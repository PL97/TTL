
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_-1_1/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_1_1/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_1_1/
pooling!! 256
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.914723085259014 	 acc:0.33578125 | test: loss:1.9227630094575734 	 acc:0.3264797507788162 	 lr:0.0001
epoch1: train: loss:1.8778599306533896 	 acc:0.4784375 | test: loss:1.8763601335038278 	 acc:0.48099688473520247 	 lr:0.0001
epoch2: train: loss:1.8430864487468592 	 acc:0.4646875 | test: loss:1.8439989231085852 	 acc:0.47102803738317756 	 lr:0.0001
epoch3: train: loss:1.8141507389208564 	 acc:0.458125 | test: loss:1.8114345855059282 	 acc:0.44922118380062304 	 lr:0.0001
epoch4: train: loss:1.8057016269942916 	 acc:0.45921875 | test: loss:1.7771715683357738 	 acc:0.4604361370716511 	 lr:0.0001
epoch5: train: loss:1.7843197205399834 	 acc:0.500625 | test: loss:1.762922734040709 	 acc:0.502803738317757 	 lr:0.0001
epoch6: train: loss:1.7774909991011967 	 acc:0.50875 | test: loss:1.7397544358006891 	 acc:0.5277258566978194 	 lr:0.0001
epoch7: train: loss:1.7652774950752586 	 acc:0.53234375 | test: loss:1.7488928235579875 	 acc:0.5514018691588785 	 lr:0.0001
epoch8: train: loss:1.753933070210346 	 acc:0.52921875 | test: loss:1.7390303876169746 	 acc:0.5433021806853583 	 lr:0.0001
epoch9: train: loss:1.7425668399283702 	 acc:0.525 | test: loss:1.7282481464641488 	 acc:0.5383177570093458 	 lr:0.0001
epoch10: train: loss:1.7314118884858631 	 acc:0.56453125 | test: loss:1.6932837998755625 	 acc:0.5800623052959502 	 lr:0.0001
epoch11: train: loss:1.7340437234704333 	 acc:0.5215625 | test: loss:1.7311301402213788 	 acc:0.5295950155763239 	 lr:0.0001
epoch12: train: loss:1.7226892976068502 	 acc:0.5434375 | test: loss:1.7004554693572618 	 acc:0.5595015576323987 	 lr:0.0001
epoch13: train: loss:1.7132002652575502 	 acc:0.55140625 | test: loss:1.6972375332009384 	 acc:0.557632398753894 	 lr:0.0001
epoch14: train: loss:1.7102046064247292 	 acc:0.58328125 | test: loss:1.6641331689008672 	 acc:0.5887850467289719 	 lr:0.0001
epoch15: train: loss:1.7228934556780897 	 acc:0.5534375 | test: loss:1.684684905233413 	 acc:0.5532710280373832 	 lr:0.0001
epoch16: train: loss:1.7045597621372768 	 acc:0.565 | test: loss:1.6808891982675713 	 acc:0.5607476635514018 	 lr:0.0001
epoch17: train: loss:1.7038815095590298 	 acc:0.55203125 | test: loss:1.6839312137473037 	 acc:0.5464174454828661 	 lr:0.0001
epoch18: train: loss:1.687175870444233 	 acc:0.6003125 | test: loss:1.654654158312955 	 acc:0.5981308411214953 	 lr:0.0001
epoch19: train: loss:1.6871957914425375 	 acc:0.57125 | test: loss:1.684937433067512 	 acc:0.5757009345794393 	 lr:0.0001
epoch20: train: loss:1.6928857998397553 	 acc:0.5884375 | test: loss:1.6270641826766303 	 acc:0.6080996884735203 	 lr:0.0001
epoch21: train: loss:1.677703745452619 	 acc:0.5825 | test: loss:1.6493051476196336 	 acc:0.5956386292834891 	 lr:0.0001
epoch22: train: loss:1.6776871525617802 	 acc:0.5896875 | test: loss:1.6560058001788607 	 acc:0.6012461059190031 	 lr:0.0001
epoch23: train: loss:1.6720729378217092 	 acc:0.60171875 | test: loss:1.6317440290688725 	 acc:0.6105919003115264 	 lr:0.0001
epoch24: train: loss:1.6765146152755415 	 acc:0.625 | test: loss:1.637764810327429 	 acc:0.6292834890965732 	 lr:0.0001
epoch25: train: loss:1.6748194960297131 	 acc:0.57 | test: loss:1.6357681842607872 	 acc:0.5937694704049844 	 lr:0.0001
epoch26: train: loss:1.6549876538409933 	 acc:0.58265625 | test: loss:1.6419448135797852 	 acc:0.5887850467289719 	 lr:0.0001
epoch27: train: loss:1.6527523210512112 	 acc:0.61109375 | test: loss:1.6338133702768343 	 acc:0.6218068535825545 	 lr:5e-05
epoch28: train: loss:1.6524331872301303 	 acc:0.615625 | test: loss:1.63148886711798 	 acc:0.6342679127725857 	 lr:5e-05
epoch29: train: loss:1.6426502403479641 	 acc:0.61484375 | test: loss:1.628585114880143 	 acc:0.6205607476635514 	 lr:5e-05
epoch30: train: loss:1.6492500966270858 	 acc:0.6325 | test: loss:1.6159829886160164 	 acc:0.6461059190031153 	 lr:5e-05
epoch31: train: loss:1.644401039284342 	 acc:0.624375 | test: loss:1.6327614522054559 	 acc:0.6311526479750779 	 lr:5e-05
epoch32: train: loss:1.6368902249600532 	 acc:0.600625 | test: loss:1.6310418481767364 	 acc:0.609968847352025 	 lr:5e-05
epoch33: train: loss:1.6398580153597044 	 acc:0.6121875 | test: loss:1.6187269960979807 	 acc:0.6137071651090342 	 lr:5e-05
epoch34: train: loss:1.6399692127427303 	 acc:0.61546875 | test: loss:1.6407425891573184 	 acc:0.6205607476635514 	 lr:5e-05
epoch35: train: loss:1.6403915393361814 	 acc:0.6240625 | test: loss:1.6268369446662356 	 acc:0.6199376947040498 	 lr:5e-05
epoch36: train: loss:1.6394789779865584 	 acc:0.64859375 | test: loss:1.605779298072292 	 acc:0.6573208722741433 	 lr:5e-05
epoch37: train: loss:1.6399627328198185 	 acc:0.63796875 | test: loss:1.6139014356975614 	 acc:0.6411214953271028 	 lr:5e-05
epoch38: train: loss:1.6320715613145553 	 acc:0.61921875 | test: loss:1.6156056245168051 	 acc:0.6386292834890965 	 lr:5e-05
epoch39: train: loss:1.6204608616467848 	 acc:0.60015625 | test: loss:1.6224592127903972 	 acc:0.6080996884735203 	 lr:5e-05
epoch40: train: loss:1.6372631731115217 	 acc:0.63046875 | test: loss:1.6295685918903053 	 acc:0.6317757009345795 	 lr:5e-05
epoch41: train: loss:1.6368408155478509 	 acc:0.6253125 | test: loss:1.6219093856781814 	 acc:0.6124610591900311 	 lr:5e-05
epoch42: train: loss:1.633942281706644 	 acc:0.62375 | test: loss:1.6359791401390718 	 acc:0.6218068535825545 	 lr:5e-05
epoch43: train: loss:1.6211448603919667 	 acc:0.62859375 | test: loss:1.6138977671709387 	 acc:0.6286604361370717 	 lr:2.5e-05
epoch44: train: loss:1.622187451400578 	 acc:0.6253125 | test: loss:1.6164129059634105 	 acc:0.6355140186915887 	 lr:2.5e-05
epoch45: train: loss:1.6171246471300802 	 acc:0.6328125 | test: loss:1.6061799703728745 	 acc:0.6386292834890965 	 lr:2.5e-05
epoch46: train: loss:1.6142667271586528 	 acc:0.643125 | test: loss:1.6025970610502724 	 acc:0.6485981308411215 	 lr:2.5e-05
epoch47: train: loss:1.611180075437738 	 acc:0.6434375 | test: loss:1.6036034930903593 	 acc:0.6517133956386293 	 lr:2.5e-05
epoch48: train: loss:1.6247813311896224 	 acc:0.62546875 | test: loss:1.620974320934569 	 acc:0.6286604361370717 	 lr:2.5e-05
epoch49: train: loss:1.6147613763623085 	 acc:0.64421875 | test: loss:1.6109123253748052 	 acc:0.6485981308411215 	 lr:2.5e-05
epoch50: train: loss:1.6137469420481436 	 acc:0.6396875 | test: loss:1.5987250888830404 	 acc:0.6517133956386293 	 lr:2.5e-05
epoch51: train: loss:1.6086414870203332 	 acc:0.64984375 | test: loss:1.5958166625269476 	 acc:0.6573208722741433 	 lr:2.5e-05
epoch52: train: loss:1.6148059877727667 	 acc:0.66609375 | test: loss:1.5890055610383411 	 acc:0.6691588785046729 	 lr:2.5e-05
epoch53: train: loss:1.6113832424619439 	 acc:0.65765625 | test: loss:1.5916706025043381 	 acc:0.6623052959501557 	 lr:2.5e-05
epoch54: train: loss:1.6092117869416593 	 acc:0.65609375 | test: loss:1.5834146902197246 	 acc:0.6716510903426791 	 lr:2.5e-05
epoch55: train: loss:1.6145834095677205 	 acc:0.65234375 | test: loss:1.5938939805342773 	 acc:0.6623052959501557 	 lr:2.5e-05
epoch56: train: loss:1.6084007292962652 	 acc:0.65 | test: loss:1.6019756211670018 	 acc:0.6392523364485981 	 lr:2.5e-05
epoch57: train: loss:1.607081488461163 	 acc:0.6484375 | test: loss:1.6001443579189503 	 acc:0.6560747663551402 	 lr:2.5e-05
epoch58: train: loss:1.6100068210717946 	 acc:0.65796875 | test: loss:1.5917264796491724 	 acc:0.6542056074766355 	 lr:2.5e-05
epoch59: train: loss:1.6117730574715798 	 acc:0.65375 | test: loss:1.596377997086427 	 acc:0.6679127725856698 	 lr:2.5e-05
epoch60: train: loss:1.6125558345416484 	 acc:0.65609375 | test: loss:1.599670199516035 	 acc:0.6498442367601246 	 lr:2.5e-05
epoch61: train: loss:1.609642780432005 	 acc:0.643125 | test: loss:1.6015070491862073 	 acc:0.6529595015576324 	 lr:1.25e-05
epoch62: train: loss:1.6084010532924107 	 acc:0.653125 | test: loss:1.5973774228883308 	 acc:0.6442367601246106 	 lr:1.25e-05
epoch63: train: loss:1.5975835176299644 	 acc:0.6546875 | test: loss:1.5950538210408338 	 acc:0.6573208722741433 	 lr:1.25e-05
epoch64: train: loss:1.5982617416203162 	 acc:0.6609375 | test: loss:1.5952801339722869 	 acc:0.6566978193146418 	 lr:1.25e-05
epoch65: train: loss:1.5990813923105422 	 acc:0.65109375 | test: loss:1.5982086225462109 	 acc:0.6492211838006231 	 lr:1.25e-05
epoch66: train: loss:1.5986183851422229 	 acc:0.64140625 | test: loss:1.5995214027779125 	 acc:0.6423676012461059 	 lr:1.25e-05
epoch67: train: loss:1.5930945391212006 	 acc:0.64703125 | test: loss:1.597110567063186 	 acc:0.6479750778816199 	 lr:6.25e-06
epoch68: train: loss:1.6023005060438622 	 acc:0.65375 | test: loss:1.5969633862979686 	 acc:0.653582554517134 	 lr:6.25e-06
epoch69: train: loss:1.5981761828146346 	 acc:0.64875 | test: loss:1.5939516676549228 	 acc:0.6523364485981309 	 lr:6.25e-06
epoch70: train: loss:1.6015950639949563 	 acc:0.64546875 | test: loss:1.601830759375266 	 acc:0.6461059190031153 	 lr:6.25e-06
epoch71: train: loss:1.5941431240585053 	 acc:0.65609375 | test: loss:1.5937001971812264 	 acc:0.6573208722741433 	 lr:6.25e-06
epoch72: train: loss:1.6020586310095568 	 acc:0.6571875 | test: loss:1.5929772172018746 	 acc:0.6598130841121496 	 lr:6.25e-06
epoch73: train: loss:1.5997121056013979 	 acc:0.65828125 | test: loss:1.593285201123199 	 acc:0.6579439252336449 	 lr:3.125e-06
epoch74: train: loss:1.6011237711016784 	 acc:0.6596875 | test: loss:1.5970823051030762 	 acc:0.6548286604361371 	 lr:3.125e-06
epoch75: train: loss:1.6017049368055047 	 acc:0.66046875 | test: loss:1.5922133957485542 	 acc:0.659190031152648 	 lr:3.125e-06
epoch76: train: loss:1.6013204224681035 	 acc:0.6590625 | test: loss:1.5919954285072018 	 acc:0.6585669781931465 	 lr:3.125e-06
epoch77: train: loss:1.5974239519105862 	 acc:0.65953125 | test: loss:1.5887312530357147 	 acc:0.6654205607476635 	 lr:3.125e-06
epoch78: train: loss:1.5936351576603363 	 acc:0.66125 | test: loss:1.593076125855015 	 acc:0.6579439252336449 	 lr:3.125e-06
epoch79: train: loss:1.5958748024576441 	 acc:0.6553125 | test: loss:1.5967828745411192 	 acc:0.6529595015576324 	 lr:1.5625e-06
epoch80: train: loss:1.594532131460102 	 acc:0.658125 | test: loss:1.594320325762312 	 acc:0.6579439252336449 	 lr:1.5625e-06
epoch81: train: loss:1.5929931855034214 	 acc:0.658125 | test: loss:1.593188434597859 	 acc:0.6573208722741433 	 lr:1.5625e-06
epoch82: train: loss:1.5990891158068952 	 acc:0.6584375 | test: loss:1.5938574670631194 	 acc:0.6566978193146418 	 lr:1.5625e-06
epoch83: train: loss:1.5924338657161763 	 acc:0.65703125 | test: loss:1.5937469968171878 	 acc:0.6566978193146418 	 lr:1.5625e-06
epoch84: train: loss:1.5989030848435366 	 acc:0.6565625 | test: loss:1.5946522347280911 	 acc:0.6585669781931465 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_2_1/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_2_1/
pooling!! 512
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.861534441457327 	 acc:0.35171875 | test: loss:1.852072316389589 	 acc:0.3414330218068536 	 lr:0.0001
epoch1: train: loss:1.7795211446555121 	 acc:0.44890625 | test: loss:1.781048683956776 	 acc:0.44984423676012464 	 lr:0.0001
epoch2: train: loss:1.7470066734629623 	 acc:0.49671875 | test: loss:1.7320981429001996 	 acc:0.5071651090342679 	 lr:0.0001
epoch3: train: loss:1.7325116243146528 	 acc:0.5353125 | test: loss:1.6760088679946472 	 acc:0.5426791277258567 	 lr:0.0001
epoch4: train: loss:1.704029225595848 	 acc:0.48859375 | test: loss:1.7435473415339104 	 acc:0.5015576323987538 	 lr:0.0001
epoch5: train: loss:1.6587653702818537 	 acc:0.56703125 | test: loss:1.647403493402903 	 acc:0.5694704049844237 	 lr:0.0001
epoch6: train: loss:1.6424462776273419 	 acc:0.5321875 | test: loss:1.682602486877798 	 acc:0.5233644859813084 	 lr:0.0001
epoch7: train: loss:1.634947765627287 	 acc:0.5578125 | test: loss:1.6707524575919748 	 acc:0.5532710280373832 	 lr:0.0001
epoch8: train: loss:1.631268036579546 	 acc:0.56796875 | test: loss:1.66064858748534 	 acc:0.5657320872274143 	 lr:0.0001
epoch9: train: loss:1.6104257242647955 	 acc:0.61328125 | test: loss:1.60718751971224 	 acc:0.6236760124610592 	 lr:0.0001
epoch10: train: loss:1.6056262931257927 	 acc:0.56921875 | test: loss:1.6246097171046652 	 acc:0.5856697819314641 	 lr:0.0001
epoch11: train: loss:1.6093666189820575 	 acc:0.6646875 | test: loss:1.6094644247185776 	 acc:0.6797507788161994 	 lr:0.0001
epoch12: train: loss:1.5846231334010294 	 acc:0.65921875 | test: loss:1.5813255079810122 	 acc:0.6498442367601246 	 lr:0.0001
epoch13: train: loss:1.589097162822929 	 acc:0.670625 | test: loss:1.564877499598209 	 acc:0.6872274143302181 	 lr:0.0001
epoch14: train: loss:1.5754080135872548 	 acc:0.63734375 | test: loss:1.5699914644811754 	 acc:0.6330218068535826 	 lr:0.0001
epoch15: train: loss:1.5483494588120852 	 acc:0.673125 | test: loss:1.5541270581361288 	 acc:0.6722741433021807 	 lr:0.0001
epoch16: train: loss:1.6058378364032921 	 acc:0.60359375 | test: loss:1.6143071250380756 	 acc:0.6130841121495327 	 lr:0.0001
epoch17: train: loss:1.5704088150012503 	 acc:0.666875 | test: loss:1.5604411486153291 	 acc:0.6672897196261682 	 lr:0.0001
epoch18: train: loss:1.545113263811384 	 acc:0.5928125 | test: loss:1.622471628976388 	 acc:0.5850467289719626 	 lr:0.0001
epoch19: train: loss:1.5451707564509538 	 acc:0.6909375 | test: loss:1.539603820990922 	 acc:0.6934579439252336 	 lr:0.0001
epoch20: train: loss:1.5346358859846128 	 acc:0.65765625 | test: loss:1.5441289212473457 	 acc:0.6560747663551402 	 lr:0.0001
epoch21: train: loss:1.5432805605552609 	 acc:0.67421875 | test: loss:1.5514654055562718 	 acc:0.6828660436137072 	 lr:0.0001
epoch22: train: loss:1.5316124444823076 	 acc:0.65515625 | test: loss:1.577282977846924 	 acc:0.6454828660436137 	 lr:0.0001
epoch23: train: loss:1.5184948244474532 	 acc:0.73609375 | test: loss:1.50301838953547 	 acc:0.7401869158878505 	 lr:0.0001
epoch24: train: loss:1.5071816151817732 	 acc:0.70875 | test: loss:1.5247560715749628 	 acc:0.7133956386292835 	 lr:0.0001
epoch25: train: loss:1.50518453391058 	 acc:0.713125 | test: loss:1.5185052459485062 	 acc:0.7258566978193146 	 lr:0.0001
epoch26: train: loss:1.5899784823677485 	 acc:0.55765625 | test: loss:1.6211988038363114 	 acc:0.5495327102803739 	 lr:0.0001
epoch27: train: loss:1.5224031756577503 	 acc:0.6453125 | test: loss:1.5439275498702147 	 acc:0.6479750778816199 	 lr:0.0001
epoch28: train: loss:1.4937288497222019 	 acc:0.66171875 | test: loss:1.5386334445246284 	 acc:0.6685358255451713 	 lr:0.0001
epoch29: train: loss:1.5379849864950783 	 acc:0.66203125 | test: loss:1.5640478940024924 	 acc:0.6361370716510903 	 lr:0.0001
epoch30: train: loss:1.4620552795553097 	 acc:0.72765625 | test: loss:1.4978589288913573 	 acc:0.7171339563862928 	 lr:5e-05
epoch31: train: loss:1.4541455902409313 	 acc:0.7496875 | test: loss:1.4833595347924395 	 acc:0.756386292834891 	 lr:5e-05
epoch32: train: loss:1.463341880161068 	 acc:0.748125 | test: loss:1.498414291548209 	 acc:0.7408099688473521 	 lr:5e-05
epoch33: train: loss:1.438639994732204 	 acc:0.745 | test: loss:1.49510172163586 	 acc:0.7383177570093458 	 lr:5e-05
epoch34: train: loss:1.4409214826787848 	 acc:0.76890625 | test: loss:1.471596088736228 	 acc:0.7657320872274144 	 lr:5e-05
epoch35: train: loss:1.4339542814756538 	 acc:0.76515625 | test: loss:1.475793840877735 	 acc:0.7551401869158878 	 lr:5e-05
epoch36: train: loss:1.4588464908168057 	 acc:0.7246875 | test: loss:1.5060419747391223 	 acc:0.7102803738317757 	 lr:5e-05
epoch37: train: loss:1.420344720940214 	 acc:0.75375 | test: loss:1.479932311019422 	 acc:0.7457943925233644 	 lr:5e-05
epoch38: train: loss:1.4341510972224762 	 acc:0.76171875 | test: loss:1.476689796729994 	 acc:0.7576323987538941 	 lr:5e-05
epoch39: train: loss:1.4286111848043521 	 acc:0.749375 | test: loss:1.4867093177599329 	 acc:0.7426791277258566 	 lr:5e-05
epoch40: train: loss:1.4213272225754416 	 acc:0.74734375 | test: loss:1.4748470903557038 	 acc:0.7476635514018691 	 lr:5e-05
epoch41: train: loss:1.4028331295016405 	 acc:0.7803125 | test: loss:1.4624590639756105 	 acc:0.7688473520249222 	 lr:2.5e-05
epoch42: train: loss:1.40565626276926 	 acc:0.77515625 | test: loss:1.4697933874397635 	 acc:0.7626168224299066 	 lr:2.5e-05
epoch43: train: loss:1.4028388685216018 	 acc:0.768125 | test: loss:1.46608227264844 	 acc:0.7538940809968847 	 lr:2.5e-05
epoch44: train: loss:1.400082721457083 	 acc:0.76625 | test: loss:1.4714748760621497 	 acc:0.7445482866043613 	 lr:2.5e-05
epoch45: train: loss:1.402813851730978 	 acc:0.77046875 | test: loss:1.4640070749591816 	 acc:0.7601246105919003 	 lr:2.5e-05
epoch46: train: loss:1.4043175535775274 	 acc:0.79609375 | test: loss:1.4507875877748768 	 acc:0.7769470404984423 	 lr:2.5e-05
epoch47: train: loss:1.397182801102959 	 acc:0.78265625 | test: loss:1.4612143975551997 	 acc:0.7607476635514019 	 lr:2.5e-05
epoch48: train: loss:1.39248377303422 	 acc:0.76515625 | test: loss:1.4674811358763793 	 acc:0.7501557632398754 	 lr:2.5e-05
epoch49: train: loss:1.3925224095745816 	 acc:0.785 | test: loss:1.4566743563268787 	 acc:0.7638629283489097 	 lr:2.5e-05
epoch50: train: loss:1.40145763960041 	 acc:0.7753125 | test: loss:1.4660937789073243 	 acc:0.7501557632398754 	 lr:2.5e-05
epoch51: train: loss:1.3922767942906542 	 acc:0.78328125 | test: loss:1.4471211073183197 	 acc:0.77196261682243 	 lr:2.5e-05
epoch52: train: loss:1.39938476467207 	 acc:0.78921875 | test: loss:1.453053443082768 	 acc:0.767601246105919 	 lr:2.5e-05
epoch53: train: loss:1.3841929713419692 	 acc:0.78125 | test: loss:1.4535173009860554 	 acc:0.767601246105919 	 lr:2.5e-05
epoch54: train: loss:1.3879921787330454 	 acc:0.78609375 | test: loss:1.453731952278042 	 acc:0.7713395638629283 	 lr:2.5e-05
epoch55: train: loss:1.3795368132044057 	 acc:0.78140625 | test: loss:1.458549811535535 	 acc:0.75202492211838 	 lr:2.5e-05
epoch56: train: loss:1.3801610418077004 	 acc:0.77546875 | test: loss:1.4605344620820517 	 acc:0.7551401869158878 	 lr:2.5e-05
epoch57: train: loss:1.386627268735363 	 acc:0.791875 | test: loss:1.4462993766659888 	 acc:0.7738317757009345 	 lr:2.5e-05
epoch58: train: loss:1.39366717133831 	 acc:0.7796875 | test: loss:1.455822643981173 	 acc:0.7657320872274144 	 lr:2.5e-05
epoch59: train: loss:1.3814962585115693 	 acc:0.78921875 | test: loss:1.4531532402721892 	 acc:0.767601246105919 	 lr:2.5e-05
epoch60: train: loss:1.38533521737092 	 acc:0.77578125 | test: loss:1.4495021134522101 	 acc:0.7595015576323988 	 lr:2.5e-05
epoch61: train: loss:1.3872032624124828 	 acc:0.79046875 | test: loss:1.4401598340625703 	 acc:0.773208722741433 	 lr:2.5e-05
epoch62: train: loss:1.3912714747504087 	 acc:0.804375 | test: loss:1.4374711207511641 	 acc:0.7881619937694704 	 lr:2.5e-05
epoch63: train: loss:1.3713559081459492 	 acc:0.7696875 | test: loss:1.4617997162067258 	 acc:0.7489096573208722 	 lr:2.5e-05
epoch64: train: loss:1.3757560712113035 	 acc:0.78875 | test: loss:1.4545304272405084 	 acc:0.7651090342679128 	 lr:2.5e-05
epoch65: train: loss:1.3625947402846896 	 acc:0.765 | test: loss:1.4514406894226313 	 acc:0.7551401869158878 	 lr:2.5e-05
epoch66: train: loss:1.373434510182627 	 acc:0.78640625 | test: loss:1.4503295843474961 	 acc:0.756386292834891 	 lr:2.5e-05
epoch67: train: loss:1.3663696384355484 	 acc:0.7815625 | test: loss:1.461129493356865 	 acc:0.756386292834891 	 lr:2.5e-05
epoch68: train: loss:1.3868155772010393 	 acc:0.7878125 | test: loss:1.4508289306705986 	 acc:0.7707165109034267 	 lr:2.5e-05
epoch69: train: loss:1.367238460305517 	 acc:0.79953125 | test: loss:1.4417601157571667 	 acc:0.7657320872274144 	 lr:1.25e-05
epoch70: train: loss:1.3684879228530873 	 acc:0.7953125 | test: loss:1.4414602242526235 	 acc:0.7744548286604361 	 lr:1.25e-05
epoch71: train: loss:1.3637808401448759 	 acc:0.80375 | test: loss:1.4392845377372432 	 acc:0.7682242990654206 	 lr:1.25e-05
epoch72: train: loss:1.3606736265803387 	 acc:0.80703125 | test: loss:1.4379001866991274 	 acc:0.7825545171339564 	 lr:1.25e-05
epoch73: train: loss:1.3626069901977824 	 acc:0.79171875 | test: loss:1.441849514479949 	 acc:0.7738317757009345 	 lr:1.25e-05
epoch74: train: loss:1.3580515924791727 	 acc:0.80234375 | test: loss:1.4391223319965731 	 acc:0.7682242990654206 	 lr:1.25e-05
epoch75: train: loss:1.3631883965163933 	 acc:0.80390625 | test: loss:1.434332507124571 	 acc:0.7813084112149533 	 lr:6.25e-06
epoch76: train: loss:1.3699851900408921 	 acc:0.79203125 | test: loss:1.4391381625445832 	 acc:0.7750778816199377 	 lr:6.25e-06
epoch77: train: loss:1.353331781010624 	 acc:0.80140625 | test: loss:1.436126968496685 	 acc:0.7750778816199377 	 lr:6.25e-06
epoch78: train: loss:1.3624264433456528 	 acc:0.80734375 | test: loss:1.432887024745763 	 acc:0.7813084112149533 	 lr:6.25e-06
epoch79: train: loss:1.3617858934365241 	 acc:0.8121875 | test: loss:1.4303580538134708 	 acc:0.7844236760124611 	 lr:6.25e-06
epoch80: train: loss:1.3559305135948783 	 acc:0.81234375 | test: loss:1.43439426058178 	 acc:0.7831775700934579 	 lr:6.25e-06
epoch81: train: loss:1.3489833215360618 	 acc:0.80609375 | test: loss:1.4385690299149985 	 acc:0.773208722741433 	 lr:6.25e-06
epoch82: train: loss:1.3600486523094446 	 acc:0.81375 | test: loss:1.4341721444857827 	 acc:0.778816199376947 	 lr:6.25e-06
epoch83: train: loss:1.351264344929346 	 acc:0.80484375 | test: loss:1.4388560093080516 	 acc:0.7713395638629283 	 lr:6.25e-06
epoch84: train: loss:1.3548143016091554 	 acc:0.806875 | test: loss:1.435930655604211 	 acc:0.7781931464174455 	 lr:6.25e-06
epoch85: train: loss:1.353898979145321 	 acc:0.80796875 | test: loss:1.4355526102666172 	 acc:0.7775700934579439 	 lr:6.25e-06
epoch86: train: loss:1.367022061850483 	 acc:0.81 | test: loss:1.4313797128534762 	 acc:0.7831775700934579 	 lr:3.125e-06
epoch87: train: loss:1.357824159096592 	 acc:0.80515625 | test: loss:1.4363451595246977 	 acc:0.7757009345794392 	 lr:3.125e-06
epoch88: train: loss:1.3577498019811793 	 acc:0.7996875 | test: loss:1.4374252518389454 	 acc:0.773208722741433 	 lr:3.125e-06
epoch89: train: loss:1.3539577367992535 	 acc:0.80328125 | test: loss:1.4382615199341582 	 acc:0.7738317757009345 	 lr:3.125e-06
epoch90: train: loss:1.350259866349684 	 acc:0.8015625 | test: loss:1.4356716656610602 	 acc:0.7781931464174455 	 lr:3.125e-06
epoch91: train: loss:1.3516167240902188 	 acc:0.80640625 | test: loss:1.4344689320923756 	 acc:0.7769470404984423 	 lr:3.125e-06
epoch92: train: loss:1.353659834255007 	 acc:0.80375 | test: loss:1.4338428932558338 	 acc:0.778816199376947 	 lr:1.5625e-06
epoch93: train: loss:1.3553362617820246 	 acc:0.80953125 | test: loss:1.4351531715779289 	 acc:0.7806853582554517 	 lr:1.5625e-06
epoch94: train: loss:1.3524566983916069 	 acc:0.80875 | test: loss:1.4316241854819183 	 acc:0.7781931464174455 	 lr:1.5625e-06
epoch95: train: loss:1.34745405972888 	 acc:0.816875 | test: loss:1.431636927283813 	 acc:0.7800623052959501 	 lr:1.5625e-06
epoch96: train: loss:1.3516790885835956 	 acc:0.81453125 | test: loss:1.4304187049004147 	 acc:0.7794392523364486 	 lr:1.5625e-06
epoch97: train: loss:1.3534315480002195 	 acc:0.8175 | test: loss:1.4317118138167717 	 acc:0.7800623052959501 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_3_1/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_3_1/
pooling!! 1024
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.75837033083437 	 acc:0.523125 | test: loss:1.745172161105266 	 acc:0.550778816199377 	 lr:0.0001
epoch1: train: loss:1.6580556567249403 	 acc:0.59390625 | test: loss:1.6457792355635457 	 acc:0.6012461059190031 	 lr:0.0001
epoch2: train: loss:1.5770733347169874 	 acc:0.6196875 | test: loss:1.6105855590457856 	 acc:0.6348909657320873 	 lr:0.0001
epoch3: train: loss:1.5235368453181413 	 acc:0.61140625 | test: loss:1.573801715277437 	 acc:0.6137071651090342 	 lr:0.0001
epoch4: train: loss:1.5294849619094883 	 acc:0.66328125 | test: loss:1.5613424421470856 	 acc:0.6841121495327103 	 lr:0.0001
epoch5: train: loss:1.5138062500190586 	 acc:0.70796875 | test: loss:1.4884427528886408 	 acc:0.7289719626168224 	 lr:0.0001
epoch6: train: loss:1.4432661951379233 	 acc:0.656875 | test: loss:1.5186307809063206 	 acc:0.6672897196261682 	 lr:0.0001
epoch7: train: loss:1.460539408794704 	 acc:0.76921875 | test: loss:1.4236297513094276 	 acc:0.8037383177570093 	 lr:0.0001
epoch8: train: loss:1.4532544269308645 	 acc:0.688125 | test: loss:1.5077684567353435 	 acc:0.6866043613707166 	 lr:0.0001
epoch9: train: loss:1.4189169533079626 	 acc:0.73890625 | test: loss:1.4618187764723354 	 acc:0.7327102803738318 	 lr:0.0001
epoch10: train: loss:1.4123650413114144 	 acc:0.76859375 | test: loss:1.4398017059605441 	 acc:0.7906542056074767 	 lr:0.0001
epoch11: train: loss:1.4097723836548899 	 acc:0.741875 | test: loss:1.4761677476104547 	 acc:0.7389408099688474 	 lr:0.0001
epoch12: train: loss:1.400678598015314 	 acc:0.71796875 | test: loss:1.4440831610718248 	 acc:0.7376947040498443 	 lr:0.0001
epoch13: train: loss:1.3739885364446112 	 acc:0.7896875 | test: loss:1.4193705806108279 	 acc:0.7962616822429907 	 lr:0.0001
epoch14: train: loss:1.3647037931944037 	 acc:0.78328125 | test: loss:1.4168122281166624 	 acc:0.7738317757009345 	 lr:0.0001
epoch15: train: loss:1.4142799538248316 	 acc:0.7825 | test: loss:1.4173133031601475 	 acc:0.7794392523364486 	 lr:0.0001
epoch16: train: loss:1.3755551292782737 	 acc:0.81140625 | test: loss:1.3936541593706124 	 acc:0.8087227414330218 	 lr:0.0001
epoch17: train: loss:1.3447053955459296 	 acc:0.79609375 | test: loss:1.3979316173684189 	 acc:0.7931464174454829 	 lr:0.0001
epoch18: train: loss:1.3911471841113814 	 acc:0.7546875 | test: loss:1.4832749215984642 	 acc:0.7121495327102804 	 lr:0.0001
epoch19: train: loss:1.3911313249765198 	 acc:0.68 | test: loss:1.487411745017934 	 acc:0.67601246105919 	 lr:0.0001
epoch20: train: loss:1.347527504413971 	 acc:0.8228125 | test: loss:1.3836939551005853 	 acc:0.8068535825545171 	 lr:0.0001
epoch21: train: loss:1.3363432337026127 	 acc:0.8484375 | test: loss:1.3611479897365393 	 acc:0.8454828660436137 	 lr:0.0001
epoch22: train: loss:1.3281237707205065 	 acc:0.8790625 | test: loss:1.338896723773992 	 acc:0.8461059190031153 	 lr:0.0001
epoch23: train: loss:1.3906507791344958 	 acc:0.7446875 | test: loss:1.4570896279403354 	 acc:0.7320872274143302 	 lr:0.0001
epoch24: train: loss:1.3309195811072891 	 acc:0.8215625 | test: loss:1.3877711481765795 	 acc:0.8137071651090343 	 lr:0.0001
epoch25: train: loss:1.2975077811486082 	 acc:0.8040625 | test: loss:1.3930166659696823 	 acc:0.8043613707165109 	 lr:0.0001
epoch26: train: loss:1.2922026206142356 	 acc:0.82953125 | test: loss:1.37162952148283 	 acc:0.811214953271028 	 lr:0.0001
epoch27: train: loss:1.319738090568739 	 acc:0.85421875 | test: loss:1.368338121506284 	 acc:0.8230529595015577 	 lr:0.0001
epoch28: train: loss:1.2833028324314806 	 acc:0.87359375 | test: loss:1.335393195657344 	 acc:0.854828660436137 	 lr:0.0001
epoch29: train: loss:1.316873750232515 	 acc:0.84328125 | test: loss:1.3806465810704454 	 acc:0.8180685358255452 	 lr:0.0001
epoch30: train: loss:1.2972451179498439 	 acc:0.8121875 | test: loss:1.3773607731608215 	 acc:0.8 	 lr:0.0001
epoch31: train: loss:1.2873092877688024 	 acc:0.8978125 | test: loss:1.3272141204073422 	 acc:0.8604361370716511 	 lr:0.0001
epoch32: train: loss:1.2857809336272932 	 acc:0.8578125 | test: loss:1.355645320200103 	 acc:0.8292834890965732 	 lr:0.0001
epoch33: train: loss:1.2886447665283776 	 acc:0.86046875 | test: loss:1.3625814225443427 	 acc:0.8342679127725857 	 lr:0.0001
epoch34: train: loss:1.2987853193915886 	 acc:0.88828125 | test: loss:1.3294296002462274 	 acc:0.8485981308411215 	 lr:0.0001
epoch35: train: loss:1.296546320204247 	 acc:0.88046875 | test: loss:1.3359654502333882 	 acc:0.8623052959501558 	 lr:0.0001
epoch36: train: loss:1.2812728083757197 	 acc:0.83390625 | test: loss:1.3475778747570477 	 acc:0.8286604361370716 	 lr:0.0001
epoch37: train: loss:1.2846472115557608 	 acc:0.87734375 | test: loss:1.3489709616450134 	 acc:0.8467289719626169 	 lr:0.0001
epoch38: train: loss:1.2499782517587272 	 acc:0.9078125 | test: loss:1.3155140700741348 	 acc:0.874766355140187 	 lr:5e-05
epoch39: train: loss:1.245503257495365 	 acc:0.9053125 | test: loss:1.3043953107524884 	 acc:0.874766355140187 	 lr:5e-05
epoch40: train: loss:1.2398254346884758 	 acc:0.9125 | test: loss:1.3111047195868328 	 acc:0.8728971962616823 	 lr:5e-05
epoch41: train: loss:1.2594024419970664 	 acc:0.91703125 | test: loss:1.3126736561457315 	 acc:0.8629283489096573 	 lr:5e-05
epoch42: train: loss:1.2396922627433402 	 acc:0.931875 | test: loss:1.3129104549639692 	 acc:0.870404984423676 	 lr:5e-05
epoch43: train: loss:1.23842201679596 	 acc:0.90015625 | test: loss:1.325528255949882 	 acc:0.8573208722741433 	 lr:5e-05
epoch44: train: loss:1.2376870625099106 	 acc:0.91171875 | test: loss:1.314233033233714 	 acc:0.8691588785046729 	 lr:5e-05
epoch45: train: loss:1.2443697260098008 	 acc:0.91296875 | test: loss:1.3177309671295023 	 acc:0.8635514018691589 	 lr:5e-05
epoch46: train: loss:1.2414628850027138 	 acc:0.9290625 | test: loss:1.3035088511642265 	 acc:0.8728971962616823 	 lr:2.5e-05
epoch47: train: loss:1.2269190458465982 	 acc:0.92828125 | test: loss:1.3069460339263963 	 acc:0.8803738317757009 	 lr:2.5e-05
epoch48: train: loss:1.229888046467146 	 acc:0.93203125 | test: loss:1.3005831702847348 	 acc:0.8828660436137071 	 lr:2.5e-05
epoch49: train: loss:1.227367659009685 	 acc:0.94046875 | test: loss:1.2946680759715143 	 acc:0.8878504672897196 	 lr:2.5e-05
epoch50: train: loss:1.2253916922814208 	 acc:0.94171875 | test: loss:1.3033511141007563 	 acc:0.8834890965732087 	 lr:2.5e-05
epoch51: train: loss:1.2248620205238216 	 acc:0.9378125 | test: loss:1.3014871676762898 	 acc:0.881619937694704 	 lr:2.5e-05
epoch52: train: loss:1.22197075039032 	 acc:0.93859375 | test: loss:1.2965113274404936 	 acc:0.8866043613707165 	 lr:2.5e-05
epoch53: train: loss:1.2210211700987388 	 acc:0.9184375 | test: loss:1.3093339417210992 	 acc:0.8741433021806854 	 lr:2.5e-05
epoch54: train: loss:1.2275073394656275 	 acc:0.941875 | test: loss:1.2961361729838767 	 acc:0.8772585669781932 	 lr:2.5e-05
epoch55: train: loss:1.2257689524404152 	 acc:0.9321875 | test: loss:1.312096077705098 	 acc:0.8710280373831776 	 lr:2.5e-05
epoch56: train: loss:1.2204491930208943 	 acc:0.94609375 | test: loss:1.2921558488566556 	 acc:0.8890965732087227 	 lr:1.25e-05
epoch57: train: loss:1.2161130739877597 	 acc:0.94234375 | test: loss:1.2934419359373526 	 acc:0.8859813084112149 	 lr:1.25e-05
epoch58: train: loss:1.2123388595640612 	 acc:0.94984375 | test: loss:1.2934665136990888 	 acc:0.8841121495327103 	 lr:1.25e-05
epoch59: train: loss:1.219759395697636 	 acc:0.93296875 | test: loss:1.3014963953665855 	 acc:0.8778816199376948 	 lr:1.25e-05
epoch60: train: loss:1.2072984729680487 	 acc:0.94046875 | test: loss:1.2994536558044292 	 acc:0.8791277258566979 	 lr:1.25e-05
epoch61: train: loss:1.2151904282581052 	 acc:0.946875 | test: loss:1.2917648524881524 	 acc:0.8897196261682243 	 lr:1.25e-05
epoch62: train: loss:1.2131469321567319 	 acc:0.94625 | test: loss:1.2912350948725906 	 acc:0.8859813084112149 	 lr:1.25e-05
epoch63: train: loss:1.20969134173665 	 acc:0.9503125 | test: loss:1.2917784883968555 	 acc:0.8903426791277259 	 lr:1.25e-05
epoch64: train: loss:1.2103704019229362 	 acc:0.9496875 | test: loss:1.2893197655306428 	 acc:0.8866043613707165 	 lr:1.25e-05
epoch65: train: loss:1.2117360623528677 	 acc:0.946875 | test: loss:1.292125058991144 	 acc:0.8890965732087227 	 lr:1.25e-05
epoch66: train: loss:1.215129347837688 	 acc:0.94234375 | test: loss:1.297787403243353 	 acc:0.8809968847352025 	 lr:1.25e-05
epoch67: train: loss:1.212813475800156 	 acc:0.9465625 | test: loss:1.292485737577777 	 acc:0.8872274143302181 	 lr:1.25e-05
epoch68: train: loss:1.214747868581827 	 acc:0.95109375 | test: loss:1.2890128818999198 	 acc:0.8897196261682243 	 lr:1.25e-05
epoch69: train: loss:1.209334722037989 	 acc:0.9525 | test: loss:1.2926583853094749 	 acc:0.8828660436137071 	 lr:1.25e-05
epoch70: train: loss:1.2169318336885855 	 acc:0.9540625 | test: loss:1.287812237056245 	 acc:0.8884735202492212 	 lr:1.25e-05
epoch71: train: loss:1.2102149647720897 	 acc:0.9440625 | test: loss:1.293505797802102 	 acc:0.8853582554517134 	 lr:1.25e-05
epoch72: train: loss:1.2098589120666838 	 acc:0.95140625 | test: loss:1.2898326826244129 	 acc:0.8853582554517134 	 lr:1.25e-05
epoch73: train: loss:1.2136800678142992 	 acc:0.9540625 | test: loss:1.2906016491655248 	 acc:0.8866043613707165 	 lr:1.25e-05
epoch74: train: loss:1.2109153753514406 	 acc:0.94046875 | test: loss:1.3004492507174008 	 acc:0.881619937694704 	 lr:1.25e-05
epoch75: train: loss:1.213173654691769 	 acc:0.9421875 | test: loss:1.2944522208513871 	 acc:0.8822429906542056 	 lr:1.25e-05
epoch76: train: loss:1.2089839509462212 	 acc:0.9496875 | test: loss:1.2937111099932423 	 acc:0.8853582554517134 	 lr:1.25e-05
epoch77: train: loss:1.206022954936329 	 acc:0.94984375 | test: loss:1.2951459031238735 	 acc:0.8841121495327103 	 lr:6.25e-06
epoch78: train: loss:1.2058912695617439 	 acc:0.95703125 | test: loss:1.290361163103692 	 acc:0.8903426791277259 	 lr:6.25e-06
epoch79: train: loss:1.2025685032674058 	 acc:0.95171875 | test: loss:1.2949210498934594 	 acc:0.8834890965732087 	 lr:6.25e-06
epoch80: train: loss:1.206281935061262 	 acc:0.9496875 | test: loss:1.2973561828381548 	 acc:0.8834890965732087 	 lr:6.25e-06
epoch81: train: loss:1.204404422661739 	 acc:0.95578125 | test: loss:1.2921626391069168 	 acc:0.8828660436137071 	 lr:6.25e-06
epoch82: train: loss:1.2093389673404262 	 acc:0.944375 | test: loss:1.2946807087396164 	 acc:0.8822429906542056 	 lr:6.25e-06
epoch83: train: loss:1.205118009878452 	 acc:0.95796875 | test: loss:1.2895024949516463 	 acc:0.8841121495327103 	 lr:3.125e-06
epoch84: train: loss:1.203657292314659 	 acc:0.95828125 | test: loss:1.2900549056745392 	 acc:0.8834890965732087 	 lr:3.125e-06
epoch85: train: loss:1.2102077177406567 	 acc:0.95015625 | test: loss:1.2926133949808614 	 acc:0.8766355140186916 	 lr:3.125e-06
epoch86: train: loss:1.2094589626481997 	 acc:0.95140625 | test: loss:1.2909683892288684 	 acc:0.8822429906542056 	 lr:3.125e-06
epoch87: train: loss:1.2045647483426645 	 acc:0.9571875 | test: loss:1.2886006430302082 	 acc:0.8841121495327103 	 lr:3.125e-06
epoch88: train: loss:1.2048666722508354 	 acc:0.94890625 | test: loss:1.2908024233078288 	 acc:0.8809968847352025 	 lr:3.125e-06
epoch89: train: loss:1.2044303280594384 	 acc:0.95578125 | test: loss:1.288952027080215 	 acc:0.8828660436137071 	 lr:1.5625e-06
epoch90: train: loss:1.202807164545826 	 acc:0.95546875 | test: loss:1.2883760746394362 	 acc:0.8841121495327103 	 lr:1.5625e-06
epoch91: train: loss:1.2146826381519564 	 acc:0.95453125 | test: loss:1.290089858952341 	 acc:0.8828660436137071 	 lr:1.5625e-06
epoch92: train: loss:1.2042179676446014 	 acc:0.9525 | test: loss:1.289368945415889 	 acc:0.8822429906542056 	 lr:1.5625e-06
epoch93: train: loss:1.2074230828385721 	 acc:0.95375 | test: loss:1.2896226848025931 	 acc:0.8841121495327103 	 lr:1.5625e-06
epoch94: train: loss:1.2105109352510857 	 acc:0.954375 | test: loss:1.2887550202485556 	 acc:0.8872274143302181 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_1_1/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_1_1/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.927408148998585 	 acc:0.15453125 | test: loss:1.9336853810188555 	 acc:0.14641744548286603 	 lr:0.0001
epoch1: train: loss:1.8439458104058415 	 acc:0.39015625 | test: loss:1.8634134242096423 	 acc:0.43177570093457945 	 lr:0.0001
epoch2: train: loss:1.7951984223865327 	 acc:0.48484375 | test: loss:1.7355235108705325 	 acc:0.5065420560747663 	 lr:0.0001
epoch3: train: loss:1.7589796556149675 	 acc:0.47578125 | test: loss:1.7406645833517531 	 acc:0.4803738317757009 	 lr:0.0001
epoch4: train: loss:1.7779066146862497 	 acc:0.54984375 | test: loss:1.7008363676962452 	 acc:0.5389408099688473 	 lr:0.0001
epoch5: train: loss:1.7077763720474421 	 acc:0.4965625 | test: loss:1.7081648947665253 	 acc:0.5071651090342679 	 lr:0.0001
epoch6: train: loss:1.728925251421009 	 acc:0.46078125 | test: loss:1.764034696902813 	 acc:0.4747663551401869 	 lr:0.0001
epoch7: train: loss:1.6896552175958113 	 acc:0.53203125 | test: loss:1.640432819027767 	 acc:0.567601246105919 	 lr:0.0001
epoch8: train: loss:1.6807164736411984 	 acc:0.57578125 | test: loss:1.6116615403107022 	 acc:0.5887850467289719 	 lr:0.0001
epoch9: train: loss:1.6709692378047851 	 acc:0.5125 | test: loss:1.6528885928269859 	 acc:0.5470404984423676 	 lr:0.0001
epoch10: train: loss:1.6603580376582625 	 acc:0.6046875 | test: loss:1.5549610934153524 	 acc:0.6554517133956387 	 lr:0.0001
epoch11: train: loss:1.6625355681062768 	 acc:0.5709375 | test: loss:1.6082854366005395 	 acc:0.6068535825545172 	 lr:0.0001
epoch12: train: loss:1.6394165182001976 	 acc:0.58015625 | test: loss:1.580124244883053 	 acc:0.6130841121495327 	 lr:0.0001
epoch13: train: loss:1.6401152778286008 	 acc:0.6009375 | test: loss:1.588680186821293 	 acc:0.6286604361370717 	 lr:0.0001
epoch14: train: loss:1.6245569281239327 	 acc:0.63609375 | test: loss:1.5495088797120662 	 acc:0.6666666666666666 	 lr:0.0001
epoch15: train: loss:1.6295050973170069 	 acc:0.61984375 | test: loss:1.5777856165003554 	 acc:0.6342679127725857 	 lr:0.0001
epoch16: train: loss:1.6145749785209613 	 acc:0.6309375 | test: loss:1.5458234993468192 	 acc:0.6741433021806854 	 lr:0.0001
epoch17: train: loss:1.6341094803940404 	 acc:0.6 | test: loss:1.552056934157636 	 acc:0.6448598130841121 	 lr:0.0001
epoch18: train: loss:1.6035803884942488 	 acc:0.60609375 | test: loss:1.5611989089633074 	 acc:0.6261682242990654 	 lr:0.0001
epoch19: train: loss:1.6068241759634503 	 acc:0.6578125 | test: loss:1.5193516483930785 	 acc:0.7009345794392523 	 lr:0.0001
epoch20: train: loss:1.59627164532485 	 acc:0.608125 | test: loss:1.557361796117646 	 acc:0.6330218068535826 	 lr:0.0001
epoch21: train: loss:1.5872902077310818 	 acc:0.611875 | test: loss:1.5835233803478728 	 acc:0.6242990654205608 	 lr:0.0001
epoch22: train: loss:1.5976441239677863 	 acc:0.6078125 | test: loss:1.5726254431258109 	 acc:0.6336448598130842 	 lr:0.0001
epoch23: train: loss:1.583568330596519 	 acc:0.6234375 | test: loss:1.55342005346423 	 acc:0.6548286604361371 	 lr:0.0001
epoch24: train: loss:1.6011452382286482 	 acc:0.6415625 | test: loss:1.5395775614497818 	 acc:0.6897196261682244 	 lr:0.0001
epoch25: train: loss:1.5771989284875707 	 acc:0.65875 | test: loss:1.4965375194668398 	 acc:0.7115264797507788 	 lr:0.0001
epoch26: train: loss:1.5629159896100153 	 acc:0.6265625 | test: loss:1.5384076018942479 	 acc:0.6654205607476635 	 lr:0.0001
epoch27: train: loss:1.566854026967152 	 acc:0.6303125 | test: loss:1.5378800881614567 	 acc:0.6610591900311527 	 lr:0.0001
epoch28: train: loss:1.5654136187205736 	 acc:0.6703125 | test: loss:1.5099800222759305 	 acc:0.7096573208722742 	 lr:0.0001
epoch29: train: loss:1.5633035056764124 	 acc:0.638125 | test: loss:1.5248224740459169 	 acc:0.6866043613707166 	 lr:0.0001
epoch30: train: loss:1.5662916911569635 	 acc:0.60796875 | test: loss:1.5584036385158884 	 acc:0.6193146417445483 	 lr:0.0001
epoch31: train: loss:1.5750961863557218 	 acc:0.64828125 | test: loss:1.5285303237653596 	 acc:0.6809968847352025 	 lr:0.0001
epoch32: train: loss:1.539806569059969 	 acc:0.6328125 | test: loss:1.5472113546924056 	 acc:0.6560747663551402 	 lr:5e-05
epoch33: train: loss:1.5267953769942915 	 acc:0.66796875 | test: loss:1.49604479583253 	 acc:0.702803738317757 	 lr:5e-05
epoch34: train: loss:1.5193425270098433 	 acc:0.6534375 | test: loss:1.5102109788734221 	 acc:0.685981308411215 	 lr:5e-05
epoch35: train: loss:1.5220326336541277 	 acc:0.69125 | test: loss:1.488806957990581 	 acc:0.7221183800623053 	 lr:5e-05
epoch36: train: loss:1.5277769995517418 	 acc:0.69859375 | test: loss:1.493322364637785 	 acc:0.7302180685358255 	 lr:5e-05
epoch37: train: loss:1.5176537828646444 	 acc:0.695 | test: loss:1.4923468968578588 	 acc:0.7246105919003115 	 lr:5e-05
epoch38: train: loss:1.5001539839328406 	 acc:0.6875 | test: loss:1.4792332378874686 	 acc:0.7277258566978193 	 lr:5e-05
epoch39: train: loss:1.5042093793644187 	 acc:0.7034375 | test: loss:1.4829872258355685 	 acc:0.7214953271028037 	 lr:5e-05
epoch40: train: loss:1.5131827647010392 	 acc:0.68484375 | test: loss:1.4927717811964754 	 acc:0.7121495327102804 	 lr:5e-05
epoch41: train: loss:1.502510045786373 	 acc:0.6865625 | test: loss:1.4953307506822722 	 acc:0.7140186915887851 	 lr:5e-05
epoch42: train: loss:1.5047159830729167 	 acc:0.65515625 | test: loss:1.511707794034964 	 acc:0.6828660436137072 	 lr:5e-05
epoch43: train: loss:1.5027326644909373 	 acc:0.7025 | test: loss:1.477350279252477 	 acc:0.7258566978193146 	 lr:5e-05
epoch44: train: loss:1.4978734679746963 	 acc:0.68453125 | test: loss:1.4868423759008864 	 acc:0.7140186915887851 	 lr:5e-05
epoch45: train: loss:1.4952617112218543 	 acc:0.67390625 | test: loss:1.501828655127053 	 acc:0.6965732087227414 	 lr:5e-05
epoch46: train: loss:1.4894165563173911 	 acc:0.67421875 | test: loss:1.4949679185296887 	 acc:0.702803738317757 	 lr:5e-05
epoch47: train: loss:1.494529213112467 	 acc:0.70359375 | test: loss:1.4796431834823989 	 acc:0.7314641744548287 	 lr:5e-05
epoch48: train: loss:1.477542998100239 	 acc:0.7009375 | test: loss:1.473814109181318 	 acc:0.7258566978193146 	 lr:5e-05
epoch49: train: loss:1.4769985749887173 	 acc:0.6721875 | test: loss:1.5068713242391187 	 acc:0.6834890965732088 	 lr:5e-05
epoch50: train: loss:1.4954748482745108 	 acc:0.66875 | test: loss:1.496805710138933 	 acc:0.7009345794392523 	 lr:5e-05
epoch51: train: loss:1.482787621384203 	 acc:0.71640625 | test: loss:1.4812487315537401 	 acc:0.7271028037383177 	 lr:5e-05
epoch52: train: loss:1.4757478069272663 	 acc:0.7003125 | test: loss:1.484255999419548 	 acc:0.7140186915887851 	 lr:5e-05
epoch53: train: loss:1.486863638217518 	 acc:0.68875 | test: loss:1.4996484761668885 	 acc:0.6897196261682244 	 lr:5e-05
epoch54: train: loss:1.4768461514784896 	 acc:0.6815625 | test: loss:1.5030984955784688 	 acc:0.6909657320872274 	 lr:5e-05
epoch55: train: loss:1.4585419153813548 	 acc:0.69890625 | test: loss:1.4876043030777453 	 acc:0.7115264797507788 	 lr:2.5e-05
epoch56: train: loss:1.4573579357900628 	 acc:0.715 | test: loss:1.4716285755329785 	 acc:0.7271028037383177 	 lr:2.5e-05
epoch57: train: loss:1.4674561356865363 	 acc:0.70515625 | test: loss:1.4817453322009506 	 acc:0.7158878504672898 	 lr:2.5e-05
epoch58: train: loss:1.4606580962062718 	 acc:0.72015625 | test: loss:1.4666369043035299 	 acc:0.7320872274143302 	 lr:2.5e-05
epoch59: train: loss:1.460801145269199 	 acc:0.721875 | test: loss:1.4639118225775032 	 acc:0.7408099688473521 	 lr:2.5e-05
epoch60: train: loss:1.4512594706932145 	 acc:0.70265625 | test: loss:1.4749642935868736 	 acc:0.7214953271028037 	 lr:2.5e-05
epoch61: train: loss:1.4470917105395564 	 acc:0.720625 | test: loss:1.4640266306303744 	 acc:0.735202492211838 	 lr:2.5e-05
epoch62: train: loss:1.44724677843009 	 acc:0.7146875 | test: loss:1.4673406040928445 	 acc:0.7221183800623053 	 lr:2.5e-05
epoch63: train: loss:1.4504510478988277 	 acc:0.71359375 | test: loss:1.4706443617277056 	 acc:0.729595015576324 	 lr:2.5e-05
epoch64: train: loss:1.4520153322599532 	 acc:0.71171875 | test: loss:1.4668353955693705 	 acc:0.729595015576324 	 lr:2.5e-05
epoch65: train: loss:1.4519560647885563 	 acc:0.7171875 | test: loss:1.467167546370319 	 acc:0.7358255451713396 	 lr:2.5e-05
epoch66: train: loss:1.4427011935065073 	 acc:0.740625 | test: loss:1.4531132679490657 	 acc:0.7489096573208722 	 lr:1.25e-05
epoch67: train: loss:1.441644334755867 	 acc:0.7178125 | test: loss:1.4607495156404013 	 acc:0.7339563862928349 	 lr:1.25e-05
epoch68: train: loss:1.4367565408151286 	 acc:0.7328125 | test: loss:1.4550130044188454 	 acc:0.7439252336448599 	 lr:1.25e-05
epoch69: train: loss:1.4297090719492522 	 acc:0.73109375 | test: loss:1.4570834942695878 	 acc:0.7370716510903427 	 lr:1.25e-05
epoch70: train: loss:1.4325622892119194 	 acc:0.7446875 | test: loss:1.4488294279092568 	 acc:0.7514018691588785 	 lr:1.25e-05
epoch71: train: loss:1.429382038339798 	 acc:0.73296875 | test: loss:1.461102423192556 	 acc:0.7358255451713396 	 lr:1.25e-05
epoch72: train: loss:1.430470906003763 	 acc:0.72953125 | test: loss:1.4560490761219156 	 acc:0.7401869158878505 	 lr:1.25e-05
epoch73: train: loss:1.4333231608072916 	 acc:0.73640625 | test: loss:1.4506074599387861 	 acc:0.7482866043613707 	 lr:1.25e-05
epoch74: train: loss:1.4261169290654274 	 acc:0.72734375 | test: loss:1.4557911819386704 	 acc:0.7364485981308411 	 lr:1.25e-05
epoch75: train: loss:1.4265279967928193 	 acc:0.72546875 | test: loss:1.4571605045104696 	 acc:0.7420560747663552 	 lr:1.25e-05
epoch76: train: loss:1.4225769733842883 	 acc:0.73234375 | test: loss:1.4517765971359062 	 acc:0.746417445482866 	 lr:1.25e-05
epoch77: train: loss:1.4303619218002903 	 acc:0.73 | test: loss:1.4520150120755966 	 acc:0.7495327102803738 	 lr:6.25e-06
epoch78: train: loss:1.424505841555212 	 acc:0.75359375 | test: loss:1.4416536118753973 	 acc:0.7601246105919003 	 lr:6.25e-06
epoch79: train: loss:1.4184395259288398 	 acc:0.73640625 | test: loss:1.4524734835758388 	 acc:0.7433021806853582 	 lr:6.25e-06
epoch80: train: loss:1.4176955664558024 	 acc:0.73578125 | test: loss:1.4479859355825502 	 acc:0.7557632398753894 	 lr:6.25e-06
epoch81: train: loss:1.429692419500299 	 acc:0.7425 | test: loss:1.4453699400120434 	 acc:0.7588785046728972 	 lr:6.25e-06
epoch82: train: loss:1.414434619102508 	 acc:0.744375 | test: loss:1.448682830118316 	 acc:0.7545171339563863 	 lr:6.25e-06
epoch83: train: loss:1.4188530355389466 	 acc:0.74640625 | test: loss:1.4435827999471504 	 acc:0.7545171339563863 	 lr:6.25e-06
epoch84: train: loss:1.417279874636362 	 acc:0.741875 | test: loss:1.4462676242133168 	 acc:0.7551401869158878 	 lr:6.25e-06
epoch85: train: loss:1.415298006890064 	 acc:0.74359375 | test: loss:1.4452157914081467 	 acc:0.7545171339563863 	 lr:3.125e-06
epoch86: train: loss:1.4192281801936963 	 acc:0.74609375 | test: loss:1.4434447412550264 	 acc:0.7576323987538941 	 lr:3.125e-06
epoch87: train: loss:1.421625363352148 	 acc:0.7453125 | test: loss:1.445188072314515 	 acc:0.756386292834891 	 lr:3.125e-06
epoch88: train: loss:1.4137682733826114 	 acc:0.74484375 | test: loss:1.441792966346503 	 acc:0.7595015576323988 	 lr:3.125e-06
epoch89: train: loss:1.410813245095842 	 acc:0.74890625 | test: loss:1.4450052769384651 	 acc:0.7514018691588785 	 lr:3.125e-06
epoch90: train: loss:1.41264231530695 	 acc:0.7453125 | test: loss:1.4456140296110112 	 acc:0.7526479750778816 	 lr:3.125e-06
epoch91: train: loss:1.4150379046902444 	 acc:0.7540625 | test: loss:1.4449841752601933 	 acc:0.7501557632398754 	 lr:1.5625e-06
epoch92: train: loss:1.4151097246299582 	 acc:0.74671875 | test: loss:1.4417403445436947 	 acc:0.7557632398753894 	 lr:1.5625e-06
epoch93: train: loss:1.4110601444527286 	 acc:0.7484375 | test: loss:1.4462758105120554 	 acc:0.7532710280373832 	 lr:1.5625e-06
epoch94: train: loss:1.4176254856130464 	 acc:0.74796875 | test: loss:1.4447503185717858 	 acc:0.756386292834891 	 lr:1.5625e-06
epoch95: train: loss:1.4171522940070624 	 acc:0.740625 | test: loss:1.4492743079907426 	 acc:0.75202492211838 	 lr:1.5625e-06
epoch96: train: loss:1.4136126408811476 	 acc:0.74546875 | test: loss:1.446462262976578 	 acc:0.7532710280373832 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_2_1/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_2_1/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.8658966707680766 	 acc:0.3990625 | test: loss:1.8550475102718746 	 acc:0.3806853582554517 	 lr:0.0001
epoch1: train: loss:1.740937375166191 	 acc:0.4975 | test: loss:1.7141500786457478 	 acc:0.5395638629283489 	 lr:0.0001
epoch2: train: loss:1.7028591563978948 	 acc:0.62046875 | test: loss:1.582838635726881 	 acc:0.6498442367601246 	 lr:0.0001
epoch3: train: loss:1.6805876184682378 	 acc:0.6628125 | test: loss:1.5589126748087994 	 acc:0.7059190031152648 	 lr:0.0001
epoch4: train: loss:1.6844822641651114 	 acc:0.5290625 | test: loss:1.640035526997575 	 acc:0.5383177570093458 	 lr:0.0001
epoch5: train: loss:1.6313148778458297 	 acc:0.63078125 | test: loss:1.5731957895362116 	 acc:0.6679127725856698 	 lr:0.0001
epoch6: train: loss:1.6160276557392297 	 acc:0.605625 | test: loss:1.5927697097772378 	 acc:0.6280373831775701 	 lr:0.0001
epoch7: train: loss:1.5960733930362936 	 acc:0.5365625 | test: loss:1.6109724144326563 	 acc:0.5744548286604362 	 lr:0.0001
epoch8: train: loss:1.6432381551773823 	 acc:0.5765625 | test: loss:1.6311379503980976 	 acc:0.5813084112149532 	 lr:0.0001
epoch9: train: loss:1.5908363383975836 	 acc:0.63625 | test: loss:1.5164337852662226 	 acc:0.6772585669781932 	 lr:0.0001
epoch10: train: loss:1.5476757898263687 	 acc:0.59578125 | test: loss:1.5751353994708195 	 acc:0.6012461059190031 	 lr:0.0001
epoch11: train: loss:1.5515723223243254 	 acc:0.63578125 | test: loss:1.551070822510764 	 acc:0.6423676012461059 	 lr:0.0001
epoch12: train: loss:1.5172568447044545 	 acc:0.65671875 | test: loss:1.4952256183386592 	 acc:0.6747663551401869 	 lr:0.0001
epoch13: train: loss:1.5366928183967299 	 acc:0.6375 | test: loss:1.5360728637451695 	 acc:0.6498442367601246 	 lr:0.0001
epoch14: train: loss:1.5366012966325746 	 acc:0.5815625 | test: loss:1.6069607862430941 	 acc:0.5937694704049844 	 lr:0.0001
epoch15: train: loss:1.5167448926772298 	 acc:0.66484375 | test: loss:1.5166675182146447 	 acc:0.6816199376947041 	 lr:0.0001
epoch16: train: loss:1.5079303464509843 	 acc:0.68640625 | test: loss:1.5095876972251963 	 acc:0.6841121495327103 	 lr:0.0001
epoch17: train: loss:1.5363645173905884 	 acc:0.68 | test: loss:1.514415384155939 	 acc:0.6816199376947041 	 lr:0.0001
epoch18: train: loss:1.5214587793044985 	 acc:0.674375 | test: loss:1.4814660063413816 	 acc:0.7158878504672898 	 lr:0.0001
epoch19: train: loss:1.5017673727686194 	 acc:0.749375 | test: loss:1.438381071105553 	 acc:0.7638629283489097 	 lr:0.0001
epoch20: train: loss:1.441119901730063 	 acc:0.721875 | test: loss:1.4476494351651439 	 acc:0.7308411214953271 	 lr:0.0001
epoch21: train: loss:1.472759247421753 	 acc:0.72515625 | test: loss:1.4539008394580022 	 acc:0.7364485981308411 	 lr:0.0001
epoch22: train: loss:1.5164149216615437 	 acc:0.69875 | test: loss:1.466278399485294 	 acc:0.7383177570093458 	 lr:0.0001
epoch23: train: loss:1.4857789110337078 	 acc:0.71703125 | test: loss:1.4766983829926108 	 acc:0.719626168224299 	 lr:0.0001
epoch24: train: loss:1.4589087790757953 	 acc:0.6384375 | test: loss:1.5181138899467443 	 acc:0.6467289719626168 	 lr:0.0001
epoch25: train: loss:1.4699550750756245 	 acc:0.75265625 | test: loss:1.4380725328796007 	 acc:0.7626168224299066 	 lr:0.0001
epoch26: train: loss:1.4650015432698758 	 acc:0.629375 | test: loss:1.5266750124013313 	 acc:0.6299065420560748 	 lr:0.0001
epoch27: train: loss:1.4348431410778322 	 acc:0.690625 | test: loss:1.4879096706336903 	 acc:0.6897196261682244 	 lr:0.0001
epoch28: train: loss:1.4544925424663653 	 acc:0.6721875 | test: loss:1.4922626895696576 	 acc:0.6741433021806854 	 lr:0.0001
epoch29: train: loss:1.4453723416116255 	 acc:0.75921875 | test: loss:1.4221254864214365 	 acc:0.7644859813084112 	 lr:0.0001
epoch30: train: loss:1.3973740898567097 	 acc:0.7353125 | test: loss:1.4389903737748524 	 acc:0.7389408099688474 	 lr:0.0001
epoch31: train: loss:1.3942827139861131 	 acc:0.74 | test: loss:1.4157852353336655 	 acc:0.7700934579439253 	 lr:0.0001
epoch32: train: loss:1.4170146980851446 	 acc:0.7134375 | test: loss:1.4396613801379814 	 acc:0.7420560747663552 	 lr:0.0001
epoch33: train: loss:1.471110226156933 	 acc:0.72984375 | test: loss:1.419482369066399 	 acc:0.7688473520249222 	 lr:0.0001
epoch34: train: loss:1.4109888867416203 	 acc:0.7478125 | test: loss:1.4326462819940204 	 acc:0.7538940809968847 	 lr:0.0001
epoch35: train: loss:1.3944430229907665 	 acc:0.81375 | test: loss:1.3870967604289546 	 acc:0.8130841121495327 	 lr:0.0001
epoch36: train: loss:1.3878507210629514 	 acc:0.77078125 | test: loss:1.4196222040883477 	 acc:0.7688473520249222 	 lr:0.0001
epoch37: train: loss:1.3874135740281641 	 acc:0.7375 | test: loss:1.4528479671923913 	 acc:0.7507788161993769 	 lr:0.0001
epoch38: train: loss:1.3761368047995646 	 acc:0.75984375 | test: loss:1.422924562258141 	 acc:0.761993769470405 	 lr:0.0001
epoch39: train: loss:1.410395478271675 	 acc:0.7096875 | test: loss:1.4569628789788838 	 acc:0.7246105919003115 	 lr:0.0001
epoch40: train: loss:1.3849148109310963 	 acc:0.7546875 | test: loss:1.4277853664208053 	 acc:0.7495327102803738 	 lr:0.0001
epoch41: train: loss:1.4048085197073514 	 acc:0.78828125 | test: loss:1.3896524486512039 	 acc:0.788785046728972 	 lr:0.0001
epoch42: train: loss:1.3624296261313182 	 acc:0.80328125 | test: loss:1.3941828149872777 	 acc:0.8049844236760124 	 lr:5e-05
epoch43: train: loss:1.3588163949101348 	 acc:0.79421875 | test: loss:1.4147613180017917 	 acc:0.7819314641744548 	 lr:5e-05
epoch44: train: loss:1.3349095388467567 	 acc:0.7375 | test: loss:1.4545166275582952 	 acc:0.7090342679127726 	 lr:5e-05
epoch45: train: loss:1.339637636040264 	 acc:0.80796875 | test: loss:1.3926243689201332 	 acc:0.8018691588785046 	 lr:5e-05
epoch46: train: loss:1.3387940834873269 	 acc:0.82375 | test: loss:1.3812204222812832 	 acc:0.8043613707165109 	 lr:5e-05
epoch47: train: loss:1.3506407238188245 	 acc:0.7925 | test: loss:1.407638275734732 	 acc:0.7775700934579439 	 lr:5e-05
epoch48: train: loss:1.3350978854296265 	 acc:0.7728125 | test: loss:1.4206174169374033 	 acc:0.7482866043613707 	 lr:5e-05
epoch49: train: loss:1.3349175410750878 	 acc:0.811875 | test: loss:1.3834109038950126 	 acc:0.7968847352024923 	 lr:5e-05
epoch50: train: loss:1.3397587820108192 	 acc:0.81140625 | test: loss:1.37798253814008 	 acc:0.811214953271028 	 lr:5e-05
epoch51: train: loss:1.3187109656114302 	 acc:0.808125 | test: loss:1.3704222350848427 	 acc:0.8161993769470405 	 lr:5e-05
epoch52: train: loss:1.3312260361968493 	 acc:0.8025 | test: loss:1.3867615022391917 	 acc:0.8074766355140187 	 lr:5e-05
epoch53: train: loss:1.3288865529402079 	 acc:0.795 | test: loss:1.3946239182510851 	 acc:0.7900311526479751 	 lr:5e-05
epoch54: train: loss:1.3199934948244474 	 acc:0.82546875 | test: loss:1.3819149829888269 	 acc:0.8124610591900312 	 lr:5e-05
epoch55: train: loss:1.3177617379038897 	 acc:0.80078125 | test: loss:1.396627983125942 	 acc:0.7912772585669782 	 lr:5e-05
epoch56: train: loss:1.3202667831909276 	 acc:0.80265625 | test: loss:1.3839869126352566 	 acc:0.7912772585669782 	 lr:5e-05
epoch57: train: loss:1.3275196894363535 	 acc:0.8146875 | test: loss:1.3834757122295296 	 acc:0.8074766355140187 	 lr:5e-05
epoch58: train: loss:1.301056265551815 	 acc:0.8415625 | test: loss:1.3698883027673883 	 acc:0.821183800623053 	 lr:2.5e-05
epoch59: train: loss:1.3037644468928389 	 acc:0.81515625 | test: loss:1.390053555452935 	 acc:0.7950155763239876 	 lr:2.5e-05
epoch60: train: loss:1.3032835508490985 	 acc:0.84078125 | test: loss:1.3721356092583725 	 acc:0.8137071651090343 	 lr:2.5e-05
epoch61: train: loss:1.2994085807710956 	 acc:0.80484375 | test: loss:1.3885971983645193 	 acc:0.7925233644859813 	 lr:2.5e-05
epoch62: train: loss:1.3023791942253977 	 acc:0.83203125 | test: loss:1.3627434529248057 	 acc:0.8267912772585669 	 lr:2.5e-05
epoch63: train: loss:1.3012846280409898 	 acc:0.82234375 | test: loss:1.3822342813943405 	 acc:0.8024922118380062 	 lr:2.5e-05
epoch64: train: loss:1.2921144650747403 	 acc:0.82578125 | test: loss:1.3711305499448212 	 acc:0.8149532710280374 	 lr:2.5e-05
epoch65: train: loss:1.2999683365237424 	 acc:0.83125 | test: loss:1.3821261195750252 	 acc:0.7987538940809968 	 lr:2.5e-05
epoch66: train: loss:1.2909869060769479 	 acc:0.8509375 | test: loss:1.356298201626335 	 acc:0.8348909657320872 	 lr:2.5e-05
epoch67: train: loss:1.2864788165602434 	 acc:0.84875 | test: loss:1.3586799795382491 	 acc:0.821183800623053 	 lr:2.5e-05
epoch68: train: loss:1.2955534626039837 	 acc:0.85859375 | test: loss:1.3557529563101653 	 acc:0.838006230529595 	 lr:2.5e-05
epoch69: train: loss:1.2956432190656104 	 acc:0.8428125 | test: loss:1.3635430844030647 	 acc:0.815576323987539 	 lr:2.5e-05
epoch70: train: loss:1.2886304892570501 	 acc:0.85171875 | test: loss:1.3655250468358073 	 acc:0.8311526479750779 	 lr:2.5e-05
epoch71: train: loss:1.2957962357746633 	 acc:0.853125 | test: loss:1.3524928881000506 	 acc:0.832398753894081 	 lr:2.5e-05
epoch72: train: loss:1.2868339353944063 	 acc:0.84171875 | test: loss:1.3682647685024225 	 acc:0.8168224299065421 	 lr:2.5e-05
epoch73: train: loss:1.2802698235135819 	 acc:0.829375 | test: loss:1.3722225961655472 	 acc:0.8080996884735202 	 lr:2.5e-05
epoch74: train: loss:1.2936177333382124 	 acc:0.86421875 | test: loss:1.348666020940026 	 acc:0.8355140186915888 	 lr:2.5e-05
epoch75: train: loss:1.2733743255162593 	 acc:0.831875 | test: loss:1.3741271601287748 	 acc:0.8093457943925234 	 lr:2.5e-05
epoch76: train: loss:1.2862265379144102 	 acc:0.8103125 | test: loss:1.3771375713318679 	 acc:0.8024922118380062 	 lr:2.5e-05
epoch77: train: loss:1.2854504194415983 	 acc:0.85015625 | test: loss:1.3643131444758716 	 acc:0.8199376947040499 	 lr:2.5e-05
epoch78: train: loss:1.2886339221867986 	 acc:0.8709375 | test: loss:1.348719933545478 	 acc:0.8367601246105919 	 lr:2.5e-05
epoch79: train: loss:1.2892041291974654 	 acc:0.869375 | test: loss:1.3451065922080543 	 acc:0.8461059190031153 	 lr:2.5e-05
epoch80: train: loss:1.2818596559237168 	 acc:0.8459375 | test: loss:1.3647075393860957 	 acc:0.8174454828660436 	 lr:2.5e-05
epoch81: train: loss:1.2731411936132355 	 acc:0.8465625 | test: loss:1.3597891347801945 	 acc:0.8249221183800624 	 lr:2.5e-05
epoch82: train: loss:1.28417471558856 	 acc:0.8503125 | test: loss:1.3580565940553897 	 acc:0.8280373831775701 	 lr:2.5e-05
epoch83: train: loss:1.27893742271739 	 acc:0.84546875 | test: loss:1.358124026925393 	 acc:0.8236760124610591 	 lr:2.5e-05
epoch84: train: loss:1.284354545360985 	 acc:0.82984375 | test: loss:1.3680710884640892 	 acc:0.8174454828660436 	 lr:2.5e-05
epoch85: train: loss:1.2835311710974093 	 acc:0.84484375 | test: loss:1.3641001534239154 	 acc:0.8274143302180685 	 lr:2.5e-05
epoch86: train: loss:1.2778953010267247 	 acc:0.84421875 | test: loss:1.3638512245219816 	 acc:0.8193146417445483 	 lr:1.25e-05
epoch87: train: loss:1.276434647785696 	 acc:0.84734375 | test: loss:1.36204526364989 	 acc:0.8180685358255452 	 lr:1.25e-05
epoch88: train: loss:1.278455547538239 	 acc:0.86453125 | test: loss:1.3494575083070084 	 acc:0.8442367601246106 	 lr:1.25e-05
epoch89: train: loss:1.265710115693306 	 acc:0.87484375 | test: loss:1.3437935709581939 	 acc:0.8417445482866044 	 lr:1.25e-05
epoch90: train: loss:1.267381013844927 	 acc:0.8671875 | test: loss:1.342906677462973 	 acc:0.8404984423676013 	 lr:1.25e-05
epoch91: train: loss:1.2739256097226288 	 acc:0.86296875 | test: loss:1.3477828401642797 	 acc:0.8367601246105919 	 lr:1.25e-05
epoch92: train: loss:1.2772104422623622 	 acc:0.869375 | test: loss:1.3422559933499012 	 acc:0.8473520249221184 	 lr:1.25e-05
epoch93: train: loss:1.269344320900267 	 acc:0.85984375 | test: loss:1.3495263854291208 	 acc:0.8342679127725857 	 lr:1.25e-05
epoch94: train: loss:1.2586594283068953 	 acc:0.87609375 | test: loss:1.3431483833960656 	 acc:0.8429906542056075 	 lr:1.25e-05
epoch95: train: loss:1.2638815719759344 	 acc:0.8778125 | test: loss:1.34129798753982 	 acc:0.838006230529595 	 lr:1.25e-05
epoch96: train: loss:1.2618431287850373 	 acc:0.84734375 | test: loss:1.3589300419311285 	 acc:0.8186915887850468 	 lr:1.25e-05
epoch97: train: loss:1.2652883632400835 	 acc:0.87609375 | test: loss:1.3408914161991108 	 acc:0.8398753894080997 	 lr:1.25e-05
epoch98: train: loss:1.2619797312775969 	 acc:0.881875 | test: loss:1.3401205139368122 	 acc:0.8467289719626169 	 lr:1.25e-05
epoch99: train: loss:1.2637025903110668 	 acc:0.86984375 | test: loss:1.346106722496009 	 acc:0.8355140186915888 	 lr:1.25e-05
epoch100: train: loss:1.2628742894002183 	 acc:0.85921875 | test: loss:1.3568777296773369 	 acc:0.8236760124610591 	 lr:1.25e-05
epoch101: train: loss:1.260381293911007 	 acc:0.873125 | test: loss:1.3426028683920888 	 acc:0.8373831775700935 	 lr:1.25e-05
epoch102: train: loss:1.2653943671554815 	 acc:0.87359375 | test: loss:1.350118798853081 	 acc:0.832398753894081 	 lr:1.25e-05
epoch103: train: loss:1.2596731645999524 	 acc:0.85734375 | test: loss:1.3579984739190694 	 acc:0.8230529595015577 	 lr:1.25e-05
epoch104: train: loss:1.2669698198543313 	 acc:0.87515625 | test: loss:1.3421917968078565 	 acc:0.8398753894080997 	 lr:1.25e-05
epoch105: train: loss:1.2555870228870878 	 acc:0.88125 | test: loss:1.3367921716327609 	 acc:0.8498442367601247 	 lr:6.25e-06
epoch106: train: loss:1.264123380305151 	 acc:0.874375 | test: loss:1.344633741393639 	 acc:0.8367601246105919 	 lr:6.25e-06
epoch107: train: loss:1.2575515044284555 	 acc:0.87875 | test: loss:1.3443112911093644 	 acc:0.8398753894080997 	 lr:6.25e-06
epoch108: train: loss:1.2605328288141588 	 acc:0.87375 | test: loss:1.3447659628413549 	 acc:0.8361370716510903 	 lr:6.25e-06
epoch109: train: loss:1.2549117910964334 	 acc:0.879375 | test: loss:1.3413112612156852 	 acc:0.8398753894080997 	 lr:6.25e-06
epoch110: train: loss:1.259280922001549 	 acc:0.87484375 | test: loss:1.3401126790269513 	 acc:0.8411214953271028 	 lr:6.25e-06
epoch111: train: loss:1.2614515699137943 	 acc:0.87765625 | test: loss:1.3400462775215554 	 acc:0.8429906542056075 	 lr:6.25e-06
epoch112: train: loss:1.260788449861406 	 acc:0.87640625 | test: loss:1.3421537654050786 	 acc:0.8423676012461059 	 lr:3.125e-06
epoch113: train: loss:1.2558946214924558 	 acc:0.8828125 | test: loss:1.3425398632002026 	 acc:0.8404984423676013 	 lr:3.125e-06
epoch114: train: loss:1.2552051434006941 	 acc:0.879375 | test: loss:1.3422459859342961 	 acc:0.8355140186915888 	 lr:3.125e-06
epoch115: train: loss:1.2561376281309462 	 acc:0.8828125 | test: loss:1.3441597553057092 	 acc:0.8367601246105919 	 lr:3.125e-06
epoch116: train: loss:1.2558779023384137 	 acc:0.87796875 | test: loss:1.3461624135109493 	 acc:0.8348909657320872 	 lr:3.125e-06
epoch117: train: loss:1.2530687032873793 	 acc:0.88015625 | test: loss:1.3435667750991394 	 acc:0.8373831775700935 	 lr:3.125e-06
epoch118: train: loss:1.2583658067999548 	 acc:0.88015625 | test: loss:1.3400190466289579 	 acc:0.8404984423676013 	 lr:1.5625e-06
epoch119: train: loss:1.2543731403574172 	 acc:0.88390625 | test: loss:1.340729748051486 	 acc:0.8436137071651091 	 lr:1.5625e-06
epoch120: train: loss:1.2604772896807608 	 acc:0.8825 | test: loss:1.3421932701752564 	 acc:0.8423676012461059 	 lr:1.5625e-06
epoch121: train: loss:1.2530903109715006 	 acc:0.87953125 | test: loss:1.3424741340946185 	 acc:0.8392523364485981 	 lr:1.5625e-06
epoch122: train: loss:1.2593826591735888 	 acc:0.88046875 | test: loss:1.3418846240296172 	 acc:0.8417445482866044 	 lr:1.5625e-06
epoch123: train: loss:1.256997730692879 	 acc:0.88140625 | test: loss:1.3408693988746572 	 acc:0.8404984423676013 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_3_1/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_3_1/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.681819525665831 	 acc:0.596875 | test: loss:1.628357890090467 	 acc:0.6205607476635514 	 lr:0.0001
epoch1: train: loss:1.569219803940403 	 acc:0.60875 | test: loss:1.5710920648782796 	 acc:0.6404984423676012 	 lr:0.0001
epoch2: train: loss:1.5405646295123134 	 acc:0.590625 | test: loss:1.5442055257310006 	 acc:0.6523364485981309 	 lr:0.0001
epoch3: train: loss:1.5101055469855398 	 acc:0.67125 | test: loss:1.4948351993739049 	 acc:0.7140186915887851 	 lr:0.0001
epoch4: train: loss:1.4505652112759806 	 acc:0.81125 | test: loss:1.3915289893699956 	 acc:0.8118380062305296 	 lr:0.0001
epoch5: train: loss:1.421496534384758 	 acc:0.731875 | test: loss:1.4324207409891383 	 acc:0.7607476635514019 	 lr:0.0001
epoch6: train: loss:1.3956418944931328 	 acc:0.7425 | test: loss:1.4302986705043235 	 acc:0.75202492211838 	 lr:0.0001
epoch7: train: loss:1.4022094178627842 	 acc:0.77734375 | test: loss:1.4025268701749427 	 acc:0.8043613707165109 	 lr:0.0001
epoch8: train: loss:1.3871994316345262 	 acc:0.7890625 | test: loss:1.422501871444726 	 acc:0.7806853582554517 	 lr:0.0001
epoch9: train: loss:1.3712535873043825 	 acc:0.801875 | test: loss:1.3890394158081103 	 acc:0.8006230529595015 	 lr:0.0001
epoch10: train: loss:1.3639000135506623 	 acc:0.7878125 | test: loss:1.389473399417794 	 acc:0.7968847352024923 	 lr:0.0001
epoch11: train: loss:1.3372733265789667 	 acc:0.8209375 | test: loss:1.3627631350841107 	 acc:0.8161993769470405 	 lr:0.0001
epoch12: train: loss:1.333732808669222 	 acc:0.82859375 | test: loss:1.3666894041489217 	 acc:0.8236760124610591 	 lr:0.0001
epoch13: train: loss:1.3125141057439562 	 acc:0.769375 | test: loss:1.3963035411181108 	 acc:0.7744548286604361 	 lr:0.0001
epoch14: train: loss:1.397587331731649 	 acc:0.8075 | test: loss:1.3679003587021634 	 acc:0.8037383177570093 	 lr:0.0001
epoch15: train: loss:1.3293207030106484 	 acc:0.83625 | test: loss:1.3608070118033626 	 acc:0.8274143302180685 	 lr:0.0001
epoch16: train: loss:1.3245158830534007 	 acc:0.864375 | test: loss:1.3422415015110718 	 acc:0.8448598130841122 	 lr:0.0001
epoch17: train: loss:1.3294482336111315 	 acc:0.79 | test: loss:1.417591449033434 	 acc:0.7570093457943925 	 lr:0.0001
epoch18: train: loss:1.2946733628093592 	 acc:0.88140625 | test: loss:1.3283828372153166 	 acc:0.8573208722741433 	 lr:0.0001
epoch19: train: loss:1.3007090655646223 	 acc:0.875625 | test: loss:1.323078700909362 	 acc:0.8542056074766355 	 lr:0.0001
epoch20: train: loss:1.2883594426580187 	 acc:0.86296875 | test: loss:1.3570258539413738 	 acc:0.822429906542056 	 lr:0.0001
epoch21: train: loss:1.2866048462962285 	 acc:0.79515625 | test: loss:1.400304225226429 	 acc:0.767601246105919 	 lr:0.0001
epoch22: train: loss:1.3361412959579004 	 acc:0.82734375 | test: loss:1.3476369795398178 	 acc:0.8124610591900312 	 lr:0.0001
epoch23: train: loss:1.2999000972179022 	 acc:0.88109375 | test: loss:1.3334482155856313 	 acc:0.8373831775700935 	 lr:0.0001
epoch24: train: loss:1.3157420293136466 	 acc:0.843125 | test: loss:1.3638795960358 	 acc:0.8161993769470405 	 lr:0.0001
epoch25: train: loss:1.283385134599434 	 acc:0.8821875 | test: loss:1.3360886963728433 	 acc:0.8417445482866044 	 lr:0.0001
epoch26: train: loss:1.2610539694673655 	 acc:0.9025 | test: loss:1.322157248455416 	 acc:0.8666666666666667 	 lr:5e-05
epoch27: train: loss:1.2501618694272663 	 acc:0.89890625 | test: loss:1.3173011637922387 	 acc:0.8623052959501558 	 lr:5e-05
epoch28: train: loss:1.2510371259559792 	 acc:0.9284375 | test: loss:1.2905004294861886 	 acc:0.8841121495327103 	 lr:5e-05
epoch29: train: loss:1.2286124688773115 	 acc:0.935 | test: loss:1.2949991519577406 	 acc:0.881619937694704 	 lr:5e-05
epoch30: train: loss:1.2400333258623635 	 acc:0.9228125 | test: loss:1.293377711616944 	 acc:0.8785046728971962 	 lr:5e-05
epoch31: train: loss:1.24572468027298 	 acc:0.9321875 | test: loss:1.2950775992461825 	 acc:0.8797507788161993 	 lr:5e-05
epoch32: train: loss:1.2269916662473774 	 acc:0.9259375 | test: loss:1.3004875799577185 	 acc:0.8728971962616823 	 lr:5e-05
epoch33: train: loss:1.2220482283509588 	 acc:0.9334375 | test: loss:1.2948913635123185 	 acc:0.8766355140186916 	 lr:5e-05
epoch34: train: loss:1.2267508299065977 	 acc:0.93234375 | test: loss:1.2889410258824952 	 acc:0.8841121495327103 	 lr:5e-05
epoch35: train: loss:1.2383549162412788 	 acc:0.87203125 | test: loss:1.3470440043838596 	 acc:0.8286604361370716 	 lr:5e-05
epoch36: train: loss:1.2404312574519114 	 acc:0.9340625 | test: loss:1.2885348553969482 	 acc:0.8841121495327103 	 lr:5e-05
epoch37: train: loss:1.2435591805641955 	 acc:0.90140625 | test: loss:1.3190442002079568 	 acc:0.8510903426791278 	 lr:5e-05
epoch38: train: loss:1.2389584734140198 	 acc:0.9175 | test: loss:1.3167445779960847 	 acc:0.8585669781931464 	 lr:5e-05
epoch39: train: loss:1.2231568288095849 	 acc:0.92765625 | test: loss:1.313609666764922 	 acc:0.8573208722741433 	 lr:5e-05
epoch40: train: loss:1.2245534118128232 	 acc:0.923125 | test: loss:1.2987104349789962 	 acc:0.8741433021806854 	 lr:5e-05
epoch41: train: loss:1.2320681426787543 	 acc:0.931875 | test: loss:1.300788518721441 	 acc:0.8691588785046729 	 lr:5e-05
epoch42: train: loss:1.2253279722453467 	 acc:0.938125 | test: loss:1.3005806567141571 	 acc:0.870404984423676 	 lr:5e-05
epoch43: train: loss:1.2180484125522968 	 acc:0.9553125 | test: loss:1.2866733821381662 	 acc:0.8884735202492212 	 lr:2.5e-05
epoch44: train: loss:1.2090294056995878 	 acc:0.94703125 | test: loss:1.289325344673941 	 acc:0.8822429906542056 	 lr:2.5e-05
epoch45: train: loss:1.2151902734070061 	 acc:0.9425 | test: loss:1.2935391898467161 	 acc:0.8828660436137071 	 lr:2.5e-05
epoch46: train: loss:1.2115084360019943 	 acc:0.95359375 | test: loss:1.2928006113503951 	 acc:0.8828660436137071 	 lr:2.5e-05
epoch47: train: loss:1.210937862113339 	 acc:0.9459375 | test: loss:1.2884429442176937 	 acc:0.8890965732087227 	 lr:2.5e-05
epoch48: train: loss:1.2077885290498755 	 acc:0.9409375 | test: loss:1.2963546767041692 	 acc:0.8735202492211838 	 lr:2.5e-05
epoch49: train: loss:1.2124570300111912 	 acc:0.94859375 | test: loss:1.2897429946055665 	 acc:0.891588785046729 	 lr:2.5e-05
epoch50: train: loss:1.2052154255136673 	 acc:0.95921875 | test: loss:1.2871767336705764 	 acc:0.8872274143302181 	 lr:1.25e-05
epoch51: train: loss:1.20425492464612 	 acc:0.95234375 | test: loss:1.2894533493808498 	 acc:0.8903426791277259 	 lr:1.25e-05
epoch52: train: loss:1.2022286003404627 	 acc:0.9628125 | test: loss:1.2748213017098258 	 acc:0.8965732087227415 	 lr:1.25e-05
epoch53: train: loss:1.2062455736408932 	 acc:0.9565625 | test: loss:1.2804672224870723 	 acc:0.8922118380062305 	 lr:1.25e-05
epoch54: train: loss:1.2042262724281567 	 acc:0.958125 | test: loss:1.2796687888950573 	 acc:0.8922118380062305 	 lr:1.25e-05
epoch55: train: loss:1.2040282893422802 	 acc:0.96171875 | test: loss:1.2785716046425413 	 acc:0.8947040498442368 	 lr:1.25e-05
epoch56: train: loss:1.1993141358946562 	 acc:0.96203125 | test: loss:1.2772349798048026 	 acc:0.8953271028037383 	 lr:1.25e-05
epoch57: train: loss:1.2025801814225947 	 acc:0.959375 | test: loss:1.2764709560299217 	 acc:0.8928348909657321 	 lr:1.25e-05
epoch58: train: loss:1.208315918540508 	 acc:0.95875 | test: loss:1.2772712641415938 	 acc:0.8940809968847352 	 lr:1.25e-05
epoch59: train: loss:1.2023897884228936 	 acc:0.95578125 | test: loss:1.2814798365500857 	 acc:0.8884735202492212 	 lr:6.25e-06
epoch60: train: loss:1.2009158942217384 	 acc:0.95640625 | test: loss:1.2778677829700837 	 acc:0.8953271028037383 	 lr:6.25e-06
epoch61: train: loss:1.2006028520046594 	 acc:0.9665625 | test: loss:1.2743656865532895 	 acc:0.8959501557632399 	 lr:6.25e-06
epoch62: train: loss:1.203676496233259 	 acc:0.96125 | test: loss:1.2776530313343273 	 acc:0.8934579439252337 	 lr:6.25e-06
epoch63: train: loss:1.1983023388138234 	 acc:0.96328125 | test: loss:1.2759354244511447 	 acc:0.8928348909657321 	 lr:6.25e-06
epoch64: train: loss:1.1985650234535092 	 acc:0.96578125 | test: loss:1.2766346072110804 	 acc:0.8990654205607477 	 lr:6.25e-06
epoch65: train: loss:1.1986158193786287 	 acc:0.96078125 | test: loss:1.2809866241205519 	 acc:0.891588785046729 	 lr:6.25e-06
epoch66: train: loss:1.1998499159325295 	 acc:0.96078125 | test: loss:1.280374329483769 	 acc:0.8922118380062305 	 lr:6.25e-06
epoch67: train: loss:1.203865883891979 	 acc:0.95890625 | test: loss:1.2776020590018633 	 acc:0.8909657320872274 	 lr:6.25e-06
epoch68: train: loss:1.198038782243334 	 acc:0.96265625 | test: loss:1.2790366989058497 	 acc:0.8884735202492212 	 lr:3.125e-06
epoch69: train: loss:1.1989486444191855 	 acc:0.96328125 | test: loss:1.279274020165298 	 acc:0.891588785046729 	 lr:3.125e-06
epoch70: train: loss:1.2041217860535287 	 acc:0.96125 | test: loss:1.280738705340947 	 acc:0.8890965732087227 	 lr:3.125e-06
epoch71: train: loss:1.1961380779138306 	 acc:0.96328125 | test: loss:1.2794599374878073 	 acc:0.8922118380062305 	 lr:3.125e-06
epoch72: train: loss:1.2006313136366547 	 acc:0.96125 | test: loss:1.2776321643609496 	 acc:0.891588785046729 	 lr:3.125e-06
epoch73: train: loss:1.2017689689261015 	 acc:0.96171875 | test: loss:1.27792332618036 	 acc:0.8897196261682243 	 lr:3.125e-06
epoch74: train: loss:1.1987144952635576 	 acc:0.9653125 | test: loss:1.2793861731561915 	 acc:0.8872274143302181 	 lr:1.5625e-06
epoch75: train: loss:1.2029895505525467 	 acc:0.9603125 | test: loss:1.2775054465201785 	 acc:0.8897196261682243 	 lr:1.5625e-06
epoch76: train: loss:1.1966897170865656 	 acc:0.9646875 | test: loss:1.2786022949812939 	 acc:0.8884735202492212 	 lr:1.5625e-06
epoch77: train: loss:1.199829506557682 	 acc:0.9628125 | test: loss:1.2789759846862603 	 acc:0.8909657320872274 	 lr:1.5625e-06
epoch78: train: loss:1.195325176349196 	 acc:0.9659375 | test: loss:1.2795895927791654 	 acc:0.8897196261682243 	 lr:1.5625e-06
epoch79: train: loss:1.1951689887661008 	 acc:0.96484375 | test: loss:1.279769781668238 	 acc:0.8922118380062305 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_1_1/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_1_1/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_2_1/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_2_1/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_3_1/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_3_1/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_4_1/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_4_1/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_5_1/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_5_1/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'
