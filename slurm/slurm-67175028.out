
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_-1_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_1_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_2_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_3_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_1_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_2_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_3_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_1_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_1_2/
0.conv1.weight
0.conv1.weight freeze
0.bn1.weight
0.bn1.weight freeze
0.bn1.bias
0.bn1.bias freeze
0.layer1.0.conv1.weight
0.layer1.0.bn1.weight
0.layer1.0.bn1.bias
0.layer1.0.conv2.weight
0.layer1.0.bn2.weight
0.layer1.0.bn2.bias
0.layer1.0.conv3.weight
0.layer1.0.bn3.weight
0.layer1.0.bn3.bias
0.layer1.0.downsample.0.weight
0.layer1.0.downsample.1.weight
0.layer1.0.downsample.1.bias
0.layer1.1.conv1.weight
0.layer1.1.bn1.weight
0.layer1.1.bn1.bias
0.layer1.1.conv2.weight
0.layer1.1.bn2.weight
0.layer1.1.bn2.bias
0.layer1.1.conv3.weight
0.layer1.1.bn3.weight
0.layer1.1.bn3.bias
0.layer1.2.conv1.weight
0.layer1.2.bn1.weight
0.layer1.2.bn1.bias
0.layer1.2.conv2.weight
0.layer1.2.bn2.weight
0.layer1.2.bn2.bias
0.layer1.2.conv3.weight
0.layer1.2.bn3.weight
0.layer1.2.bn3.bias
0.layer2.0.conv1.weight
0.layer2.0.bn1.weight
0.layer2.0.bn1.bias
0.layer2.0.conv2.weight
0.layer2.0.bn2.weight
0.layer2.0.bn2.bias
0.layer2.0.conv3.weight
0.layer2.0.bn3.weight
0.layer2.0.bn3.bias
0.layer2.0.downsample.0.weight
0.layer2.0.downsample.1.weight
0.layer2.0.downsample.1.bias
0.layer2.1.conv1.weight
0.layer2.1.bn1.weight
0.layer2.1.bn1.bias
0.layer2.1.conv2.weight
0.layer2.1.bn2.weight
0.layer2.1.bn2.bias
0.layer2.1.conv3.weight
0.layer2.1.bn3.weight
0.layer2.1.bn3.bias
0.layer2.2.conv1.weight
0.layer2.2.bn1.weight
0.layer2.2.bn1.bias
0.layer2.2.conv2.weight
0.layer2.2.bn2.weight
0.layer2.2.bn2.bias
0.layer2.2.conv3.weight
0.layer2.2.bn3.weight
0.layer2.2.bn3.bias
0.layer2.3.conv1.weight
0.layer2.3.bn1.weight
0.layer2.3.bn1.bias
0.layer2.3.conv2.weight
0.layer2.3.bn2.weight
0.layer2.3.bn2.bias
0.layer2.3.conv3.weight
0.layer2.3.bn3.weight
0.layer2.3.bn3.bias
0.layer3.0.conv1.weight
0.layer3.0.bn1.weight
0.layer3.0.bn1.bias
0.layer3.0.conv2.weight
0.layer3.0.bn2.weight
0.layer3.0.bn2.bias
0.layer3.0.conv3.weight
0.layer3.0.bn3.weight
0.layer3.0.bn3.bias
0.layer3.0.downsample.0.weight
0.layer3.0.downsample.1.weight
0.layer3.0.downsample.1.bias
0.layer3.1.conv1.weight
0.layer3.1.bn1.weight
0.layer3.1.bn1.bias
0.layer3.1.conv2.weight
0.layer3.1.bn2.weight
0.layer3.1.bn2.bias
0.layer3.1.conv3.weight
0.layer3.1.bn3.weight
0.layer3.1.bn3.bias
0.layer3.2.conv1.weight
0.layer3.2.bn1.weight
0.layer3.2.bn1.bias
0.layer3.2.conv2.weight
0.layer3.2.bn2.weight
0.layer3.2.bn2.bias
0.layer3.2.conv3.weight
0.layer3.2.bn3.weight
0.layer3.2.bn3.bias
0.layer3.3.conv1.weight
0.layer3.3.bn1.weight
0.layer3.3.bn1.bias
0.layer3.3.conv2.weight
0.layer3.3.bn2.weight
0.layer3.3.bn2.bias
0.layer3.3.conv3.weight
0.layer3.3.bn3.weight
0.layer3.3.bn3.bias
0.layer3.4.conv1.weight
0.layer3.4.bn1.weight
0.layer3.4.bn1.bias
0.layer3.4.conv2.weight
0.layer3.4.bn2.weight
0.layer3.4.bn2.bias
0.layer3.4.conv3.weight
0.layer3.4.bn3.weight
0.layer3.4.bn3.bias
0.layer3.5.conv1.weight
0.layer3.5.bn1.weight
0.layer3.5.bn1.bias
0.layer3.5.conv2.weight
0.layer3.5.bn2.weight
0.layer3.5.bn2.bias
0.layer3.5.conv3.weight
0.layer3.5.bn3.weight
0.layer3.5.bn3.bias
0.layer4.0.conv1.weight
0.layer4.0.bn1.weight
0.layer4.0.bn1.bias
0.layer4.0.conv2.weight
0.layer4.0.bn2.weight
0.layer4.0.bn2.bias
0.layer4.0.conv3.weight
0.layer4.0.bn3.weight
0.layer4.0.bn3.bias
0.layer4.0.downsample.0.weight
0.layer4.0.downsample.1.weight
0.layer4.0.downsample.1.bias
0.layer4.1.conv1.weight
0.layer4.1.bn1.weight
0.layer4.1.bn1.bias
0.layer4.1.conv2.weight
0.layer4.1.bn2.weight
0.layer4.1.bn2.bias
0.layer4.1.conv3.weight
0.layer4.1.bn3.weight
0.layer4.1.bn3.bias
0.layer4.2.conv1.weight
0.layer4.2.bn1.weight
0.layer4.2.bn1.bias
0.layer4.2.conv2.weight
0.layer4.2.bn2.weight
0.layer4.2.bn2.bias
0.layer4.2.conv3.weight
0.layer4.2.bn3.weight
0.layer4.2.bn3.bias
1.1.weight
1.1.bias
training with  freeze_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.523358256736833 	 acc:0.72203125 | test: loss:1.4948726923666267 	 acc:0.7227414330218068 	 lr:0.0001
epoch1: train: loss:1.4545036869808439 	 acc:0.7515625 | test: loss:1.4529205196743071 	 acc:0.7445482866043613 	 lr:0.0001
epoch2: train: loss:1.4262230973612229 	 acc:0.72203125 | test: loss:1.4865080057274886 	 acc:0.7015576323987539 	 lr:0.0001
epoch3: train: loss:1.3752191095404287 	 acc:0.77078125 | test: loss:1.435863060149077 	 acc:0.7557632398753894 	 lr:0.0001
epoch4: train: loss:1.351297070922375 	 acc:0.80421875 | test: loss:1.4201409640713272 	 acc:0.7476635514018691 	 lr:0.0001
epoch5: train: loss:1.3379249971793277 	 acc:0.8259375 | test: loss:1.379654355361083 	 acc:0.8 	 lr:0.0001
epoch6: train: loss:1.3251117715232545 	 acc:0.84359375 | test: loss:1.3611825290127335 	 acc:0.811214953271028 	 lr:0.0001
epoch7: train: loss:1.299932503979435 	 acc:0.85515625 | test: loss:1.378970022884856 	 acc:0.7950155763239876 	 lr:0.0001
epoch8: train: loss:1.3092328014269552 	 acc:0.8696875 | test: loss:1.3656985884514925 	 acc:0.8124610591900312 	 lr:0.0001
epoch9: train: loss:1.3068898018592043 	 acc:0.85984375 | test: loss:1.3687217218482235 	 acc:0.7912772585669782 	 lr:0.0001
epoch10: train: loss:1.2985200284887906 	 acc:0.8671875 | test: loss:1.3453426800041555 	 acc:0.821183800623053 	 lr:0.0001
epoch11: train: loss:1.3030457233842885 	 acc:0.88859375 | test: loss:1.3443083958462392 	 acc:0.8299065420560747 	 lr:0.0001
epoch12: train: loss:1.2696381186992278 	 acc:0.87 | test: loss:1.3444944133639707 	 acc:0.8261682242990654 	 lr:0.0001
epoch13: train: loss:1.2820402885395321 	 acc:0.87203125 | test: loss:1.3490375066471991 	 acc:0.822429906542056 	 lr:0.0001
epoch14: train: loss:1.3161500762534457 	 acc:0.80015625 | test: loss:1.4318224964854873 	 acc:0.7433021806853582 	 lr:0.0001
epoch15: train: loss:1.2795205586036604 	 acc:0.85671875 | test: loss:1.3629006800993209 	 acc:0.8087227414330218 	 lr:0.0001
epoch16: train: loss:1.2791465360237972 	 acc:0.883125 | test: loss:1.3409805295623352 	 acc:0.8330218068535825 	 lr:0.0001
epoch17: train: loss:1.2635322874547168 	 acc:0.86828125 | test: loss:1.3643218562610424 	 acc:0.7987538940809968 	 lr:0.0001
epoch18: train: loss:1.2614359561583663 	 acc:0.87296875 | test: loss:1.3453808305419495 	 acc:0.815576323987539 	 lr:0.0001
epoch19: train: loss:1.2721275192606178 	 acc:0.9075 | test: loss:1.3275665623376673 	 acc:0.8454828660436137 	 lr:0.0001
epoch20: train: loss:1.2548415863727984 	 acc:0.90546875 | test: loss:1.338447149817446 	 acc:0.8348909657320872 	 lr:0.0001
epoch21: train: loss:1.2365757057017224 	 acc:0.926875 | test: loss:1.3092160331125944 	 acc:0.8610591900311526 	 lr:0.0001
epoch22: train: loss:1.2439602568222152 	 acc:0.90546875 | test: loss:1.3254058379621891 	 acc:0.8411214953271028 	 lr:0.0001
epoch23: train: loss:1.2495480444354996 	 acc:0.9003125 | test: loss:1.327774799055771 	 acc:0.8442367601246106 	 lr:0.0001
epoch24: train: loss:1.2342200202554767 	 acc:0.92453125 | test: loss:1.311082461912684 	 acc:0.8629283489096573 	 lr:0.0001
epoch25: train: loss:1.2463538069356521 	 acc:0.91515625 | test: loss:1.3302662924442707 	 acc:0.8373831775700935 	 lr:0.0001
epoch26: train: loss:1.257442496401737 	 acc:0.8896875 | test: loss:1.333314962030571 	 acc:0.8292834890965732 	 lr:0.0001
epoch27: train: loss:1.228143989509386 	 acc:0.92265625 | test: loss:1.3255862805702234 	 acc:0.8386292834890966 	 lr:0.0001
epoch28: train: loss:1.2168775524225763 	 acc:0.93609375 | test: loss:1.308522820546991 	 acc:0.859190031152648 	 lr:5e-05
epoch29: train: loss:1.2155964502219945 	 acc:0.94484375 | test: loss:1.2941749432376612 	 acc:0.8728971962616823 	 lr:5e-05
epoch30: train: loss:1.2158636636607447 	 acc:0.95390625 | test: loss:1.2930041879136986 	 acc:0.8697819314641745 	 lr:5e-05
epoch31: train: loss:1.2077965074549608 	 acc:0.94875 | test: loss:1.2960046649350556 	 acc:0.8691588785046729 	 lr:5e-05
epoch32: train: loss:1.2092090996795106 	 acc:0.94453125 | test: loss:1.3037408048118757 	 acc:0.8610591900311526 	 lr:5e-05
epoch33: train: loss:1.2051669213848129 	 acc:0.95890625 | test: loss:1.2905511304968242 	 acc:0.8753894080996885 	 lr:5e-05
epoch34: train: loss:1.2080950174919802 	 acc:0.95359375 | test: loss:1.2965985834412856 	 acc:0.8679127725856698 	 lr:5e-05
epoch35: train: loss:1.2046301407706077 	 acc:0.95171875 | test: loss:1.2887752151192162 	 acc:0.8778816199376948 	 lr:5e-05
epoch36: train: loss:1.202386951074295 	 acc:0.9565625 | test: loss:1.2846882935996367 	 acc:0.8841121495327103 	 lr:5e-05
epoch37: train: loss:1.2072557507409982 	 acc:0.9553125 | test: loss:1.290223986411763 	 acc:0.8760124610591901 	 lr:5e-05
epoch38: train: loss:1.2036418715275237 	 acc:0.95375 | test: loss:1.2945270433604161 	 acc:0.8741433021806854 	 lr:5e-05
epoch39: train: loss:1.2059000746334652 	 acc:0.96171875 | test: loss:1.2868111973610994 	 acc:0.8797507788161993 	 lr:5e-05
epoch40: train: loss:1.1993459041931962 	 acc:0.963125 | test: loss:1.289885173185592 	 acc:0.8766355140186916 	 lr:5e-05
epoch41: train: loss:1.1988847195031958 	 acc:0.9603125 | test: loss:1.2858303584042368 	 acc:0.8803738317757009 	 lr:5e-05
epoch42: train: loss:1.2064196215115144 	 acc:0.9571875 | test: loss:1.2892018684345614 	 acc:0.8735202492211838 	 lr:5e-05
epoch43: train: loss:1.197942996118145 	 acc:0.9646875 | test: loss:1.2848528523311438 	 acc:0.8791277258566979 	 lr:2.5e-05
epoch44: train: loss:1.2008648052706932 	 acc:0.96703125 | test: loss:1.2804846459088668 	 acc:0.8859813084112149 	 lr:2.5e-05
epoch45: train: loss:1.1914642596040081 	 acc:0.9715625 | test: loss:1.2807864076994662 	 acc:0.8872274143302181 	 lr:2.5e-05
epoch46: train: loss:1.1937491971566099 	 acc:0.96671875 | test: loss:1.2780890345202056 	 acc:0.8897196261682243 	 lr:2.5e-05
epoch47: train: loss:1.1948089075497963 	 acc:0.97203125 | test: loss:1.2815454884852948 	 acc:0.8841121495327103 	 lr:2.5e-05
epoch48: train: loss:1.1911386148897956 	 acc:0.9703125 | test: loss:1.2838234919996647 	 acc:0.8828660436137071 	 lr:2.5e-05
epoch49: train: loss:1.195565102641979 	 acc:0.9715625 | test: loss:1.2772322353915633 	 acc:0.8884735202492212 	 lr:2.5e-05
epoch50: train: loss:1.1938523399001635 	 acc:0.96921875 | test: loss:1.2784704995675251 	 acc:0.8834890965732087 	 lr:2.5e-05
epoch51: train: loss:1.1875165857438648 	 acc:0.97171875 | test: loss:1.2795923319932456 	 acc:0.8847352024922118 	 lr:2.5e-05
epoch52: train: loss:1.1928984047192135 	 acc:0.97390625 | test: loss:1.272452869073624 	 acc:0.8940809968847352 	 lr:2.5e-05
epoch53: train: loss:1.1927480157141943 	 acc:0.9696875 | test: loss:1.2786577229930605 	 acc:0.8841121495327103 	 lr:2.5e-05
epoch54: train: loss:1.1983447513386758 	 acc:0.96421875 | test: loss:1.2817077043271883 	 acc:0.8847352024922118 	 lr:2.5e-05
epoch55: train: loss:1.199422267225922 	 acc:0.97640625 | test: loss:1.2800113319979278 	 acc:0.8847352024922118 	 lr:2.5e-05
epoch56: train: loss:1.1928917818568257 	 acc:0.9684375 | test: loss:1.2809897867689994 	 acc:0.8834890965732087 	 lr:2.5e-05
epoch57: train: loss:1.1934085652383393 	 acc:0.97109375 | test: loss:1.2812129610424101 	 acc:0.8834890965732087 	 lr:2.5e-05
epoch58: train: loss:1.1928697810891455 	 acc:0.969375 | test: loss:1.2821776860970946 	 acc:0.8797507788161993 	 lr:2.5e-05
epoch59: train: loss:1.1941388763737437 	 acc:0.97078125 | test: loss:1.278904720555956 	 acc:0.8866043613707165 	 lr:1.25e-05
epoch60: train: loss:1.1976643999324563 	 acc:0.975 | test: loss:1.2789314117015709 	 acc:0.8841121495327103 	 lr:1.25e-05
epoch61: train: loss:1.1952414638450795 	 acc:0.97390625 | test: loss:1.2802173938335288 	 acc:0.8847352024922118 	 lr:1.25e-05
epoch62: train: loss:1.189960157526926 	 acc:0.97375 | test: loss:1.277001173251143 	 acc:0.8872274143302181 	 lr:1.25e-05
epoch63: train: loss:1.191761480803419 	 acc:0.974375 | test: loss:1.2755646900224538 	 acc:0.8897196261682243 	 lr:1.25e-05
epoch64: train: loss:1.1857592092092664 	 acc:0.97953125 | test: loss:1.2744167289258534 	 acc:0.8922118380062305 	 lr:1.25e-05
epoch65: train: loss:1.1875900780754476 	 acc:0.97765625 | test: loss:1.2748964992267693 	 acc:0.8947040498442368 	 lr:6.25e-06
epoch66: train: loss:1.1886811636836152 	 acc:0.9784375 | test: loss:1.2726508380468018 	 acc:0.897196261682243 	 lr:6.25e-06
epoch67: train: loss:1.1893893906699784 	 acc:0.97859375 | test: loss:1.273789593141027 	 acc:0.8947040498442368 	 lr:6.25e-06
epoch68: train: loss:1.1870286332546593 	 acc:0.98015625 | test: loss:1.2731856866789013 	 acc:0.8940809968847352 	 lr:6.25e-06
epoch69: train: loss:1.1912165812269773 	 acc:0.9759375 | test: loss:1.273919219465642 	 acc:0.8940809968847352 	 lr:6.25e-06
epoch70: train: loss:1.1881951837591787 	 acc:0.9790625 | test: loss:1.2748146116547867 	 acc:0.8934579439252337 	 lr:6.25e-06
epoch71: train: loss:1.1873330109571685 	 acc:0.97875 | test: loss:1.2739580704787068 	 acc:0.8909657320872274 	 lr:3.125e-06
epoch72: train: loss:1.1886631961914824 	 acc:0.97796875 | test: loss:1.275170283599806 	 acc:0.8897196261682243 	 lr:3.125e-06
epoch73: train: loss:1.187283029787062 	 acc:0.9759375 | test: loss:1.2743581818643017 	 acc:0.8903426791277259 	 lr:3.125e-06
epoch74: train: loss:1.1864438464174412 	 acc:0.97890625 | test: loss:1.2728008468574452 	 acc:0.8928348909657321 	 lr:3.125e-06
epoch75: train: loss:1.18808096558856 	 acc:0.97890625 | test: loss:1.2731977164188277 	 acc:0.8928348909657321 	 lr:3.125e-06
epoch76: train: loss:1.1879269649794473 	 acc:0.9775 | test: loss:1.2737211655976246 	 acc:0.8897196261682243 	 lr:3.125e-06
epoch77: train: loss:1.1824318021466078 	 acc:0.97875 | test: loss:1.2733642673938075 	 acc:0.891588785046729 	 lr:1.5625e-06
epoch78: train: loss:1.1872678067328686 	 acc:0.97734375 | test: loss:1.274074151359986 	 acc:0.8940809968847352 	 lr:1.5625e-06
epoch79: train: loss:1.1905555600025615 	 acc:0.9771875 | test: loss:1.2723287127842413 	 acc:0.8934579439252337 	 lr:1.5625e-06
epoch80: train: loss:1.1849253431881526 	 acc:0.976875 | test: loss:1.2731948628975223 	 acc:0.8959501557632399 	 lr:1.5625e-06
epoch81: train: loss:1.1842854650945611 	 acc:0.979375 | test: loss:1.273650585825198 	 acc:0.8934579439252337 	 lr:1.5625e-06
epoch82: train: loss:1.1855252268163605 	 acc:0.97859375 | test: loss:1.27212877021029 	 acc:0.897196261682243 	 lr:1.5625e-06
epoch83: train: loss:1.1855814258536728 	 acc:0.9771875 | test: loss:1.272636035595356 	 acc:0.8934579439252337 	 lr:1.5625e-06
epoch84: train: loss:1.1855747672564159 	 acc:0.9825 | test: loss:1.271786100916402 	 acc:0.8947040498442368 	 lr:1.5625e-06
epoch85: train: loss:1.1873247800256757 	 acc:0.981875 | test: loss:1.2717867760643409 	 acc:0.8940809968847352 	 lr:1.5625e-06
epoch86: train: loss:1.1858842216926473 	 acc:0.97828125 | test: loss:1.2721915672872668 	 acc:0.8965732087227415 	 lr:1.5625e-06
epoch87: train: loss:1.1846330724592604 	 acc:0.97765625 | test: loss:1.2740362977684472 	 acc:0.8934579439252337 	 lr:1.5625e-06
epoch88: train: loss:1.1857254111701674 	 acc:0.97578125 | test: loss:1.2724992908049966 	 acc:0.8922118380062305 	 lr:1.5625e-06
epoch89: train: loss:1.1864260385410568 	 acc:0.97890625 | test: loss:1.272786586455467 	 acc:0.8940809968847352 	 lr:1.5625e-06
epoch90: train: loss:1.1858593597531226 	 acc:0.97765625 | test: loss:1.2739165990152093 	 acc:0.8909657320872274 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_2_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_2_2/
0.conv1.weight
0.conv1.weight freeze
0.bn1.weight
0.bn1.weight freeze
0.bn1.bias
0.bn1.bias freeze
0.layer1.0.conv1.weight
0.layer1.0.conv1.weight freeze
0.layer1.0.bn1.weight
0.layer1.0.bn1.weight freeze
0.layer1.0.bn1.bias
0.layer1.0.bn1.bias freeze
0.layer1.0.conv2.weight
0.layer1.0.conv2.weight freeze
0.layer1.0.bn2.weight
0.layer1.0.bn2.weight freeze
0.layer1.0.bn2.bias
0.layer1.0.bn2.bias freeze
0.layer1.0.conv3.weight
0.layer1.0.conv3.weight freeze
0.layer1.0.bn3.weight
0.layer1.0.bn3.weight freeze
0.layer1.0.bn3.bias
0.layer1.0.bn3.bias freeze
0.layer1.0.downsample.0.weight
0.layer1.0.downsample.0.weight freeze
0.layer1.0.downsample.1.weight
0.layer1.0.downsample.1.weight freeze
0.layer1.0.downsample.1.bias
0.layer1.0.downsample.1.bias freeze
0.layer1.1.conv1.weight
0.layer1.1.conv1.weight freeze
0.layer1.1.bn1.weight
0.layer1.1.bn1.weight freeze
0.layer1.1.bn1.bias
0.layer1.1.bn1.bias freeze
0.layer1.1.conv2.weight
0.layer1.1.conv2.weight freeze
0.layer1.1.bn2.weight
0.layer1.1.bn2.weight freeze
0.layer1.1.bn2.bias
0.layer1.1.bn2.bias freeze
0.layer1.1.conv3.weight
0.layer1.1.conv3.weight freeze
0.layer1.1.bn3.weight
0.layer1.1.bn3.weight freeze
0.layer1.1.bn3.bias
0.layer1.1.bn3.bias freeze
0.layer1.2.conv1.weight
0.layer1.2.conv1.weight freeze
0.layer1.2.bn1.weight
0.layer1.2.bn1.weight freeze
0.layer1.2.bn1.bias
0.layer1.2.bn1.bias freeze
0.layer1.2.conv2.weight
0.layer1.2.conv2.weight freeze
0.layer1.2.bn2.weight
0.layer1.2.bn2.weight freeze
0.layer1.2.bn2.bias
0.layer1.2.bn2.bias freeze
0.layer1.2.conv3.weight
0.layer1.2.conv3.weight freeze
0.layer1.2.bn3.weight
0.layer1.2.bn3.weight freeze
0.layer1.2.bn3.bias
0.layer1.2.bn3.bias freeze
0.layer2.0.conv1.weight
0.layer2.0.bn1.weight
0.layer2.0.bn1.bias
0.layer2.0.conv2.weight
0.layer2.0.bn2.weight
0.layer2.0.bn2.bias
0.layer2.0.conv3.weight
0.layer2.0.bn3.weight
0.layer2.0.bn3.bias
0.layer2.0.downsample.0.weight
0.layer2.0.downsample.1.weight
0.layer2.0.downsample.1.bias
0.layer2.1.conv1.weight
0.layer2.1.bn1.weight
0.layer2.1.bn1.bias
0.layer2.1.conv2.weight
0.layer2.1.bn2.weight
0.layer2.1.bn2.bias
0.layer2.1.conv3.weight
0.layer2.1.bn3.weight
0.layer2.1.bn3.bias
0.layer2.2.conv1.weight
0.layer2.2.bn1.weight
0.layer2.2.bn1.bias
0.layer2.2.conv2.weight
0.layer2.2.bn2.weight
0.layer2.2.bn2.bias
0.layer2.2.conv3.weight
0.layer2.2.bn3.weight
0.layer2.2.bn3.bias
0.layer2.3.conv1.weight
0.layer2.3.bn1.weight
0.layer2.3.bn1.bias
0.layer2.3.conv2.weight
0.layer2.3.bn2.weight
0.layer2.3.bn2.bias
0.layer2.3.conv3.weight
0.layer2.3.bn3.weight
0.layer2.3.bn3.bias
0.layer3.0.conv1.weight
0.layer3.0.bn1.weight
0.layer3.0.bn1.bias
0.layer3.0.conv2.weight
0.layer3.0.bn2.weight
0.layer3.0.bn2.bias
0.layer3.0.conv3.weight
0.layer3.0.bn3.weight
0.layer3.0.bn3.bias
0.layer3.0.downsample.0.weight
0.layer3.0.downsample.1.weight
0.layer3.0.downsample.1.bias
0.layer3.1.conv1.weight
0.layer3.1.bn1.weight
0.layer3.1.bn1.bias
0.layer3.1.conv2.weight
0.layer3.1.bn2.weight
0.layer3.1.bn2.bias
0.layer3.1.conv3.weight
0.layer3.1.bn3.weight
0.layer3.1.bn3.bias
0.layer3.2.conv1.weight
0.layer3.2.bn1.weight
0.layer3.2.bn1.bias
0.layer3.2.conv2.weight
0.layer3.2.bn2.weight
0.layer3.2.bn2.bias
0.layer3.2.conv3.weight
0.layer3.2.bn3.weight
0.layer3.2.bn3.bias
0.layer3.3.conv1.weight
0.layer3.3.bn1.weight
0.layer3.3.bn1.bias
0.layer3.3.conv2.weight
0.layer3.3.bn2.weight
0.layer3.3.bn2.bias
0.layer3.3.conv3.weight
0.layer3.3.bn3.weight
0.layer3.3.bn3.bias
0.layer3.4.conv1.weight
0.layer3.4.bn1.weight
0.layer3.4.bn1.bias
0.layer3.4.conv2.weight
0.layer3.4.bn2.weight
0.layer3.4.bn2.bias
0.layer3.4.conv3.weight
0.layer3.4.bn3.weight
0.layer3.4.bn3.bias
0.layer3.5.conv1.weight
0.layer3.5.bn1.weight
0.layer3.5.bn1.bias
0.layer3.5.conv2.weight
0.layer3.5.bn2.weight
0.layer3.5.bn2.bias
0.layer3.5.conv3.weight
0.layer3.5.bn3.weight
0.layer3.5.bn3.bias
0.layer4.0.conv1.weight
0.layer4.0.bn1.weight
0.layer4.0.bn1.bias
0.layer4.0.conv2.weight
0.layer4.0.bn2.weight
0.layer4.0.bn2.bias
0.layer4.0.conv3.weight
0.layer4.0.bn3.weight
0.layer4.0.bn3.bias
0.layer4.0.downsample.0.weight
0.layer4.0.downsample.1.weight
0.layer4.0.downsample.1.bias
0.layer4.1.conv1.weight
0.layer4.1.bn1.weight
0.layer4.1.bn1.bias
0.layer4.1.conv2.weight
0.layer4.1.bn2.weight
0.layer4.1.bn2.bias
0.layer4.1.conv3.weight
0.layer4.1.bn3.weight
0.layer4.1.bn3.bias
0.layer4.2.conv1.weight
0.layer4.2.bn1.weight
0.layer4.2.bn1.bias
0.layer4.2.conv2.weight
0.layer4.2.bn2.weight
0.layer4.2.bn2.bias
0.layer4.2.conv3.weight
0.layer4.2.bn3.weight
0.layer4.2.bn3.bias
1.1.weight
1.1.bias
training with  freeze_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.5226388995299387 	 acc:0.67609375 | test: loss:1.5236340951325367 	 acc:0.6853582554517134 	 lr:0.0001
epoch1: train: loss:1.4494066296472483 	 acc:0.75609375 | test: loss:1.4433181732985831 	 acc:0.7545171339563863 	 lr:0.0001
epoch2: train: loss:1.3966411962814391 	 acc:0.7471875 | test: loss:1.4692253917919884 	 acc:0.7071651090342679 	 lr:0.0001
epoch3: train: loss:1.4010811060504185 	 acc:0.71921875 | test: loss:1.474579793864693 	 acc:0.7140186915887851 	 lr:0.0001
epoch4: train: loss:1.3370610042813231 	 acc:0.84546875 | test: loss:1.367999956838067 	 acc:0.8118380062305296 	 lr:0.0001
epoch5: train: loss:1.3403828914978837 	 acc:0.79953125 | test: loss:1.4312420091153677 	 acc:0.7445482866043613 	 lr:0.0001
epoch6: train: loss:1.328110267704674 	 acc:0.8478125 | test: loss:1.364855239992944 	 acc:0.811214953271028 	 lr:0.0001
epoch7: train: loss:1.3015627120268893 	 acc:0.8540625 | test: loss:1.382297978297201 	 acc:0.7962616822429907 	 lr:0.0001
epoch8: train: loss:1.2935850121098325 	 acc:0.87140625 | test: loss:1.3739537942446651 	 acc:0.8137071651090343 	 lr:0.0001
epoch9: train: loss:1.2888054567049667 	 acc:0.88484375 | test: loss:1.3423398472438348 	 acc:0.832398753894081 	 lr:0.0001
epoch10: train: loss:1.2855609712146578 	 acc:0.85875 | test: loss:1.3398640049580843 	 acc:0.8317757009345794 	 lr:0.0001
epoch11: train: loss:1.2870322472410776 	 acc:0.85265625 | test: loss:1.3851702994646684 	 acc:0.7875389408099689 	 lr:0.0001
epoch12: train: loss:1.2551003878233862 	 acc:0.88375 | test: loss:1.33097120638577 	 acc:0.838006230529595 	 lr:0.0001
epoch13: train: loss:1.3040448560275482 	 acc:0.8709375 | test: loss:1.3420252111470587 	 acc:0.8280373831775701 	 lr:0.0001
epoch14: train: loss:1.2885077018648456 	 acc:0.8571875 | test: loss:1.3585433561111164 	 acc:0.8137071651090343 	 lr:0.0001
epoch15: train: loss:1.2668106376892136 	 acc:0.90234375 | test: loss:1.3422599641704855 	 acc:0.8255451713395638 	 lr:0.0001
epoch16: train: loss:1.264832043256916 	 acc:0.85765625 | test: loss:1.3645247772103901 	 acc:0.7993769470404984 	 lr:0.0001
epoch17: train: loss:1.264918147614931 	 acc:0.91171875 | test: loss:1.316333254178365 	 acc:0.8492211838006231 	 lr:0.0001
epoch18: train: loss:1.2463731752346494 	 acc:0.88640625 | test: loss:1.341512104655352 	 acc:0.8249221183800624 	 lr:0.0001
epoch19: train: loss:1.2617935835803327 	 acc:0.88375 | test: loss:1.3553497284743645 	 acc:0.8118380062305296 	 lr:0.0001
epoch20: train: loss:1.3168390802626122 	 acc:0.81390625 | test: loss:1.38836872934181 	 acc:0.7781931464174455 	 lr:0.0001
epoch21: train: loss:1.2573137960798753 	 acc:0.83609375 | test: loss:1.3506048605820844 	 acc:0.8193146417445483 	 lr:0.0001
epoch22: train: loss:1.2404013283079625 	 acc:0.92375 | test: loss:1.3240747677574276 	 acc:0.8442367601246106 	 lr:0.0001
epoch23: train: loss:1.246782005959242 	 acc:0.91390625 | test: loss:1.3092581317432201 	 acc:0.8616822429906542 	 lr:0.0001
epoch24: train: loss:1.2432213979061464 	 acc:0.864375 | test: loss:1.3508273106126398 	 acc:0.815576323987539 	 lr:0.0001
epoch25: train: loss:1.2505542192302765 	 acc:0.91515625 | test: loss:1.3201471203212798 	 acc:0.8566978193146417 	 lr:0.0001
epoch26: train: loss:1.2447127211940745 	 acc:0.91046875 | test: loss:1.323963423607134 	 acc:0.8442367601246106 	 lr:0.0001
epoch27: train: loss:1.2325676970887611 	 acc:0.90359375 | test: loss:1.3248101559012107 	 acc:0.8373831775700935 	 lr:0.0001
epoch28: train: loss:1.2307676745614997 	 acc:0.92109375 | test: loss:1.3170555662886005 	 acc:0.8542056074766355 	 lr:0.0001
epoch29: train: loss:1.2269674737410654 	 acc:0.934375 | test: loss:1.30166068671277 	 acc:0.8697819314641745 	 lr:0.0001
epoch30: train: loss:1.2248116200645858 	 acc:0.91421875 | test: loss:1.3122345042748615 	 acc:0.854828660436137 	 lr:0.0001
epoch31: train: loss:1.2186542281687585 	 acc:0.93640625 | test: loss:1.2931953274943746 	 acc:0.8772585669781932 	 lr:0.0001
epoch32: train: loss:1.220637201313671 	 acc:0.9434375 | test: loss:1.3016550621139669 	 acc:0.8679127725856698 	 lr:0.0001
epoch33: train: loss:1.241909132517473 	 acc:0.91515625 | test: loss:1.321804160343895 	 acc:0.838006230529595 	 lr:0.0001
epoch34: train: loss:1.2258898053850447 	 acc:0.91453125 | test: loss:1.325215937489661 	 acc:0.8461059190031153 	 lr:0.0001
epoch35: train: loss:1.2279362484219483 	 acc:0.93125 | test: loss:1.3098474494393368 	 acc:0.854828660436137 	 lr:0.0001
epoch36: train: loss:1.2175803262679303 	 acc:0.93640625 | test: loss:1.3082987369406631 	 acc:0.864797507788162 	 lr:0.0001
epoch37: train: loss:1.2395617947366255 	 acc:0.9225 | test: loss:1.3169379413313584 	 acc:0.8473520249221184 	 lr:0.0001
epoch38: train: loss:1.2167501719085432 	 acc:0.955 | test: loss:1.2997998623833107 	 acc:0.870404984423676 	 lr:5e-05
epoch39: train: loss:1.2152559755371475 	 acc:0.95234375 | test: loss:1.2975187033507682 	 acc:0.8685358255451714 	 lr:5e-05
epoch40: train: loss:1.201273633612961 	 acc:0.9603125 | test: loss:1.2910848626466556 	 acc:0.8822429906542056 	 lr:5e-05
epoch41: train: loss:1.2046589454573453 	 acc:0.95640625 | test: loss:1.285605265938233 	 acc:0.8847352024922118 	 lr:5e-05
epoch42: train: loss:1.2043664603192392 	 acc:0.94859375 | test: loss:1.2943845691710618 	 acc:0.8741433021806854 	 lr:5e-05
epoch43: train: loss:1.2011836555951465 	 acc:0.96640625 | test: loss:1.285109383294887 	 acc:0.8828660436137071 	 lr:5e-05
epoch44: train: loss:1.2013798614668716 	 acc:0.9596875 | test: loss:1.2867916151741954 	 acc:0.8760124610591901 	 lr:5e-05
epoch45: train: loss:1.1974372000176863 	 acc:0.9596875 | test: loss:1.2924310485150583 	 acc:0.8766355140186916 	 lr:5e-05
epoch46: train: loss:1.2058511011866644 	 acc:0.966875 | test: loss:1.2806587567581937 	 acc:0.8859813084112149 	 lr:5e-05
epoch47: train: loss:1.204462039182188 	 acc:0.9625 | test: loss:1.2869235387843718 	 acc:0.8803738317757009 	 lr:5e-05
epoch48: train: loss:1.1975104493521602 	 acc:0.96234375 | test: loss:1.2851583968070437 	 acc:0.8841121495327103 	 lr:5e-05
epoch49: train: loss:1.2004557386959651 	 acc:0.9684375 | test: loss:1.2859091491342705 	 acc:0.8859813084112149 	 lr:5e-05
epoch50: train: loss:1.198573290119871 	 acc:0.9653125 | test: loss:1.282155165568319 	 acc:0.8853582554517134 	 lr:5e-05
epoch51: train: loss:1.1943515155205588 	 acc:0.96453125 | test: loss:1.2867856979370118 	 acc:0.8841121495327103 	 lr:5e-05
epoch52: train: loss:1.1983373161035995 	 acc:0.96265625 | test: loss:1.282995447191494 	 acc:0.881619937694704 	 lr:5e-05
epoch53: train: loss:1.1925789754898821 	 acc:0.9678125 | test: loss:1.282813787757422 	 acc:0.8834890965732087 	 lr:2.5e-05
epoch54: train: loss:1.199307987114864 	 acc:0.96453125 | test: loss:1.2825883260025785 	 acc:0.8828660436137071 	 lr:2.5e-05
epoch55: train: loss:1.1987841806404298 	 acc:0.975 | test: loss:1.2802572743543583 	 acc:0.8853582554517134 	 lr:2.5e-05
epoch56: train: loss:1.1959290670473812 	 acc:0.9753125 | test: loss:1.2808837380364677 	 acc:0.8847352024922118 	 lr:2.5e-05
epoch57: train: loss:1.193089140773657 	 acc:0.9725 | test: loss:1.2829915045949158 	 acc:0.8822429906542056 	 lr:2.5e-05
epoch58: train: loss:1.1896935539632734 	 acc:0.9725 | test: loss:1.2848394630111266 	 acc:0.8772585669781932 	 lr:2.5e-05
epoch59: train: loss:1.1908533339012795 	 acc:0.9709375 | test: loss:1.2769335307807566 	 acc:0.8878504672897196 	 lr:2.5e-05
epoch60: train: loss:1.1979734731967517 	 acc:0.97359375 | test: loss:1.2771377384476943 	 acc:0.8890965732087227 	 lr:2.5e-05
epoch61: train: loss:1.1964990667958078 	 acc:0.971875 | test: loss:1.2840320602012942 	 acc:0.8841121495327103 	 lr:2.5e-05
epoch62: train: loss:1.1888805761642516 	 acc:0.975 | test: loss:1.276802325768634 	 acc:0.8897196261682243 	 lr:2.5e-05
epoch63: train: loss:1.188892976163794 	 acc:0.97625 | test: loss:1.2736815385729352 	 acc:0.8928348909657321 	 lr:2.5e-05
epoch64: train: loss:1.1880197326993682 	 acc:0.9759375 | test: loss:1.2724134535804343 	 acc:0.8897196261682243 	 lr:2.5e-05
epoch65: train: loss:1.1934947029488985 	 acc:0.97421875 | test: loss:1.274829274991591 	 acc:0.8928348909657321 	 lr:2.5e-05
epoch66: train: loss:1.1906721938503244 	 acc:0.976875 | test: loss:1.2748641501334597 	 acc:0.8884735202492212 	 lr:2.5e-05
epoch67: train: loss:1.1902011582481034 	 acc:0.9759375 | test: loss:1.2773028983505343 	 acc:0.8897196261682243 	 lr:2.5e-05
epoch68: train: loss:1.1903287484067013 	 acc:0.97875 | test: loss:1.2754014314520767 	 acc:0.8884735202492212 	 lr:2.5e-05
epoch69: train: loss:1.1952364966982887 	 acc:0.9696875 | test: loss:1.2773796288766595 	 acc:0.8878504672897196 	 lr:2.5e-05
epoch70: train: loss:1.1901052029778676 	 acc:0.97578125 | test: loss:1.274998033380954 	 acc:0.8878504672897196 	 lr:2.5e-05
epoch71: train: loss:1.186059899073295 	 acc:0.97640625 | test: loss:1.2742131354281465 	 acc:0.8903426791277259 	 lr:1.25e-05
epoch72: train: loss:1.1880157851875062 	 acc:0.9753125 | test: loss:1.2741926326929967 	 acc:0.8940809968847352 	 lr:1.25e-05
epoch73: train: loss:1.188500178483759 	 acc:0.978125 | test: loss:1.2776061535624328 	 acc:0.8878504672897196 	 lr:1.25e-05
epoch74: train: loss:1.1877090370720202 	 acc:0.97671875 | test: loss:1.2740249502324612 	 acc:0.897196261682243 	 lr:1.25e-05
epoch75: train: loss:1.1897397734428363 	 acc:0.978125 | test: loss:1.274477411653394 	 acc:0.8928348909657321 	 lr:1.25e-05
epoch76: train: loss:1.1883243057524944 	 acc:0.975625 | test: loss:1.2761151346462167 	 acc:0.8878504672897196 	 lr:1.25e-05
epoch77: train: loss:1.1826656749525821 	 acc:0.98078125 | test: loss:1.274807719278187 	 acc:0.8884735202492212 	 lr:6.25e-06
epoch78: train: loss:1.1859622571172423 	 acc:0.978125 | test: loss:1.2750643459807305 	 acc:0.8897196261682243 	 lr:6.25e-06
epoch79: train: loss:1.1906156064941025 	 acc:0.979375 | test: loss:1.2720425052434856 	 acc:0.8928348909657321 	 lr:6.25e-06
epoch80: train: loss:1.1841168393947294 	 acc:0.9759375 | test: loss:1.2738056247479448 	 acc:0.8940809968847352 	 lr:6.25e-06
epoch81: train: loss:1.1832764053046936 	 acc:0.98015625 | test: loss:1.2729398297372265 	 acc:0.8940809968847352 	 lr:6.25e-06
epoch82: train: loss:1.1852468235989644 	 acc:0.97953125 | test: loss:1.272028290816928 	 acc:0.8947040498442368 	 lr:6.25e-06
epoch83: train: loss:1.1843562415761002 	 acc:0.9784375 | test: loss:1.2708887877865371 	 acc:0.8940809968847352 	 lr:6.25e-06
epoch84: train: loss:1.1860562755575783 	 acc:0.9828125 | test: loss:1.2708693621685943 	 acc:0.8990654205607477 	 lr:6.25e-06
epoch85: train: loss:1.1864697780207114 	 acc:0.9825 | test: loss:1.26944472641217 	 acc:0.8978193146417446 	 lr:6.25e-06
epoch86: train: loss:1.1869566489345482 	 acc:0.98 | test: loss:1.269681626391188 	 acc:0.8965732087227415 	 lr:6.25e-06
epoch87: train: loss:1.1857602907846347 	 acc:0.97953125 | test: loss:1.272042556492339 	 acc:0.8928348909657321 	 lr:6.25e-06
epoch88: train: loss:1.1836341851954344 	 acc:0.980625 | test: loss:1.270356903120736 	 acc:0.8953271028037383 	 lr:6.25e-06
epoch89: train: loss:1.1843385766391918 	 acc:0.981875 | test: loss:1.2715057370075926 	 acc:0.8940809968847352 	 lr:6.25e-06
epoch90: train: loss:1.1835817835835345 	 acc:0.98109375 | test: loss:1.2724495794171484 	 acc:0.8947040498442368 	 lr:6.25e-06
epoch91: train: loss:1.187984810203058 	 acc:0.97859375 | test: loss:1.2726995483737125 	 acc:0.8934579439252337 	 lr:6.25e-06
epoch92: train: loss:1.1855216175945917 	 acc:0.97890625 | test: loss:1.2716938702116873 	 acc:0.8953271028037383 	 lr:3.125e-06
epoch93: train: loss:1.1866356854881746 	 acc:0.98328125 | test: loss:1.271164490129346 	 acc:0.897196261682243 	 lr:3.125e-06
epoch94: train: loss:1.1817694253794948 	 acc:0.9875 | test: loss:1.2699267224730733 	 acc:0.8990654205607477 	 lr:3.125e-06
epoch95: train: loss:1.186808201524078 	 acc:0.9821875 | test: loss:1.2699218392000762 	 acc:0.897196261682243 	 lr:3.125e-06
epoch96: train: loss:1.187308372956156 	 acc:0.97890625 | test: loss:1.2689801497622815 	 acc:0.8978193146417446 	 lr:3.125e-06
epoch97: train: loss:1.185591426852343 	 acc:0.980625 | test: loss:1.2700836715668533 	 acc:0.897196261682243 	 lr:3.125e-06
epoch98: train: loss:1.184217885692635 	 acc:0.98234375 | test: loss:1.269420847788778 	 acc:0.8953271028037383 	 lr:3.125e-06
epoch99: train: loss:1.180644115296125 	 acc:0.98546875 | test: loss:1.2687903832795093 	 acc:0.897196261682243 	 lr:3.125e-06
epoch100: train: loss:1.185583722414587 	 acc:0.97984375 | test: loss:1.2678724632084926 	 acc:0.9003115264797508 	 lr:3.125e-06
epoch101: train: loss:1.1846236598947661 	 acc:0.98265625 | test: loss:1.2687531373211156 	 acc:0.8990654205607477 	 lr:3.125e-06
epoch102: train: loss:1.183990228371542 	 acc:0.98078125 | test: loss:1.269164610800342 	 acc:0.9003115264797508 	 lr:3.125e-06
epoch103: train: loss:1.182893429948984 	 acc:0.98296875 | test: loss:1.2694712034267057 	 acc:0.8959501557632399 	 lr:3.125e-06
epoch104: train: loss:1.1825552875599201 	 acc:0.98203125 | test: loss:1.2697464050904983 	 acc:0.897196261682243 	 lr:3.125e-06
epoch105: train: loss:1.1805832135500525 	 acc:0.98390625 | test: loss:1.269782743201449 	 acc:0.8965732087227415 	 lr:3.125e-06
epoch106: train: loss:1.1831983579684755 	 acc:0.98296875 | test: loss:1.2706690737762927 	 acc:0.8940809968847352 	 lr:3.125e-06
epoch107: train: loss:1.180422470962322 	 acc:0.98203125 | test: loss:1.269763094465309 	 acc:0.8984423676012461 	 lr:1.5625e-06
epoch108: train: loss:1.1814894260046167 	 acc:0.985 | test: loss:1.2687377245626716 	 acc:0.8984423676012461 	 lr:1.5625e-06
epoch109: train: loss:1.1895671883195196 	 acc:0.98375 | test: loss:1.2690712659901175 	 acc:0.897196261682243 	 lr:1.5625e-06
epoch110: train: loss:1.1829684612622584 	 acc:0.98171875 | test: loss:1.2703227369956138 	 acc:0.8953271028037383 	 lr:1.5625e-06
epoch111: train: loss:1.1780675359483253 	 acc:0.98375 | test: loss:1.2685170113483322 	 acc:0.897196261682243 	 lr:1.5625e-06
epoch112: train: loss:1.185809373818367 	 acc:0.98046875 | test: loss:1.2703797436948878 	 acc:0.8978193146417446 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_3_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_3_2/
0.conv1.weight
0.conv1.weight freeze
0.bn1.weight
0.bn1.weight freeze
0.bn1.bias
0.bn1.bias freeze
0.layer1.0.conv1.weight
0.layer1.0.conv1.weight freeze
0.layer1.0.bn1.weight
0.layer1.0.bn1.weight freeze
0.layer1.0.bn1.bias
0.layer1.0.bn1.bias freeze
0.layer1.0.conv2.weight
0.layer1.0.conv2.weight freeze
0.layer1.0.bn2.weight
0.layer1.0.bn2.weight freeze
0.layer1.0.bn2.bias
0.layer1.0.bn2.bias freeze
0.layer1.0.conv3.weight
0.layer1.0.conv3.weight freeze
0.layer1.0.bn3.weight
0.layer1.0.bn3.weight freeze
0.layer1.0.bn3.bias
0.layer1.0.bn3.bias freeze
0.layer1.0.downsample.0.weight
0.layer1.0.downsample.0.weight freeze
0.layer1.0.downsample.1.weight
0.layer1.0.downsample.1.weight freeze
0.layer1.0.downsample.1.bias
0.layer1.0.downsample.1.bias freeze
0.layer1.1.conv1.weight
0.layer1.1.conv1.weight freeze
0.layer1.1.bn1.weight
0.layer1.1.bn1.weight freeze
0.layer1.1.bn1.bias
0.layer1.1.bn1.bias freeze
0.layer1.1.conv2.weight
0.layer1.1.conv2.weight freeze
0.layer1.1.bn2.weight
0.layer1.1.bn2.weight freeze
0.layer1.1.bn2.bias
0.layer1.1.bn2.bias freeze
0.layer1.1.conv3.weight
0.layer1.1.conv3.weight freeze
0.layer1.1.bn3.weight
0.layer1.1.bn3.weight freeze
0.layer1.1.bn3.bias
0.layer1.1.bn3.bias freeze
0.layer1.2.conv1.weight
0.layer1.2.conv1.weight freeze
0.layer1.2.bn1.weight
0.layer1.2.bn1.weight freeze
0.layer1.2.bn1.bias
0.layer1.2.bn1.bias freeze
0.layer1.2.conv2.weight
0.layer1.2.conv2.weight freeze
0.layer1.2.bn2.weight
0.layer1.2.bn2.weight freeze
0.layer1.2.bn2.bias
0.layer1.2.bn2.bias freeze
0.layer1.2.conv3.weight
0.layer1.2.conv3.weight freeze
0.layer1.2.bn3.weight
0.layer1.2.bn3.weight freeze
0.layer1.2.bn3.bias
0.layer1.2.bn3.bias freeze
0.layer2.0.conv1.weight
0.layer2.0.conv1.weight freeze
0.layer2.0.bn1.weight
0.layer2.0.bn1.weight freeze
0.layer2.0.bn1.bias
0.layer2.0.bn1.bias freeze
0.layer2.0.conv2.weight
0.layer2.0.conv2.weight freeze
0.layer2.0.bn2.weight
0.layer2.0.bn2.weight freeze
0.layer2.0.bn2.bias
0.layer2.0.bn2.bias freeze
0.layer2.0.conv3.weight
0.layer2.0.conv3.weight freeze
0.layer2.0.bn3.weight
0.layer2.0.bn3.weight freeze
0.layer2.0.bn3.bias
0.layer2.0.bn3.bias freeze
0.layer2.0.downsample.0.weight
0.layer2.0.downsample.0.weight freeze
0.layer2.0.downsample.1.weight
0.layer2.0.downsample.1.weight freeze
0.layer2.0.downsample.1.bias
0.layer2.0.downsample.1.bias freeze
0.layer2.1.conv1.weight
0.layer2.1.conv1.weight freeze
0.layer2.1.bn1.weight
0.layer2.1.bn1.weight freeze
0.layer2.1.bn1.bias
0.layer2.1.bn1.bias freeze
0.layer2.1.conv2.weight
0.layer2.1.conv2.weight freeze
0.layer2.1.bn2.weight
0.layer2.1.bn2.weight freeze
0.layer2.1.bn2.bias
0.layer2.1.bn2.bias freeze
0.layer2.1.conv3.weight
0.layer2.1.conv3.weight freeze
0.layer2.1.bn3.weight
0.layer2.1.bn3.weight freeze
0.layer2.1.bn3.bias
0.layer2.1.bn3.bias freeze
0.layer2.2.conv1.weight
0.layer2.2.conv1.weight freeze
0.layer2.2.bn1.weight
0.layer2.2.bn1.weight freeze
0.layer2.2.bn1.bias
0.layer2.2.bn1.bias freeze
0.layer2.2.conv2.weight
0.layer2.2.conv2.weight freeze
0.layer2.2.bn2.weight
0.layer2.2.bn2.weight freeze
0.layer2.2.bn2.bias
0.layer2.2.bn2.bias freeze
0.layer2.2.conv3.weight
0.layer2.2.conv3.weight freeze
0.layer2.2.bn3.weight
0.layer2.2.bn3.weight freeze
0.layer2.2.bn3.bias
0.layer2.2.bn3.bias freeze
0.layer2.3.conv1.weight
0.layer2.3.conv1.weight freeze
0.layer2.3.bn1.weight
0.layer2.3.bn1.weight freeze
0.layer2.3.bn1.bias
0.layer2.3.bn1.bias freeze
0.layer2.3.conv2.weight
0.layer2.3.conv2.weight freeze
0.layer2.3.bn2.weight
0.layer2.3.bn2.weight freeze
0.layer2.3.bn2.bias
0.layer2.3.bn2.bias freeze
0.layer2.3.conv3.weight
0.layer2.3.conv3.weight freeze
0.layer2.3.bn3.weight
0.layer2.3.bn3.weight freeze
0.layer2.3.bn3.bias
0.layer2.3.bn3.bias freeze
0.layer3.0.conv1.weight
0.layer3.0.bn1.weight
0.layer3.0.bn1.bias
0.layer3.0.conv2.weight
0.layer3.0.bn2.weight
0.layer3.0.bn2.bias
0.layer3.0.conv3.weight
0.layer3.0.bn3.weight
0.layer3.0.bn3.bias
0.layer3.0.downsample.0.weight
0.layer3.0.downsample.1.weight
0.layer3.0.downsample.1.bias
0.layer3.1.conv1.weight
0.layer3.1.bn1.weight
0.layer3.1.bn1.bias
0.layer3.1.conv2.weight
0.layer3.1.bn2.weight
0.layer3.1.bn2.bias
0.layer3.1.conv3.weight
0.layer3.1.bn3.weight
0.layer3.1.bn3.bias
0.layer3.2.conv1.weight
0.layer3.2.bn1.weight
0.layer3.2.bn1.bias
0.layer3.2.conv2.weight
0.layer3.2.bn2.weight
0.layer3.2.bn2.bias
0.layer3.2.conv3.weight
0.layer3.2.bn3.weight
0.layer3.2.bn3.bias
0.layer3.3.conv1.weight
0.layer3.3.bn1.weight
0.layer3.3.bn1.bias
0.layer3.3.conv2.weight
0.layer3.3.bn2.weight
0.layer3.3.bn2.bias
0.layer3.3.conv3.weight
0.layer3.3.bn3.weight
0.layer3.3.bn3.bias
0.layer3.4.conv1.weight
0.layer3.4.bn1.weight
0.layer3.4.bn1.bias
0.layer3.4.conv2.weight
0.layer3.4.bn2.weight
0.layer3.4.bn2.bias
0.layer3.4.conv3.weight
0.layer3.4.bn3.weight
0.layer3.4.bn3.bias
0.layer3.5.conv1.weight
0.layer3.5.bn1.weight
0.layer3.5.bn1.bias
0.layer3.5.conv2.weight
0.layer3.5.bn2.weight
0.layer3.5.bn2.bias
0.layer3.5.conv3.weight
0.layer3.5.bn3.weight
0.layer3.5.bn3.bias
0.layer4.0.conv1.weight
0.layer4.0.bn1.weight
0.layer4.0.bn1.bias
0.layer4.0.conv2.weight
0.layer4.0.bn2.weight
0.layer4.0.bn2.bias
0.layer4.0.conv3.weight
0.layer4.0.bn3.weight
0.layer4.0.bn3.bias
0.layer4.0.downsample.0.weight
0.layer4.0.downsample.1.weight
0.layer4.0.downsample.1.bias
0.layer4.1.conv1.weight
0.layer4.1.bn1.weight
0.layer4.1.bn1.bias
0.layer4.1.conv2.weight
0.layer4.1.bn2.weight
0.layer4.1.bn2.bias
0.layer4.1.conv3.weight
0.layer4.1.bn3.weight
0.layer4.1.bn3.bias
0.layer4.2.conv1.weight
0.layer4.2.bn1.weight
0.layer4.2.bn1.bias
0.layer4.2.conv2.weight
0.layer4.2.bn2.weight
0.layer4.2.bn2.bias
0.layer4.2.conv3.weight
0.layer4.2.bn3.weight
0.layer4.2.bn3.bias
1.1.weight
1.1.bias
training with  freeze_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.5137177585717945 	 acc:0.68578125 | test: loss:1.5119147440354772 	 acc:0.6953271028037383 	 lr:0.0001
epoch1: train: loss:1.4462677430771553 	 acc:0.7496875 | test: loss:1.4489648045780503 	 acc:0.7389408099688474 	 lr:0.0001
epoch2: train: loss:1.3964048720634514 	 acc:0.78171875 | test: loss:1.4350225459749453 	 acc:0.7576323987538941 	 lr:0.0001
epoch3: train: loss:1.3892295152484022 	 acc:0.755 | test: loss:1.4421373103637933 	 acc:0.7289719626168224 	 lr:0.0001
epoch4: train: loss:1.340389380950094 	 acc:0.80890625 | test: loss:1.4024045107891998 	 acc:0.7869158878504673 	 lr:0.0001
epoch5: train: loss:1.3376739621069356 	 acc:0.81328125 | test: loss:1.4088710672016085 	 acc:0.7588785046728972 	 lr:0.0001
epoch6: train: loss:1.3409580918609119 	 acc:0.83171875 | test: loss:1.3618961473120337 	 acc:0.8099688473520249 	 lr:0.0001
epoch7: train: loss:1.3550281819470873 	 acc:0.859375 | test: loss:1.3924562339842135 	 acc:0.7900311526479751 	 lr:0.0001
epoch8: train: loss:1.3127260468696635 	 acc:0.8721875 | test: loss:1.3601666284870135 	 acc:0.8261682242990654 	 lr:0.0001
epoch9: train: loss:1.2950244960144663 	 acc:0.8534375 | test: loss:1.3816958299678435 	 acc:0.7925233644859813 	 lr:0.0001
epoch10: train: loss:1.274986435043728 	 acc:0.8521875 | test: loss:1.3682066999120504 	 acc:0.7987538940809968 	 lr:0.0001
epoch11: train: loss:1.2851003154007183 	 acc:0.84984375 | test: loss:1.3812687662160286 	 acc:0.794392523364486 	 lr:0.0001
epoch12: train: loss:1.2643651552818234 	 acc:0.8728125 | test: loss:1.3372916847003211 	 acc:0.8255451713395638 	 lr:0.0001
epoch13: train: loss:1.2714025564439402 	 acc:0.89984375 | test: loss:1.3336173228385664 	 acc:0.8311526479750779 	 lr:0.0001
epoch14: train: loss:1.262361839467152 	 acc:0.88375 | test: loss:1.3338557221065057 	 acc:0.832398753894081 	 lr:0.0001
epoch15: train: loss:1.2526161997882208 	 acc:0.903125 | test: loss:1.3338089218763547 	 acc:0.8274143302180685 	 lr:0.0001
epoch16: train: loss:1.2695500536880673 	 acc:0.88359375 | test: loss:1.3329128817234455 	 acc:0.8267912772585669 	 lr:0.0001
epoch17: train: loss:1.2648934190855838 	 acc:0.85734375 | test: loss:1.3683646237738778 	 acc:0.7993769470404984 	 lr:0.0001
epoch18: train: loss:1.2586137067331735 	 acc:0.8921875 | test: loss:1.3366999125554926 	 acc:0.8249221183800624 	 lr:0.0001
epoch19: train: loss:1.2511817568824404 	 acc:0.9053125 | test: loss:1.3420036872970724 	 acc:0.8255451713395638 	 lr:0.0001
epoch20: train: loss:1.2409576487485363 	 acc:0.8884375 | test: loss:1.342433585407578 	 acc:0.8236760124610591 	 lr:0.0001
epoch21: train: loss:1.2319198906189013 	 acc:0.9290625 | test: loss:1.3091087526992846 	 acc:0.8517133956386292 	 lr:0.0001
epoch22: train: loss:1.2303485402830125 	 acc:0.9203125 | test: loss:1.319793423982424 	 acc:0.8492211838006231 	 lr:0.0001
epoch23: train: loss:1.2412425056832737 	 acc:0.89578125 | test: loss:1.3236657873492375 	 acc:0.8348909657320872 	 lr:0.0001
epoch24: train: loss:1.2480024494108606 	 acc:0.8965625 | test: loss:1.3368316802651712 	 acc:0.8348909657320872 	 lr:0.0001
epoch25: train: loss:1.2565570506707102 	 acc:0.874375 | test: loss:1.3453953157704195 	 acc:0.8180685358255452 	 lr:0.0001
epoch26: train: loss:1.2499407253816293 	 acc:0.8759375 | test: loss:1.365914761537332 	 acc:0.8 	 lr:0.0001
epoch27: train: loss:1.2390379453803486 	 acc:0.93515625 | test: loss:1.321810581453864 	 acc:0.8429906542056075 	 lr:0.0001
epoch28: train: loss:1.2139127994867902 	 acc:0.94078125 | test: loss:1.2995723973182132 	 acc:0.8616822429906542 	 lr:5e-05
epoch29: train: loss:1.2162281593245328 	 acc:0.94640625 | test: loss:1.2950342885430357 	 acc:0.8679127725856698 	 lr:5e-05
epoch30: train: loss:1.2111005748835137 	 acc:0.95453125 | test: loss:1.298266540509518 	 acc:0.8760124610591901 	 lr:5e-05
epoch31: train: loss:1.2111727307309963 	 acc:0.93921875 | test: loss:1.3103025281169334 	 acc:0.8573208722741433 	 lr:5e-05
epoch32: train: loss:1.211538496173796 	 acc:0.95671875 | test: loss:1.3051506983528256 	 acc:0.864797507788162 	 lr:5e-05
epoch33: train: loss:1.2095288529049875 	 acc:0.95984375 | test: loss:1.2988741054713169 	 acc:0.8728971962616823 	 lr:5e-05
epoch34: train: loss:1.2126460102924046 	 acc:0.9575 | test: loss:1.2969390860970518 	 acc:0.8691588785046729 	 lr:5e-05
epoch35: train: loss:1.2054359477725838 	 acc:0.9575 | test: loss:1.296205089396777 	 acc:0.870404984423676 	 lr:5e-05
epoch36: train: loss:1.2000155256094176 	 acc:0.95796875 | test: loss:1.2960054509736296 	 acc:0.8697819314641745 	 lr:2.5e-05
epoch37: train: loss:1.2045558408309853 	 acc:0.96265625 | test: loss:1.297882841606378 	 acc:0.8716510903426792 	 lr:2.5e-05
epoch38: train: loss:1.2002094563611498 	 acc:0.96765625 | test: loss:1.2978132989919073 	 acc:0.8716510903426792 	 lr:2.5e-05
epoch39: train: loss:1.2035899177926486 	 acc:0.96140625 | test: loss:1.2940077959191392 	 acc:0.8691588785046729 	 lr:2.5e-05
epoch40: train: loss:1.1958371307587456 	 acc:0.96640625 | test: loss:1.2932725818729103 	 acc:0.8722741433021807 	 lr:2.5e-05
epoch41: train: loss:1.1980376458745 	 acc:0.965 | test: loss:1.2951303135197483 	 acc:0.8741433021806854 	 lr:2.5e-05
epoch42: train: loss:1.2009729556605557 	 acc:0.96390625 | test: loss:1.2955586001137707 	 acc:0.8728971962616823 	 lr:2.5e-05
epoch43: train: loss:1.196955660746304 	 acc:0.96609375 | test: loss:1.2930356836764612 	 acc:0.8753894080996885 	 lr:2.5e-05
epoch44: train: loss:1.1965386538836846 	 acc:0.96421875 | test: loss:1.2938443688217354 	 acc:0.8722741433021807 	 lr:2.5e-05
epoch45: train: loss:1.1928412646636843 	 acc:0.97171875 | test: loss:1.2942586315012423 	 acc:0.874766355140187 	 lr:2.5e-05
epoch46: train: loss:1.196149682216957 	 acc:0.96484375 | test: loss:1.2926836684485463 	 acc:0.8728971962616823 	 lr:2.5e-05
epoch47: train: loss:1.195917307893156 	 acc:0.97109375 | test: loss:1.2896001147332592 	 acc:0.8753894080996885 	 lr:2.5e-05
epoch48: train: loss:1.1916389959664386 	 acc:0.968125 | test: loss:1.2891912021369578 	 acc:0.8785046728971962 	 lr:2.5e-05
epoch49: train: loss:1.196901975061445 	 acc:0.97171875 | test: loss:1.2876200485823681 	 acc:0.874766355140187 	 lr:2.5e-05
epoch50: train: loss:1.1929750107490487 	 acc:0.96375 | test: loss:1.288891138837345 	 acc:0.8760124610591901 	 lr:2.5e-05
epoch51: train: loss:1.1907933684087748 	 acc:0.9675 | test: loss:1.287378755165409 	 acc:0.8822429906542056 	 lr:2.5e-05
epoch52: train: loss:1.1929690192026798 	 acc:0.9675 | test: loss:1.2895775474120523 	 acc:0.8710280373831776 	 lr:2.5e-05
epoch53: train: loss:1.1920985130292192 	 acc:0.97109375 | test: loss:1.2832391504186706 	 acc:0.8753894080996885 	 lr:2.5e-05
epoch54: train: loss:1.195677031398657 	 acc:0.97390625 | test: loss:1.2866708680476726 	 acc:0.8760124610591901 	 lr:2.5e-05
epoch55: train: loss:1.2010888605169912 	 acc:0.975625 | test: loss:1.286541256800619 	 acc:0.8778816199376948 	 lr:2.5e-05
epoch56: train: loss:1.195901062821709 	 acc:0.97203125 | test: loss:1.2900980069258503 	 acc:0.8679127725856698 	 lr:2.5e-05
epoch57: train: loss:1.1936861441714237 	 acc:0.97328125 | test: loss:1.2886971348171294 	 acc:0.8716510903426792 	 lr:2.5e-05
epoch58: train: loss:1.1916830522952648 	 acc:0.9721875 | test: loss:1.284632540863251 	 acc:0.8766355140186916 	 lr:2.5e-05
epoch59: train: loss:1.1919533198741523 	 acc:0.97515625 | test: loss:1.2831516902394755 	 acc:0.8797507788161993 	 lr:2.5e-05
epoch60: train: loss:1.2011599848179217 	 acc:0.973125 | test: loss:1.2833677194571569 	 acc:0.8760124610591901 	 lr:1.25e-05
epoch61: train: loss:1.1966947056742778 	 acc:0.9715625 | test: loss:1.2848835564848047 	 acc:0.8772585669781932 	 lr:1.25e-05
epoch62: train: loss:1.1899450655005854 	 acc:0.9725 | test: loss:1.2809088059303546 	 acc:0.8791277258566979 	 lr:1.25e-05
epoch63: train: loss:1.1926467145075563 	 acc:0.97078125 | test: loss:1.280527206447637 	 acc:0.8834890965732087 	 lr:1.25e-05
epoch64: train: loss:1.1893529053687304 	 acc:0.97640625 | test: loss:1.2815015775020993 	 acc:0.8772585669781932 	 lr:1.25e-05
epoch65: train: loss:1.1896586243199894 	 acc:0.976875 | test: loss:1.2836037929927078 	 acc:0.8797507788161993 	 lr:1.25e-05
epoch66: train: loss:1.1918758300018906 	 acc:0.97578125 | test: loss:1.2824820131527672 	 acc:0.8797507788161993 	 lr:1.25e-05
epoch67: train: loss:1.1914573651566158 	 acc:0.97703125 | test: loss:1.2831059890372731 	 acc:0.8809968847352025 	 lr:1.25e-05
epoch68: train: loss:1.1884977175424474 	 acc:0.975 | test: loss:1.2829142372927562 	 acc:0.8760124610591901 	 lr:1.25e-05
epoch69: train: loss:1.1932393630159543 	 acc:0.97515625 | test: loss:1.2846465942644256 	 acc:0.8772585669781932 	 lr:1.25e-05
epoch70: train: loss:1.1867432021796935 	 acc:0.980625 | test: loss:1.2824840316148562 	 acc:0.8772585669781932 	 lr:6.25e-06
epoch71: train: loss:1.1879620470915058 	 acc:0.97453125 | test: loss:1.2821355653329058 	 acc:0.8778816199376948 	 lr:6.25e-06
epoch72: train: loss:1.1893648408149762 	 acc:0.97515625 | test: loss:1.2811276590341347 	 acc:0.881619937694704 	 lr:6.25e-06
epoch73: train: loss:1.1926511670722335 	 acc:0.97359375 | test: loss:1.2820634740163976 	 acc:0.8785046728971962 	 lr:6.25e-06
epoch74: train: loss:1.18705983217762 	 acc:0.97640625 | test: loss:1.2833279153639654 	 acc:0.8791277258566979 	 lr:6.25e-06
epoch75: train: loss:1.1910050069792582 	 acc:0.976875 | test: loss:1.2811736191544578 	 acc:0.8803738317757009 	 lr:6.25e-06
epoch76: train: loss:1.1905299404838139 	 acc:0.975625 | test: loss:1.2814479261172524 	 acc:0.8797507788161993 	 lr:3.125e-06
epoch77: train: loss:1.183438191350599 	 acc:0.9778125 | test: loss:1.2811531817058908 	 acc:0.8809968847352025 	 lr:3.125e-06
epoch78: train: loss:1.1868534966616962 	 acc:0.97765625 | test: loss:1.2812694270291431 	 acc:0.8797507788161993 	 lr:3.125e-06
epoch79: train: loss:1.1915581398695172 	 acc:0.97703125 | test: loss:1.279213150936495 	 acc:0.881619937694704 	 lr:3.125e-06
epoch80: train: loss:1.1867242150526323 	 acc:0.9734375 | test: loss:1.2792874870270583 	 acc:0.8772585669781932 	 lr:3.125e-06
epoch81: train: loss:1.1858351029240461 	 acc:0.97765625 | test: loss:1.2805440042620508 	 acc:0.8797507788161993 	 lr:3.125e-06
epoch82: train: loss:1.1859940373274054 	 acc:0.97640625 | test: loss:1.2813734423705723 	 acc:0.874766355140187 	 lr:3.125e-06
epoch83: train: loss:1.1862389402962774 	 acc:0.97625 | test: loss:1.2795827594501579 	 acc:0.8791277258566979 	 lr:3.125e-06
epoch84: train: loss:1.187066912446331 	 acc:0.98234375 | test: loss:1.2791542459499798 	 acc:0.8841121495327103 	 lr:3.125e-06
epoch85: train: loss:1.1897650689654384 	 acc:0.9778125 | test: loss:1.279659526370396 	 acc:0.8778816199376948 	 lr:3.125e-06
epoch86: train: loss:1.1881549748845812 	 acc:0.97671875 | test: loss:1.2800093001665727 	 acc:0.8809968847352025 	 lr:1.5625e-06
epoch87: train: loss:1.1876816069865765 	 acc:0.97734375 | test: loss:1.2816510283687033 	 acc:0.8772585669781932 	 lr:1.5625e-06
epoch88: train: loss:1.1874270770439963 	 acc:0.9759375 | test: loss:1.2797073333805595 	 acc:0.8809968847352025 	 lr:1.5625e-06
epoch89: train: loss:1.188172720820526 	 acc:0.97859375 | test: loss:1.2804737167566365 	 acc:0.8803738317757009 	 lr:1.5625e-06
epoch90: train: loss:1.1870620787097177 	 acc:0.97796875 | test: loss:1.2811809153571678 	 acc:0.8785046728971962 	 lr:1.5625e-06
epoch91: train: loss:1.1875033829009318 	 acc:0.97484375 | test: loss:1.2802410308445726 	 acc:0.8785046728971962 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_4_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_4_2/
0.conv1.weight
0.conv1.weight freeze
0.bn1.weight
0.bn1.weight freeze
0.bn1.bias
0.bn1.bias freeze
0.layer1.0.conv1.weight
0.layer1.0.conv1.weight freeze
0.layer1.0.bn1.weight
0.layer1.0.bn1.weight freeze
0.layer1.0.bn1.bias
0.layer1.0.bn1.bias freeze
0.layer1.0.conv2.weight
0.layer1.0.conv2.weight freeze
0.layer1.0.bn2.weight
0.layer1.0.bn2.weight freeze
0.layer1.0.bn2.bias
0.layer1.0.bn2.bias freeze
0.layer1.0.conv3.weight
0.layer1.0.conv3.weight freeze
0.layer1.0.bn3.weight
0.layer1.0.bn3.weight freeze
0.layer1.0.bn3.bias
0.layer1.0.bn3.bias freeze
0.layer1.0.downsample.0.weight
0.layer1.0.downsample.0.weight freeze
0.layer1.0.downsample.1.weight
0.layer1.0.downsample.1.weight freeze
0.layer1.0.downsample.1.bias
0.layer1.0.downsample.1.bias freeze
0.layer1.1.conv1.weight
0.layer1.1.conv1.weight freeze
0.layer1.1.bn1.weight
0.layer1.1.bn1.weight freeze
0.layer1.1.bn1.bias
0.layer1.1.bn1.bias freeze
0.layer1.1.conv2.weight
0.layer1.1.conv2.weight freeze
0.layer1.1.bn2.weight
0.layer1.1.bn2.weight freeze
0.layer1.1.bn2.bias
0.layer1.1.bn2.bias freeze
0.layer1.1.conv3.weight
0.layer1.1.conv3.weight freeze
0.layer1.1.bn3.weight
0.layer1.1.bn3.weight freeze
0.layer1.1.bn3.bias
0.layer1.1.bn3.bias freeze
0.layer1.2.conv1.weight
0.layer1.2.conv1.weight freeze
0.layer1.2.bn1.weight
0.layer1.2.bn1.weight freeze
0.layer1.2.bn1.bias
0.layer1.2.bn1.bias freeze
0.layer1.2.conv2.weight
0.layer1.2.conv2.weight freeze
0.layer1.2.bn2.weight
0.layer1.2.bn2.weight freeze
0.layer1.2.bn2.bias
0.layer1.2.bn2.bias freeze
0.layer1.2.conv3.weight
0.layer1.2.conv3.weight freeze
0.layer1.2.bn3.weight
0.layer1.2.bn3.weight freeze
0.layer1.2.bn3.bias
0.layer1.2.bn3.bias freeze
0.layer2.0.conv1.weight
0.layer2.0.conv1.weight freeze
0.layer2.0.bn1.weight
0.layer2.0.bn1.weight freeze
0.layer2.0.bn1.bias
0.layer2.0.bn1.bias freeze
0.layer2.0.conv2.weight
0.layer2.0.conv2.weight freeze
0.layer2.0.bn2.weight
0.layer2.0.bn2.weight freeze
0.layer2.0.bn2.bias
0.layer2.0.bn2.bias freeze
0.layer2.0.conv3.weight
0.layer2.0.conv3.weight freeze
0.layer2.0.bn3.weight
0.layer2.0.bn3.weight freeze
0.layer2.0.bn3.bias
0.layer2.0.bn3.bias freeze
0.layer2.0.downsample.0.weight
0.layer2.0.downsample.0.weight freeze
0.layer2.0.downsample.1.weight
0.layer2.0.downsample.1.weight freeze
0.layer2.0.downsample.1.bias
0.layer2.0.downsample.1.bias freeze
0.layer2.1.conv1.weight
0.layer2.1.conv1.weight freeze
0.layer2.1.bn1.weight
0.layer2.1.bn1.weight freeze
0.layer2.1.bn1.bias
0.layer2.1.bn1.bias freeze
0.layer2.1.conv2.weight
0.layer2.1.conv2.weight freeze
0.layer2.1.bn2.weight
0.layer2.1.bn2.weight freeze
0.layer2.1.bn2.bias
0.layer2.1.bn2.bias freeze
0.layer2.1.conv3.weight
0.layer2.1.conv3.weight freeze
0.layer2.1.bn3.weight
0.layer2.1.bn3.weight freeze
0.layer2.1.bn3.bias
0.layer2.1.bn3.bias freeze
0.layer2.2.conv1.weight
0.layer2.2.conv1.weight freeze
0.layer2.2.bn1.weight
0.layer2.2.bn1.weight freeze
0.layer2.2.bn1.bias
0.layer2.2.bn1.bias freeze
0.layer2.2.conv2.weight
0.layer2.2.conv2.weight freeze
0.layer2.2.bn2.weight
0.layer2.2.bn2.weight freeze
0.layer2.2.bn2.bias
0.layer2.2.bn2.bias freeze
0.layer2.2.conv3.weight
0.layer2.2.conv3.weight freeze
0.layer2.2.bn3.weight
0.layer2.2.bn3.weight freeze
0.layer2.2.bn3.bias
0.layer2.2.bn3.bias freeze
0.layer2.3.conv1.weight
0.layer2.3.conv1.weight freeze
0.layer2.3.bn1.weight
0.layer2.3.bn1.weight freeze
0.layer2.3.bn1.bias
0.layer2.3.bn1.bias freeze
0.layer2.3.conv2.weight
0.layer2.3.conv2.weight freeze
0.layer2.3.bn2.weight
0.layer2.3.bn2.weight freeze
0.layer2.3.bn2.bias
0.layer2.3.bn2.bias freeze
0.layer2.3.conv3.weight
0.layer2.3.conv3.weight freeze
0.layer2.3.bn3.weight
0.layer2.3.bn3.weight freeze
0.layer2.3.bn3.bias
0.layer2.3.bn3.bias freeze
0.layer3.0.conv1.weight
0.layer3.0.conv1.weight freeze
0.layer3.0.bn1.weight
0.layer3.0.bn1.weight freeze
0.layer3.0.bn1.bias
0.layer3.0.bn1.bias freeze
0.layer3.0.conv2.weight
0.layer3.0.conv2.weight freeze
0.layer3.0.bn2.weight
0.layer3.0.bn2.weight freeze
0.layer3.0.bn2.bias
0.layer3.0.bn2.bias freeze
0.layer3.0.conv3.weight
0.layer3.0.conv3.weight freeze
0.layer3.0.bn3.weight
0.layer3.0.bn3.weight freeze
0.layer3.0.bn3.bias
0.layer3.0.bn3.bias freeze
0.layer3.0.downsample.0.weight
0.layer3.0.downsample.0.weight freeze
0.layer3.0.downsample.1.weight
0.layer3.0.downsample.1.weight freeze
0.layer3.0.downsample.1.bias
0.layer3.0.downsample.1.bias freeze
0.layer3.1.conv1.weight
0.layer3.1.conv1.weight freeze
0.layer3.1.bn1.weight
0.layer3.1.bn1.weight freeze
0.layer3.1.bn1.bias
0.layer3.1.bn1.bias freeze
0.layer3.1.conv2.weight
0.layer3.1.conv2.weight freeze
0.layer3.1.bn2.weight
0.layer3.1.bn2.weight freeze
0.layer3.1.bn2.bias
0.layer3.1.bn2.bias freeze
0.layer3.1.conv3.weight
0.layer3.1.conv3.weight freeze
0.layer3.1.bn3.weight
0.layer3.1.bn3.weight freeze
0.layer3.1.bn3.bias
0.layer3.1.bn3.bias freeze
0.layer3.2.conv1.weight
0.layer3.2.conv1.weight freeze
0.layer3.2.bn1.weight
0.layer3.2.bn1.weight freeze
0.layer3.2.bn1.bias
0.layer3.2.bn1.bias freeze
0.layer3.2.conv2.weight
0.layer3.2.conv2.weight freeze
0.layer3.2.bn2.weight
0.layer3.2.bn2.weight freeze
0.layer3.2.bn2.bias
0.layer3.2.bn2.bias freeze
0.layer3.2.conv3.weight
0.layer3.2.conv3.weight freeze
0.layer3.2.bn3.weight
0.layer3.2.bn3.weight freeze
0.layer3.2.bn3.bias
0.layer3.2.bn3.bias freeze
0.layer3.3.conv1.weight
0.layer3.3.conv1.weight freeze
0.layer3.3.bn1.weight
0.layer3.3.bn1.weight freeze
0.layer3.3.bn1.bias
0.layer3.3.bn1.bias freeze
0.layer3.3.conv2.weight
0.layer3.3.conv2.weight freeze
0.layer3.3.bn2.weight
0.layer3.3.bn2.weight freeze
0.layer3.3.bn2.bias
0.layer3.3.bn2.bias freeze
0.layer3.3.conv3.weight
0.layer3.3.conv3.weight freeze
0.layer3.3.bn3.weight
0.layer3.3.bn3.weight freeze
0.layer3.3.bn3.bias
0.layer3.3.bn3.bias freeze
0.layer3.4.conv1.weight
0.layer3.4.conv1.weight freeze
0.layer3.4.bn1.weight
0.layer3.4.bn1.weight freeze
0.layer3.4.bn1.bias
0.layer3.4.bn1.bias freeze
0.layer3.4.conv2.weight
0.layer3.4.conv2.weight freeze
0.layer3.4.bn2.weight
0.layer3.4.bn2.weight freeze
0.layer3.4.bn2.bias
0.layer3.4.bn2.bias freeze
0.layer3.4.conv3.weight
0.layer3.4.conv3.weight freeze
0.layer3.4.bn3.weight
0.layer3.4.bn3.weight freeze
0.layer3.4.bn3.bias
0.layer3.4.bn3.bias freeze
0.layer3.5.conv1.weight
0.layer3.5.conv1.weight freeze
0.layer3.5.bn1.weight
0.layer3.5.bn1.weight freeze
0.layer3.5.bn1.bias
0.layer3.5.bn1.bias freeze
0.layer3.5.conv2.weight
0.layer3.5.conv2.weight freeze
0.layer3.5.bn2.weight
0.layer3.5.bn2.weight freeze
0.layer3.5.bn2.bias
0.layer3.5.bn2.bias freeze
0.layer3.5.conv3.weight
0.layer3.5.conv3.weight freeze
0.layer3.5.bn3.weight
0.layer3.5.bn3.weight freeze
0.layer3.5.bn3.bias
0.layer3.5.bn3.bias freeze
0.layer4.0.conv1.weight
0.layer4.0.bn1.weight
0.layer4.0.bn1.bias
0.layer4.0.conv2.weight
0.layer4.0.bn2.weight
0.layer4.0.bn2.bias
0.layer4.0.conv3.weight
0.layer4.0.bn3.weight
0.layer4.0.bn3.bias
0.layer4.0.downsample.0.weight
0.layer4.0.downsample.1.weight
0.layer4.0.downsample.1.bias
0.layer4.1.conv1.weight
0.layer4.1.bn1.weight
0.layer4.1.bn1.bias
0.layer4.1.conv2.weight
0.layer4.1.bn2.weight
0.layer4.1.bn2.bias
0.layer4.1.conv3.weight
0.layer4.1.bn3.weight
0.layer4.1.bn3.bias
0.layer4.2.conv1.weight
0.layer4.2.bn1.weight
0.layer4.2.bn1.bias
0.layer4.2.conv2.weight
0.layer4.2.bn2.weight
0.layer4.2.bn2.bias
0.layer4.2.conv3.weight
0.layer4.2.bn3.weight
0.layer4.2.bn3.bias
1.1.weight
1.1.bias
training with  freeze_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.5394119250783689 	 acc:0.688125 | test: loss:1.5101193610010117 	 acc:0.7034267912772586 	 lr:0.0001
epoch1: train: loss:1.4592999639220763 	 acc:0.745625 | test: loss:1.4545016323665962 	 acc:0.7395638629283489 	 lr:0.0001
epoch2: train: loss:1.418303633369011 	 acc:0.77375 | test: loss:1.445920242104575 	 acc:0.7433021806853582 	 lr:0.0001
epoch3: train: loss:1.3964918340583223 	 acc:0.79328125 | test: loss:1.4244914483429858 	 acc:0.7607476635514019 	 lr:0.0001
epoch4: train: loss:1.3560453154350238 	 acc:0.78125 | test: loss:1.4338808159961878 	 acc:0.7588785046728972 	 lr:0.0001
epoch5: train: loss:1.3563088934464347 	 acc:0.81140625 | test: loss:1.4051478596862603 	 acc:0.7669781931464175 	 lr:0.0001
epoch6: train: loss:1.3391904689482095 	 acc:0.84296875 | test: loss:1.3771619328828615 	 acc:0.8087227414330218 	 lr:0.0001
epoch7: train: loss:1.311975133391119 	 acc:0.85171875 | test: loss:1.3642104298154885 	 acc:0.8018691588785046 	 lr:0.0001
epoch8: train: loss:1.3100248457397174 	 acc:0.87421875 | test: loss:1.3823994525125094 	 acc:0.8062305295950156 	 lr:0.0001
epoch9: train: loss:1.3080872844663287 	 acc:0.8215625 | test: loss:1.3955762897325081 	 acc:0.7651090342679128 	 lr:0.0001
epoch10: train: loss:1.3028353140932987 	 acc:0.8334375 | test: loss:1.391390253450269 	 acc:0.7819314641744548 	 lr:0.0001
epoch11: train: loss:1.2833915406702832 	 acc:0.86390625 | test: loss:1.3653882935039723 	 acc:0.8099688473520249 	 lr:0.0001
epoch12: train: loss:1.2912739714265893 	 acc:0.85203125 | test: loss:1.3845004327572024 	 acc:0.7881619937694704 	 lr:0.0001
epoch13: train: loss:1.2846298539387258 	 acc:0.87375 | test: loss:1.3546761585544573 	 acc:0.8161993769470405 	 lr:0.0001
epoch14: train: loss:1.2848340763327295 	 acc:0.81484375 | test: loss:1.3997286834449412 	 acc:0.7725856697819314 	 lr:0.0001
epoch15: train: loss:1.2703716721784128 	 acc:0.87625 | test: loss:1.3591079959988224 	 acc:0.8186915887850468 	 lr:0.0001
epoch16: train: loss:1.275539801401798 	 acc:0.899375 | test: loss:1.3380348885913504 	 acc:0.8286604361370716 	 lr:0.0001
epoch17: train: loss:1.2727618513025407 	 acc:0.86859375 | test: loss:1.3669756306294711 	 acc:0.8043613707165109 	 lr:0.0001
epoch18: train: loss:1.2694550037011796 	 acc:0.883125 | test: loss:1.3603603021378086 	 acc:0.8043613707165109 	 lr:0.0001
epoch19: train: loss:1.2631082336759307 	 acc:0.90859375 | test: loss:1.331493646853438 	 acc:0.8373831775700935 	 lr:0.0001
epoch20: train: loss:1.249066881571404 	 acc:0.90546875 | test: loss:1.3433946853114809 	 acc:0.8274143302180685 	 lr:0.0001
epoch21: train: loss:1.2479413141969775 	 acc:0.89171875 | test: loss:1.3382572751921658 	 acc:0.8286604361370716 	 lr:0.0001
epoch22: train: loss:1.2521411475867243 	 acc:0.890625 | test: loss:1.3547383740683583 	 acc:0.8031152647975078 	 lr:0.0001
epoch23: train: loss:1.2630301911043618 	 acc:0.89109375 | test: loss:1.3599005587004427 	 acc:0.8174454828660436 	 lr:0.0001
epoch24: train: loss:1.2492511996433755 	 acc:0.8984375 | test: loss:1.346353356934782 	 acc:0.8280373831775701 	 lr:0.0001
epoch25: train: loss:1.2511148755016224 	 acc:0.885625 | test: loss:1.3562600477462246 	 acc:0.8137071651090343 	 lr:0.0001
epoch26: train: loss:1.229129204612333 	 acc:0.9175 | test: loss:1.3351319071659788 	 acc:0.838006230529595 	 lr:5e-05
epoch27: train: loss:1.216304872558975 	 acc:0.94453125 | test: loss:1.3245583665705172 	 acc:0.8473520249221184 	 lr:5e-05
epoch28: train: loss:1.218563340102947 	 acc:0.93796875 | test: loss:1.3169633825248648 	 acc:0.8542056074766355 	 lr:5e-05
epoch29: train: loss:1.2241497316740158 	 acc:0.9359375 | test: loss:1.323680551326906 	 acc:0.8517133956386292 	 lr:5e-05
epoch30: train: loss:1.2228016289763857 	 acc:0.9428125 | test: loss:1.320819581854752 	 acc:0.8529595015576324 	 lr:5e-05
epoch31: train: loss:1.2150691226718018 	 acc:0.93671875 | test: loss:1.3250157432764118 	 acc:0.8467289719626169 	 lr:5e-05
epoch32: train: loss:1.219274402055584 	 acc:0.94703125 | test: loss:1.3277817911819505 	 acc:0.8367601246105919 	 lr:5e-05
epoch33: train: loss:1.2190759496517614 	 acc:0.94859375 | test: loss:1.3158976139680618 	 acc:0.859190031152648 	 lr:5e-05
epoch34: train: loss:1.2166638436120158 	 acc:0.94671875 | test: loss:1.3097049330625208 	 acc:0.8623052959501558 	 lr:5e-05
epoch35: train: loss:1.2132783578579356 	 acc:0.94046875 | test: loss:1.3098120584666173 	 acc:0.8579439252336448 	 lr:5e-05
epoch36: train: loss:1.2099835433781287 	 acc:0.9490625 | test: loss:1.315217589663568 	 acc:0.8492211838006231 	 lr:5e-05
epoch37: train: loss:1.218281916097958 	 acc:0.94796875 | test: loss:1.3107609760724124 	 acc:0.8566978193146417 	 lr:5e-05
epoch38: train: loss:1.2136540552119925 	 acc:0.9465625 | test: loss:1.3217879415672515 	 acc:0.8386292834890966 	 lr:5e-05
epoch39: train: loss:1.2197734537951002 	 acc:0.9446875 | test: loss:1.3168813311793723 	 acc:0.8566978193146417 	 lr:5e-05
epoch40: train: loss:1.2115558704671034 	 acc:0.95890625 | test: loss:1.3110470043164548 	 acc:0.8542056074766355 	 lr:5e-05
epoch41: train: loss:1.2065473426979654 	 acc:0.9515625 | test: loss:1.3173511938142628 	 acc:0.8529595015576324 	 lr:2.5e-05
epoch42: train: loss:1.2093040758143356 	 acc:0.95875 | test: loss:1.3125133775104987 	 acc:0.854828660436137 	 lr:2.5e-05
epoch43: train: loss:1.2095693309822648 	 acc:0.953125 | test: loss:1.3162662937633716 	 acc:0.8517133956386292 	 lr:2.5e-05
epoch44: train: loss:1.2065818792576906 	 acc:0.9528125 | test: loss:1.315399804664921 	 acc:0.8504672897196262 	 lr:2.5e-05
epoch45: train: loss:1.203557372856289 	 acc:0.9603125 | test: loss:1.315275674668428 	 acc:0.8504672897196262 	 lr:2.5e-05
epoch46: train: loss:1.2096465182993024 	 acc:0.9578125 | test: loss:1.3113683225952577 	 acc:0.8560747663551402 	 lr:2.5e-05
epoch47: train: loss:1.2065189144185147 	 acc:0.95703125 | test: loss:1.3087952029296541 	 acc:0.8579439252336448 	 lr:1.25e-05
epoch48: train: loss:1.2009324609070062 	 acc:0.96015625 | test: loss:1.3104202933029223 	 acc:0.8566978193146417 	 lr:1.25e-05
epoch49: train: loss:1.2069390016268418 	 acc:0.96578125 | test: loss:1.3058352454800473 	 acc:0.8629283489096573 	 lr:1.25e-05
epoch50: train: loss:1.2013859173360046 	 acc:0.95671875 | test: loss:1.3100817417429986 	 acc:0.8604361370716511 	 lr:1.25e-05
epoch51: train: loss:1.1980705481595495 	 acc:0.96375 | test: loss:1.3053790674774075 	 acc:0.8629283489096573 	 lr:1.25e-05
epoch52: train: loss:1.2013592186242132 	 acc:0.9625 | test: loss:1.3073487991113157 	 acc:0.8623052959501558 	 lr:1.25e-05
epoch53: train: loss:1.2039054685975312 	 acc:0.95984375 | test: loss:1.307767835807206 	 acc:0.8610591900311526 	 lr:1.25e-05
epoch54: train: loss:1.2056600697239706 	 acc:0.95875 | test: loss:1.3069544673337372 	 acc:0.8654205607476636 	 lr:1.25e-05
epoch55: train: loss:1.2078700259921142 	 acc:0.96640625 | test: loss:1.3083646319736943 	 acc:0.8604361370716511 	 lr:1.25e-05
epoch56: train: loss:1.20287386248765 	 acc:0.96125 | test: loss:1.3109310073644573 	 acc:0.8566978193146417 	 lr:1.25e-05
epoch57: train: loss:1.2027258605718798 	 acc:0.9646875 | test: loss:1.3072621636672925 	 acc:0.8623052959501558 	 lr:1.25e-05
epoch58: train: loss:1.1969103322561414 	 acc:0.96203125 | test: loss:1.3081299633251915 	 acc:0.8635514018691589 	 lr:6.25e-06
epoch59: train: loss:1.1984044833633696 	 acc:0.96359375 | test: loss:1.30917941454415 	 acc:0.8585669781931464 	 lr:6.25e-06
epoch60: train: loss:1.2095855736713872 	 acc:0.96171875 | test: loss:1.3077295153311852 	 acc:0.859190031152648 	 lr:6.25e-06
epoch61: train: loss:1.2078022178870267 	 acc:0.96203125 | test: loss:1.3097256978352865 	 acc:0.8566978193146417 	 lr:6.25e-06
epoch62: train: loss:1.199855707363632 	 acc:0.96109375 | test: loss:1.3098891977951905 	 acc:0.8610591900311526 	 lr:6.25e-06
epoch63: train: loss:1.2009131664600714 	 acc:0.9628125 | test: loss:1.3088617991064198 	 acc:0.8629283489096573 	 lr:6.25e-06
epoch64: train: loss:1.1993669282077906 	 acc:0.96546875 | test: loss:1.3079353219623506 	 acc:0.8585669781931464 	 lr:3.125e-06
epoch65: train: loss:1.1978793427871597 	 acc:0.9675 | test: loss:1.3077489849934325 	 acc:0.8623052959501558 	 lr:3.125e-06
epoch66: train: loss:1.1981483453516846 	 acc:0.96703125 | test: loss:1.3084380478873803 	 acc:0.859190031152648 	 lr:3.125e-06
epoch67: train: loss:1.199061356960657 	 acc:0.966875 | test: loss:1.3089325066296111 	 acc:0.8629283489096573 	 lr:3.125e-06
epoch68: train: loss:1.198828196469738 	 acc:0.96828125 | test: loss:1.308440817479404 	 acc:0.8598130841121495 	 lr:3.125e-06
epoch69: train: loss:1.2067194942382051 	 acc:0.96296875 | test: loss:1.3096582328790445 	 acc:0.8635514018691589 	 lr:3.125e-06
epoch70: train: loss:1.198491240478325 	 acc:0.96546875 | test: loss:1.3091229288748862 	 acc:0.864797507788162 	 lr:1.5625e-06
epoch71: train: loss:1.1994539735840224 	 acc:0.96703125 | test: loss:1.3083090857181965 	 acc:0.8629283489096573 	 lr:1.5625e-06
epoch72: train: loss:1.19607469616785 	 acc:0.9678125 | test: loss:1.3095322628258916 	 acc:0.8629283489096573 	 lr:1.5625e-06
epoch73: train: loss:1.1983628617702844 	 acc:0.96578125 | test: loss:1.3080140060353502 	 acc:0.8623052959501558 	 lr:1.5625e-06
epoch74: train: loss:1.193805205607954 	 acc:0.965 | test: loss:1.3082715213484482 	 acc:0.8660436137071651 	 lr:1.5625e-06
epoch75: train: loss:1.2037717129828687 	 acc:0.96578125 | test: loss:1.3077306024753417 	 acc:0.8660436137071651 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_5_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_5_2/
0.conv1.weight
0.conv1.weight freeze
0.bn1.weight
0.bn1.weight freeze
0.bn1.bias
0.bn1.bias freeze
0.layer1.0.conv1.weight
0.layer1.0.conv1.weight freeze
0.layer1.0.bn1.weight
0.layer1.0.bn1.weight freeze
0.layer1.0.bn1.bias
0.layer1.0.bn1.bias freeze
0.layer1.0.conv2.weight
0.layer1.0.conv2.weight freeze
0.layer1.0.bn2.weight
0.layer1.0.bn2.weight freeze
0.layer1.0.bn2.bias
0.layer1.0.bn2.bias freeze
0.layer1.0.conv3.weight
0.layer1.0.conv3.weight freeze
0.layer1.0.bn3.weight
0.layer1.0.bn3.weight freeze
0.layer1.0.bn3.bias
0.layer1.0.bn3.bias freeze
0.layer1.0.downsample.0.weight
0.layer1.0.downsample.0.weight freeze
0.layer1.0.downsample.1.weight
0.layer1.0.downsample.1.weight freeze
0.layer1.0.downsample.1.bias
0.layer1.0.downsample.1.bias freeze
0.layer1.1.conv1.weight
0.layer1.1.conv1.weight freeze
0.layer1.1.bn1.weight
0.layer1.1.bn1.weight freeze
0.layer1.1.bn1.bias
0.layer1.1.bn1.bias freeze
0.layer1.1.conv2.weight
0.layer1.1.conv2.weight freeze
0.layer1.1.bn2.weight
0.layer1.1.bn2.weight freeze
0.layer1.1.bn2.bias
0.layer1.1.bn2.bias freeze
0.layer1.1.conv3.weight
0.layer1.1.conv3.weight freeze
0.layer1.1.bn3.weight
0.layer1.1.bn3.weight freeze
0.layer1.1.bn3.bias
0.layer1.1.bn3.bias freeze
0.layer1.2.conv1.weight
0.layer1.2.conv1.weight freeze
0.layer1.2.bn1.weight
0.layer1.2.bn1.weight freeze
0.layer1.2.bn1.bias
0.layer1.2.bn1.bias freeze
0.layer1.2.conv2.weight
0.layer1.2.conv2.weight freeze
0.layer1.2.bn2.weight
0.layer1.2.bn2.weight freeze
0.layer1.2.bn2.bias
0.layer1.2.bn2.bias freeze
0.layer1.2.conv3.weight
0.layer1.2.conv3.weight freeze
0.layer1.2.bn3.weight
0.layer1.2.bn3.weight freeze
0.layer1.2.bn3.bias
0.layer1.2.bn3.bias freeze
0.layer2.0.conv1.weight
0.layer2.0.conv1.weight freeze
0.layer2.0.bn1.weight
0.layer2.0.bn1.weight freeze
0.layer2.0.bn1.bias
0.layer2.0.bn1.bias freeze
0.layer2.0.conv2.weight
0.layer2.0.conv2.weight freeze
0.layer2.0.bn2.weight
0.layer2.0.bn2.weight freeze
0.layer2.0.bn2.bias
0.layer2.0.bn2.bias freeze
0.layer2.0.conv3.weight
0.layer2.0.conv3.weight freeze
0.layer2.0.bn3.weight
0.layer2.0.bn3.weight freeze
0.layer2.0.bn3.bias
0.layer2.0.bn3.bias freeze
0.layer2.0.downsample.0.weight
0.layer2.0.downsample.0.weight freeze
0.layer2.0.downsample.1.weight
0.layer2.0.downsample.1.weight freeze
0.layer2.0.downsample.1.bias
0.layer2.0.downsample.1.bias freeze
0.layer2.1.conv1.weight
0.layer2.1.conv1.weight freeze
0.layer2.1.bn1.weight
0.layer2.1.bn1.weight freeze
0.layer2.1.bn1.bias
0.layer2.1.bn1.bias freeze
0.layer2.1.conv2.weight
0.layer2.1.conv2.weight freeze
0.layer2.1.bn2.weight
0.layer2.1.bn2.weight freeze
0.layer2.1.bn2.bias
0.layer2.1.bn2.bias freeze
0.layer2.1.conv3.weight
0.layer2.1.conv3.weight freeze
0.layer2.1.bn3.weight
0.layer2.1.bn3.weight freeze
0.layer2.1.bn3.bias
0.layer2.1.bn3.bias freeze
0.layer2.2.conv1.weight
0.layer2.2.conv1.weight freeze
0.layer2.2.bn1.weight
0.layer2.2.bn1.weight freeze
0.layer2.2.bn1.bias
0.layer2.2.bn1.bias freeze
0.layer2.2.conv2.weight
0.layer2.2.conv2.weight freeze
0.layer2.2.bn2.weight
0.layer2.2.bn2.weight freeze
0.layer2.2.bn2.bias
0.layer2.2.bn2.bias freeze
0.layer2.2.conv3.weight
0.layer2.2.conv3.weight freeze
0.layer2.2.bn3.weight
0.layer2.2.bn3.weight freeze
0.layer2.2.bn3.bias
0.layer2.2.bn3.bias freeze
0.layer2.3.conv1.weight
0.layer2.3.conv1.weight freeze
0.layer2.3.bn1.weight
0.layer2.3.bn1.weight freeze
0.layer2.3.bn1.bias
0.layer2.3.bn1.bias freeze
0.layer2.3.conv2.weight
0.layer2.3.conv2.weight freeze
0.layer2.3.bn2.weight
0.layer2.3.bn2.weight freeze
0.layer2.3.bn2.bias
0.layer2.3.bn2.bias freeze
0.layer2.3.conv3.weight
0.layer2.3.conv3.weight freeze
0.layer2.3.bn3.weight
0.layer2.3.bn3.weight freeze
0.layer2.3.bn3.bias
0.layer2.3.bn3.bias freeze
0.layer3.0.conv1.weight
0.layer3.0.conv1.weight freeze
0.layer3.0.bn1.weight
0.layer3.0.bn1.weight freeze
0.layer3.0.bn1.bias
0.layer3.0.bn1.bias freeze
0.layer3.0.conv2.weight
0.layer3.0.conv2.weight freeze
0.layer3.0.bn2.weight
0.layer3.0.bn2.weight freeze
0.layer3.0.bn2.bias
0.layer3.0.bn2.bias freeze
0.layer3.0.conv3.weight
0.layer3.0.conv3.weight freeze
0.layer3.0.bn3.weight
0.layer3.0.bn3.weight freeze
0.layer3.0.bn3.bias
0.layer3.0.bn3.bias freeze
0.layer3.0.downsample.0.weight
0.layer3.0.downsample.0.weight freeze
0.layer3.0.downsample.1.weight
0.layer3.0.downsample.1.weight freeze
0.layer3.0.downsample.1.bias
0.layer3.0.downsample.1.bias freeze
0.layer3.1.conv1.weight
0.layer3.1.conv1.weight freeze
0.layer3.1.bn1.weight
0.layer3.1.bn1.weight freeze
0.layer3.1.bn1.bias
0.layer3.1.bn1.bias freeze
0.layer3.1.conv2.weight
0.layer3.1.conv2.weight freeze
0.layer3.1.bn2.weight
0.layer3.1.bn2.weight freeze
0.layer3.1.bn2.bias
0.layer3.1.bn2.bias freeze
0.layer3.1.conv3.weight
0.layer3.1.conv3.weight freeze
0.layer3.1.bn3.weight
0.layer3.1.bn3.weight freeze
0.layer3.1.bn3.bias
0.layer3.1.bn3.bias freeze
0.layer3.2.conv1.weight
0.layer3.2.conv1.weight freeze
0.layer3.2.bn1.weight
0.layer3.2.bn1.weight freeze
0.layer3.2.bn1.bias
0.layer3.2.bn1.bias freeze
0.layer3.2.conv2.weight
0.layer3.2.conv2.weight freeze
0.layer3.2.bn2.weight
0.layer3.2.bn2.weight freeze
0.layer3.2.bn2.bias
0.layer3.2.bn2.bias freeze
0.layer3.2.conv3.weight
0.layer3.2.conv3.weight freeze
0.layer3.2.bn3.weight
0.layer3.2.bn3.weight freeze
0.layer3.2.bn3.bias
0.layer3.2.bn3.bias freeze
0.layer3.3.conv1.weight
0.layer3.3.conv1.weight freeze
0.layer3.3.bn1.weight
0.layer3.3.bn1.weight freeze
0.layer3.3.bn1.bias
0.layer3.3.bn1.bias freeze
0.layer3.3.conv2.weight
0.layer3.3.conv2.weight freeze
0.layer3.3.bn2.weight
0.layer3.3.bn2.weight freeze
0.layer3.3.bn2.bias
0.layer3.3.bn2.bias freeze
0.layer3.3.conv3.weight
0.layer3.3.conv3.weight freeze
0.layer3.3.bn3.weight
0.layer3.3.bn3.weight freeze
0.layer3.3.bn3.bias
0.layer3.3.bn3.bias freeze
0.layer3.4.conv1.weight
0.layer3.4.conv1.weight freeze
0.layer3.4.bn1.weight
0.layer3.4.bn1.weight freeze
0.layer3.4.bn1.bias
0.layer3.4.bn1.bias freeze
0.layer3.4.conv2.weight
0.layer3.4.conv2.weight freeze
0.layer3.4.bn2.weight
0.layer3.4.bn2.weight freeze
0.layer3.4.bn2.bias
0.layer3.4.bn2.bias freeze
0.layer3.4.conv3.weight
0.layer3.4.conv3.weight freeze
0.layer3.4.bn3.weight
0.layer3.4.bn3.weight freeze
0.layer3.4.bn3.bias
0.layer3.4.bn3.bias freeze
0.layer3.5.conv1.weight
0.layer3.5.conv1.weight freeze
0.layer3.5.bn1.weight
0.layer3.5.bn1.weight freeze
0.layer3.5.bn1.bias
0.layer3.5.bn1.bias freeze
0.layer3.5.conv2.weight
0.layer3.5.conv2.weight freeze
0.layer3.5.bn2.weight
0.layer3.5.bn2.weight freeze
0.layer3.5.bn2.bias
0.layer3.5.bn2.bias freeze
0.layer3.5.conv3.weight
0.layer3.5.conv3.weight freeze
0.layer3.5.bn3.weight
0.layer3.5.bn3.weight freeze
0.layer3.5.bn3.bias
0.layer3.5.bn3.bias freeze
0.layer4.0.conv1.weight
0.layer4.0.conv1.weight freeze
0.layer4.0.bn1.weight
0.layer4.0.bn1.weight freeze
0.layer4.0.bn1.bias
0.layer4.0.bn1.bias freeze
0.layer4.0.conv2.weight
0.layer4.0.conv2.weight freeze
0.layer4.0.bn2.weight
0.layer4.0.bn2.weight freeze
0.layer4.0.bn2.bias
0.layer4.0.bn2.bias freeze
0.layer4.0.conv3.weight
0.layer4.0.conv3.weight freeze
0.layer4.0.bn3.weight
0.layer4.0.bn3.weight freeze
0.layer4.0.bn3.bias
0.layer4.0.bn3.bias freeze
0.layer4.0.downsample.0.weight
0.layer4.0.downsample.0.weight freeze
0.layer4.0.downsample.1.weight
0.layer4.0.downsample.1.weight freeze
0.layer4.0.downsample.1.bias
0.layer4.0.downsample.1.bias freeze
0.layer4.1.conv1.weight
0.layer4.1.conv1.weight freeze
0.layer4.1.bn1.weight
0.layer4.1.bn1.weight freeze
0.layer4.1.bn1.bias
0.layer4.1.bn1.bias freeze
0.layer4.1.conv2.weight
0.layer4.1.conv2.weight freeze
0.layer4.1.bn2.weight
0.layer4.1.bn2.weight freeze
0.layer4.1.bn2.bias
0.layer4.1.bn2.bias freeze
0.layer4.1.conv3.weight
0.layer4.1.conv3.weight freeze
0.layer4.1.bn3.weight
0.layer4.1.bn3.weight freeze
0.layer4.1.bn3.bias
0.layer4.1.bn3.bias freeze
0.layer4.2.conv1.weight
0.layer4.2.conv1.weight freeze
0.layer4.2.bn1.weight
0.layer4.2.bn1.weight freeze
0.layer4.2.bn1.bias
0.layer4.2.bn1.bias freeze
0.layer4.2.conv2.weight
0.layer4.2.conv2.weight freeze
0.layer4.2.bn2.weight
0.layer4.2.bn2.weight freeze
0.layer4.2.bn2.bias
0.layer4.2.bn2.bias freeze
0.layer4.2.conv3.weight
0.layer4.2.conv3.weight freeze
0.layer4.2.bn3.weight
0.layer4.2.bn3.weight freeze
0.layer4.2.bn3.bias
0.layer4.2.bn3.bias freeze
1.1.weight
1.1.bias
training with  freeze_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.927562333046692 	 acc:0.20515625 | test: loss:1.9339229516151166 	 acc:0.18006230529595016 	 lr:0.0001
epoch1: train: loss:1.9065655889220763 	 acc:0.39390625 | test: loss:1.8848722916897211 	 acc:0.38691588785046727 	 lr:0.0001
epoch2: train: loss:1.883364186819227 	 acc:0.44328125 | test: loss:1.8649059716043443 	 acc:0.44922118380062304 	 lr:0.0001
epoch3: train: loss:1.8652552965746365 	 acc:0.45296875 | test: loss:1.8509276850572627 	 acc:0.4753894080996885 	 lr:0.0001
epoch4: train: loss:1.8472403342420471 	 acc:0.5396875 | test: loss:1.802930390203482 	 acc:0.5781931464174455 	 lr:0.0001
epoch5: train: loss:1.8292668648570147 	 acc:0.535625 | test: loss:1.7913859582764338 	 acc:0.5788161993769471 	 lr:0.0001
epoch6: train: loss:1.8141213100650737 	 acc:0.53796875 | test: loss:1.7768461338084807 	 acc:0.5744548286604362 	 lr:0.0001
epoch7: train: loss:1.8039405411802913 	 acc:0.5853125 | test: loss:1.7405153089594618 	 acc:0.6137071651090342 	 lr:0.0001
epoch8: train: loss:1.7883509496708199 	 acc:0.6015625 | test: loss:1.7221265215740025 	 acc:0.6292834890965732 	 lr:0.0001
epoch9: train: loss:1.7834111257608192 	 acc:0.62875 | test: loss:1.6952389399210612 	 acc:0.6504672897196262 	 lr:0.0001
epoch10: train: loss:1.7679682759174047 	 acc:0.578125 | test: loss:1.7185063847871584 	 acc:0.6087227414330219 	 lr:0.0001
epoch11: train: loss:1.7589756747505612 	 acc:0.6134375 | test: loss:1.6872204112115308 	 acc:0.6454828660436137 	 lr:0.0001
epoch12: train: loss:1.7501704267372291 	 acc:0.59765625 | test: loss:1.691060946960687 	 acc:0.6261682242990654 	 lr:0.0001
epoch13: train: loss:1.740733957997902 	 acc:0.61640625 | test: loss:1.6720710957904472 	 acc:0.6454828660436137 	 lr:0.0001
epoch14: train: loss:1.736888588303053 	 acc:0.61421875 | test: loss:1.6620141696335742 	 acc:0.6411214953271028 	 lr:0.0001
epoch15: train: loss:1.729465624580711 	 acc:0.58203125 | test: loss:1.6797798433036448 	 acc:0.6130841121495327 	 lr:0.0001
epoch16: train: loss:1.7208826379232534 	 acc:0.61734375 | test: loss:1.6497576703906431 	 acc:0.6517133956386293 	 lr:0.0001
epoch17: train: loss:1.7152928253340591 	 acc:0.60484375 | test: loss:1.652869557220245 	 acc:0.6386292834890965 	 lr:0.0001
epoch18: train: loss:1.7059604808560207 	 acc:0.61875 | test: loss:1.6453472774719524 	 acc:0.6392523364485981 	 lr:0.0001
epoch19: train: loss:1.7012697766294338 	 acc:0.6321875 | test: loss:1.6276189878350849 	 acc:0.6529595015576324 	 lr:0.0001
epoch20: train: loss:1.7009190627134563 	 acc:0.6003125 | test: loss:1.6561711136054398 	 acc:0.6174454828660436 	 lr:0.0001
epoch21: train: loss:1.6976635232370036 	 acc:0.616875 | test: loss:1.6402214554611396 	 acc:0.6417445482866043 	 lr:0.0001
epoch22: train: loss:1.6966790943011747 	 acc:0.61484375 | test: loss:1.631833450296586 	 acc:0.6423676012461059 	 lr:0.0001
epoch23: train: loss:1.685537038381727 	 acc:0.63171875 | test: loss:1.6228871130126288 	 acc:0.6566978193146418 	 lr:0.0001
epoch24: train: loss:1.6897692142102423 	 acc:0.6178125 | test: loss:1.6310433498424162 	 acc:0.6411214953271028 	 lr:0.0001
epoch25: train: loss:1.6797319660886576 	 acc:0.646875 | test: loss:1.6126386407008424 	 acc:0.6573208722741433 	 lr:0.0001
epoch26: train: loss:1.6799948436966359 	 acc:0.654375 | test: loss:1.6056674750794502 	 acc:0.6585669781931465 	 lr:0.0001
epoch27: train: loss:1.675974592317556 	 acc:0.62578125 | test: loss:1.6214707912314346 	 acc:0.6448598130841121 	 lr:0.0001
epoch28: train: loss:1.673666343718744 	 acc:0.63578125 | test: loss:1.6184279386870957 	 acc:0.643613707165109 	 lr:0.0001
epoch29: train: loss:1.6671877477617585 	 acc:0.6296875 | test: loss:1.6111795484091262 	 acc:0.6498442367601246 	 lr:0.0001
epoch30: train: loss:1.663529166609491 	 acc:0.6275 | test: loss:1.6150741294908375 	 acc:0.6417445482866043 	 lr:0.0001
epoch31: train: loss:1.660755418930828 	 acc:0.624375 | test: loss:1.6169832337311123 	 acc:0.6392523364485981 	 lr:0.0001
epoch32: train: loss:1.6563809325600116 	 acc:0.61828125 | test: loss:1.6206424232583923 	 acc:0.6336448598130842 	 lr:0.0001
epoch33: train: loss:1.652931705328936 	 acc:0.64703125 | test: loss:1.602429666400327 	 acc:0.6610591900311527 	 lr:5e-05
epoch34: train: loss:1.6609132717588189 	 acc:0.63 | test: loss:1.6031722865000693 	 acc:0.6566978193146418 	 lr:5e-05
epoch35: train: loss:1.6551505024036701 	 acc:0.63140625 | test: loss:1.6023040461763043 	 acc:0.6529595015576324 	 lr:5e-05
epoch36: train: loss:1.651229736899883 	 acc:0.62671875 | test: loss:1.6084069198537096 	 acc:0.6454828660436137 	 lr:5e-05
epoch37: train: loss:1.6568585505251026 	 acc:0.6571875 | test: loss:1.586286465846861 	 acc:0.670404984423676 	 lr:5e-05
epoch38: train: loss:1.6541078470722945 	 acc:0.63484375 | test: loss:1.6022288775518305 	 acc:0.6492211838006231 	 lr:5e-05
epoch39: train: loss:1.6516956076223714 	 acc:0.62953125 | test: loss:1.6024855217086935 	 acc:0.6442367601246106 	 lr:5e-05
epoch40: train: loss:1.6475038717539399 	 acc:0.6440625 | test: loss:1.5917503986774575 	 acc:0.659190031152648 	 lr:5e-05
epoch41: train: loss:1.6486214035474909 	 acc:0.6365625 | test: loss:1.5961824712723587 	 acc:0.6504672897196262 	 lr:5e-05
epoch42: train: loss:1.64512302419527 	 acc:0.64453125 | test: loss:1.5931583146811275 	 acc:0.6573208722741433 	 lr:5e-05
epoch43: train: loss:1.6446173329170937 	 acc:0.64625 | test: loss:1.597438083630856 	 acc:0.6529595015576324 	 lr:5e-05
epoch44: train: loss:1.6431122612338993 	 acc:0.65328125 | test: loss:1.5944152636691418 	 acc:0.653582554517134 	 lr:2.5e-05
epoch45: train: loss:1.6430081917660764 	 acc:0.66515625 | test: loss:1.5849892126808285 	 acc:0.664797507788162 	 lr:2.5e-05
epoch46: train: loss:1.6456705776813159 	 acc:0.64171875 | test: loss:1.5969158044856657 	 acc:0.6542056074766355 	 lr:2.5e-05
epoch47: train: loss:1.6468672645920241 	 acc:0.64609375 | test: loss:1.5936825955025504 	 acc:0.6566978193146418 	 lr:2.5e-05
epoch48: train: loss:1.64695050539587 	 acc:0.64890625 | test: loss:1.5906972265689172 	 acc:0.6573208722741433 	 lr:2.5e-05
epoch49: train: loss:1.6495038856667155 	 acc:0.6471875 | test: loss:1.586126440262126 	 acc:0.6654205607476635 	 lr:2.5e-05
epoch50: train: loss:1.6480263274503257 	 acc:0.6390625 | test: loss:1.5935264276938275 	 acc:0.6523364485981309 	 lr:2.5e-05
epoch51: train: loss:1.6414695864818136 	 acc:0.654375 | test: loss:1.5879727368785586 	 acc:0.6654205607476635 	 lr:2.5e-05
epoch52: train: loss:1.6402201120226203 	 acc:0.6540625 | test: loss:1.5900153755027557 	 acc:0.6623052959501557 	 lr:1.25e-05
epoch53: train: loss:1.6453118543900334 	 acc:0.6478125 | test: loss:1.587154424004837 	 acc:0.6654205607476635 	 lr:1.25e-05
epoch54: train: loss:1.6479147179251439 	 acc:0.63953125 | test: loss:1.593472025921783 	 acc:0.6529595015576324 	 lr:1.25e-05
epoch55: train: loss:1.6499419880136672 	 acc:0.65671875 | test: loss:1.5855034846011724 	 acc:0.6641744548286604 	 lr:1.25e-05
epoch56: train: loss:1.6408795871183706 	 acc:0.636875 | test: loss:1.5944518549791378 	 acc:0.6542056074766355 	 lr:1.25e-05
epoch57: train: loss:1.6428708983993827 	 acc:0.65953125 | test: loss:1.586542955885795 	 acc:0.659190031152648 	 lr:1.25e-05
epoch58: train: loss:1.6449796106366792 	 acc:0.65453125 | test: loss:1.5867434778688854 	 acc:0.660436137071651 	 lr:6.25e-06
epoch59: train: loss:1.644032179201887 	 acc:0.64421875 | test: loss:1.590375936885489 	 acc:0.6573208722741433 	 lr:6.25e-06
epoch60: train: loss:1.6465501247766332 	 acc:0.64390625 | test: loss:1.5893413307510804 	 acc:0.6560747663551402 	 lr:6.25e-06
epoch61: train: loss:1.6531579308729447 	 acc:0.63390625 | test: loss:1.5937948848599586 	 acc:0.6467289719626168 	 lr:6.25e-06
epoch62: train: loss:1.6374530967932768 	 acc:0.64046875 | test: loss:1.5909997039123487 	 acc:0.6542056074766355 	 lr:6.25e-06
epoch63: train: loss:1.6415086137234094 	 acc:0.6459375 | test: loss:1.5911150592138463 	 acc:0.6560747663551402 	 lr:6.25e-06
epoch64: train: loss:1.6450158839110376 	 acc:0.64609375 | test: loss:1.590164798813817 	 acc:0.6573208722741433 	 lr:3.125e-06
epoch65: train: loss:1.6406033399047375 	 acc:0.64859375 | test: loss:1.585878621529196 	 acc:0.660436137071651 	 lr:3.125e-06
epoch66: train: loss:1.6403889306907444 	 acc:0.65703125 | test: loss:1.5806452697682603 	 acc:0.6629283489096574 	 lr:3.125e-06
epoch67: train: loss:1.6436490489950784 	 acc:0.64453125 | test: loss:1.5898732183878295 	 acc:0.6548286604361371 	 lr:3.125e-06
epoch68: train: loss:1.6482085633705967 	 acc:0.64953125 | test: loss:1.587862785657247 	 acc:0.660436137071651 	 lr:3.125e-06
epoch69: train: loss:1.6385016864952309 	 acc:0.64453125 | test: loss:1.5909097657396787 	 acc:0.6610591900311527 	 lr:3.125e-06
epoch70: train: loss:1.6418994960144662 	 acc:0.64375 | test: loss:1.5914406979195426 	 acc:0.6566978193146418 	 lr:3.125e-06
epoch71: train: loss:1.6412283522928255 	 acc:0.648125 | test: loss:1.584502821993605 	 acc:0.6641744548286604 	 lr:3.125e-06
epoch72: train: loss:1.6400981727379733 	 acc:0.64640625 | test: loss:1.5935495671453506 	 acc:0.6548286604361371 	 lr:3.125e-06
epoch73: train: loss:1.636825106499439 	 acc:0.6425 | test: loss:1.592243198590858 	 acc:0.6542056074766355 	 lr:1.5625e-06
epoch74: train: loss:1.6376418769592238 	 acc:0.65109375 | test: loss:1.5875982918100564 	 acc:0.660436137071651 	 lr:1.5625e-06
epoch75: train: loss:1.6400529752756636 	 acc:0.6509375 | test: loss:1.589093970138336 	 acc:0.6598130841121496 	 lr:1.5625e-06
epoch76: train: loss:1.6386497956156079 	 acc:0.65375 | test: loss:1.5847777449081992 	 acc:0.6641744548286604 	 lr:1.5625e-06
epoch77: train: loss:1.641445872822746 	 acc:0.6540625 | test: loss:1.5859692974625348 	 acc:0.6641744548286604 	 lr:1.5625e-06
epoch78: train: loss:1.6412655284682072 	 acc:0.64359375 | test: loss:1.5894908477212781 	 acc:0.6579439252336449 	 lr:1.5625e-06
