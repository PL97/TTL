
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/resnet50_imagenet_-1_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/resnet50_imagenet_-1_2/
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.626843349559248 	 acc:0.5623140495867769 | test: loss:0.625147120842081 	 acc:0.5668874172185431 	 lr:0.0001
epoch1: train: loss:0.6245730570722218 	 acc:0.5497520661157025 | test: loss:0.6270747609485854 	 acc:0.5496688741721855 	 lr:0.0001
epoch2: train: loss:0.5888934434346916 	 acc:0.6271074380165289 | test: loss:0.5901650864556925 	 acc:0.6198675496688741 	 lr:0.0001
epoch3: train: loss:0.5634778235372433 	 acc:0.7305785123966942 | test: loss:0.5808622750225446 	 acc:0.7072847682119205 	 lr:0.0001
epoch4: train: loss:0.5541535766853773 	 acc:0.7358677685950413 | test: loss:0.5586717144542972 	 acc:0.7099337748344371 	 lr:0.0001
epoch5: train: loss:0.5290900146271572 	 acc:0.7441322314049587 | test: loss:0.5608783830870067 	 acc:0.695364238410596 	 lr:0.0001
epoch6: train: loss:0.5166743799674609 	 acc:0.7897520661157025 | test: loss:0.5587187868080392 	 acc:0.7364238410596027 	 lr:0.0001
epoch7: train: loss:0.5078723687672418 	 acc:0.7828099173553719 | test: loss:0.5538877455603998 	 acc:0.7099337748344371 	 lr:0.0001
epoch8: train: loss:0.5149295087392665 	 acc:0.7841322314049587 | test: loss:0.551684755836891 	 acc:0.7298013245033113 	 lr:0.0001
epoch9: train: loss:0.4978794946650828 	 acc:0.7854545454545454 | test: loss:0.5361056172295122 	 acc:0.7350993377483444 	 lr:0.0001
epoch10: train: loss:0.543177329055534 	 acc:0.6912396694214876 | test: loss:0.5980013492091603 	 acc:0.6185430463576159 	 lr:0.0001
epoch11: train: loss:0.49735731094336705 	 acc:0.7623140495867768 | test: loss:0.5433676487562672 	 acc:0.7152317880794702 	 lr:0.0001
epoch12: train: loss:0.4992028519634373 	 acc:0.7738842975206611 | test: loss:0.5612012366585384 	 acc:0.6781456953642384 	 lr:0.0001
epoch13: train: loss:0.4697700907967307 	 acc:0.8416528925619835 | test: loss:0.5435218868666137 	 acc:0.7298013245033113 	 lr:0.0001
epoch14: train: loss:0.49343807037211646 	 acc:0.8228099173553719 | test: loss:0.5626038464489362 	 acc:0.7165562913907285 	 lr:0.0001
epoch15: train: loss:0.47055594117188254 	 acc:0.8489256198347107 | test: loss:0.5816309963630525 	 acc:0.7178807947019867 	 lr:0.0001
epoch16: train: loss:0.4463257521735735 	 acc:0.8803305785123967 | test: loss:0.5503244330551451 	 acc:0.7470198675496689 	 lr:5e-05
epoch17: train: loss:0.4425204745698566 	 acc:0.8671074380165289 | test: loss:0.5442053766440083 	 acc:0.7311258278145696 	 lr:5e-05
epoch18: train: loss:0.4328670545156337 	 acc:0.8750413223140496 | test: loss:0.5447717028737857 	 acc:0.7377483443708609 	 lr:5e-05
epoch19: train: loss:0.42446444923227483 	 acc:0.8935537190082644 | test: loss:0.5428445565779477 	 acc:0.7417218543046358 	 lr:5e-05
epoch20: train: loss:0.426241730609216 	 acc:0.8968595041322314 | test: loss:0.5460214494079944 	 acc:0.7417218543046358 	 lr:5e-05
epoch21: train: loss:0.4194461785072138 	 acc:0.895206611570248 | test: loss:0.5415715171801333 	 acc:0.7377483443708609 	 lr:5e-05
epoch22: train: loss:0.421218882099656 	 acc:0.9087603305785124 | test: loss:0.5662941123476092 	 acc:0.7403973509933774 	 lr:2.5e-05
epoch23: train: loss:0.40582184835898977 	 acc:0.9186776859504132 | test: loss:0.5537292373890909 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch24: train: loss:0.40434962810563646 	 acc:0.9090909090909091 | test: loss:0.5517612645957644 	 acc:0.7337748344370861 	 lr:2.5e-05
epoch25: train: loss:0.4002955311389009 	 acc:0.9196694214876033 | test: loss:0.5455101496336476 	 acc:0.743046357615894 	 lr:2.5e-05
epoch26: train: loss:0.4015448449071774 	 acc:0.9077685950413223 | test: loss:0.5339374277765387 	 acc:0.752317880794702 	 lr:2.5e-05
epoch27: train: loss:0.406822831896711 	 acc:0.9087603305785124 | test: loss:0.5454079301152008 	 acc:0.7562913907284768 	 lr:2.5e-05
epoch28: train: loss:0.39495221549814397 	 acc:0.9196694214876033 | test: loss:0.5406045234755964 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch29: train: loss:0.3911115718676039 	 acc:0.923305785123967 | test: loss:0.5302655385819492 	 acc:0.7602649006622516 	 lr:2.5e-05
epoch30: train: loss:0.39067580593518975 	 acc:0.9246280991735537 | test: loss:0.5338686674635932 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch31: train: loss:0.3926724044547593 	 acc:0.9203305785123967 | test: loss:0.5383322788390102 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch32: train: loss:0.38894598788466334 	 acc:0.9190082644628099 | test: loss:0.5432935517355306 	 acc:0.7364238410596027 	 lr:2.5e-05
epoch33: train: loss:0.3831049472044322 	 acc:0.9338842975206612 | test: loss:0.5499161835537841 	 acc:0.7417218543046358 	 lr:2.5e-05
epoch34: train: loss:0.39183864668381113 	 acc:0.9302479338842975 | test: loss:0.5570578056455447 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch35: train: loss:0.3855631687030319 	 acc:0.936198347107438 | test: loss:0.5653646969637334 	 acc:0.7284768211920529 	 lr:2.5e-05
epoch36: train: loss:0.38885078277469665 	 acc:0.9289256198347108 | test: loss:0.546214655456164 	 acc:0.743046357615894 	 lr:1.25e-05
epoch37: train: loss:0.38416379485248536 	 acc:0.9335537190082644 | test: loss:0.553573380558696 	 acc:0.7417218543046358 	 lr:1.25e-05
epoch38: train: loss:0.38325109649295647 	 acc:0.9292561983471075 | test: loss:0.5351180357648837 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch39: train: loss:0.3773054335038524 	 acc:0.9441322314049587 | test: loss:0.5509401226675273 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch40: train: loss:0.3794525620464451 	 acc:0.9322314049586777 | test: loss:0.5400918619522196 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch41: train: loss:0.37737035661689505 	 acc:0.9385123966942148 | test: loss:0.5494910114648326 	 acc:0.743046357615894 	 lr:1.25e-05
epoch42: train: loss:0.3761647484617785 	 acc:0.9398347107438016 | test: loss:0.5384081966040151 	 acc:0.7470198675496689 	 lr:6.25e-06
epoch43: train: loss:0.3796324028850587 	 acc:0.9355371900826446 | test: loss:0.5344186621785953 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch44: train: loss:0.37813524399907134 	 acc:0.9454545454545454 | test: loss:0.5455322466149235 	 acc:0.743046357615894 	 lr:6.25e-06
epoch45: train: loss:0.3798628394170241 	 acc:0.9385123966942148 | test: loss:0.5446735543131039 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch46: train: loss:0.37843772598534575 	 acc:0.9404958677685951 | test: loss:0.5409327482545613 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch47: train: loss:0.37693827296091503 	 acc:0.9404958677685951 | test: loss:0.5424748588871482 	 acc:0.7470198675496689 	 lr:6.25e-06
epoch48: train: loss:0.37477962339219967 	 acc:0.9421487603305785 | test: loss:0.5415496061179812 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch49: train: loss:0.37634726813016844 	 acc:0.9365289256198347 | test: loss:0.5435889150922661 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch50: train: loss:0.3714281901840336 	 acc:0.9441322314049587 | test: loss:0.544554700677758 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch51: train: loss:0.3742205753109672 	 acc:0.9404958677685951 | test: loss:0.5445884659590311 	 acc:0.743046357615894 	 lr:3.125e-06
epoch52: train: loss:0.3718164952136268 	 acc:0.9457851239669421 | test: loss:0.5411439928787434 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch53: train: loss:0.3739831967393229 	 acc:0.944793388429752 | test: loss:0.5417964699252552 	 acc:0.743046357615894 	 lr:3.125e-06
epoch54: train: loss:0.3741907478265526 	 acc:0.9444628099173553 | test: loss:0.5440747086575489 	 acc:0.743046357615894 	 lr:1.5625e-06
epoch55: train: loss:0.37059080861816723 	 acc:0.9477685950413223 | test: loss:0.5450363676279586 	 acc:0.7417218543046358 	 lr:1.5625e-06
epoch56: train: loss:0.3729154682454984 	 acc:0.9461157024793388 | test: loss:0.5465328903387714 	 acc:0.7390728476821192 	 lr:1.5625e-06
epoch57: train: loss:0.37220391348373794 	 acc:0.9467768595041323 | test: loss:0.5449860072293818 	 acc:0.743046357615894 	 lr:1.5625e-06
epoch58: train: loss:0.37093277912494566 	 acc:0.9454545454545454 | test: loss:0.5419990046134847 	 acc:0.743046357615894 	 lr:1.5625e-06
epoch59: train: loss:0.3767527718386374 	 acc:0.9411570247933885 | test: loss:0.5440190888398531 	 acc:0.7417218543046358 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_1_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_1_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7238816218533792 	 acc:0.48826446280991737 | test: loss:0.7223922280286321 	 acc:0.49933774834437084 	 lr:0.0001
epoch1: train: loss:0.6928401641215175 	 acc:0.5213223140495867 | test: loss:0.692830929061435 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.7305696956973431 	 acc:0.509090909090909 | test: loss:0.7333600337931652 	 acc:0.4966887417218543 	 lr:0.0001
epoch3: train: loss:0.6636589576587204 	 acc:0.5282644628099173 | test: loss:0.6564444211145111 	 acc:0.543046357615894 	 lr:0.0001
epoch4: train: loss:0.6586437785921018 	 acc:0.5209917355371901 | test: loss:0.6533237479380425 	 acc:0.5218543046357615 	 lr:0.0001
epoch5: train: loss:0.6667816692147374 	 acc:0.5768595041322314 | test: loss:0.661887951007742 	 acc:0.5867549668874172 	 lr:0.0001
epoch6: train: loss:0.6720446160016966 	 acc:0.5583471074380165 | test: loss:0.6643589926871243 	 acc:0.5748344370860927 	 lr:0.0001
epoch7: train: loss:0.6723432594094395 	 acc:0.5851239669421487 | test: loss:0.6704568898440987 	 acc:0.5920529801324503 	 lr:0.0001
epoch8: train: loss:0.6583320578661832 	 acc:0.5246280991735537 | test: loss:0.6532322435979022 	 acc:0.5258278145695364 	 lr:0.0001
epoch9: train: loss:0.6544632458095708 	 acc:0.5292561983471075 | test: loss:0.652502480642685 	 acc:0.5417218543046357 	 lr:0.0001
epoch10: train: loss:0.6527377223968506 	 acc:0.524297520661157 | test: loss:0.6507163792256488 	 acc:0.5258278145695364 	 lr:0.0001
epoch11: train: loss:0.6538238287563166 	 acc:0.5239669421487604 | test: loss:0.6518601203596355 	 acc:0.5324503311258278 	 lr:0.0001
epoch12: train: loss:0.6540492615423912 	 acc:0.52 | test: loss:0.6520776848919344 	 acc:0.5258278145695364 	 lr:0.0001
epoch13: train: loss:0.6682969250560792 	 acc:0.5828099173553719 | test: loss:0.667068067370661 	 acc:0.6066225165562914 	 lr:0.0001
epoch14: train: loss:0.6578685756557244 	 acc:0.5692561983471074 | test: loss:0.6579894950847752 	 acc:0.5947019867549669 	 lr:0.0001
epoch15: train: loss:0.6510661014052462 	 acc:0.5272727272727272 | test: loss:0.6473717348465067 	 acc:0.5271523178807948 	 lr:0.0001
epoch16: train: loss:0.6984884382082411 	 acc:0.5457851239669421 | test: loss:0.6960434706795294 	 acc:0.5470198675496689 	 lr:0.0001
epoch17: train: loss:0.6602638779395869 	 acc:0.5213223140495867 | test: loss:0.6587839035008917 	 acc:0.5218543046357615 	 lr:0.0001
epoch18: train: loss:0.6659553989103018 	 acc:0.572892561983471 | test: loss:0.661061764947626 	 acc:0.6172185430463576 	 lr:0.0001
epoch19: train: loss:0.6536832425416994 	 acc:0.5461157024793388 | test: loss:0.646800665665936 	 acc:0.5536423841059602 	 lr:0.0001
epoch20: train: loss:0.6516750841495419 	 acc:0.5613223140495868 | test: loss:0.6475381321464942 	 acc:0.5841059602649007 	 lr:0.0001
epoch21: train: loss:0.653495691630466 	 acc:0.5216528925619834 | test: loss:0.6492316557871585 	 acc:0.5231788079470199 	 lr:0.0001
epoch22: train: loss:0.6820692032427828 	 acc:0.5775206611570248 | test: loss:0.6791204097255177 	 acc:0.5867549668874172 	 lr:0.0001
epoch23: train: loss:0.6501026019774193 	 acc:0.5566942148760331 | test: loss:0.6451968444104226 	 acc:0.5602649006622517 	 lr:0.0001
epoch24: train: loss:0.656135766348563 	 acc:0.5213223140495867 | test: loss:0.6572970749526624 	 acc:0.5218543046357615 	 lr:0.0001
epoch25: train: loss:0.6470103071937876 	 acc:0.5262809917355372 | test: loss:0.6446911427358918 	 acc:0.5350993377483444 	 lr:0.0001
epoch26: train: loss:0.7502623560408915 	 acc:0.4975206611570248 | test: loss:0.7598079674291295 	 acc:0.49801324503311256 	 lr:0.0001
epoch27: train: loss:0.6574790933900628 	 acc:0.5907438016528925 | test: loss:0.6523117986735919 	 acc:0.6158940397350994 	 lr:0.0001
epoch28: train: loss:0.6528027038928891 	 acc:0.5834710743801653 | test: loss:0.6500978933264877 	 acc:0.6172185430463576 	 lr:0.0001
epoch29: train: loss:0.6543161203841532 	 acc:0.5811570247933884 | test: loss:0.6510432113874827 	 acc:0.6052980132450331 	 lr:0.0001
epoch30: train: loss:0.6443988142919934 	 acc:0.5282644628099173 | test: loss:0.6442537443527323 	 acc:0.5350993377483444 	 lr:0.0001
epoch31: train: loss:0.6526955002989651 	 acc:0.5527272727272727 | test: loss:0.648027011732392 	 acc:0.5655629139072847 	 lr:0.0001
epoch32: train: loss:0.6599566666941997 	 acc:0.5930578512396695 | test: loss:0.6606761156328467 	 acc:0.6185430463576159 	 lr:0.0001
epoch33: train: loss:0.6457066714468082 	 acc:0.5368595041322314 | test: loss:0.6428996631641262 	 acc:0.543046357615894 	 lr:0.0001
epoch34: train: loss:0.661693056713451 	 acc:0.5216528925619834 | test: loss:0.6645770763719319 	 acc:0.5218543046357615 	 lr:0.0001
epoch35: train: loss:0.6769477866306778 	 acc:0.5732231404958678 | test: loss:0.6711817474554707 	 acc:0.6013245033112583 	 lr:0.0001
epoch36: train: loss:0.6444954836073 	 acc:0.540495867768595 | test: loss:0.6414297916241829 	 acc:0.5602649006622517 	 lr:0.0001
epoch37: train: loss:0.6462995729958716 	 acc:0.5272727272727272 | test: loss:0.6449049124654556 	 acc:0.528476821192053 	 lr:0.0001
epoch38: train: loss:0.6635108106195434 	 acc:0.5996694214876033 | test: loss:0.6527384048266127 	 acc:0.6225165562913907 	 lr:0.0001
epoch39: train: loss:0.6780689133100273 	 acc:0.5914049586776859 | test: loss:0.6775279361680644 	 acc:0.5960264900662252 	 lr:0.0001
epoch40: train: loss:0.6691014910926504 	 acc:0.6039669421487603 | test: loss:0.659468878578666 	 acc:0.6119205298013245 	 lr:0.0001
epoch41: train: loss:0.6461100918990521 	 acc:0.5633057851239669 | test: loss:0.6411575589748408 	 acc:0.5748344370860927 	 lr:0.0001
epoch42: train: loss:0.6896658665483648 	 acc:0.5712396694214876 | test: loss:0.6872932950392464 	 acc:0.5668874172185431 	 lr:0.0001
epoch43: train: loss:0.7448038061788259 	 acc:0.5077685950413223 | test: loss:0.7462973389404499 	 acc:0.5019867549668874 	 lr:0.0001
epoch44: train: loss:0.6622392292456193 	 acc:0.5953719008264463 | test: loss:0.657654552980764 	 acc:0.6278145695364239 	 lr:0.0001
epoch45: train: loss:0.6512697869490001 	 acc:0.5877685950413223 | test: loss:0.6434509787338459 	 acc:0.5933774834437087 	 lr:0.0001
epoch46: train: loss:0.6462277668763783 	 acc:0.5798347107438017 | test: loss:0.6428195906790677 	 acc:0.5854304635761589 	 lr:0.0001
epoch47: train: loss:0.6509838841769321 	 acc:0.6082644628099173 | test: loss:0.6459030844517891 	 acc:0.6013245033112583 	 lr:0.0001
epoch48: train: loss:0.6591276530785994 	 acc:0.6148760330578512 | test: loss:0.6515817703000757 	 acc:0.614569536423841 	 lr:5e-05
epoch49: train: loss:0.6526814612475309 	 acc:0.6046280991735538 | test: loss:0.645747642169725 	 acc:0.609271523178808 	 lr:5e-05
epoch50: train: loss:0.6506601117268082 	 acc:0.6049586776859505 | test: loss:0.6460825159060245 	 acc:0.6052980132450331 	 lr:5e-05
epoch51: train: loss:0.6421290238238563 	 acc:0.5619834710743802 | test: loss:0.6397045980226125 	 acc:0.5655629139072847 	 lr:5e-05
epoch52: train: loss:0.6427565656023577 	 acc:0.5570247933884298 | test: loss:0.6399278962848992 	 acc:0.5748344370860927 	 lr:5e-05
epoch53: train: loss:0.6475894944923969 	 acc:0.5986776859504133 | test: loss:0.6444647168481586 	 acc:0.6 	 lr:5e-05
epoch54: train: loss:0.6618619958428312 	 acc:0.608595041322314 | test: loss:0.653521243941705 	 acc:0.6264900662251656 	 lr:5e-05
epoch55: train: loss:0.6431333915852319 	 acc:0.5619834710743802 | test: loss:0.6385924550871186 	 acc:0.5814569536423841 	 lr:5e-05
epoch56: train: loss:0.6589614418715485 	 acc:0.6155371900826446 | test: loss:0.6565211289765819 	 acc:0.6304635761589404 	 lr:5e-05
epoch57: train: loss:0.688351689941627 	 acc:0.5679338842975207 | test: loss:0.6894156643096975 	 acc:0.5536423841059602 	 lr:5e-05
epoch58: train: loss:0.6491103325402441 	 acc:0.610909090909091 | test: loss:0.644352026254136 	 acc:0.6251655629139072 	 lr:5e-05
epoch59: train: loss:0.6517434664206071 	 acc:0.6168595041322315 | test: loss:0.6511459773739442 	 acc:0.6291390728476821 	 lr:5e-05
epoch60: train: loss:0.6487008849254324 	 acc:0.6 | test: loss:0.6463701620007193 	 acc:0.6384105960264901 	 lr:5e-05
epoch61: train: loss:0.6445125115804435 	 acc:0.6052892561983471 | test: loss:0.6416273694164706 	 acc:0.6079470198675496 	 lr:5e-05
epoch62: train: loss:0.6425436371811165 	 acc:0.5623140495867769 | test: loss:0.6376816772467253 	 acc:0.5708609271523178 	 lr:2.5e-05
epoch63: train: loss:0.6595145350842436 	 acc:0.6178512396694215 | test: loss:0.6554552048247382 	 acc:0.6423841059602649 	 lr:2.5e-05
epoch64: train: loss:0.6486673146239982 	 acc:0.5884297520661157 | test: loss:0.641156831719228 	 acc:0.6132450331125828 	 lr:2.5e-05
epoch65: train: loss:0.6456179195593211 	 acc:0.5963636363636363 | test: loss:0.6391314299690802 	 acc:0.5933774834437087 	 lr:2.5e-05
epoch66: train: loss:0.6491652921999782 	 acc:0.6 | test: loss:0.6417707933495377 	 acc:0.614569536423841 	 lr:2.5e-05
epoch67: train: loss:0.645426086670111 	 acc:0.5960330578512397 | test: loss:0.6389919416004459 	 acc:0.6039735099337749 	 lr:2.5e-05
epoch68: train: loss:0.6478830459886346 	 acc:0.6165289256198347 | test: loss:0.6448198247429551 	 acc:0.614569536423841 	 lr:2.5e-05
epoch69: train: loss:0.6432434696205391 	 acc:0.5801652892561984 | test: loss:0.6384223092470738 	 acc:0.5947019867549669 	 lr:1.25e-05
epoch70: train: loss:0.6439824786067995 	 acc:0.5758677685950413 | test: loss:0.6371337758784262 	 acc:0.5867549668874172 	 lr:1.25e-05
epoch71: train: loss:0.6436641108103035 	 acc:0.6052892561983471 | test: loss:0.6391850621495025 	 acc:0.590728476821192 	 lr:1.25e-05
epoch72: train: loss:0.6546610582958569 	 acc:0.6082644628099173 | test: loss:0.6450909319302893 	 acc:0.6211920529801325 	 lr:1.25e-05
epoch73: train: loss:0.648580408707138 	 acc:0.6211570247933884 | test: loss:0.6436741818655406 	 acc:0.6225165562913907 	 lr:1.25e-05
epoch74: train: loss:0.648185296235991 	 acc:0.6181818181818182 | test: loss:0.6452600054393541 	 acc:0.6264900662251656 	 lr:1.25e-05
epoch75: train: loss:0.64804554639769 	 acc:0.6072727272727273 | test: loss:0.6427132456507904 	 acc:0.6132450331125828 	 lr:1.25e-05
epoch76: train: loss:0.642468807500256 	 acc:0.5771900826446281 | test: loss:0.6382343690126937 	 acc:0.5788079470198676 	 lr:1.25e-05
epoch77: train: loss:0.6450691518113633 	 acc:0.5973553719008264 | test: loss:0.6403817145240228 	 acc:0.5986754966887418 	 lr:6.25e-06
epoch78: train: loss:0.6452873725339401 	 acc:0.6016528925619835 | test: loss:0.6405656532736014 	 acc:0.6 	 lr:6.25e-06
epoch79: train: loss:0.6408886365851095 	 acc:0.5963636363636363 | test: loss:0.6385236944583867 	 acc:0.5801324503311258 	 lr:6.25e-06
epoch80: train: loss:0.6415816761245412 	 acc:0.5619834710743802 | test: loss:0.6370259624443306 	 acc:0.5735099337748344 	 lr:6.25e-06
epoch81: train: loss:0.6529576873582257 	 acc:0.6105785123966943 | test: loss:0.6444230550172313 	 acc:0.6251655629139072 	 lr:6.25e-06
epoch82: train: loss:0.6498661923408509 	 acc:0.6039669421487603 | test: loss:0.6425354588900181 	 acc:0.614569536423841 	 lr:6.25e-06
epoch83: train: loss:0.6435330115862129 	 acc:0.6013223140495868 | test: loss:0.6397457644639426 	 acc:0.6026490066225165 	 lr:6.25e-06
epoch84: train: loss:0.6403231690540787 	 acc:0.5682644628099174 | test: loss:0.6369152749611052 	 acc:0.5708609271523178 	 lr:6.25e-06
epoch85: train: loss:0.6409834353588829 	 acc:0.576198347107438 | test: loss:0.6373520058511898 	 acc:0.5814569536423841 	 lr:6.25e-06
epoch86: train: loss:0.6427220379813644 	 acc:0.6036363636363636 | test: loss:0.6393949458930666 	 acc:0.6039735099337749 	 lr:6.25e-06
epoch87: train: loss:0.6468196406640297 	 acc:0.5973553719008264 | test: loss:0.640724308838118 	 acc:0.609271523178808 	 lr:6.25e-06
epoch88: train: loss:0.6409001322817212 	 acc:0.5914049586776859 | test: loss:0.6382979633792347 	 acc:0.5827814569536424 	 lr:6.25e-06
epoch89: train: loss:0.6445873181090868 	 acc:0.5798347107438017 | test: loss:0.6382310264157933 	 acc:0.5827814569536424 	 lr:6.25e-06
epoch90: train: loss:0.6428831735327224 	 acc:0.5851239669421487 | test: loss:0.6384573902515386 	 acc:0.5867549668874172 	 lr:6.25e-06
epoch91: train: loss:0.6467428738026579 	 acc:0.5914049586776859 | test: loss:0.6398467868369146 	 acc:0.6 	 lr:3.125e-06
epoch92: train: loss:0.6457742301097587 	 acc:0.6072727272727273 | test: loss:0.6405816498181678 	 acc:0.6026490066225165 	 lr:3.125e-06
epoch93: train: loss:0.6438446015562893 	 acc:0.5986776859504133 | test: loss:0.6393021038036473 	 acc:0.6013245033112583 	 lr:3.125e-06
epoch94: train: loss:0.640993670983748 	 acc:0.5887603305785124 | test: loss:0.6381338587659874 	 acc:0.5867549668874172 	 lr:3.125e-06
epoch95: train: loss:0.6427039708972963 	 acc:0.5871074380165289 | test: loss:0.6379150876935744 	 acc:0.5854304635761589 	 lr:3.125e-06
epoch96: train: loss:0.6464994526697584 	 acc:0.6231404958677685 | test: loss:0.6416922451644543 	 acc:0.6052980132450331 	 lr:3.125e-06
epoch97: train: loss:0.6447062806649642 	 acc:0.5943801652892562 | test: loss:0.6389466207548482 	 acc:0.5947019867549669 	 lr:1.5625e-06
epoch98: train: loss:0.6478719256338009 	 acc:0.5917355371900826 | test: loss:0.6401355216045254 	 acc:0.6013245033112583 	 lr:1.5625e-06
epoch99: train: loss:0.6462397672519211 	 acc:0.5910743801652892 | test: loss:0.6390944788787538 	 acc:0.6 	 lr:1.5625e-06
epoch100: train: loss:0.642414877020623 	 acc:0.6 | test: loss:0.6391006291307361 	 acc:0.5960264900662252 	 lr:1.5625e-06
epoch101: train: loss:0.6437632111478443 	 acc:0.5887603305785124 | test: loss:0.6387117317180759 	 acc:0.5947019867549669 	 lr:1.5625e-06
epoch102: train: loss:0.6456134162067382 	 acc:0.5900826446280992 | test: loss:0.6392277822589243 	 acc:0.6 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_2_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_2_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6805493661982954 	 acc:0.5213223140495867 | test: loss:0.6762442750646579 	 acc:0.5218543046357615 	 lr:0.0001
epoch1: train: loss:0.680240951077012 	 acc:0.5861157024793389 | test: loss:0.6668527527360727 	 acc:0.6079470198675496 	 lr:0.0001
epoch2: train: loss:0.6722762141148906 	 acc:0.5877685950413223 | test: loss:0.6641383066872097 	 acc:0.6079470198675496 	 lr:0.0001
epoch3: train: loss:0.6477523067174864 	 acc:0.5619834710743802 | test: loss:0.6427636720486825 	 acc:0.5655629139072847 	 lr:0.0001
epoch4: train: loss:0.6569339546290311 	 acc:0.6181818181818182 | test: loss:0.6517891547537797 	 acc:0.609271523178808 	 lr:0.0001
epoch5: train: loss:0.6418965103803588 	 acc:0.5828099173553719 | test: loss:0.6370898050188228 	 acc:0.5827814569536424 	 lr:0.0001
epoch6: train: loss:0.6370867049004421 	 acc:0.5507438016528926 | test: loss:0.6345974830602179 	 acc:0.552317880794702 	 lr:0.0001
epoch7: train: loss:0.6536021194379191 	 acc:0.6006611570247934 | test: loss:0.6494331377231523 	 acc:0.6291390728476821 	 lr:0.0001
epoch8: train: loss:0.644986280744726 	 acc:0.5299173553719009 | test: loss:0.6485029638208301 	 acc:0.5231788079470199 	 lr:0.0001
epoch9: train: loss:0.6633693610341096 	 acc:0.5223140495867769 | test: loss:0.6602885767324082 	 acc:0.5218543046357615 	 lr:0.0001
epoch10: train: loss:0.6476899352546566 	 acc:0.5272727272727272 | test: loss:0.6491396522679865 	 acc:0.5218543046357615 	 lr:0.0001
epoch11: train: loss:0.6323303825993183 	 acc:0.6264462809917355 | test: loss:0.6294243453354236 	 acc:0.6357615894039735 	 lr:0.0001
epoch12: train: loss:0.6439762023854847 	 acc:0.6304132231404959 | test: loss:0.6351493518873556 	 acc:0.6317880794701987 	 lr:0.0001
epoch13: train: loss:0.6267316810749779 	 acc:0.5814876033057851 | test: loss:0.6288978065875982 	 acc:0.5748344370860927 	 lr:0.0001
epoch14: train: loss:0.6262367945269119 	 acc:0.5963636363636363 | test: loss:0.6232992481711684 	 acc:0.5867549668874172 	 lr:0.0001
epoch15: train: loss:0.6271300094186767 	 acc:0.5722314049586776 | test: loss:0.6322077324848301 	 acc:0.5456953642384106 	 lr:0.0001
epoch16: train: loss:0.6273290644992482 	 acc:0.5695867768595041 | test: loss:0.6264164066472591 	 acc:0.5761589403973509 	 lr:0.0001
epoch17: train: loss:0.6500864986151703 	 acc:0.5246280991735537 | test: loss:0.6538607032883246 	 acc:0.5231788079470199 	 lr:0.0001
epoch18: train: loss:0.6304072556219811 	 acc:0.6502479338842975 | test: loss:0.6316876528278881 	 acc:0.623841059602649 	 lr:0.0001
epoch19: train: loss:0.6215130041059383 	 acc:0.5887603305785124 | test: loss:0.6254087145754833 	 acc:0.5695364238410596 	 lr:0.0001
epoch20: train: loss:0.6468956196997776 	 acc:0.6403305785123967 | test: loss:0.645393826787835 	 acc:0.6476821192052981 	 lr:0.0001
epoch21: train: loss:0.6367659294112655 	 acc:0.6476033057851239 | test: loss:0.6295329344983133 	 acc:0.6463576158940397 	 lr:5e-05
epoch22: train: loss:0.6224012819400504 	 acc:0.6482644628099173 | test: loss:0.6229347087689583 	 acc:0.6370860927152318 	 lr:5e-05
epoch23: train: loss:0.6224207818803709 	 acc:0.6363636363636364 | test: loss:0.6223498487314642 	 acc:0.6251655629139072 	 lr:5e-05
epoch24: train: loss:0.6275695804130933 	 acc:0.6502479338842975 | test: loss:0.6243941094701654 	 acc:0.6490066225165563 	 lr:5e-05
epoch25: train: loss:0.6213865059072321 	 acc:0.6261157024793389 | test: loss:0.6208256755443599 	 acc:0.5960264900662252 	 lr:5e-05
epoch26: train: loss:0.684288366097064 	 acc:0.5927272727272728 | test: loss:0.6773353189032599 	 acc:0.6039735099337749 	 lr:5e-05
epoch27: train: loss:0.6389851415846959 	 acc:0.6654545454545454 | test: loss:0.631973928568379 	 acc:0.6688741721854304 	 lr:5e-05
epoch28: train: loss:0.6214225130632889 	 acc:0.6413223140495867 | test: loss:0.6196418120371585 	 acc:0.6476821192052981 	 lr:5e-05
epoch29: train: loss:0.6177123707779183 	 acc:0.6175206611570248 | test: loss:0.6188524633053912 	 acc:0.609271523178808 	 lr:5e-05
epoch30: train: loss:0.6186811526353694 	 acc:0.6601652892561983 | test: loss:0.6199888934362803 	 acc:0.6450331125827815 	 lr:5e-05
epoch31: train: loss:0.6230335090574154 	 acc:0.5583471074380165 | test: loss:0.6274027173092823 	 acc:0.5509933774834437 	 lr:5e-05
epoch32: train: loss:0.6288340137812717 	 acc:0.6631404958677686 | test: loss:0.6222275718158444 	 acc:0.6688741721854304 	 lr:5e-05
epoch33: train: loss:0.6271258804423749 	 acc:0.6558677685950414 | test: loss:0.618620475791148 	 acc:0.6516556291390728 	 lr:5e-05
epoch34: train: loss:0.6130200227430044 	 acc:0.6360330578512396 | test: loss:0.6155471189922055 	 acc:0.6211920529801325 	 lr:5e-05
epoch35: train: loss:0.6159740025543969 	 acc:0.6105785123966943 | test: loss:0.6169537059518675 	 acc:0.590728476821192 	 lr:5e-05
epoch36: train: loss:0.6149048815679944 	 acc:0.6386776859504132 | test: loss:0.6155033807091366 	 acc:0.6172185430463576 	 lr:5e-05
epoch37: train: loss:0.6351295787243804 	 acc:0.6664462809917355 | test: loss:0.6307461142539978 	 acc:0.6728476821192053 	 lr:5e-05
epoch38: train: loss:0.6213949844856893 	 acc:0.6730578512396694 | test: loss:0.6221530608783494 	 acc:0.6649006622516557 	 lr:5e-05
epoch39: train: loss:0.6210788416271368 	 acc:0.5603305785123966 | test: loss:0.63006251679351 	 acc:0.543046357615894 	 lr:5e-05
epoch40: train: loss:0.6171773814957989 	 acc:0.6575206611570248 | test: loss:0.6164659447227883 	 acc:0.6529801324503312 	 lr:5e-05
epoch41: train: loss:0.6197721155615877 	 acc:0.6360330578512396 | test: loss:0.6163629450545406 	 acc:0.6185430463576159 	 lr:2.5e-05
epoch42: train: loss:0.6108173121893702 	 acc:0.644297520661157 | test: loss:0.6141047990085273 	 acc:0.6185430463576159 	 lr:2.5e-05
epoch43: train: loss:0.6238394618625484 	 acc:0.6829752066115703 | test: loss:0.6266212641008643 	 acc:0.6794701986754967 	 lr:2.5e-05
epoch44: train: loss:0.6047393131256104 	 acc:0.6469421487603306 | test: loss:0.6135621572172405 	 acc:0.633112582781457 	 lr:2.5e-05
epoch45: train: loss:0.6109017143761816 	 acc:0.6019834710743802 | test: loss:0.6164643535550857 	 acc:0.5880794701986755 	 lr:2.5e-05
epoch46: train: loss:0.6236368897532628 	 acc:0.6740495867768596 | test: loss:0.6195787760595612 	 acc:0.6781456953642384 	 lr:2.5e-05
epoch47: train: loss:0.6267010469081973 	 acc:0.6717355371900826 | test: loss:0.6252868577344528 	 acc:0.680794701986755 	 lr:2.5e-05
epoch48: train: loss:0.6121226555847925 	 acc:0.6664462809917355 | test: loss:0.6142163760614711 	 acc:0.6529801324503312 	 lr:2.5e-05
epoch49: train: loss:0.6125253183782593 	 acc:0.6717355371900826 | test: loss:0.6146470759878095 	 acc:0.6450331125827815 	 lr:2.5e-05
epoch50: train: loss:0.6110002487750092 	 acc:0.6257851239669422 | test: loss:0.61138133923739 	 acc:0.6132450331125828 	 lr:2.5e-05
epoch51: train: loss:0.6099863489797293 	 acc:0.6505785123966942 | test: loss:0.612915135772023 	 acc:0.6423841059602649 	 lr:2.5e-05
epoch52: train: loss:0.6117047104953734 	 acc:0.6585123966942149 | test: loss:0.6122816205024719 	 acc:0.6476821192052981 	 lr:2.5e-05
epoch53: train: loss:0.6341093963630928 	 acc:0.6690909090909091 | test: loss:0.6285278106367351 	 acc:0.6834437086092715 	 lr:2.5e-05
epoch54: train: loss:0.6107455957428483 	 acc:0.660495867768595 | test: loss:0.6097513287272674 	 acc:0.6384105960264901 	 lr:2.5e-05
epoch55: train: loss:0.6111939243048676 	 acc:0.6717355371900826 | test: loss:0.6116607120494969 	 acc:0.6529801324503312 	 lr:2.5e-05
epoch56: train: loss:0.6089616534138514 	 acc:0.6581818181818182 | test: loss:0.6105061894221022 	 acc:0.6185430463576159 	 lr:2.5e-05
epoch57: train: loss:0.6114035611704361 	 acc:0.607603305785124 | test: loss:0.6124136092647022 	 acc:0.5894039735099338 	 lr:2.5e-05
epoch58: train: loss:0.6541990550293411 	 acc:0.6360330578512396 | test: loss:0.6512760073143915 	 acc:0.6529801324503312 	 lr:2.5e-05
epoch59: train: loss:0.6075706590502715 	 acc:0.6747107438016529 | test: loss:0.6107640849043992 	 acc:0.6582781456953642 	 lr:2.5e-05
epoch60: train: loss:0.6083075412836941 	 acc:0.6492561983471075 | test: loss:0.6110802572294576 	 acc:0.6185430463576159 	 lr:2.5e-05
epoch61: train: loss:0.6096266237566293 	 acc:0.6674380165289256 | test: loss:0.6105923282389609 	 acc:0.6463576158940397 	 lr:1.25e-05
epoch62: train: loss:0.6114952377642482 	 acc:0.6866115702479338 | test: loss:0.612814464474356 	 acc:0.6900662251655629 	 lr:1.25e-05
epoch63: train: loss:0.612566552871515 	 acc:0.6618181818181819 | test: loss:0.6112064093943463 	 acc:0.6529801324503312 	 lr:1.25e-05
epoch64: train: loss:0.6082268862290816 	 acc:0.6773553719008264 | test: loss:0.6099973116489436 	 acc:0.6728476821192053 	 lr:1.25e-05
epoch65: train: loss:0.6113003837766726 	 acc:0.6651239669421488 | test: loss:0.6097513414376619 	 acc:0.6741721854304635 	 lr:1.25e-05
epoch66: train: loss:0.6087167004711371 	 acc:0.6621487603305786 | test: loss:0.6095411231186216 	 acc:0.6397350993377483 	 lr:1.25e-05
epoch67: train: loss:0.6104000216279148 	 acc:0.6628099173553719 | test: loss:0.610015210410617 	 acc:0.6543046357615894 	 lr:1.25e-05
epoch68: train: loss:0.6104650024737208 	 acc:0.6776859504132231 | test: loss:0.612143478567237 	 acc:0.6635761589403973 	 lr:1.25e-05
epoch69: train: loss:0.6100418235447781 	 acc:0.6733884297520661 | test: loss:0.6096446375183712 	 acc:0.6569536423841059 	 lr:1.25e-05
epoch70: train: loss:0.6149611641158742 	 acc:0.6783471074380165 | test: loss:0.6135728442905755 	 acc:0.6794701986754967 	 lr:1.25e-05
epoch71: train: loss:0.6164008809515268 	 acc:0.692892561983471 | test: loss:0.613599800116179 	 acc:0.6900662251655629 	 lr:1.25e-05
epoch72: train: loss:0.6092878613196129 	 acc:0.6614876033057852 | test: loss:0.6071226949723351 	 acc:0.6463576158940397 	 lr:1.25e-05
epoch73: train: loss:0.6081856572923582 	 acc:0.6770247933884298 | test: loss:0.6092649992728075 	 acc:0.6569536423841059 	 lr:1.25e-05
epoch74: train: loss:0.607827578477623 	 acc:0.6452892561983471 | test: loss:0.609105164641576 	 acc:0.6198675496688741 	 lr:1.25e-05
epoch75: train: loss:0.6060621289773421 	 acc:0.6211570247933884 | test: loss:0.6107339176121137 	 acc:0.590728476821192 	 lr:1.25e-05
epoch76: train: loss:0.6110442575147329 	 acc:0.6836363636363636 | test: loss:0.6089958354337326 	 acc:0.6741721854304635 	 lr:1.25e-05
epoch77: train: loss:0.606275978206603 	 acc:0.6390082644628099 | test: loss:0.6083620754298785 	 acc:0.6198675496688741 	 lr:1.25e-05
epoch78: train: loss:0.614161006813207 	 acc:0.6803305785123966 | test: loss:0.6123209309104263 	 acc:0.6847682119205298 	 lr:1.25e-05
epoch79: train: loss:0.606465589211992 	 acc:0.6571900826446281 | test: loss:0.6079129725102558 	 acc:0.6423841059602649 	 lr:6.25e-06
epoch80: train: loss:0.6081788949138862 	 acc:0.6575206611570248 | test: loss:0.6076988782314275 	 acc:0.6463576158940397 	 lr:6.25e-06
epoch81: train: loss:0.6108733207529241 	 acc:0.6614876033057852 | test: loss:0.6084673377062311 	 acc:0.6635761589403973 	 lr:6.25e-06
epoch82: train: loss:0.6132639282202917 	 acc:0.6859504132231405 | test: loss:0.6128317791105106 	 acc:0.6781456953642384 	 lr:6.25e-06
epoch83: train: loss:0.6139419587781607 	 acc:0.675702479338843 | test: loss:0.6109449240545564 	 acc:0.6794701986754967 	 lr:6.25e-06
epoch84: train: loss:0.6133712366001665 	 acc:0.6836363636363636 | test: loss:0.6107686513307079 	 acc:0.6781456953642384 	 lr:6.25e-06
epoch85: train: loss:0.6051226881909961 	 acc:0.6895867768595041 | test: loss:0.6084552212266733 	 acc:0.6701986754966888 	 lr:3.125e-06
epoch86: train: loss:0.6060351936876281 	 acc:0.6770247933884298 | test: loss:0.6081285385106573 	 acc:0.6556291390728477 	 lr:3.125e-06
epoch87: train: loss:0.6097852219628893 	 acc:0.6816528925619835 | test: loss:0.6091819026612288 	 acc:0.6741721854304635 	 lr:3.125e-06
epoch88: train: loss:0.6059614140928284 	 acc:0.6809917355371901 | test: loss:0.607770533988018 	 acc:0.671523178807947 	 lr:3.125e-06
epoch89: train: loss:0.6057865514439985 	 acc:0.6766942148760331 | test: loss:0.6072727292578741 	 acc:0.6688741721854304 	 lr:3.125e-06
epoch90: train: loss:0.6132973642585692 	 acc:0.6889256198347108 | test: loss:0.610065905226777 	 acc:0.6874172185430464 	 lr:3.125e-06
epoch91: train: loss:0.6079006765696628 	 acc:0.6628099173553719 | test: loss:0.6067936253863455 	 acc:0.6490066225165563 	 lr:1.5625e-06
epoch92: train: loss:0.604725052955722 	 acc:0.6826446280991736 | test: loss:0.6076433506233013 	 acc:0.6649006622516557 	 lr:1.5625e-06
epoch93: train: loss:0.6132865457692422 	 acc:0.6700826446280992 | test: loss:0.6078504233960285 	 acc:0.6675496688741722 	 lr:1.5625e-06
epoch94: train: loss:0.6099571097980846 	 acc:0.6747107438016529 | test: loss:0.6079584130388223 	 acc:0.6701986754966888 	 lr:1.5625e-06
epoch95: train: loss:0.6088402492349798 	 acc:0.6641322314049587 | test: loss:0.6076332863592944 	 acc:0.6662251655629139 	 lr:1.5625e-06
epoch96: train: loss:0.6107741583280327 	 acc:0.6654545454545454 | test: loss:0.6083776076108415 	 acc:0.6701986754966888 	 lr:1.5625e-06
epoch97: train: loss:0.6091104040067058 	 acc:0.6776859504132231 | test: loss:0.6080295280904959 	 acc:0.6688741721854304 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_3_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_3_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6707252249638896 	 acc:0.5213223140495867 | test: loss:0.6653171427202541 	 acc:0.5218543046357615 	 lr:0.0001
epoch1: train: loss:0.7273433277823709 	 acc:0.5216528925619834 | test: loss:0.7194717140387226 	 acc:0.5258278145695364 	 lr:0.0001
epoch2: train: loss:0.6529195377452315 	 acc:0.567603305785124 | test: loss:0.6492375360419419 	 acc:0.5549668874172186 	 lr:0.0001
epoch3: train: loss:0.6438885247805887 	 acc:0.5385123966942149 | test: loss:0.6400038647335886 	 acc:0.5443708609271524 	 lr:0.0001
epoch4: train: loss:0.6553922896345785 	 acc:0.6155371900826446 | test: loss:0.6494558398297291 	 acc:0.6079470198675496 	 lr:0.0001
epoch5: train: loss:0.6375242733167222 	 acc:0.5831404958677686 | test: loss:0.6340215415354596 	 acc:0.590728476821192 	 lr:0.0001
epoch6: train: loss:0.6349156826980843 	 acc:0.5553719008264463 | test: loss:0.6324328958593457 	 acc:0.5576158940397351 	 lr:0.0001
epoch7: train: loss:0.6489309608049629 	 acc:0.6185123966942149 | test: loss:0.6442577635215607 	 acc:0.6344370860927152 	 lr:0.0001
epoch8: train: loss:0.6424135758463017 	 acc:0.5322314049586777 | test: loss:0.6488814437626214 	 acc:0.5231788079470199 	 lr:0.0001
epoch9: train: loss:0.6536461475072813 	 acc:0.524297520661157 | test: loss:0.654420393981681 	 acc:0.5231788079470199 	 lr:0.0001
epoch10: train: loss:0.6452509213676138 	 acc:0.5266115702479339 | test: loss:0.6481314025967326 	 acc:0.5245033112582781 	 lr:0.0001
epoch11: train: loss:0.6315711550476137 	 acc:0.643305785123967 | test: loss:0.6315482774317659 	 acc:0.6490066225165563 	 lr:0.0001
epoch12: train: loss:0.6707335859093785 	 acc:0.6145454545454545 | test: loss:0.654286648342941 	 acc:0.633112582781457 	 lr:0.0001
epoch13: train: loss:0.6246879091735714 	 acc:0.5590082644628099 | test: loss:0.6294011824967846 	 acc:0.5536423841059602 	 lr:0.0001
epoch14: train: loss:0.6192390942179468 	 acc:0.6251239669421488 | test: loss:0.6159968705366778 	 acc:0.6198675496688741 	 lr:0.0001
epoch15: train: loss:0.6281907867006034 	 acc:0.6023140495867768 | test: loss:0.6334208444254288 	 acc:0.5854304635761589 	 lr:0.0001
epoch16: train: loss:0.6165263268770265 	 acc:0.6198347107438017 | test: loss:0.6173940399624654 	 acc:0.6105960264900663 	 lr:0.0001
epoch17: train: loss:0.612063519402969 	 acc:0.6264462809917355 | test: loss:0.6178544297123587 	 acc:0.614569536423841 	 lr:0.0001
epoch18: train: loss:0.7312448574097689 	 acc:0.5444628099173554 | test: loss:0.7413260289375355 	 acc:0.5443708609271524 	 lr:0.0001
epoch19: train: loss:0.6219791028322267 	 acc:0.6066115702479339 | test: loss:0.6242267285751191 	 acc:0.5788079470198676 	 lr:0.0001
epoch20: train: loss:0.6216121546296048 	 acc:0.660495867768595 | test: loss:0.6223425558071263 	 acc:0.6596026490066225 	 lr:0.0001
epoch21: train: loss:0.6185362946691592 	 acc:0.6499173553719009 | test: loss:0.6160107051299897 	 acc:0.6556291390728477 	 lr:5e-05
epoch22: train: loss:0.6139912387950361 	 acc:0.6710743801652893 | test: loss:0.6123422153738161 	 acc:0.6635761589403973 	 lr:5e-05
epoch23: train: loss:0.625345705264856 	 acc:0.6654545454545454 | test: loss:0.6223295830732939 	 acc:0.671523178807947 	 lr:5e-05
epoch24: train: loss:0.6150807802342186 	 acc:0.6611570247933884 | test: loss:0.6140649686585988 	 acc:0.6556291390728477 	 lr:5e-05
epoch25: train: loss:0.6109159441427751 	 acc:0.6072727272727273 | test: loss:0.616585293116159 	 acc:0.5814569536423841 	 lr:5e-05
epoch26: train: loss:0.6889880013859961 	 acc:0.5894214876033058 | test: loss:0.67488432822638 	 acc:0.6264900662251656 	 lr:5e-05
epoch27: train: loss:0.6249717381374895 	 acc:0.6833057851239669 | test: loss:0.6209854924915642 	 acc:0.6781456953642384 	 lr:5e-05
epoch28: train: loss:0.6091233844402408 	 acc:0.6300826446280992 | test: loss:0.6081318953968832 	 acc:0.6278145695364239 	 lr:5e-05
epoch29: train: loss:0.6103734665074624 	 acc:0.6608264462809917 | test: loss:0.6085363307536044 	 acc:0.6768211920529801 	 lr:5e-05
epoch30: train: loss:0.6120486172368704 	 acc:0.68 | test: loss:0.6154517401922618 	 acc:0.6596026490066225 	 lr:5e-05
epoch31: train: loss:0.6197699635679071 	 acc:0.5609917355371901 | test: loss:0.6291428597557623 	 acc:0.5470198675496689 	 lr:5e-05
epoch32: train: loss:0.6559202561693743 	 acc:0.6386776859504132 | test: loss:0.6444123327337353 	 acc:0.6543046357615894 	 lr:5e-05
epoch33: train: loss:0.6059495527882222 	 acc:0.6492561983471075 | test: loss:0.6054478501641987 	 acc:0.6251655629139072 	 lr:5e-05
epoch34: train: loss:0.6018909775521144 	 acc:0.6690909090909091 | test: loss:0.6058088731292067 	 acc:0.6503311258278146 	 lr:5e-05
epoch35: train: loss:0.606675321110024 	 acc:0.6558677685950414 | test: loss:0.6050536839377801 	 acc:0.6490066225165563 	 lr:5e-05
epoch36: train: loss:0.6036947471642297 	 acc:0.6304132231404959 | test: loss:0.6075895080503249 	 acc:0.6211920529801325 	 lr:5e-05
epoch37: train: loss:0.6233018523996526 	 acc:0.6889256198347108 | test: loss:0.6184240079083979 	 acc:0.6794701986754967 	 lr:5e-05
epoch38: train: loss:0.6054272167150639 	 acc:0.6816528925619835 | test: loss:0.6069764746735428 	 acc:0.6834437086092715 	 lr:5e-05
epoch39: train: loss:0.6110391265301665 	 acc:0.5824793388429752 | test: loss:0.6171565961364089 	 acc:0.5682119205298013 	 lr:5e-05
epoch40: train: loss:0.6074208036927152 	 acc:0.6925619834710743 | test: loss:0.6052710187356204 	 acc:0.6887417218543046 	 lr:5e-05
epoch41: train: loss:0.6121980288009014 	 acc:0.6674380165289256 | test: loss:0.6045998645144582 	 acc:0.6649006622516557 	 lr:5e-05
epoch42: train: loss:0.6130191025852172 	 acc:0.6862809917355371 | test: loss:0.6130886898135507 	 acc:0.6940397350993377 	 lr:5e-05
epoch43: train: loss:0.6179015729052961 	 acc:0.696198347107438 | test: loss:0.6207851230703443 	 acc:0.6781456953642384 	 lr:5e-05
epoch44: train: loss:0.6200365566419176 	 acc:0.6925619834710743 | test: loss:0.6214944965792019 	 acc:0.6821192052980133 	 lr:5e-05
epoch45: train: loss:0.5996018744894296 	 acc:0.6677685950413224 | test: loss:0.5997420543866442 	 acc:0.6556291390728477 	 lr:5e-05
epoch46: train: loss:0.6313376859199902 	 acc:0.6750413223140496 | test: loss:0.6223870494507796 	 acc:0.6927152317880795 	 lr:5e-05
epoch47: train: loss:0.6177014388919861 	 acc:0.5639669421487603 | test: loss:0.6250897843316691 	 acc:0.5456953642384106 	 lr:5e-05
epoch48: train: loss:0.640956280527036 	 acc:0.656198347107438 | test: loss:0.6328613990190013 	 acc:0.6728476821192053 	 lr:5e-05
epoch49: train: loss:0.6097213737432622 	 acc:0.7044628099173553 | test: loss:0.6123970140684519 	 acc:0.6887417218543046 	 lr:5e-05
epoch50: train: loss:0.6051164338214339 	 acc:0.6816528925619835 | test: loss:0.604132741413369 	 acc:0.6754966887417219 	 lr:5e-05
epoch51: train: loss:0.6068282922831448 	 acc:0.687603305785124 | test: loss:0.6033950058829706 	 acc:0.6980132450331126 	 lr:5e-05
epoch52: train: loss:0.5948675698485256 	 acc:0.6760330578512397 | test: loss:0.5968215666069889 	 acc:0.6582781456953642 	 lr:2.5e-05
epoch53: train: loss:0.6080776474495565 	 acc:0.6905785123966942 | test: loss:0.603875824394605 	 acc:0.6980132450331126 	 lr:2.5e-05
epoch54: train: loss:0.5961444841337598 	 acc:0.6753719008264463 | test: loss:0.5981859657148652 	 acc:0.6582781456953642 	 lr:2.5e-05
epoch55: train: loss:0.5981085346158871 	 acc:0.6981818181818182 | test: loss:0.5988332824201773 	 acc:0.6728476821192053 	 lr:2.5e-05
epoch56: train: loss:0.5950388741690266 	 acc:0.692892561983471 | test: loss:0.5989244866844834 	 acc:0.6701986754966888 	 lr:2.5e-05
epoch57: train: loss:0.5998646256155219 	 acc:0.6271074380165289 | test: loss:0.6016509566086018 	 acc:0.6185430463576159 	 lr:2.5e-05
epoch58: train: loss:0.6468577481695443 	 acc:0.6492561983471075 | test: loss:0.6385291630069152 	 acc:0.6596026490066225 	 lr:2.5e-05
epoch59: train: loss:0.5943943858737788 	 acc:0.7014876033057851 | test: loss:0.5978039452571743 	 acc:0.6794701986754967 	 lr:1.25e-05
epoch60: train: loss:0.5934374151742163 	 acc:0.6796694214876033 | test: loss:0.5962932269304794 	 acc:0.6543046357615894 	 lr:1.25e-05
epoch61: train: loss:0.6007488191620377 	 acc:0.6965289256198347 | test: loss:0.5992211802116293 	 acc:0.6900662251655629 	 lr:1.25e-05
epoch62: train: loss:0.5968845918947014 	 acc:0.7057851239669422 | test: loss:0.5996711751483134 	 acc:0.6993377483443709 	 lr:1.25e-05
epoch63: train: loss:0.5992180800043847 	 acc:0.6796694214876033 | test: loss:0.5988756042442575 	 acc:0.6768211920529801 	 lr:1.25e-05
epoch64: train: loss:0.5925811886393334 	 acc:0.6981818181818182 | test: loss:0.5964450702761972 	 acc:0.6847682119205298 	 lr:1.25e-05
epoch65: train: loss:0.5981254000112045 	 acc:0.6895867768595041 | test: loss:0.5967100708689911 	 acc:0.6900662251655629 	 lr:1.25e-05
epoch66: train: loss:0.5961051518857972 	 acc:0.6998347107438017 | test: loss:0.5979473867953219 	 acc:0.686092715231788 	 lr:1.25e-05
epoch67: train: loss:0.5989358884835047 	 acc:0.7024793388429752 | test: loss:0.5980539136375023 	 acc:0.686092715231788 	 lr:6.25e-06
epoch68: train: loss:0.5968593167076426 	 acc:0.6985123966942148 | test: loss:0.5978504401958541 	 acc:0.6834437086092715 	 lr:6.25e-06
epoch69: train: loss:0.5973533994698328 	 acc:0.6846280991735537 | test: loss:0.596343775142897 	 acc:0.6821192052980133 	 lr:6.25e-06
epoch70: train: loss:0.5939026028656762 	 acc:0.6852892561983471 | test: loss:0.597036605560227 	 acc:0.671523178807947 	 lr:6.25e-06
epoch71: train: loss:0.59372417753393 	 acc:0.6909090909090909 | test: loss:0.5959190974172377 	 acc:0.6781456953642384 	 lr:6.25e-06
epoch72: train: loss:0.5953724047369208 	 acc:0.675702479338843 | test: loss:0.5954917488508666 	 acc:0.6622516556291391 	 lr:6.25e-06
epoch73: train: loss:0.5970757125625925 	 acc:0.6971900826446281 | test: loss:0.5961479280958113 	 acc:0.6874172185430464 	 lr:6.25e-06
epoch74: train: loss:0.6018287126880046 	 acc:0.6985123966942148 | test: loss:0.6002753511169888 	 acc:0.6980132450331126 	 lr:6.25e-06
epoch75: train: loss:0.5940251148633721 	 acc:0.7024793388429752 | test: loss:0.5970231429630557 	 acc:0.680794701986755 	 lr:6.25e-06
epoch76: train: loss:0.5967875347255676 	 acc:0.6935537190082645 | test: loss:0.5956490378506136 	 acc:0.6781456953642384 	 lr:6.25e-06
epoch77: train: loss:0.5940349354231653 	 acc:0.6902479338842975 | test: loss:0.5960777123242814 	 acc:0.6821192052980133 	 lr:6.25e-06
epoch78: train: loss:0.5965108946729297 	 acc:0.6895867768595041 | test: loss:0.595838711198592 	 acc:0.6781456953642384 	 lr:6.25e-06
epoch79: train: loss:0.5954533096384411 	 acc:0.6882644628099174 | test: loss:0.5965574921361658 	 acc:0.680794701986755 	 lr:3.125e-06
epoch80: train: loss:0.5977920437647292 	 acc:0.691900826446281 | test: loss:0.5974998415700647 	 acc:0.6900662251655629 	 lr:3.125e-06
epoch81: train: loss:0.5984034430094002 	 acc:0.6975206611570248 | test: loss:0.5977028740952347 	 acc:0.686092715231788 	 lr:3.125e-06
epoch82: train: loss:0.5998100060470833 	 acc:0.696198347107438 | test: loss:0.5989712885673473 	 acc:0.6887417218543046 	 lr:3.125e-06
epoch83: train: loss:0.5971356915048331 	 acc:0.6862809917355371 | test: loss:0.5970148534964252 	 acc:0.6887417218543046 	 lr:3.125e-06
epoch84: train: loss:0.599629814230706 	 acc:0.6991735537190082 | test: loss:0.5983301410611892 	 acc:0.6821192052980133 	 lr:3.125e-06
epoch85: train: loss:0.5904576564229225 	 acc:0.7011570247933885 | test: loss:0.596573426865584 	 acc:0.6847682119205298 	 lr:1.5625e-06
epoch86: train: loss:0.5947434537075768 	 acc:0.699504132231405 | test: loss:0.5967914204723788 	 acc:0.6887417218543046 	 lr:1.5625e-06
epoch87: train: loss:0.5967022114548801 	 acc:0.703801652892562 | test: loss:0.5974055844426944 	 acc:0.6940397350993377 	 lr:1.5625e-06
epoch88: train: loss:0.5918808920127301 	 acc:0.7031404958677686 | test: loss:0.5962811185034695 	 acc:0.6913907284768211 	 lr:1.5625e-06
epoch89: train: loss:0.5952827746020861 	 acc:0.6971900826446281 | test: loss:0.5963837928329871 	 acc:0.6900662251655629 	 lr:1.5625e-06
epoch90: train: loss:0.5981112859465859 	 acc:0.707107438016529 | test: loss:0.5969545347801107 	 acc:0.6927152317880795 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_4_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_4_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6641806082489077 	 acc:0.5216528925619834 | test: loss:0.6582041299106269 	 acc:0.5218543046357615 	 lr:0.0001
epoch1: train: loss:0.7009428367930011 	 acc:0.5517355371900826 | test: loss:0.6912078042693486 	 acc:0.5642384105960265 	 lr:0.0001
epoch2: train: loss:0.6663823123608739 	 acc:0.6095867768595041 | test: loss:0.655717717338082 	 acc:0.6105960264900663 	 lr:0.0001
epoch3: train: loss:0.6463319269881761 	 acc:0.5289256198347108 | test: loss:0.6442399353381024 	 acc:0.5258278145695364 	 lr:0.0001
epoch4: train: loss:0.6463491298147469 	 acc:0.5904132231404958 | test: loss:0.641170932046625 	 acc:0.5774834437086093 	 lr:0.0001
epoch5: train: loss:0.6373182062669234 	 acc:0.5500826446280992 | test: loss:0.632976944004463 	 acc:0.5483443708609271 	 lr:0.0001
epoch6: train: loss:0.6351950627910204 	 acc:0.5527272727272727 | test: loss:0.6306845158930646 	 acc:0.5509933774834437 	 lr:0.0001
epoch7: train: loss:0.6683410481579047 	 acc:0.6033057851239669 | test: loss:0.6636644638137312 	 acc:0.6317880794701987 	 lr:0.0001
epoch8: train: loss:0.6435617302074905 	 acc:0.528595041322314 | test: loss:0.6508683736750621 	 acc:0.5231788079470199 	 lr:0.0001
epoch9: train: loss:0.6498208037683787 	 acc:0.5249586776859504 | test: loss:0.6507452745311307 	 acc:0.5231788079470199 	 lr:0.0001
epoch10: train: loss:0.6270491383095418 	 acc:0.564297520661157 | test: loss:0.6268851787838715 	 acc:0.552317880794702 	 lr:0.0001
epoch11: train: loss:0.6692854078348018 	 acc:0.6241322314049587 | test: loss:0.6682444829025016 	 acc:0.6357615894039735 	 lr:0.0001
epoch12: train: loss:0.6227348536499275 	 acc:0.617190082644628 | test: loss:0.6196732273164964 	 acc:0.5814569536423841 	 lr:0.0001
epoch13: train: loss:0.6261674015581116 	 acc:0.5507438016528926 | test: loss:0.6318329917674033 	 acc:0.5350993377483444 	 lr:0.0001
epoch14: train: loss:0.6305903927550828 	 acc:0.6542148760330578 | test: loss:0.6262802821910934 	 acc:0.686092715231788 	 lr:0.0001
epoch15: train: loss:0.6199737449519891 	 acc:0.5864462809917356 | test: loss:0.6253032003017451 	 acc:0.5761589403973509 	 lr:0.0001
epoch16: train: loss:0.6138415089323501 	 acc:0.6138842975206612 | test: loss:0.6130146632131361 	 acc:0.5947019867549669 	 lr:0.0001
epoch17: train: loss:0.6090300079810718 	 acc:0.6214876033057851 | test: loss:0.6130552055819934 	 acc:0.609271523178808 	 lr:0.0001
epoch18: train: loss:0.7331174354907894 	 acc:0.5441322314049587 | test: loss:0.7454744704511781 	 acc:0.5417218543046357 	 lr:0.0001
epoch19: train: loss:0.6189303417442259 	 acc:0.6231404958677685 | test: loss:0.61568970103927 	 acc:0.6066225165562914 	 lr:0.0001
epoch20: train: loss:0.6204580852611006 	 acc:0.6710743801652893 | test: loss:0.6193681115346239 	 acc:0.6649006622516557 	 lr:0.0001
epoch21: train: loss:0.6293093458877123 	 acc:0.6462809917355372 | test: loss:0.6170071222924238 	 acc:0.6582781456953642 	 lr:0.0001
epoch22: train: loss:0.6224834895725092 	 acc:0.6717355371900826 | test: loss:0.618526216532221 	 acc:0.6675496688741722 	 lr:0.0001
epoch23: train: loss:0.6202749411330736 	 acc:0.6730578512396694 | test: loss:0.6142144726601657 	 acc:0.6768211920529801 	 lr:5e-05
epoch24: train: loss:0.6168942113750238 	 acc:0.6790082644628099 | test: loss:0.613949676144202 	 acc:0.6966887417218544 	 lr:5e-05
epoch25: train: loss:0.607506335175727 	 acc:0.6095867768595041 | test: loss:0.6093790685104219 	 acc:0.5894039735099338 	 lr:5e-05
epoch26: train: loss:0.684948275227192 	 acc:0.5927272727272728 | test: loss:0.6662190314949743 	 acc:0.623841059602649 	 lr:5e-05
epoch27: train: loss:0.6142619972780716 	 acc:0.6952066115702479 | test: loss:0.6100565032453726 	 acc:0.6940397350993377 	 lr:5e-05
epoch28: train: loss:0.6021572098850219 	 acc:0.6310743801652893 | test: loss:0.6056022053522779 	 acc:0.6132450331125828 	 lr:5e-05
epoch29: train: loss:0.6078546986304039 	 acc:0.6790082644628099 | test: loss:0.6033787934195917 	 acc:0.6913907284768211 	 lr:5e-05
epoch30: train: loss:0.6071869538441177 	 acc:0.7021487603305785 | test: loss:0.6093202055684778 	 acc:0.6728476821192053 	 lr:5e-05
epoch31: train: loss:0.6121936195349891 	 acc:0.5801652892561984 | test: loss:0.6222971491466295 	 acc:0.5629139072847682 	 lr:5e-05
epoch32: train: loss:0.6497438164978974 	 acc:0.6446280991735537 | test: loss:0.6422335325487402 	 acc:0.6529801324503312 	 lr:5e-05
epoch33: train: loss:0.5985692319397099 	 acc:0.6631404958677686 | test: loss:0.5955127591328905 	 acc:0.6569536423841059 	 lr:5e-05
epoch34: train: loss:0.595687515440066 	 acc:0.6823140495867769 | test: loss:0.5984505194701896 	 acc:0.6609271523178808 	 lr:5e-05
epoch35: train: loss:0.5991571933375902 	 acc:0.675702479338843 | test: loss:0.5960205622066725 	 acc:0.680794701986755 	 lr:5e-05
epoch36: train: loss:0.5990008988656288 	 acc:0.6700826446280992 | test: loss:0.5948105098395947 	 acc:0.6768211920529801 	 lr:5e-05
epoch37: train: loss:0.621615217973378 	 acc:0.6866115702479338 | test: loss:0.6156367676937028 	 acc:0.6980132450331126 	 lr:5e-05
epoch38: train: loss:0.5991569178163513 	 acc:0.6955371900826446 | test: loss:0.5976372730653018 	 acc:0.6940397350993377 	 lr:5e-05
epoch39: train: loss:0.6047447978563545 	 acc:0.5976859504132231 | test: loss:0.6081366576106343 	 acc:0.5827814569536424 	 lr:5e-05
epoch40: train: loss:0.6120792507534185 	 acc:0.692892561983471 | test: loss:0.6027108539019199 	 acc:0.7152317880794702 	 lr:5e-05
epoch41: train: loss:0.6059039942292143 	 acc:0.6545454545454545 | test: loss:0.5945631945370049 	 acc:0.6516556291390728 	 lr:5e-05
epoch42: train: loss:0.6199071328107976 	 acc:0.6942148760330579 | test: loss:0.6170255336540424 	 acc:0.695364238410596 	 lr:5e-05
epoch43: train: loss:0.6062113619441828 	 acc:0.7160330578512397 | test: loss:0.6078792234919719 	 acc:0.7033112582781457 	 lr:5e-05
epoch44: train: loss:0.6311977643414962 	 acc:0.6793388429752066 | test: loss:0.6325540978387492 	 acc:0.6569536423841059 	 lr:5e-05
epoch45: train: loss:0.593459343634361 	 acc:0.6634710743801653 | test: loss:0.5916224193099319 	 acc:0.6450331125827815 	 lr:5e-05
epoch46: train: loss:0.6569316776133766 	 acc:0.643305785123967 | test: loss:0.6421733540414974 	 acc:0.6701986754966888 	 lr:5e-05
epoch47: train: loss:0.605911746813246 	 acc:0.5907438016528925 | test: loss:0.6134840246067931 	 acc:0.5735099337748344 	 lr:5e-05
epoch48: train: loss:0.6246577162387943 	 acc:0.6750413223140496 | test: loss:0.6197894441370932 	 acc:0.6874172185430464 	 lr:5e-05
epoch49: train: loss:0.6054370232061906 	 acc:0.707107438016529 | test: loss:0.605138061536069 	 acc:0.7006622516556291 	 lr:5e-05
epoch50: train: loss:0.5923492552623276 	 acc:0.6925619834710743 | test: loss:0.5939858237639168 	 acc:0.6688741721854304 	 lr:5e-05
epoch51: train: loss:0.5988966991487613 	 acc:0.7087603305785124 | test: loss:0.5934505049756031 	 acc:0.7006622516556291 	 lr:5e-05
epoch52: train: loss:0.5902150646714139 	 acc:0.6968595041322314 | test: loss:0.5913679622656463 	 acc:0.6900662251655629 	 lr:2.5e-05
epoch53: train: loss:0.6048784548191984 	 acc:0.6998347107438017 | test: loss:0.5981667047304823 	 acc:0.7192052980132451 	 lr:2.5e-05
epoch54: train: loss:0.5917913553931496 	 acc:0.6862809917355371 | test: loss:0.5905485938716408 	 acc:0.6768211920529801 	 lr:2.5e-05
epoch55: train: loss:0.595281352858898 	 acc:0.7130578512396695 | test: loss:0.592480227173559 	 acc:0.7059602649006622 	 lr:2.5e-05
epoch56: train: loss:0.5883700550292149 	 acc:0.7034710743801653 | test: loss:0.5904088306111216 	 acc:0.6847682119205298 	 lr:2.5e-05
epoch57: train: loss:0.592472339188757 	 acc:0.6459504132231405 | test: loss:0.593825461532896 	 acc:0.6304635761589404 	 lr:2.5e-05
epoch58: train: loss:0.6469506536042394 	 acc:0.648595041322314 | test: loss:0.6347532638650857 	 acc:0.6649006622516557 	 lr:2.5e-05
epoch59: train: loss:0.5857832501151345 	 acc:0.7143801652892562 | test: loss:0.5871117384228486 	 acc:0.6966887417218544 	 lr:2.5e-05
epoch60: train: loss:0.5891873031214249 	 acc:0.6780165289256198 | test: loss:0.5902234563764357 	 acc:0.6609271523178808 	 lr:2.5e-05
epoch61: train: loss:0.601159548128932 	 acc:0.7064462809917356 | test: loss:0.5951793245132396 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch62: train: loss:0.582700183293051 	 acc:0.7028099173553719 | test: loss:0.5896434835250804 	 acc:0.671523178807947 	 lr:2.5e-05
epoch63: train: loss:0.6081765649535439 	 acc:0.7057851239669422 | test: loss:0.6016096386688434 	 acc:0.7099337748344371 	 lr:2.5e-05
epoch64: train: loss:0.5858756740625239 	 acc:0.6975206611570248 | test: loss:0.5868975336188512 	 acc:0.6913907284768211 	 lr:2.5e-05
epoch65: train: loss:0.590599700616411 	 acc:0.6968595041322314 | test: loss:0.5892506389428448 	 acc:0.695364238410596 	 lr:2.5e-05
epoch66: train: loss:0.5942577942146743 	 acc:0.715702479338843 | test: loss:0.593971897197875 	 acc:0.7033112582781457 	 lr:2.5e-05
epoch67: train: loss:0.589386208609116 	 acc:0.6469421487603306 | test: loss:0.5926407330872997 	 acc:0.6291390728476821 	 lr:2.5e-05
epoch68: train: loss:0.5879216101149882 	 acc:0.7097520661157025 | test: loss:0.5870047051385539 	 acc:0.7072847682119205 	 lr:2.5e-05
epoch69: train: loss:0.5867570671759361 	 acc:0.6760330578512397 | test: loss:0.5859293244532402 	 acc:0.6635761589403973 	 lr:2.5e-05
epoch70: train: loss:0.5972346390377391 	 acc:0.7110743801652892 | test: loss:0.5933140913382271 	 acc:0.7006622516556291 	 lr:2.5e-05
epoch71: train: loss:0.5936535797631445 	 acc:0.7123966942148761 | test: loss:0.5870528616652584 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch72: train: loss:0.5927663381631709 	 acc:0.715702479338843 | test: loss:0.5897429778086428 	 acc:0.713907284768212 	 lr:2.5e-05
epoch73: train: loss:0.6147784583627685 	 acc:0.6895867768595041 | test: loss:0.6057254323106728 	 acc:0.7006622516556291 	 lr:2.5e-05
epoch74: train: loss:0.5854514194125972 	 acc:0.6694214876033058 | test: loss:0.588905872888123 	 acc:0.6476821192052981 	 lr:2.5e-05
epoch75: train: loss:0.5837408163330772 	 acc:0.660495867768595 | test: loss:0.5906317399037595 	 acc:0.6304635761589404 	 lr:2.5e-05
epoch76: train: loss:0.5889064687539723 	 acc:0.7137190082644628 | test: loss:0.5880504334209771 	 acc:0.7086092715231788 	 lr:1.25e-05
epoch77: train: loss:0.5836775571452685 	 acc:0.6760330578512397 | test: loss:0.5867109042129769 	 acc:0.6675496688741722 	 lr:1.25e-05
epoch78: train: loss:0.5939660586207366 	 acc:0.7090909090909091 | test: loss:0.5890310274054672 	 acc:0.7152317880794702 	 lr:1.25e-05
epoch79: train: loss:0.593008126660812 	 acc:0.7140495867768595 | test: loss:0.5912160302629534 	 acc:0.7086092715231788 	 lr:1.25e-05
epoch80: train: loss:0.5862718188467104 	 acc:0.7107438016528925 | test: loss:0.5863235859681439 	 acc:0.7072847682119205 	 lr:1.25e-05
epoch81: train: loss:0.5862787228182328 	 acc:0.6991735537190082 | test: loss:0.5863709066877302 	 acc:0.7006622516556291 	 lr:1.25e-05
epoch82: train: loss:0.5880148534735372 	 acc:0.7117355371900826 | test: loss:0.5855891324036958 	 acc:0.7205298013245033 	 lr:6.25e-06
epoch83: train: loss:0.5852242023097582 	 acc:0.7054545454545454 | test: loss:0.58397101921751 	 acc:0.7072847682119205 	 lr:6.25e-06
epoch84: train: loss:0.5919719889144267 	 acc:0.71900826446281 | test: loss:0.5874936334344725 	 acc:0.7192052980132451 	 lr:6.25e-06
epoch85: train: loss:0.5869909576936202 	 acc:0.7262809917355372 | test: loss:0.5895162552397772 	 acc:0.7165562913907285 	 lr:6.25e-06
epoch86: train: loss:0.5880681831777589 	 acc:0.7262809917355372 | test: loss:0.5870456651346573 	 acc:0.7205298013245033 	 lr:6.25e-06
epoch87: train: loss:0.5839493844331789 	 acc:0.715702479338843 | test: loss:0.5853887717455428 	 acc:0.713907284768212 	 lr:6.25e-06
epoch88: train: loss:0.5802857179484091 	 acc:0.7031404958677686 | test: loss:0.5838694919024082 	 acc:0.6821192052980133 	 lr:6.25e-06
epoch89: train: loss:0.5823205587883626 	 acc:0.716694214876033 | test: loss:0.5842090712477829 	 acc:0.7165562913907285 	 lr:6.25e-06
epoch90: train: loss:0.5876370584669192 	 acc:0.7153719008264463 | test: loss:0.5858039537802437 	 acc:0.7099337748344371 	 lr:6.25e-06
epoch91: train: loss:0.5885803872100578 	 acc:0.7229752066115702 | test: loss:0.5860518955237028 	 acc:0.7152317880794702 	 lr:6.25e-06
epoch92: train: loss:0.5808322125624034 	 acc:0.7054545454545454 | test: loss:0.5851574442244524 	 acc:0.6980132450331126 	 lr:6.25e-06
epoch93: train: loss:0.5858742345857226 	 acc:0.6885950413223141 | test: loss:0.5848523267057558 	 acc:0.6768211920529801 	 lr:6.25e-06
epoch94: train: loss:0.5825103666762675 	 acc:0.6988429752066115 | test: loss:0.5850410456688988 	 acc:0.6821192052980133 	 lr:6.25e-06
epoch95: train: loss:0.5855829406966848 	 acc:0.7173553719008264 | test: loss:0.586697408616148 	 acc:0.7125827814569536 	 lr:3.125e-06
epoch96: train: loss:0.5845513392874032 	 acc:0.7034710743801653 | test: loss:0.584430811263078 	 acc:0.7059602649006622 	 lr:3.125e-06
epoch97: train: loss:0.5837291145521747 	 acc:0.7097520661157025 | test: loss:0.5846180771360334 	 acc:0.7072847682119205 	 lr:3.125e-06
epoch98: train: loss:0.5830623113813479 	 acc:0.7120661157024794 | test: loss:0.5849044649806243 	 acc:0.7086092715231788 	 lr:3.125e-06
epoch99: train: loss:0.5839949904985664 	 acc:0.72 | test: loss:0.5855813646158635 	 acc:0.7125827814569536 	 lr:3.125e-06
epoch100: train: loss:0.5809760973079146 	 acc:0.7127272727272728 | test: loss:0.5848459535876647 	 acc:0.7099337748344371 	 lr:3.125e-06
epoch101: train: loss:0.5817631709871214 	 acc:0.7216528925619835 | test: loss:0.5851285943132363 	 acc:0.7112582781456953 	 lr:1.5625e-06
epoch102: train: loss:0.5829883740559098 	 acc:0.7120661157024794 | test: loss:0.5853379861408512 	 acc:0.7125827814569536 	 lr:1.5625e-06
epoch103: train: loss:0.5860082810772352 	 acc:0.7130578512396695 | test: loss:0.5856455105029984 	 acc:0.7086092715231788 	 lr:1.5625e-06
epoch104: train: loss:0.5834881569137258 	 acc:0.7034710743801653 | test: loss:0.5849375765844687 	 acc:0.695364238410596 	 lr:1.5625e-06
epoch105: train: loss:0.5809794034051501 	 acc:0.7305785123966942 | test: loss:0.5858222389852764 	 acc:0.7165562913907285 	 lr:1.5625e-06
epoch106: train: loss:0.5878388047415363 	 acc:0.7097520661157025 | test: loss:0.5853748011273264 	 acc:0.713907284768212 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_5_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_5_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7758742170294455 	 acc:0.4826446280991736 | test: loss:0.7718429536219464 	 acc:0.48079470198675495 	 lr:0.0001
epoch1: train: loss:0.6592000071667443 	 acc:0.5213223140495867 | test: loss:0.6636319125724944 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6608885170408517 	 acc:0.5735537190082645 | test: loss:0.6507232847592689 	 acc:0.5721854304635762 	 lr:0.0001
epoch3: train: loss:0.6507182261569441 	 acc:0.611900826446281 | test: loss:0.6466127408261331 	 acc:0.6132450331125828 	 lr:0.0001
epoch4: train: loss:0.6427828821268948 	 acc:0.6244628099173554 | test: loss:0.6332065116490749 	 acc:0.6105960264900663 	 lr:0.0001
epoch5: train: loss:0.638581268885904 	 acc:0.6056198347107438 | test: loss:0.630323576058773 	 acc:0.6211920529801325 	 lr:0.0001
epoch6: train: loss:0.6308860789645802 	 acc:0.6310743801652893 | test: loss:0.6254001062437399 	 acc:0.6556291390728477 	 lr:0.0001
epoch7: train: loss:0.6422042672496197 	 acc:0.6558677685950414 | test: loss:0.6335874983806484 	 acc:0.6741721854304635 	 lr:0.0001
epoch8: train: loss:0.6372445635362105 	 acc:0.6565289256198347 | test: loss:0.6351169415657094 	 acc:0.6688741721854304 	 lr:0.0001
epoch9: train: loss:0.6307516737811821 	 acc:0.6826446280991736 | test: loss:0.6273245299888762 	 acc:0.6847682119205298 	 lr:0.0001
epoch10: train: loss:0.617074425772202 	 acc:0.6469421487603306 | test: loss:0.6113390733864134 	 acc:0.6450331125827815 	 lr:0.0001
epoch11: train: loss:0.6156618396112742 	 acc:0.6489256198347108 | test: loss:0.6124741562944375 	 acc:0.6609271523178808 	 lr:0.0001
epoch12: train: loss:0.6071156986094703 	 acc:0.6423140495867768 | test: loss:0.610364552286287 	 acc:0.6158940397350994 	 lr:0.0001
epoch13: train: loss:0.6160327714927926 	 acc:0.6743801652892562 | test: loss:0.614729309476764 	 acc:0.6543046357615894 	 lr:0.0001
epoch14: train: loss:0.6143102234848274 	 acc:0.5814876033057851 | test: loss:0.6195225898793202 	 acc:0.5629139072847682 	 lr:0.0001
epoch15: train: loss:0.6365794269309556 	 acc:0.5335537190082644 | test: loss:0.6435179326707954 	 acc:0.5271523178807948 	 lr:0.0001
epoch16: train: loss:0.6096981559509088 	 acc:0.584793388429752 | test: loss:0.609832745277329 	 acc:0.5841059602649007 	 lr:0.0001
epoch17: train: loss:0.6064558539311747 	 acc:0.6803305785123966 | test: loss:0.6026659284206416 	 acc:0.6900662251655629 	 lr:0.0001
epoch18: train: loss:0.5973363318522115 	 acc:0.6608264462809917 | test: loss:0.602551169505972 	 acc:0.6357615894039735 	 lr:0.0001
epoch19: train: loss:0.6313369968114806 	 acc:0.6823140495867769 | test: loss:0.6179108427849827 	 acc:0.686092715231788 	 lr:0.0001
epoch20: train: loss:0.5981013889746233 	 acc:0.7011570247933885 | test: loss:0.5925986196821099 	 acc:0.7112582781456953 	 lr:0.0001
epoch21: train: loss:0.6098245098768187 	 acc:0.6859504132231405 | test: loss:0.5937189501642391 	 acc:0.7231788079470198 	 lr:0.0001
epoch22: train: loss:0.5960185648193044 	 acc:0.620495867768595 | test: loss:0.5901262678057942 	 acc:0.6291390728476821 	 lr:0.0001
epoch23: train: loss:0.5936353306533877 	 acc:0.6925619834710743 | test: loss:0.5870101383190282 	 acc:0.6993377483443709 	 lr:0.0001
epoch24: train: loss:0.5890578185034192 	 acc:0.6611570247933884 | test: loss:0.5841238702369842 	 acc:0.6556291390728477 	 lr:0.0001
epoch25: train: loss:0.6025874474422991 	 acc:0.7044628099173553 | test: loss:0.5981530358459776 	 acc:0.7178807947019867 	 lr:0.0001
epoch26: train: loss:0.6558133020873897 	 acc:0.6343801652892562 | test: loss:0.632484911214437 	 acc:0.6768211920529801 	 lr:0.0001
epoch27: train: loss:0.6486590104457761 	 acc:0.651900826446281 | test: loss:0.6409009642948378 	 acc:0.6609271523178808 	 lr:0.0001
epoch28: train: loss:0.5845039820474042 	 acc:0.7097520661157025 | test: loss:0.5833106153848155 	 acc:0.7112582781456953 	 lr:0.0001
epoch29: train: loss:0.5897023575759132 	 acc:0.7090909090909091 | test: loss:0.5869098282018245 	 acc:0.695364238410596 	 lr:0.0001
epoch30: train: loss:0.6259725854613564 	 acc:0.6773553719008264 | test: loss:0.6140145237872142 	 acc:0.6940397350993377 	 lr:0.0001
epoch31: train: loss:0.5888521356227969 	 acc:0.7120661157024794 | test: loss:0.5823328664760716 	 acc:0.7125827814569536 	 lr:0.0001
epoch32: train: loss:0.5798408689577718 	 acc:0.6952066115702479 | test: loss:0.5729338042783422 	 acc:0.7152317880794702 	 lr:0.0001
epoch33: train: loss:0.6020863479228059 	 acc:0.6029752066115702 | test: loss:0.5858901969644408 	 acc:0.6264900662251656 	 lr:0.0001
epoch34: train: loss:0.5970107724843932 	 acc:0.71900826446281 | test: loss:0.5832428981926268 	 acc:0.7284768211920529 	 lr:0.0001
epoch35: train: loss:0.5941247224019579 	 acc:0.628099173553719 | test: loss:0.6071196316883264 	 acc:0.5920529801324503 	 lr:0.0001
epoch36: train: loss:0.5988349677117403 	 acc:0.6849586776859504 | test: loss:0.5945583755606847 	 acc:0.713907284768212 	 lr:0.0001
epoch37: train: loss:0.5790197685730358 	 acc:0.6998347107438017 | test: loss:0.5741153416254663 	 acc:0.6701986754966888 	 lr:0.0001
epoch38: train: loss:0.5829574231470912 	 acc:0.7282644628099173 | test: loss:0.5731274790321754 	 acc:0.7298013245033113 	 lr:0.0001
epoch39: train: loss:0.5749175989529318 	 acc:0.7209917355371901 | test: loss:0.568348391324479 	 acc:0.7245033112582782 	 lr:5e-05
epoch40: train: loss:0.5698153267222003 	 acc:0.6925619834710743 | test: loss:0.5684846248847759 	 acc:0.6874172185430464 	 lr:5e-05
epoch41: train: loss:0.5761183069954233 	 acc:0.6565289256198347 | test: loss:0.5803912997245788 	 acc:0.6450331125827815 	 lr:5e-05
epoch42: train: loss:0.5912274475925224 	 acc:0.7322314049586777 | test: loss:0.5878352049960206 	 acc:0.7231788079470198 	 lr:5e-05
epoch43: train: loss:0.568281368421129 	 acc:0.7163636363636363 | test: loss:0.5610810328793052 	 acc:0.743046357615894 	 lr:5e-05
epoch44: train: loss:0.5713405373667882 	 acc:0.7269421487603306 | test: loss:0.5677425205312817 	 acc:0.704635761589404 	 lr:5e-05
epoch45: train: loss:0.5703007487817244 	 acc:0.7408264462809917 | test: loss:0.560976493200719 	 acc:0.7549668874172185 	 lr:5e-05
epoch46: train: loss:0.564898495339165 	 acc:0.7117355371900826 | test: loss:0.5606712424991936 	 acc:0.7298013245033113 	 lr:5e-05
epoch47: train: loss:0.5812820195757653 	 acc:0.7388429752066116 | test: loss:0.5716228564843436 	 acc:0.7443708609271523 	 lr:5e-05
epoch48: train: loss:0.5601505468699558 	 acc:0.7173553719008264 | test: loss:0.5687320730544084 	 acc:0.680794701986755 	 lr:5e-05
epoch49: train: loss:0.5889367242489965 	 acc:0.7150413223140496 | test: loss:0.5769368041430089 	 acc:0.7178807947019867 	 lr:5e-05
epoch50: train: loss:0.5699468359080228 	 acc:0.7173553719008264 | test: loss:0.5597997637773982 	 acc:0.7298013245033113 	 lr:5e-05
epoch51: train: loss:0.5721714083222318 	 acc:0.703801652892562 | test: loss:0.5656062654312083 	 acc:0.7072847682119205 	 lr:5e-05
epoch52: train: loss:0.5634033307162198 	 acc:0.7434710743801652 | test: loss:0.561175314953785 	 acc:0.7231788079470198 	 lr:5e-05
epoch53: train: loss:0.5691101881295196 	 acc:0.7411570247933884 | test: loss:0.5637842475973218 	 acc:0.7470198675496689 	 lr:5e-05
epoch54: train: loss:0.5623190824650536 	 acc:0.7279338842975207 | test: loss:0.5561306175017199 	 acc:0.7470198675496689 	 lr:5e-05
epoch55: train: loss:0.5867930524802405 	 acc:0.7209917355371901 | test: loss:0.578777479809641 	 acc:0.7112582781456953 	 lr:5e-05
epoch56: train: loss:0.5630279219052023 	 acc:0.7262809917355372 | test: loss:0.5655061041282502 	 acc:0.686092715231788 	 lr:5e-05
epoch57: train: loss:0.5678966975803218 	 acc:0.7028099173553719 | test: loss:0.5575916340019529 	 acc:0.7311258278145696 	 lr:5e-05
epoch58: train: loss:0.561822668225312 	 acc:0.7570247933884298 | test: loss:0.5583957194492517 	 acc:0.752317880794702 	 lr:5e-05
epoch59: train: loss:0.568661072332997 	 acc:0.6763636363636364 | test: loss:0.5610848945497677 	 acc:0.6874172185430464 	 lr:5e-05
epoch60: train: loss:0.5611068505098011 	 acc:0.7537190082644628 | test: loss:0.5608981689080498 	 acc:0.7350993377483444 	 lr:5e-05
epoch61: train: loss:0.5563728293111502 	 acc:0.7626446280991735 | test: loss:0.5547723358040614 	 acc:0.7496688741721854 	 lr:2.5e-05
epoch62: train: loss:0.557529295535127 	 acc:0.7543801652892562 | test: loss:0.5526361164667748 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch63: train: loss:0.5502364279415982 	 acc:0.7543801652892562 | test: loss:0.555393307572169 	 acc:0.7324503311258278 	 lr:2.5e-05
epoch64: train: loss:0.5575520836814376 	 acc:0.7414876033057851 | test: loss:0.5537600164381874 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch65: train: loss:0.5605943917637028 	 acc:0.6998347107438017 | test: loss:0.5571520934831228 	 acc:0.6940397350993377 	 lr:2.5e-05
epoch66: train: loss:0.5577557374229116 	 acc:0.748099173553719 | test: loss:0.554392331720188 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch67: train: loss:0.5522438577186963 	 acc:0.7378512396694215 | test: loss:0.5518273871466024 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch68: train: loss:0.5784909777798929 	 acc:0.7471074380165289 | test: loss:0.568504233233976 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch69: train: loss:0.5717314711484042 	 acc:0.7467768595041322 | test: loss:0.5627418902536102 	 acc:0.7311258278145696 	 lr:2.5e-05
epoch70: train: loss:0.5629202666755551 	 acc:0.7520661157024794 | test: loss:0.5584499809915656 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch71: train: loss:0.5564176805945468 	 acc:0.7398347107438017 | test: loss:0.548830637079201 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch72: train: loss:0.5556330660354992 	 acc:0.731900826446281 | test: loss:0.5492924832350371 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch73: train: loss:0.5641438822312789 	 acc:0.7547107438016529 | test: loss:0.5565769690551505 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch74: train: loss:0.5508869723643153 	 acc:0.7609917355371901 | test: loss:0.5478086436821136 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch75: train: loss:0.5513048386967871 	 acc:0.7517355371900827 | test: loss:0.5499009538170517 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch76: train: loss:0.5570083658360253 	 acc:0.7623140495867768 | test: loss:0.5547428582677778 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch77: train: loss:0.5710158751030598 	 acc:0.7500826446280991 | test: loss:0.5658154192349769 	 acc:0.7417218543046358 	 lr:2.5e-05
epoch78: train: loss:0.557589571968583 	 acc:0.7451239669421488 | test: loss:0.5525128079565945 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch79: train: loss:0.5754547227709746 	 acc:0.7510743801652893 | test: loss:0.5704152796442146 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch80: train: loss:0.554380645594321 	 acc:0.7193388429752066 | test: loss:0.5554083476792898 	 acc:0.704635761589404 	 lr:2.5e-05
epoch81: train: loss:0.5548720645707501 	 acc:0.7527272727272727 | test: loss:0.5519935962380162 	 acc:0.7483443708609272 	 lr:1.25e-05
epoch82: train: loss:0.5533866825970737 	 acc:0.7428099173553719 | test: loss:0.5510595474811579 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch83: train: loss:0.5509021314116549 	 acc:0.7504132231404959 | test: loss:0.5491213035109817 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch84: train: loss:0.5570085959592141 	 acc:0.7355371900826446 | test: loss:0.5514993278396051 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch85: train: loss:0.5508044826885885 	 acc:0.7504132231404959 | test: loss:0.5502072300342534 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch86: train: loss:0.5555579785867171 	 acc:0.7576859504132232 | test: loss:0.5539183958476742 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch87: train: loss:0.5496658234556845 	 acc:0.7530578512396694 | test: loss:0.5503092648177746 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch88: train: loss:0.555692933315088 	 acc:0.7586776859504132 | test: loss:0.5500767277566013 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch89: train: loss:0.5508568891611966 	 acc:0.7490909090909091 | test: loss:0.5499062568936127 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch90: train: loss:0.5581222184039345 	 acc:0.7652892561983471 | test: loss:0.5547441135179129 	 acc:0.743046357615894 	 lr:6.25e-06
epoch91: train: loss:0.5532367108281979 	 acc:0.7606611570247934 | test: loss:0.553251909814923 	 acc:0.7417218543046358 	 lr:6.25e-06
epoch92: train: loss:0.5494602086327293 	 acc:0.7517355371900827 | test: loss:0.5496264868224694 	 acc:0.752317880794702 	 lr:6.25e-06
epoch93: train: loss:0.5506750912311649 	 acc:0.7576859504132232 | test: loss:0.5493188299090657 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch94: train: loss:0.5479422370855473 	 acc:0.7580165289256199 | test: loss:0.549311638588937 	 acc:0.743046357615894 	 lr:3.125e-06
epoch95: train: loss:0.5524285339323942 	 acc:0.7590082644628099 | test: loss:0.5499983101491107 	 acc:0.743046357615894 	 lr:3.125e-06
epoch96: train: loss:0.5517829916102827 	 acc:0.7695867768595042 | test: loss:0.5517025708362756 	 acc:0.7509933774834437 	 lr:3.125e-06
epoch97: train: loss:0.5528920020741864 	 acc:0.7596694214876033 | test: loss:0.551534144452076 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch98: train: loss:0.5465990754395477 	 acc:0.7662809917355372 | test: loss:0.5502363407848686 	 acc:0.7509933774834437 	 lr:3.125e-06
epoch99: train: loss:0.5513669805881406 	 acc:0.7464462809917355 | test: loss:0.5499320250473275 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch100: train: loss:0.5478702566052271 	 acc:0.763305785123967 | test: loss:0.5499183092685724 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch101: train: loss:0.5491885081598581 	 acc:0.7553719008264462 | test: loss:0.5495413166797714 	 acc:0.7456953642384105 	 lr:1.5625e-06
epoch102: train: loss:0.5485369682903132 	 acc:0.7646280991735537 | test: loss:0.5504755206455458 	 acc:0.7509933774834437 	 lr:1.5625e-06
epoch103: train: loss:0.5468054535566282 	 acc:0.7629752066115703 | test: loss:0.5497466514442141 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch104: train: loss:0.554098580671736 	 acc:0.7553719008264462 | test: loss:0.5503903376345602 	 acc:0.7483443708609272 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_6_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_6_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.8181583285528766 	 acc:0.48198347107438017 | test: loss:0.8175582727059624 	 acc:0.4781456953642384 	 lr:0.0001
epoch1: train: loss:0.6667027473843787 	 acc:0.5213223140495867 | test: loss:0.671376369804736 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.672948152349015 	 acc:0.596694214876033 | test: loss:0.6590889427835578 	 acc:0.6026490066225165 	 lr:0.0001
epoch3: train: loss:0.6434661764349819 	 acc:0.5808264462809918 | test: loss:0.6396176901084698 	 acc:0.5576158940397351 	 lr:0.0001
epoch4: train: loss:0.6402192650353613 	 acc:0.5953719008264463 | test: loss:0.6330584733691437 	 acc:0.5695364238410596 	 lr:0.0001
epoch5: train: loss:0.641521238433428 	 acc:0.6066115702479339 | test: loss:0.634611786596033 	 acc:0.6066225165562914 	 lr:0.0001
epoch6: train: loss:0.6305514277505481 	 acc:0.6072727272727273 | test: loss:0.6252194210393539 	 acc:0.6119205298013245 	 lr:0.0001
epoch7: train: loss:0.6501486800918894 	 acc:0.6416528925619834 | test: loss:0.6417802724617206 	 acc:0.6781456953642384 	 lr:0.0001
epoch8: train: loss:0.6343538055735186 	 acc:0.6608264462809917 | test: loss:0.626420130713886 	 acc:0.6874172185430464 	 lr:0.0001
epoch9: train: loss:0.6217507946195682 	 acc:0.6783471074380165 | test: loss:0.6181735731118562 	 acc:0.6834437086092715 	 lr:0.0001
epoch10: train: loss:0.6182997129771335 	 acc:0.6363636363636364 | test: loss:0.6137214253280336 	 acc:0.6450331125827815 	 lr:0.0001
epoch11: train: loss:0.6139301747921084 	 acc:0.6304132231404959 | test: loss:0.6092786947622993 	 acc:0.633112582781457 	 lr:0.0001
epoch12: train: loss:0.6050191081850982 	 acc:0.6697520661157025 | test: loss:0.6067452988877202 	 acc:0.6529801324503312 	 lr:0.0001
epoch13: train: loss:0.6119111394685162 	 acc:0.6611570247933884 | test: loss:0.6107722075569708 	 acc:0.6423841059602649 	 lr:0.0001
epoch14: train: loss:0.6191072858857715 	 acc:0.5689256198347108 | test: loss:0.6275899411037268 	 acc:0.5496688741721855 	 lr:0.0001
epoch15: train: loss:0.6406053211472251 	 acc:0.5322314049586777 | test: loss:0.6451730407626424 	 acc:0.5298013245033113 	 lr:0.0001
epoch16: train: loss:0.6112823072346774 	 acc:0.5804958677685951 | test: loss:0.6114719089293322 	 acc:0.5761589403973509 	 lr:0.0001
epoch17: train: loss:0.6161187433014231 	 acc:0.687603305785124 | test: loss:0.6109046012360528 	 acc:0.695364238410596 	 lr:0.0001
epoch18: train: loss:0.6027019369306643 	 acc:0.6710743801652893 | test: loss:0.6005648523766474 	 acc:0.6834437086092715 	 lr:0.0001
epoch19: train: loss:0.614631808178484 	 acc:0.7034710743801653 | test: loss:0.6024517537742261 	 acc:0.7165562913907285 	 lr:0.0001
epoch20: train: loss:0.5898527500058008 	 acc:0.691900826446281 | test: loss:0.5872916496352644 	 acc:0.6887417218543046 	 lr:0.0001
epoch21: train: loss:0.5980671905091971 	 acc:0.699504132231405 | test: loss:0.5830651276948436 	 acc:0.7311258278145696 	 lr:0.0001
epoch22: train: loss:0.5987967165639578 	 acc:0.6079338842975207 | test: loss:0.6011527610140921 	 acc:0.6 	 lr:0.0001
epoch23: train: loss:0.5942605831209293 	 acc:0.6945454545454546 | test: loss:0.5935423897591647 	 acc:0.7218543046357616 	 lr:0.0001
epoch24: train: loss:0.583202344957462 	 acc:0.6581818181818182 | test: loss:0.5795706910802828 	 acc:0.6569536423841059 	 lr:0.0001
epoch25: train: loss:0.6005612107938971 	 acc:0.707107438016529 | test: loss:0.6042010448626335 	 acc:0.7165562913907285 	 lr:0.0001
epoch26: train: loss:0.6399458566381911 	 acc:0.6608264462809917 | test: loss:0.6170472071660276 	 acc:0.695364238410596 	 lr:0.0001
epoch27: train: loss:0.6152318488861903 	 acc:0.6938842975206612 | test: loss:0.6021873361227528 	 acc:0.7099337748344371 	 lr:0.0001
epoch28: train: loss:0.5815631445774362 	 acc:0.6981818181818182 | test: loss:0.5799425961955493 	 acc:0.686092715231788 	 lr:0.0001
epoch29: train: loss:0.5854615541923145 	 acc:0.6958677685950413 | test: loss:0.5779111742183862 	 acc:0.695364238410596 	 lr:0.0001
epoch30: train: loss:0.625281242161743 	 acc:0.68 | test: loss:0.6066882937159759 	 acc:0.6993377483443709 	 lr:0.0001
epoch31: train: loss:0.6019476026739956 	 acc:0.7004958677685951 | test: loss:0.5912192938343579 	 acc:0.7178807947019867 	 lr:0.0001
epoch32: train: loss:0.5804165562716397 	 acc:0.6753719008264463 | test: loss:0.5783452953723882 	 acc:0.6384105960264901 	 lr:0.0001
epoch33: train: loss:0.5804267303017545 	 acc:0.6690909090909091 | test: loss:0.5635989320199222 	 acc:0.7165562913907285 	 lr:0.0001
epoch34: train: loss:0.5963456117023122 	 acc:0.7107438016528925 | test: loss:0.5918490490376555 	 acc:0.7059602649006622 	 lr:0.0001
epoch35: train: loss:0.5926213823271191 	 acc:0.6145454545454545 | test: loss:0.6088213357704365 	 acc:0.5880794701986755 	 lr:0.0001
epoch36: train: loss:0.5778337011652545 	 acc:0.7342148760330579 | test: loss:0.5780810350613879 	 acc:0.7231788079470198 	 lr:0.0001
epoch37: train: loss:0.5676261668559933 	 acc:0.7117355371900826 | test: loss:0.5584071675673226 	 acc:0.7086092715231788 	 lr:0.0001
epoch38: train: loss:0.5695464052050567 	 acc:0.6793388429752066 | test: loss:0.5741356305728685 	 acc:0.6476821192052981 	 lr:0.0001
epoch39: train: loss:0.5697339787759071 	 acc:0.7322314049586777 | test: loss:0.563788959522121 	 acc:0.7403973509933774 	 lr:0.0001
epoch40: train: loss:0.5601509759839901 	 acc:0.7444628099173554 | test: loss:0.5550151994686253 	 acc:0.7364238410596027 	 lr:0.0001
epoch41: train: loss:0.6093411701966909 	 acc:0.5669421487603306 | test: loss:0.6193160422590395 	 acc:0.5629139072847682 	 lr:0.0001
epoch42: train: loss:0.5994185191343638 	 acc:0.7137190082644628 | test: loss:0.5932161928012671 	 acc:0.7178807947019867 	 lr:0.0001
epoch43: train: loss:0.5705948158729175 	 acc:0.6833057851239669 | test: loss:0.5542167166210957 	 acc:0.7059602649006622 	 lr:0.0001
epoch44: train: loss:0.5695388553753372 	 acc:0.7299173553719008 | test: loss:0.5612250031224939 	 acc:0.7165562913907285 	 lr:0.0001
epoch45: train: loss:0.575179446117937 	 acc:0.7305785123966942 | test: loss:0.5674162473899639 	 acc:0.7364238410596027 	 lr:0.0001
epoch46: train: loss:0.5571760588243974 	 acc:0.7084297520661157 | test: loss:0.5678904748910311 	 acc:0.6635761589403973 	 lr:0.0001
epoch47: train: loss:0.5573725339991987 	 acc:0.7024793388429752 | test: loss:0.5669671917593242 	 acc:0.6741721854304635 	 lr:0.0001
epoch48: train: loss:0.5685772749215118 	 acc:0.7487603305785124 | test: loss:0.5659436980620125 	 acc:0.7324503311258278 	 lr:0.0001
epoch49: train: loss:0.5700706826753853 	 acc:0.6565289256198347 | test: loss:0.5697165840508922 	 acc:0.6635761589403973 	 lr:0.0001
epoch50: train: loss:0.5571862357904103 	 acc:0.748099173553719 | test: loss:0.5543149862857844 	 acc:0.7245033112582782 	 lr:5e-05
epoch51: train: loss:0.56036361243114 	 acc:0.6988429752066115 | test: loss:0.5571853854798323 	 acc:0.7072847682119205 	 lr:5e-05
epoch52: train: loss:0.5488990334439868 	 acc:0.7447933884297521 | test: loss:0.5523556699815965 	 acc:0.7178807947019867 	 lr:5e-05
epoch53: train: loss:0.5548363496843448 	 acc:0.7527272727272727 | test: loss:0.5533473111935798 	 acc:0.7536423841059603 	 lr:5e-05
epoch54: train: loss:0.5465057172854084 	 acc:0.7510743801652893 | test: loss:0.5490053198984917 	 acc:0.7350993377483444 	 lr:5e-05
epoch55: train: loss:0.5533332052900771 	 acc:0.7583471074380165 | test: loss:0.5572846523973326 	 acc:0.7337748344370861 	 lr:5e-05
epoch56: train: loss:0.5469830963828347 	 acc:0.739504132231405 | test: loss:0.5502333476053958 	 acc:0.7152317880794702 	 lr:5e-05
epoch57: train: loss:0.55218049788278 	 acc:0.736198347107438 | test: loss:0.5459993198217935 	 acc:0.7470198675496689 	 lr:5e-05
epoch58: train: loss:0.5497571308356671 	 acc:0.7742148760330578 | test: loss:0.5484213952197144 	 acc:0.7456953642384105 	 lr:5e-05
epoch59: train: loss:0.5548476574637673 	 acc:0.7057851239669422 | test: loss:0.5535633758993338 	 acc:0.7099337748344371 	 lr:5e-05
epoch60: train: loss:0.5622112469436709 	 acc:0.7656198347107438 | test: loss:0.5600625950769084 	 acc:0.7364238410596027 	 lr:5e-05
epoch61: train: loss:0.548615180854955 	 acc:0.7147107438016529 | test: loss:0.5604161567245888 	 acc:0.6821192052980133 	 lr:5e-05
epoch62: train: loss:0.5490849048047026 	 acc:0.708099173553719 | test: loss:0.5517994594889761 	 acc:0.7086092715231788 	 lr:5e-05
epoch63: train: loss:0.5470590988466563 	 acc:0.7695867768595042 | test: loss:0.557033739816274 	 acc:0.7456953642384105 	 lr:5e-05
epoch64: train: loss:0.542176946332632 	 acc:0.7596694214876033 | test: loss:0.5437401654704517 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch65: train: loss:0.5453067695010793 	 acc:0.7570247933884298 | test: loss:0.5435111104257849 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch66: train: loss:0.5504763880642978 	 acc:0.768595041322314 | test: loss:0.5481700205645025 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch67: train: loss:0.5380620450224758 	 acc:0.7530578512396694 | test: loss:0.5455690822853947 	 acc:0.7284768211920529 	 lr:2.5e-05
epoch68: train: loss:0.5497469931003476 	 acc:0.7738842975206611 | test: loss:0.5499308169282825 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch69: train: loss:0.5500222959400208 	 acc:0.771900826446281 | test: loss:0.54753765933561 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch70: train: loss:0.5451159714667265 	 acc:0.7656198347107438 | test: loss:0.5460642235958023 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch71: train: loss:0.5468208717511706 	 acc:0.7669421487603306 | test: loss:0.543776645486718 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch72: train: loss:0.5425204183641544 	 acc:0.7689256198347107 | test: loss:0.5431001510841168 	 acc:0.743046357615894 	 lr:1.25e-05
epoch73: train: loss:0.5403390827454811 	 acc:0.7652892561983471 | test: loss:0.5409591307703233 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch74: train: loss:0.5342459839434663 	 acc:0.7695867768595042 | test: loss:0.5393275573553629 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch75: train: loss:0.5354637590715707 	 acc:0.7563636363636363 | test: loss:0.5422031200484724 	 acc:0.7311258278145696 	 lr:1.25e-05
epoch76: train: loss:0.5372652651061697 	 acc:0.7646280991735537 | test: loss:0.5408954180629049 	 acc:0.752317880794702 	 lr:1.25e-05
epoch77: train: loss:0.5420863651047069 	 acc:0.7748760330578512 | test: loss:0.5436862493982378 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch78: train: loss:0.5424902378626106 	 acc:0.7289256198347107 | test: loss:0.5478781011720367 	 acc:0.7086092715231788 	 lr:1.25e-05
epoch79: train: loss:0.5475584066603795 	 acc:0.7580165289256199 | test: loss:0.544814793242524 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch80: train: loss:0.5373009162894951 	 acc:0.7652892561983471 | test: loss:0.5410201291374813 	 acc:0.743046357615894 	 lr:1.25e-05
epoch81: train: loss:0.5422518732331015 	 acc:0.7728925619834711 | test: loss:0.5419100177998575 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch82: train: loss:0.5397925940229873 	 acc:0.7490909090909091 | test: loss:0.5422695341489173 	 acc:0.7417218543046358 	 lr:6.25e-06
epoch83: train: loss:0.5399911961279624 	 acc:0.7755371900826447 | test: loss:0.5418832853929886 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch84: train: loss:0.5411367111560726 	 acc:0.7629752066115703 | test: loss:0.5405340841274388 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch85: train: loss:0.5376993587194395 	 acc:0.7623140495867768 | test: loss:0.5417792336040774 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch86: train: loss:0.5386464206640386 	 acc:0.7586776859504132 | test: loss:0.5417829187500556 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch87: train: loss:0.537532552943742 	 acc:0.7745454545454545 | test: loss:0.5411862442035549 	 acc:0.7390728476821192 	 lr:3.125e-06
epoch88: train: loss:0.5397340269719274 	 acc:0.7689256198347107 | test: loss:0.5409770345845759 	 acc:0.743046357615894 	 lr:3.125e-06
epoch89: train: loss:0.5380541679287745 	 acc:0.7709090909090909 | test: loss:0.5408282422861516 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch90: train: loss:0.5367342061444748 	 acc:0.7811570247933884 | test: loss:0.5410488614972854 	 acc:0.7549668874172185 	 lr:3.125e-06
epoch91: train: loss:0.5391150977788878 	 acc:0.771900826446281 | test: loss:0.5415149083990135 	 acc:0.7549668874172185 	 lr:3.125e-06
epoch92: train: loss:0.5383136828871798 	 acc:0.7732231404958678 | test: loss:0.5410399513528836 	 acc:0.7576158940397351 	 lr:3.125e-06
epoch93: train: loss:0.5382415129724613 	 acc:0.768595041322314 | test: loss:0.54115765600015 	 acc:0.7562913907284768 	 lr:1.5625e-06
epoch94: train: loss:0.536378428049324 	 acc:0.7748760330578512 | test: loss:0.540325985365356 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch95: train: loss:0.5390110114980335 	 acc:0.7715702479338843 | test: loss:0.540295152632606 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch96: train: loss:0.5349650776090701 	 acc:0.7877685950413224 | test: loss:0.5413253104449898 	 acc:0.7562913907284768 	 lr:1.5625e-06
epoch97: train: loss:0.5363922396573153 	 acc:0.7732231404958678 | test: loss:0.5408410337587066 	 acc:0.7615894039735099 	 lr:1.5625e-06
epoch98: train: loss:0.5333766440320606 	 acc:0.7781818181818182 | test: loss:0.540135469973482 	 acc:0.7576158940397351 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_7_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_7_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.8327376657872161 	 acc:0.4803305785123967 | test: loss:0.8349108365197845 	 acc:0.4781456953642384 	 lr:0.0001
epoch1: train: loss:0.6578638411750478 	 acc:0.5547107438016529 | test: loss:0.650674735868214 	 acc:0.5509933774834437 	 lr:0.0001
epoch2: train: loss:0.656282213443567 	 acc:0.5219834710743801 | test: loss:0.6568139977802504 	 acc:0.5218543046357615 	 lr:0.0001
epoch3: train: loss:0.6608940062444072 	 acc:0.622809917355372 | test: loss:0.6561499534063782 	 acc:0.6423841059602649 	 lr:0.0001
epoch4: train: loss:0.6424702118842069 	 acc:0.5943801652892562 | test: loss:0.6366480729437822 	 acc:0.5761589403973509 	 lr:0.0001
epoch5: train: loss:0.640428882058987 	 acc:0.5917355371900826 | test: loss:0.6334223259363743 	 acc:0.5933774834437087 	 lr:0.0001
epoch6: train: loss:0.6297256769227587 	 acc:0.5841322314049586 | test: loss:0.6257101767110509 	 acc:0.5827814569536424 	 lr:0.0001
epoch7: train: loss:0.6303756553673547 	 acc:0.6046280991735538 | test: loss:0.6233706958246547 	 acc:0.6013245033112583 	 lr:0.0001
epoch8: train: loss:0.6205552473147053 	 acc:0.6095867768595041 | test: loss:0.6160871559420958 	 acc:0.6 	 lr:0.0001
epoch9: train: loss:0.6625546830547743 	 acc:0.6271074380165289 | test: loss:0.6617873202886013 	 acc:0.6384105960264901 	 lr:0.0001
epoch10: train: loss:0.6260728445131917 	 acc:0.6644628099173554 | test: loss:0.6226685025044625 	 acc:0.6768211920529801 	 lr:0.0001
epoch11: train: loss:0.6215238455898505 	 acc:0.6720661157024793 | test: loss:0.6198175794241444 	 acc:0.6927152317880795 	 lr:0.0001
epoch12: train: loss:0.5989366053352672 	 acc:0.6618181818181819 | test: loss:0.602499200887238 	 acc:0.6384105960264901 	 lr:0.0001
epoch13: train: loss:0.602146398134468 	 acc:0.6565289256198347 | test: loss:0.6036194192652671 	 acc:0.6251655629139072 	 lr:0.0001
epoch14: train: loss:0.6011580132058829 	 acc:0.6138842975206612 | test: loss:0.6051069129381749 	 acc:0.6052980132450331 	 lr:0.0001
epoch15: train: loss:0.5940830758780488 	 acc:0.643305785123967 | test: loss:0.5983161402064443 	 acc:0.6251655629139072 	 lr:0.0001
epoch16: train: loss:0.6125617652688145 	 acc:0.6955371900826446 | test: loss:0.6139383822876886 	 acc:0.6980132450331126 	 lr:0.0001
epoch17: train: loss:0.6138904681481606 	 acc:0.7061157024793389 | test: loss:0.6007614421528696 	 acc:0.7059602649006622 	 lr:0.0001
epoch18: train: loss:0.6019443280834798 	 acc:0.7137190082644628 | test: loss:0.599629356529539 	 acc:0.695364238410596 	 lr:0.0001
epoch19: train: loss:0.6110820402192675 	 acc:0.6945454545454546 | test: loss:0.5991183893570048 	 acc:0.7192052980132451 	 lr:0.0001
epoch20: train: loss:0.583875178739059 	 acc:0.7153719008264463 | test: loss:0.5799496696484799 	 acc:0.7218543046357616 	 lr:0.0001
epoch21: train: loss:0.6420611881027537 	 acc:0.6608264462809917 | test: loss:0.6235154971381687 	 acc:0.7006622516556291 	 lr:0.0001
epoch22: train: loss:0.589635591506958 	 acc:0.727603305785124 | test: loss:0.5765374657334081 	 acc:0.7284768211920529 	 lr:0.0001
epoch23: train: loss:0.6132225793649343 	 acc:0.6948760330578513 | test: loss:0.6204107443228463 	 acc:0.680794701986755 	 lr:0.0001
epoch24: train: loss:0.5757738602062887 	 acc:0.7090909090909091 | test: loss:0.5666349598114064 	 acc:0.7298013245033113 	 lr:0.0001
epoch25: train: loss:0.5852216052023832 	 acc:0.7160330578512397 | test: loss:0.5834498928872165 	 acc:0.7364238410596027 	 lr:0.0001
epoch26: train: loss:0.6395925581750791 	 acc:0.6608264462809917 | test: loss:0.6176452362774224 	 acc:0.7006622516556291 	 lr:0.0001
epoch27: train: loss:0.5851728414306956 	 acc:0.6704132231404959 | test: loss:0.5849418271456334 	 acc:0.6397350993377483 	 lr:0.0001
epoch28: train: loss:0.5773812373610567 	 acc:0.6773553719008264 | test: loss:0.5808140959960736 	 acc:0.6476821192052981 	 lr:0.0001
epoch29: train: loss:0.5700344596027342 	 acc:0.7282644628099173 | test: loss:0.567765285637205 	 acc:0.7178807947019867 	 lr:0.0001
epoch30: train: loss:0.5724638059119548 	 acc:0.739504132231405 | test: loss:0.5593024425948693 	 acc:0.7284768211920529 	 lr:0.0001
epoch31: train: loss:0.5616800405368332 	 acc:0.7163636363636363 | test: loss:0.559960857606092 	 acc:0.7165562913907285 	 lr:0.0001
epoch32: train: loss:0.5682888189426138 	 acc:0.708099173553719 | test: loss:0.5574965446990058 	 acc:0.7072847682119205 	 lr:0.0001
epoch33: train: loss:0.5796458295554169 	 acc:0.648595041322314 | test: loss:0.5659505639644649 	 acc:0.6993377483443709 	 lr:0.0001
epoch34: train: loss:0.5982048760563874 	 acc:0.7117355371900826 | test: loss:0.5816136822795236 	 acc:0.7377483443708609 	 lr:0.0001
epoch35: train: loss:0.5995443038309901 	 acc:0.6105785123966943 | test: loss:0.6187201221257645 	 acc:0.5788079470198676 	 lr:0.0001
epoch36: train: loss:0.5743481483932369 	 acc:0.7104132231404958 | test: loss:0.5760921345641281 	 acc:0.7192052980132451 	 lr:0.0001
epoch37: train: loss:0.5616731529984592 	 acc:0.715702479338843 | test: loss:0.5533661551033424 	 acc:0.7231788079470198 	 lr:0.0001
epoch38: train: loss:0.5748846148065299 	 acc:0.6439669421487604 | test: loss:0.5845510709364683 	 acc:0.6357615894039735 	 lr:0.0001
epoch39: train: loss:0.5569204634477284 	 acc:0.7216528925619835 | test: loss:0.5567168849193497 	 acc:0.7006622516556291 	 lr:0.0001
epoch40: train: loss:0.5506935873307472 	 acc:0.7603305785123967 | test: loss:0.5497641755255642 	 acc:0.7443708609271523 	 lr:0.0001
epoch41: train: loss:0.5952845102105259 	 acc:0.5970247933884297 | test: loss:0.6160373447746631 	 acc:0.5695364238410596 	 lr:0.0001
epoch42: train: loss:0.5641376348763458 	 acc:0.7659504132231405 | test: loss:0.5672589837320593 	 acc:0.7443708609271523 	 lr:0.0001
epoch43: train: loss:0.5727778763613425 	 acc:0.7378512396694215 | test: loss:0.5716007868975204 	 acc:0.7377483443708609 	 lr:0.0001
epoch44: train: loss:0.5548111070089103 	 acc:0.7233057851239669 | test: loss:0.5526004140740198 	 acc:0.7112582781456953 	 lr:0.0001
epoch45: train: loss:0.5506085534923333 	 acc:0.7454545454545455 | test: loss:0.5516357483453308 	 acc:0.7337748344370861 	 lr:0.0001
epoch46: train: loss:0.5629835945515593 	 acc:0.6571900826446281 | test: loss:0.5742864878761847 	 acc:0.6463576158940397 	 lr:0.0001
epoch47: train: loss:0.5485540229821008 	 acc:0.7649586776859504 | test: loss:0.5491034417752398 	 acc:0.7456953642384105 	 lr:5e-05
epoch48: train: loss:0.5359532338922675 	 acc:0.7649586776859504 | test: loss:0.5435065147892528 	 acc:0.7390728476821192 	 lr:5e-05
epoch49: train: loss:0.5454580428974688 	 acc:0.7401652892561984 | test: loss:0.5442546232646664 	 acc:0.7417218543046358 	 lr:5e-05
epoch50: train: loss:0.5458753481384151 	 acc:0.7619834710743801 | test: loss:0.5466661284301455 	 acc:0.7549668874172185 	 lr:5e-05
epoch51: train: loss:0.5433334983478892 	 acc:0.7613223140495867 | test: loss:0.5472086918275088 	 acc:0.743046357615894 	 lr:5e-05
epoch52: train: loss:0.5366075436142851 	 acc:0.7471074380165289 | test: loss:0.5401556806848539 	 acc:0.7589403973509934 	 lr:5e-05
epoch53: train: loss:0.5505998639429897 	 acc:0.7669421487603306 | test: loss:0.5602161728783159 	 acc:0.7456953642384105 	 lr:5e-05
epoch54: train: loss:0.5315214601232986 	 acc:0.7709090909090909 | test: loss:0.5381964584849528 	 acc:0.7562913907284768 	 lr:5e-05
epoch55: train: loss:0.5423436280321484 	 acc:0.7527272727272727 | test: loss:0.547446502912913 	 acc:0.7417218543046358 	 lr:5e-05
epoch56: train: loss:0.5392456277539908 	 acc:0.7203305785123967 | test: loss:0.5501828215769584 	 acc:0.7059602649006622 	 lr:5e-05
epoch57: train: loss:0.5472552552499061 	 acc:0.7652892561983471 | test: loss:0.5517017959758935 	 acc:0.7536423841059603 	 lr:5e-05
epoch58: train: loss:0.5405386446133132 	 acc:0.7818181818181819 | test: loss:0.5508660856461682 	 acc:0.7549668874172185 	 lr:5e-05
epoch59: train: loss:0.5431561507272327 	 acc:0.7219834710743802 | test: loss:0.5493341471185748 	 acc:0.7072847682119205 	 lr:5e-05
epoch60: train: loss:0.5426074121412167 	 acc:0.780495867768595 | test: loss:0.5504491044196071 	 acc:0.7483443708609272 	 lr:5e-05
epoch61: train: loss:0.5500136671775628 	 acc:0.7761983471074381 | test: loss:0.5492055014269241 	 acc:0.743046357615894 	 lr:2.5e-05
epoch62: train: loss:0.5381878930083976 	 acc:0.7814876033057852 | test: loss:0.5434385813624654 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch63: train: loss:0.5266682980671402 	 acc:0.7940495867768596 | test: loss:0.5404629949702332 	 acc:0.7562913907284768 	 lr:2.5e-05
epoch64: train: loss:0.5282766762449722 	 acc:0.7798347107438016 | test: loss:0.5381867955062563 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch65: train: loss:0.530724204315627 	 acc:0.7735537190082644 | test: loss:0.5355036734745202 	 acc:0.752317880794702 	 lr:2.5e-05
epoch66: train: loss:0.5268208504905385 	 acc:0.7841322314049587 | test: loss:0.5334919136881039 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch67: train: loss:0.5220591963618255 	 acc:0.7841322314049587 | test: loss:0.5333939610727575 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch68: train: loss:0.5322612272215284 	 acc:0.7874380165289256 | test: loss:0.5397171314978442 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch69: train: loss:0.5309404922319838 	 acc:0.7798347107438016 | test: loss:0.534954924378174 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch70: train: loss:0.5254655975940798 	 acc:0.7738842975206611 | test: loss:0.5367747877607283 	 acc:0.752317880794702 	 lr:2.5e-05
epoch71: train: loss:0.5330814442358727 	 acc:0.7910743801652893 | test: loss:0.5394985606338805 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch72: train: loss:0.5364117848380539 	 acc:0.7871074380165289 | test: loss:0.5440389876334083 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch73: train: loss:0.5453440593490916 	 acc:0.7877685950413224 | test: loss:0.5493669963830354 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch74: train: loss:0.5229776402741424 	 acc:0.7877685950413224 | test: loss:0.5343923674513962 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch75: train: loss:0.5281959936047389 	 acc:0.7421487603305785 | test: loss:0.5426256744277398 	 acc:0.7218543046357616 	 lr:1.25e-05
epoch76: train: loss:0.5210463986909094 	 acc:0.7910743801652893 | test: loss:0.5341841172698317 	 acc:0.766887417218543 	 lr:1.25e-05
epoch77: train: loss:0.5246223262912971 	 acc:0.7943801652892561 | test: loss:0.5381468170526011 	 acc:0.766887417218543 	 lr:1.25e-05
epoch78: train: loss:0.5256150057493163 	 acc:0.752396694214876 | test: loss:0.542383220337874 	 acc:0.7178807947019867 	 lr:1.25e-05
epoch79: train: loss:0.5328717378348359 	 acc:0.7877685950413224 | test: loss:0.5402530244644115 	 acc:0.7682119205298014 	 lr:1.25e-05
epoch80: train: loss:0.5207804202442327 	 acc:0.7900826446280992 | test: loss:0.5348451068859227 	 acc:0.766887417218543 	 lr:6.25e-06
epoch81: train: loss:0.5262707082299162 	 acc:0.783801652892562 | test: loss:0.5350420243692714 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch82: train: loss:0.5249126556688104 	 acc:0.7649586776859504 | test: loss:0.535556860080618 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch83: train: loss:0.524458709334539 	 acc:0.7953719008264463 | test: loss:0.5370332441582585 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch84: train: loss:0.5242872262986238 	 acc:0.7828099173553719 | test: loss:0.5348982146244176 	 acc:0.7695364238410596 	 lr:6.25e-06
epoch85: train: loss:0.5230980308784926 	 acc:0.7871074380165289 | test: loss:0.5357939403578146 	 acc:0.7682119205298014 	 lr:6.25e-06
epoch86: train: loss:0.520847905430912 	 acc:0.7824793388429752 | test: loss:0.5346023813778201 	 acc:0.7562913907284768 	 lr:3.125e-06
epoch87: train: loss:0.5224547077407522 	 acc:0.7937190082644628 | test: loss:0.5353340422870307 	 acc:0.7615894039735099 	 lr:3.125e-06
epoch88: train: loss:0.5251186379519376 	 acc:0.7765289256198347 | test: loss:0.534581488173529 	 acc:0.7589403973509934 	 lr:3.125e-06
epoch89: train: loss:0.5196847951609241 	 acc:0.7976859504132231 | test: loss:0.5347507430228177 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch90: train: loss:0.5205436531866877 	 acc:0.8016528925619835 | test: loss:0.5352760437308558 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch91: train: loss:0.522698836070447 	 acc:0.7900826446280992 | test: loss:0.535297423087998 	 acc:0.7682119205298014 	 lr:3.125e-06
epoch92: train: loss:0.5240825312984876 	 acc:0.7927272727272727 | test: loss:0.5350636397765962 	 acc:0.7655629139072848 	 lr:1.5625e-06
epoch93: train: loss:0.5212551120293042 	 acc:0.7857851239669421 | test: loss:0.5352410944881818 	 acc:0.7695364238410596 	 lr:1.5625e-06
epoch94: train: loss:0.5195326439014151 	 acc:0.7947107438016529 | test: loss:0.5341061927624886 	 acc:0.766887417218543 	 lr:1.5625e-06
epoch95: train: loss:0.5217336088566741 	 acc:0.792396694214876 | test: loss:0.5341361363991997 	 acc:0.7682119205298014 	 lr:1.5625e-06
epoch96: train: loss:0.5207142429509439 	 acc:0.8013223140495868 | test: loss:0.5355137944221496 	 acc:0.7629139072847683 	 lr:1.5625e-06
epoch97: train: loss:0.5203123463283885 	 acc:0.7933884297520661 | test: loss:0.5347635851790573 	 acc:0.7695364238410596 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_8_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_8_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.8541742920087388 	 acc:0.4786776859504132 | test: loss:0.8561539661805362 	 acc:0.4781456953642384 	 lr:0.0001
epoch1: train: loss:0.6552249523824897 	 acc:0.5216528925619834 | test: loss:0.6563295497010085 	 acc:0.5231788079470199 	 lr:0.0001
epoch2: train: loss:0.6499968253679512 	 acc:0.5259504132231405 | test: loss:0.6448924987521393 	 acc:0.5218543046357615 	 lr:0.0001
epoch3: train: loss:0.6455913961820366 	 acc:0.6115702479338843 | test: loss:0.6404737874372116 	 acc:0.5894039735099338 	 lr:0.0001
epoch4: train: loss:0.6461827936645381 	 acc:0.6145454545454545 | test: loss:0.6360342355753412 	 acc:0.6251655629139072 	 lr:0.0001
epoch5: train: loss:0.6376210128374337 	 acc:0.5907438016528925 | test: loss:0.6307479396561124 	 acc:0.5867549668874172 	 lr:0.0001
epoch6: train: loss:0.6277438139127306 	 acc:0.5861157024793389 | test: loss:0.6237480353045938 	 acc:0.5854304635761589 	 lr:0.0001
epoch7: train: loss:0.628890540580119 	 acc:0.6046280991735538 | test: loss:0.6230146143610115 	 acc:0.6079470198675496 	 lr:0.0001
epoch8: train: loss:0.6183876200549858 	 acc:0.6300826446280992 | test: loss:0.6143368037331183 	 acc:0.6291390728476821 	 lr:0.0001
epoch9: train: loss:0.6482422411540323 	 acc:0.6545454545454545 | test: loss:0.6479090317195615 	 acc:0.6688741721854304 	 lr:0.0001
epoch10: train: loss:0.6204029900968567 	 acc:0.6680991735537191 | test: loss:0.615858665365257 	 acc:0.6701986754966888 	 lr:0.0001
epoch11: train: loss:0.6188116891719093 	 acc:0.6766942148760331 | test: loss:0.6177703584266814 	 acc:0.6940397350993377 	 lr:0.0001
epoch12: train: loss:0.5976884511876698 	 acc:0.6737190082644628 | test: loss:0.598407298997538 	 acc:0.6503311258278146 	 lr:0.0001
epoch13: train: loss:0.6013068361518796 	 acc:0.6687603305785124 | test: loss:0.6020138401858854 	 acc:0.6569536423841059 	 lr:0.0001
epoch14: train: loss:0.5963237664522218 	 acc:0.6386776859504132 | test: loss:0.5968857261518768 	 acc:0.6158940397350994 	 lr:0.0001
epoch15: train: loss:0.5935542593317583 	 acc:0.6330578512396694 | test: loss:0.599275302176444 	 acc:0.6105960264900663 	 lr:0.0001
epoch16: train: loss:0.592565007525042 	 acc:0.7120661157024794 | test: loss:0.5908958804528445 	 acc:0.7245033112582782 	 lr:0.0001
epoch17: train: loss:0.589781142719521 	 acc:0.7114049586776859 | test: loss:0.5797020952433151 	 acc:0.704635761589404 	 lr:0.0001
epoch18: train: loss:0.5916061851603925 	 acc:0.7180165289256198 | test: loss:0.5903113960430322 	 acc:0.7205298013245033 	 lr:0.0001
epoch19: train: loss:0.5928337875476554 	 acc:0.7107438016528925 | test: loss:0.5801514848968051 	 acc:0.7298013245033113 	 lr:0.0001
epoch20: train: loss:0.5883065805159324 	 acc:0.7328925619834711 | test: loss:0.5829160377679282 	 acc:0.7245033112582782 	 lr:0.0001
epoch21: train: loss:0.6243411346112401 	 acc:0.6813223140495868 | test: loss:0.5983453486928877 	 acc:0.7231788079470198 	 lr:0.0001
epoch22: train: loss:0.5782243393472404 	 acc:0.7338842975206612 | test: loss:0.5684464618859701 	 acc:0.7337748344370861 	 lr:0.0001
epoch23: train: loss:0.6142175290210188 	 acc:0.7014876033057851 | test: loss:0.6217854727182957 	 acc:0.6874172185430464 	 lr:0.0001
epoch24: train: loss:0.5688655994549271 	 acc:0.6849586776859504 | test: loss:0.5679791389711645 	 acc:0.6887417218543046 	 lr:0.0001
epoch25: train: loss:0.5779693188549073 	 acc:0.731900826446281 | test: loss:0.5741950584563198 	 acc:0.7364238410596027 	 lr:0.0001
epoch26: train: loss:0.6273865475930458 	 acc:0.6750413223140496 | test: loss:0.6218329538572703 	 acc:0.6887417218543046 	 lr:0.0001
epoch27: train: loss:0.5758299366304697 	 acc:0.731900826446281 | test: loss:0.5652670703976359 	 acc:0.7165562913907285 	 lr:0.0001
epoch28: train: loss:0.5698516088477836 	 acc:0.7114049586776859 | test: loss:0.5745633786087794 	 acc:0.6701986754966888 	 lr:0.0001
epoch29: train: loss:0.5642939268656013 	 acc:0.7332231404958678 | test: loss:0.5681573023859239 	 acc:0.7006622516556291 	 lr:0.0001
epoch30: train: loss:0.5646219182211506 	 acc:0.707107438016529 | test: loss:0.5692607793586933 	 acc:0.6649006622516557 	 lr:0.0001
epoch31: train: loss:0.5696929886715472 	 acc:0.6733884297520661 | test: loss:0.5651793308605422 	 acc:0.6688741721854304 	 lr:0.0001
epoch32: train: loss:0.56293405921006 	 acc:0.7163636363636363 | test: loss:0.5546632893827578 	 acc:0.7178807947019867 	 lr:0.0001
epoch33: train: loss:0.5554058021947372 	 acc:0.724297520661157 | test: loss:0.5534272264171121 	 acc:0.7324503311258278 	 lr:0.0001
epoch34: train: loss:0.5648377520781903 	 acc:0.7566942148760331 | test: loss:0.5600919800088895 	 acc:0.7443708609271523 	 lr:0.0001
epoch35: train: loss:0.6022241947670613 	 acc:0.5933884297520661 | test: loss:0.6302763064176041 	 acc:0.5549668874172186 	 lr:0.0001
epoch36: train: loss:0.5501131105028894 	 acc:0.7408264462809917 | test: loss:0.5537172638027873 	 acc:0.7470198675496689 	 lr:0.0001
epoch37: train: loss:0.5519430035007886 	 acc:0.7659504132231405 | test: loss:0.547803348263368 	 acc:0.7549668874172185 	 lr:0.0001
epoch38: train: loss:0.5672222365820704 	 acc:0.6548760330578512 | test: loss:0.5761574447549731 	 acc:0.6543046357615894 	 lr:0.0001
epoch39: train: loss:0.5576077511881994 	 acc:0.6991735537190082 | test: loss:0.5583280043096732 	 acc:0.6834437086092715 	 lr:0.0001
epoch40: train: loss:0.5619451635731153 	 acc:0.7586776859504132 | test: loss:0.5604971670157073 	 acc:0.743046357615894 	 lr:0.0001
epoch41: train: loss:0.6040629342567823 	 acc:0.5791735537190082 | test: loss:0.6295609718126967 	 acc:0.5562913907284768 	 lr:0.0001
epoch42: train: loss:0.5423470077041752 	 acc:0.7695867768595042 | test: loss:0.5491274979730315 	 acc:0.7536423841059603 	 lr:0.0001
epoch43: train: loss:0.5697488082144871 	 acc:0.7371900826446282 | test: loss:0.5815366186843013 	 acc:0.7298013245033113 	 lr:0.0001
epoch44: train: loss:0.5421930942653624 	 acc:0.7213223140495868 | test: loss:0.5508508400411796 	 acc:0.7112582781456953 	 lr:5e-05
epoch45: train: loss:0.534426911074268 	 acc:0.7639669421487604 | test: loss:0.5385888762821425 	 acc:0.7589403973509934 	 lr:5e-05
epoch46: train: loss:0.5307427470546123 	 acc:0.7884297520661157 | test: loss:0.5428476503353246 	 acc:0.7549668874172185 	 lr:5e-05
epoch47: train: loss:0.5293949604034424 	 acc:0.7689256198347107 | test: loss:0.5434334955468083 	 acc:0.7350993377483444 	 lr:5e-05
epoch48: train: loss:0.5441835791020354 	 acc:0.7814876033057852 | test: loss:0.5533452988460364 	 acc:0.7350993377483444 	 lr:5e-05
epoch49: train: loss:0.5389047821691214 	 acc:0.7259504132231405 | test: loss:0.5517150094967015 	 acc:0.704635761589404 	 lr:5e-05
epoch50: train: loss:0.5361533198671893 	 acc:0.7695867768595042 | test: loss:0.5411984202877576 	 acc:0.7417218543046358 	 lr:5e-05
epoch51: train: loss:0.5450963884542797 	 acc:0.7282644628099173 | test: loss:0.5499512149008694 	 acc:0.7205298013245033 	 lr:5e-05
epoch52: train: loss:0.5316115682184204 	 acc:0.7699173553719009 | test: loss:0.5380258819125346 	 acc:0.7695364238410596 	 lr:2.5e-05
epoch53: train: loss:0.5295044781747928 	 acc:0.7818181818181819 | test: loss:0.5406171860284363 	 acc:0.766887417218543 	 lr:2.5e-05
epoch54: train: loss:0.5229747845909812 	 acc:0.7662809917355372 | test: loss:0.5348650383633493 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch55: train: loss:0.5275079832195251 	 acc:0.7811570247933884 | test: loss:0.5412152359817202 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch56: train: loss:0.5252693491533769 	 acc:0.7778512396694215 | test: loss:0.5375543469624804 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch57: train: loss:0.5388980517899694 	 acc:0.7867768595041322 | test: loss:0.5445192247826532 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch58: train: loss:0.5201707225397598 	 acc:0.7864462809917355 | test: loss:0.5363131421291275 	 acc:0.7576158940397351 	 lr:2.5e-05
epoch59: train: loss:0.5242786345403057 	 acc:0.7722314049586777 | test: loss:0.5374232839274881 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch60: train: loss:0.522878511227852 	 acc:0.7656198347107438 | test: loss:0.5388678950189755 	 acc:0.7364238410596027 	 lr:2.5e-05
epoch61: train: loss:0.5211547467728291 	 acc:0.7950413223140496 | test: loss:0.5370298708511504 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch62: train: loss:0.5274616200864808 	 acc:0.7993388429752066 | test: loss:0.5433519813398652 	 acc:0.752317880794702 	 lr:1.25e-05
epoch63: train: loss:0.5139220130739133 	 acc:0.7890909090909091 | test: loss:0.5344595593332455 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch64: train: loss:0.5229511900775689 	 acc:0.7920661157024793 | test: loss:0.5386849084437288 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch65: train: loss:0.5227067925319199 	 acc:0.7828099173553719 | test: loss:0.5358733803231195 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch66: train: loss:0.5168945563135068 	 acc:0.7847933884297521 | test: loss:0.533263788396949 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch67: train: loss:0.5190754091838174 	 acc:0.7973553719008264 | test: loss:0.5343233103783714 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch68: train: loss:0.5250945792907525 	 acc:0.7943801652892561 | test: loss:0.5365088900193473 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch69: train: loss:0.5218763500599821 	 acc:0.7834710743801653 | test: loss:0.5327424856211176 	 acc:0.7602649006622516 	 lr:1.25e-05
epoch70: train: loss:0.5142000379345634 	 acc:0.7993388429752066 | test: loss:0.5337472273813968 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch71: train: loss:0.5182986005869779 	 acc:0.7930578512396694 | test: loss:0.5335816305204732 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch72: train: loss:0.5213640583251133 	 acc:0.7950413223140496 | test: loss:0.5353724087310943 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch73: train: loss:0.523712935644733 	 acc:0.7990082644628099 | test: loss:0.5384081525518405 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch74: train: loss:0.5162038152868097 	 acc:0.7980165289256198 | test: loss:0.5340995778311167 	 acc:0.7602649006622516 	 lr:1.25e-05
epoch75: train: loss:0.5180374015461314 	 acc:0.771900826446281 | test: loss:0.5386448334384438 	 acc:0.7337748344370861 	 lr:1.25e-05
epoch76: train: loss:0.5175264853288319 	 acc:0.8046280991735537 | test: loss:0.535401318799581 	 acc:0.7642384105960265 	 lr:6.25e-06
epoch77: train: loss:0.515516245542479 	 acc:0.8049586776859504 | test: loss:0.5345658704145065 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch78: train: loss:0.5168276047214003 	 acc:0.7904132231404959 | test: loss:0.5348697560512466 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch79: train: loss:0.521890147697827 	 acc:0.7933884297520661 | test: loss:0.5359197733418042 	 acc:0.7642384105960265 	 lr:6.25e-06
epoch80: train: loss:0.5158317572617334 	 acc:0.7996694214876033 | test: loss:0.5347163871424088 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch81: train: loss:0.5221553945738422 	 acc:0.7930578512396694 | test: loss:0.5349124155297185 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch82: train: loss:0.5210660421355697 	 acc:0.7765289256198347 | test: loss:0.5338962174409273 	 acc:0.7576158940397351 	 lr:3.125e-06
epoch83: train: loss:0.5155854285551497 	 acc:0.7976859504132231 | test: loss:0.5337740906816445 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch84: train: loss:0.5202621317697951 	 acc:0.7970247933884298 | test: loss:0.5357558180954283 	 acc:0.7629139072847683 	 lr:3.125e-06
epoch85: train: loss:0.5155878017953605 	 acc:0.7917355371900826 | test: loss:0.5345167868974193 	 acc:0.7602649006622516 	 lr:3.125e-06
epoch86: train: loss:0.5160418040299218 	 acc:0.7887603305785124 | test: loss:0.5341324644372952 	 acc:0.7629139072847683 	 lr:3.125e-06
epoch87: train: loss:0.517584906097286 	 acc:0.7927272727272727 | test: loss:0.5344216225163037 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch88: train: loss:0.5210682769255205 	 acc:0.7897520661157025 | test: loss:0.5341365851313863 	 acc:0.7629139072847683 	 lr:1.5625e-06
epoch89: train: loss:0.5153318097965777 	 acc:0.8036363636363636 | test: loss:0.5341514835294509 	 acc:0.7629139072847683 	 lr:1.5625e-06
epoch90: train: loss:0.5150092183459889 	 acc:0.8013223140495868 | test: loss:0.5345639540659671 	 acc:0.7602649006622516 	 lr:1.5625e-06
epoch91: train: loss:0.5200657857351066 	 acc:0.7947107438016529 | test: loss:0.5348426559113508 	 acc:0.7589403973509934 	 lr:1.5625e-06
epoch92: train: loss:0.5179720879783315 	 acc:0.7996694214876033 | test: loss:0.5345475620781349 	 acc:0.7642384105960265 	 lr:1.5625e-06
epoch93: train: loss:0.5148537540829872 	 acc:0.8003305785123966 | test: loss:0.5347478913945078 	 acc:0.7602649006622516 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_9_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_9_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6999731092019514 	 acc:0.5358677685950414 | test: loss:0.692663900267999 	 acc:0.5602649006622517 	 lr:0.0001
epoch1: train: loss:0.6461380926833665 	 acc:0.5500826446280992 | test: loss:0.640767823386666 	 acc:0.5350993377483444 	 lr:0.0001
epoch2: train: loss:0.6395790167682427 	 acc:0.531900826446281 | test: loss:0.6376387270870588 	 acc:0.5231788079470199 	 lr:0.0001
epoch3: train: loss:0.6278873043217935 	 acc:0.643305785123967 | test: loss:0.6224797318313295 	 acc:0.6344370860927152 	 lr:0.0001
epoch4: train: loss:0.6427936909021424 	 acc:0.655206611570248 | test: loss:0.6427428343438155 	 acc:0.6781456953642384 	 lr:0.0001
epoch5: train: loss:0.6104081353077219 	 acc:0.6409917355371901 | test: loss:0.6123688116768338 	 acc:0.6079470198675496 	 lr:0.0001
epoch6: train: loss:0.6281412359899726 	 acc:0.6892561983471074 | test: loss:0.6206905745512602 	 acc:0.7033112582781457 	 lr:0.0001
epoch7: train: loss:0.5877146762461701 	 acc:0.675702479338843 | test: loss:0.5930205313575189 	 acc:0.6344370860927152 	 lr:0.0001
epoch8: train: loss:0.6066195637726587 	 acc:0.7031404958677686 | test: loss:0.6060040616831243 	 acc:0.7059602649006622 	 lr:0.0001
epoch9: train: loss:0.6061943040406408 	 acc:0.5844628099173553 | test: loss:0.6239593396913137 	 acc:0.5682119205298013 	 lr:0.0001
epoch10: train: loss:0.5813256867069844 	 acc:0.7219834710743802 | test: loss:0.5801843095299424 	 acc:0.6993377483443709 	 lr:0.0001
epoch11: train: loss:0.5872036099236859 	 acc:0.6287603305785124 | test: loss:0.5945539911851189 	 acc:0.614569536423841 	 lr:0.0001
epoch12: train: loss:0.5818653628845846 	 acc:0.7147107438016529 | test: loss:0.5794804023591098 	 acc:0.7324503311258278 	 lr:0.0001
epoch13: train: loss:0.5702529098573795 	 acc:0.7213223140495868 | test: loss:0.5626924536086076 	 acc:0.7298013245033113 	 lr:0.0001
epoch14: train: loss:0.5618606014094076 	 acc:0.7256198347107438 | test: loss:0.5680236902457989 	 acc:0.6887417218543046 	 lr:0.0001
epoch15: train: loss:0.5685651806760426 	 acc:0.7461157024793389 | test: loss:0.5689846720127081 	 acc:0.7337748344370861 	 lr:0.0001
epoch16: train: loss:0.5559750531724662 	 acc:0.7213223140495868 | test: loss:0.559642508250988 	 acc:0.7072847682119205 	 lr:0.0001
epoch17: train: loss:0.5614602944082465 	 acc:0.6991735537190082 | test: loss:0.5652983735728737 	 acc:0.7059602649006622 	 lr:0.0001
epoch18: train: loss:0.5777614098343967 	 acc:0.7457851239669422 | test: loss:0.5887263815134567 	 acc:0.7152317880794702 	 lr:0.0001
epoch19: train: loss:0.5524391424951475 	 acc:0.6902479338842975 | test: loss:0.561194257941467 	 acc:0.6913907284768211 	 lr:0.0001
epoch20: train: loss:0.5495699980633318 	 acc:0.7570247933884298 | test: loss:0.5555907180767186 	 acc:0.7218543046357616 	 lr:0.0001
epoch21: train: loss:0.5495123339487501 	 acc:0.7206611570247934 | test: loss:0.5710235326495392 	 acc:0.6688741721854304 	 lr:0.0001
epoch22: train: loss:0.5601743256553146 	 acc:0.6899173553719008 | test: loss:0.5657869593986612 	 acc:0.6834437086092715 	 lr:0.0001
epoch23: train: loss:0.5435045702595356 	 acc:0.7143801652892562 | test: loss:0.5561650265131565 	 acc:0.6966887417218544 	 lr:0.0001
epoch24: train: loss:0.5582549939667883 	 acc:0.6816528925619835 | test: loss:0.5840833124735497 	 acc:0.6397350993377483 	 lr:0.0001
epoch25: train: loss:0.5451720545114564 	 acc:0.7186776859504133 | test: loss:0.553375364849899 	 acc:0.7165562913907285 	 lr:0.0001
epoch26: train: loss:0.5614570065372246 	 acc:0.6935537190082645 | test: loss:0.5850070276797212 	 acc:0.6503311258278146 	 lr:0.0001
epoch27: train: loss:0.5598450307609621 	 acc:0.6697520661157025 | test: loss:0.5809373053493879 	 acc:0.6423841059602649 	 lr:0.0001
epoch28: train: loss:0.547244887667254 	 acc:0.7001652892561984 | test: loss:0.5835026007614388 	 acc:0.6397350993377483 	 lr:0.0001
epoch29: train: loss:0.571830277226188 	 acc:0.6409917355371901 | test: loss:0.598093913722512 	 acc:0.614569536423841 	 lr:0.0001
epoch30: train: loss:0.5310214916930711 	 acc:0.7408264462809917 | test: loss:0.5472224945264147 	 acc:0.7218543046357616 	 lr:0.0001
epoch31: train: loss:0.5273743430050937 	 acc:0.7864462809917355 | test: loss:0.5525288988422874 	 acc:0.7324503311258278 	 lr:0.0001
epoch32: train: loss:0.5521406378627809 	 acc:0.6846280991735537 | test: loss:0.5780040244393001 	 acc:0.6529801324503312 	 lr:0.0001
epoch33: train: loss:0.534423954664183 	 acc:0.7342148760330579 | test: loss:0.562808919110835 	 acc:0.6874172185430464 	 lr:0.0001
epoch34: train: loss:0.5338098117733789 	 acc:0.731900826446281 | test: loss:0.5677958674778212 	 acc:0.6741721854304635 	 lr:0.0001
epoch35: train: loss:0.5208936095237732 	 acc:0.7636363636363637 | test: loss:0.5487711720908715 	 acc:0.7298013245033113 	 lr:0.0001
epoch36: train: loss:0.609746305489343 	 acc:0.7143801652892562 | test: loss:0.6885338884315744 	 acc:0.6158940397350994 	 lr:0.0001
epoch37: train: loss:0.5057265423447632 	 acc:0.7930578512396694 | test: loss:0.537063067717268 	 acc:0.7337748344370861 	 lr:5e-05
epoch38: train: loss:0.5036293289976672 	 acc:0.7831404958677686 | test: loss:0.5387488264911222 	 acc:0.7390728476821192 	 lr:5e-05
epoch39: train: loss:0.5117081868155928 	 acc:0.7910743801652893 | test: loss:0.5399065709271967 	 acc:0.7364238410596027 	 lr:5e-05
epoch40: train: loss:0.5223722479363119 	 acc:0.8109090909090909 | test: loss:0.5733901147810828 	 acc:0.7271523178807947 	 lr:5e-05
epoch41: train: loss:0.5621405480912894 	 acc:0.7563636363636363 | test: loss:0.5967468406190936 	 acc:0.7231788079470198 	 lr:5e-05
epoch42: train: loss:0.5105841106915276 	 acc:0.7692561983471075 | test: loss:0.5516580178248172 	 acc:0.6966887417218544 	 lr:5e-05
epoch43: train: loss:0.5007454707208744 	 acc:0.8013223140495868 | test: loss:0.5396941105261543 	 acc:0.7456953642384105 	 lr:5e-05
epoch44: train: loss:0.5032097087517258 	 acc:0.8221487603305785 | test: loss:0.546938490472882 	 acc:0.7615894039735099 	 lr:2.5e-05
epoch45: train: loss:0.49115129726977386 	 acc:0.8353719008264463 | test: loss:0.5391487316580008 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch46: train: loss:0.4966131945680981 	 acc:0.8274380165289256 | test: loss:0.5401294088521541 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch47: train: loss:0.48898753803623607 	 acc:0.8343801652892562 | test: loss:0.5409622334486601 	 acc:0.7496688741721854 	 lr:2.5e-05
epoch48: train: loss:0.4871106340471378 	 acc:0.8191735537190082 | test: loss:0.5379211051574606 	 acc:0.7311258278145696 	 lr:2.5e-05
epoch49: train: loss:0.48611443825989714 	 acc:0.8221487603305785 | test: loss:0.5405234737901499 	 acc:0.7324503311258278 	 lr:2.5e-05
epoch50: train: loss:0.49301698340857325 	 acc:0.8320661157024793 | test: loss:0.541735623135472 	 acc:0.752317880794702 	 lr:1.25e-05
epoch51: train: loss:0.488032820293726 	 acc:0.8112396694214876 | test: loss:0.5397000479382394 	 acc:0.7298013245033113 	 lr:1.25e-05
epoch52: train: loss:0.48858524651566815 	 acc:0.8337190082644628 | test: loss:0.5411650984492523 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch53: train: loss:0.486391930383099 	 acc:0.832396694214876 | test: loss:0.5371902845553215 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch54: train: loss:0.486796277278711 	 acc:0.8363636363636363 | test: loss:0.5391569790461206 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch55: train: loss:0.48851634964470037 	 acc:0.8224793388429752 | test: loss:0.5387625942956533 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch56: train: loss:0.48502393871299493 	 acc:0.8363636363636363 | test: loss:0.5406823847467536 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch57: train: loss:0.48378354135623647 	 acc:0.8373553719008264 | test: loss:0.5395598448664937 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch58: train: loss:0.48153978467972813 	 acc:0.8330578512396695 | test: loss:0.5364386953265462 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch59: train: loss:0.4831163637500164 	 acc:0.8307438016528925 | test: loss:0.5384820334958714 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch60: train: loss:0.4829032799724705 	 acc:0.8383471074380165 | test: loss:0.5383551732593814 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch61: train: loss:0.48217638461057805 	 acc:0.8350413223140496 | test: loss:0.5380320286119221 	 acc:0.7496688741721854 	 lr:6.25e-06
epoch62: train: loss:0.4825680306529211 	 acc:0.8406611570247934 | test: loss:0.536888974470808 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch63: train: loss:0.4790250898195692 	 acc:0.8343801652892562 | test: loss:0.5366997649338072 	 acc:0.7337748344370861 	 lr:6.25e-06
epoch64: train: loss:0.48175320290336926 	 acc:0.8373553719008264 | test: loss:0.536102564366448 	 acc:0.7311258278145696 	 lr:6.25e-06
epoch65: train: loss:0.48389588402322503 	 acc:0.8423140495867769 | test: loss:0.5368095783997845 	 acc:0.7470198675496689 	 lr:6.25e-06
epoch66: train: loss:0.4809932267764383 	 acc:0.8373553719008264 | test: loss:0.5368292746954406 	 acc:0.7470198675496689 	 lr:6.25e-06
epoch67: train: loss:0.4892687893307899 	 acc:0.8343801652892562 | test: loss:0.5394491750672953 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch68: train: loss:0.48142014874899686 	 acc:0.8380165289256198 | test: loss:0.5384362734706196 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch69: train: loss:0.4840397469465398 	 acc:0.8380165289256198 | test: loss:0.5422098182684538 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch70: train: loss:0.4796912150737668 	 acc:0.8376859504132231 | test: loss:0.5373111237753306 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch71: train: loss:0.48017069227439313 	 acc:0.8386776859504133 | test: loss:0.5363050040030322 	 acc:0.7417218543046358 	 lr:3.125e-06
epoch72: train: loss:0.4796288644183766 	 acc:0.8472727272727273 | test: loss:0.537644392765121 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch73: train: loss:0.48313703826636323 	 acc:0.8343801652892562 | test: loss:0.5386298825409239 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch74: train: loss:0.48031007533231057 	 acc:0.8287603305785124 | test: loss:0.5372767776053473 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch75: train: loss:0.47733029820702294 	 acc:0.8433057851239669 | test: loss:0.5365900861506431 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch76: train: loss:0.48144096981395373 	 acc:0.8340495867768595 | test: loss:0.5356730338753454 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch77: train: loss:0.4816825170457856 	 acc:0.8393388429752067 | test: loss:0.5357222021020801 	 acc:0.7417218543046358 	 lr:3.125e-06
epoch78: train: loss:0.4793969308147746 	 acc:0.8423140495867769 | test: loss:0.5377476386676561 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch79: train: loss:0.47854644363576715 	 acc:0.850909090909091 | test: loss:0.5380385938859144 	 acc:0.7496688741721854 	 lr:3.125e-06
epoch80: train: loss:0.4778687338099992 	 acc:0.8406611570247934 | test: loss:0.5368914495240774 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch81: train: loss:0.47584621609735095 	 acc:0.8406611570247934 | test: loss:0.5356753256936736 	 acc:0.743046357615894 	 lr:3.125e-06
epoch82: train: loss:0.4810107270351126 	 acc:0.8360330578512397 | test: loss:0.5372321859100797 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch83: train: loss:0.4751627993091079 	 acc:0.847603305785124 | test: loss:0.5375653047435331 	 acc:0.7483443708609272 	 lr:1.5625e-06
epoch84: train: loss:0.47715497504581106 	 acc:0.8439669421487603 | test: loss:0.536593217012898 	 acc:0.7509933774834437 	 lr:1.5625e-06
epoch85: train: loss:0.4755749699793571 	 acc:0.8522314049586777 | test: loss:0.5366964445208872 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch86: train: loss:0.48099573542263885 	 acc:0.8403305785123967 | test: loss:0.5364643982704113 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch87: train: loss:0.47842217328134645 	 acc:0.836694214876033 | test: loss:0.5362252380674248 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch88: train: loss:0.48215074190423507 	 acc:0.84 | test: loss:0.536368685289724 	 acc:0.7456953642384105 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_10_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_10_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.687302416553182 	 acc:0.5702479338842975 | test: loss:0.6815914844835042 	 acc:0.6052980132450331 	 lr:0.0001
epoch1: train: loss:0.6438543935846691 	 acc:0.5666115702479338 | test: loss:0.6387108812268996 	 acc:0.5509933774834437 	 lr:0.0001
epoch2: train: loss:0.6358775713227012 	 acc:0.5385123966942149 | test: loss:0.633514776766695 	 acc:0.528476821192053 	 lr:0.0001
epoch3: train: loss:0.6194339978990476 	 acc:0.6581818181818182 | test: loss:0.6132279377899422 	 acc:0.6423841059602649 	 lr:0.0001
epoch4: train: loss:0.6736067473001717 	 acc:0.6132231404958678 | test: loss:0.6911319903980028 	 acc:0.5947019867549669 	 lr:0.0001
epoch5: train: loss:0.5933921437618161 	 acc:0.6866115702479338 | test: loss:0.5952034573681307 	 acc:0.6437086092715232 	 lr:0.0001
epoch6: train: loss:0.5922448031764385 	 acc:0.731900826446281 | test: loss:0.5923528985471915 	 acc:0.713907284768212 	 lr:0.0001
epoch7: train: loss:0.5800045725727869 	 acc:0.6621487603305786 | test: loss:0.5845825762148724 	 acc:0.6423841059602649 	 lr:0.0001
epoch8: train: loss:0.5765660921601224 	 acc:0.699504132231405 | test: loss:0.5833112290363438 	 acc:0.6847682119205298 	 lr:0.0001
epoch9: train: loss:0.6042415073686395 	 acc:0.5943801652892562 | test: loss:0.6268446375992125 	 acc:0.5668874172185431 	 lr:0.0001
epoch10: train: loss:0.5604509649789038 	 acc:0.7282644628099173 | test: loss:0.5721851735715046 	 acc:0.6768211920529801 	 lr:0.0001
epoch11: train: loss:0.5707813968737263 	 acc:0.6727272727272727 | test: loss:0.56559003236278 	 acc:0.6754966887417219 	 lr:0.0001
epoch12: train: loss:0.5829236580517666 	 acc:0.7312396694214875 | test: loss:0.608186501540885 	 acc:0.6993377483443709 	 lr:0.0001
epoch13: train: loss:0.5533746375525294 	 acc:0.7094214876033058 | test: loss:0.5674310616310069 	 acc:0.6728476821192053 	 lr:0.0001
epoch14: train: loss:0.5483489573888543 	 acc:0.728595041322314 | test: loss:0.5593810700422881 	 acc:0.6940397350993377 	 lr:0.0001
epoch15: train: loss:0.581110473861379 	 acc:0.7289256198347107 | test: loss:0.5812326434432276 	 acc:0.7258278145695364 	 lr:0.0001
epoch16: train: loss:0.5383785507304609 	 acc:0.7603305785123967 | test: loss:0.5511672862318178 	 acc:0.7311258278145696 	 lr:0.0001
epoch17: train: loss:0.5450982264644844 	 acc:0.763305785123967 | test: loss:0.5658348040864957 	 acc:0.7417218543046358 	 lr:0.0001
epoch18: train: loss:0.5503308686540147 	 acc:0.7662809917355372 | test: loss:0.5533476670056778 	 acc:0.7417218543046358 	 lr:0.0001
epoch19: train: loss:0.5241491560305446 	 acc:0.7712396694214876 | test: loss:0.5455823070955592 	 acc:0.7284768211920529 	 lr:0.0001
epoch20: train: loss:0.5248459512537176 	 acc:0.7867768595041322 | test: loss:0.5475586647229479 	 acc:0.7536423841059603 	 lr:0.0001
epoch21: train: loss:0.5293046551302445 	 acc:0.780495867768595 | test: loss:0.5467452898720242 	 acc:0.7562913907284768 	 lr:0.0001
epoch22: train: loss:0.5302888403845227 	 acc:0.7517355371900827 | test: loss:0.5475343358437746 	 acc:0.7271523178807947 	 lr:0.0001
epoch23: train: loss:0.5355582037050862 	 acc:0.7100826446280992 | test: loss:0.5702910350647983 	 acc:0.6741721854304635 	 lr:0.0001
epoch24: train: loss:0.5117724919713234 	 acc:0.7890909090909091 | test: loss:0.5337012784370524 	 acc:0.7284768211920529 	 lr:0.0001
epoch25: train: loss:0.5122152986211225 	 acc:0.7798347107438016 | test: loss:0.5437491855873967 	 acc:0.7364238410596027 	 lr:0.0001
epoch26: train: loss:0.5411277450411773 	 acc:0.7758677685950414 | test: loss:0.5712925804371866 	 acc:0.7218543046357616 	 lr:0.0001
epoch27: train: loss:0.5315744935382496 	 acc:0.7104132231404958 | test: loss:0.5720384486463685 	 acc:0.6635761589403973 	 lr:0.0001
epoch28: train: loss:0.5713984983617609 	 acc:0.6383471074380165 | test: loss:0.6190237627913621 	 acc:0.5867549668874172 	 lr:0.0001
epoch29: train: loss:0.5413913470063327 	 acc:0.6856198347107438 | test: loss:0.576878409985675 	 acc:0.6463576158940397 	 lr:0.0001
epoch30: train: loss:0.5674465056096226 	 acc:0.7494214876033057 | test: loss:0.620319562005681 	 acc:0.6940397350993377 	 lr:0.0001
epoch31: train: loss:0.49627569884308115 	 acc:0.8191735537190082 | test: loss:0.5499529534617796 	 acc:0.7165562913907285 	 lr:5e-05
epoch32: train: loss:0.4975698201715454 	 acc:0.7947107438016529 | test: loss:0.5458012431662603 	 acc:0.7258278145695364 	 lr:5e-05
epoch33: train: loss:0.4948101063996307 	 acc:0.8211570247933885 | test: loss:0.5427091476143591 	 acc:0.7298013245033113 	 lr:5e-05
epoch34: train: loss:0.47749067929165423 	 acc:0.8403305785123967 | test: loss:0.5369247873887321 	 acc:0.7350993377483444 	 lr:5e-05
epoch35: train: loss:0.4910513732847103 	 acc:0.7943801652892561 | test: loss:0.5479001998111902 	 acc:0.7152317880794702 	 lr:5e-05
epoch36: train: loss:0.5899377123383451 	 acc:0.7345454545454545 | test: loss:0.6729807455808121 	 acc:0.6569536423841059 	 lr:5e-05
epoch37: train: loss:0.49523997188599644 	 acc:0.775206611570248 | test: loss:0.5500481750791436 	 acc:0.6966887417218544 	 lr:2.5e-05
epoch38: train: loss:0.4723294717122701 	 acc:0.8552066115702479 | test: loss:0.5432644581163166 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch39: train: loss:0.47673811890862205 	 acc:0.8472727272727273 | test: loss:0.5348638100339876 	 acc:0.743046357615894 	 lr:2.5e-05
epoch40: train: loss:0.47923434340264187 	 acc:0.8436363636363636 | test: loss:0.5477626667117441 	 acc:0.7536423841059603 	 lr:2.5e-05
epoch41: train: loss:0.48482852016598726 	 acc:0.8393388429752067 | test: loss:0.5527571570794314 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch42: train: loss:0.4767021993763191 	 acc:0.8092561983471075 | test: loss:0.5398394808074496 	 acc:0.7152317880794702 	 lr:2.5e-05
epoch43: train: loss:0.46660195687585626 	 acc:0.8598347107438017 | test: loss:0.5410349847465161 	 acc:0.743046357615894 	 lr:1.25e-05
epoch44: train: loss:0.4622153954170952 	 acc:0.8515702479338843 | test: loss:0.537714858323533 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch45: train: loss:0.46348808513199985 	 acc:0.8588429752066116 | test: loss:0.5345580267590403 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch46: train: loss:0.4607660215255643 	 acc:0.8657851239669422 | test: loss:0.5369751683923583 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch47: train: loss:0.45979876304460954 	 acc:0.8575206611570247 | test: loss:0.5367922539742577 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch48: train: loss:0.4572160985745674 	 acc:0.8611570247933884 | test: loss:0.536118564305716 	 acc:0.743046357615894 	 lr:1.25e-05
epoch49: train: loss:0.45577801287666825 	 acc:0.8720661157024794 | test: loss:0.5363925674893208 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch50: train: loss:0.4611739362172844 	 acc:0.8571900826446281 | test: loss:0.5345622197681705 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch51: train: loss:0.4597610777567241 	 acc:0.8581818181818182 | test: loss:0.534040132500478 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch52: train: loss:0.45952046166766775 	 acc:0.8565289256198347 | test: loss:0.5344383784477285 	 acc:0.7470198675496689 	 lr:6.25e-06
epoch53: train: loss:0.45867395240413256 	 acc:0.8628099173553719 | test: loss:0.5330085390450938 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch54: train: loss:0.4563798146208456 	 acc:0.8624793388429752 | test: loss:0.5324675961835494 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch55: train: loss:0.45960223736841815 	 acc:0.8568595041322314 | test: loss:0.5353203668499624 	 acc:0.743046357615894 	 lr:6.25e-06
epoch56: train: loss:0.45540342819592183 	 acc:0.8687603305785124 | test: loss:0.5392816173319785 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch57: train: loss:0.45588094966470705 	 acc:0.8730578512396694 | test: loss:0.5378668200101284 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch58: train: loss:0.4549279498364315 	 acc:0.856198347107438 | test: loss:0.5332829094090998 	 acc:0.7417218543046358 	 lr:6.25e-06
epoch59: train: loss:0.456617150730338 	 acc:0.8598347107438017 | test: loss:0.5375560636551965 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch60: train: loss:0.4549117607321621 	 acc:0.8700826446280991 | test: loss:0.5357516921119184 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch61: train: loss:0.4527956272846411 	 acc:0.868099173553719 | test: loss:0.5334058703965698 	 acc:0.743046357615894 	 lr:3.125e-06
epoch62: train: loss:0.4538335854258419 	 acc:0.8641322314049587 | test: loss:0.5318958025894418 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch63: train: loss:0.4521105099611046 	 acc:0.8651239669421488 | test: loss:0.5319992427004884 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch64: train: loss:0.45417716924809226 	 acc:0.859504132231405 | test: loss:0.5324976178194514 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch65: train: loss:0.4547713192238295 	 acc:0.8697520661157024 | test: loss:0.532549538833416 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch66: train: loss:0.4534921290756257 	 acc:0.8624793388429752 | test: loss:0.5321024559980986 	 acc:0.7417218543046358 	 lr:3.125e-06
epoch67: train: loss:0.4568881544002817 	 acc:0.8661157024793389 | test: loss:0.533377398323539 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch68: train: loss:0.4533467235052881 	 acc:0.8710743801652893 | test: loss:0.5354227913136513 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch69: train: loss:0.4518426445890064 	 acc:0.8720661157024794 | test: loss:0.5354797254335012 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch70: train: loss:0.450149512793407 	 acc:0.8740495867768595 | test: loss:0.5339230658202772 	 acc:0.7417218543046358 	 lr:1.5625e-06
epoch71: train: loss:0.45217774972442754 	 acc:0.8664462809917355 | test: loss:0.532859029596215 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch72: train: loss:0.45170095627958123 	 acc:0.8647933884297521 | test: loss:0.5339236655772127 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch73: train: loss:0.45462469082233337 	 acc:0.8611570247933884 | test: loss:0.535308379763799 	 acc:0.7417218543046358 	 lr:1.5625e-06
epoch74: train: loss:0.45478445460973693 	 acc:0.860495867768595 | test: loss:0.5354259558071364 	 acc:0.7443708609271523 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_11_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_11_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6831294385066702 	 acc:0.5811570247933884 | test: loss:0.6762785235777596 	 acc:0.609271523178808 	 lr:0.0001
epoch1: train: loss:0.6423418748280234 	 acc:0.5447933884297521 | test: loss:0.636726991388182 	 acc:0.5350993377483444 	 lr:0.0001
epoch2: train: loss:0.6357972080057318 	 acc:0.5371900826446281 | test: loss:0.6353302278266048 	 acc:0.5258278145695364 	 lr:0.0001
epoch3: train: loss:0.6214539765129404 	 acc:0.6770247933884298 | test: loss:0.6101524420131911 	 acc:0.6701986754966888 	 lr:0.0001
epoch4: train: loss:0.6639233965124965 	 acc:0.6201652892561983 | test: loss:0.6901749570638138 	 acc:0.5986754966887418 	 lr:0.0001
epoch5: train: loss:0.5954719333018154 	 acc:0.6386776859504132 | test: loss:0.5951797393773566 	 acc:0.6304635761589404 	 lr:0.0001
epoch6: train: loss:0.5881782492133212 	 acc:0.7289256198347107 | test: loss:0.5807481911798187 	 acc:0.7231788079470198 	 lr:0.0001
epoch7: train: loss:0.5833149243386324 	 acc:0.6393388429752066 | test: loss:0.6023175388771966 	 acc:0.6039735099337749 	 lr:0.0001
epoch8: train: loss:0.5718276635674405 	 acc:0.7282644628099173 | test: loss:0.576037970994482 	 acc:0.7033112582781457 	 lr:0.0001
epoch9: train: loss:0.5976685776592287 	 acc:0.6006611570247934 | test: loss:0.6227441804298502 	 acc:0.5761589403973509 	 lr:0.0001
epoch10: train: loss:0.5626704373044416 	 acc:0.6902479338842975 | test: loss:0.588484520075337 	 acc:0.633112582781457 	 lr:0.0001
epoch11: train: loss:0.5496855680410527 	 acc:0.7219834710743802 | test: loss:0.5584751219938923 	 acc:0.6993377483443709 	 lr:0.0001
epoch12: train: loss:0.5796703585908433 	 acc:0.7365289256198347 | test: loss:0.6111988497885648 	 acc:0.7112582781456953 	 lr:0.0001
epoch13: train: loss:0.5478234683383595 	 acc:0.7338842975206612 | test: loss:0.5516821922055933 	 acc:0.7072847682119205 	 lr:0.0001
epoch14: train: loss:0.5491991563473851 	 acc:0.7312396694214875 | test: loss:0.5590595380359927 	 acc:0.7059602649006622 	 lr:0.0001
epoch15: train: loss:0.6046714799660297 	 acc:0.7087603305785124 | test: loss:0.6033291838026994 	 acc:0.7165562913907285 	 lr:0.0001
epoch16: train: loss:0.5341567170915524 	 acc:0.7408264462809917 | test: loss:0.5505755599760851 	 acc:0.7165562913907285 	 lr:0.0001
epoch17: train: loss:0.5361065752447144 	 acc:0.7818181818181819 | test: loss:0.5583324991314617 	 acc:0.7417218543046358 	 lr:0.0001
epoch18: train: loss:0.5345856425584841 	 acc:0.7781818181818182 | test: loss:0.5542779219071596 	 acc:0.7364238410596027 	 lr:0.0001
epoch19: train: loss:0.5153526506739214 	 acc:0.7867768595041322 | test: loss:0.5386211591051114 	 acc:0.7364238410596027 	 lr:0.0001
epoch20: train: loss:0.541871175509839 	 acc:0.7798347107438016 | test: loss:0.5806079422401277 	 acc:0.7258278145695364 	 lr:0.0001
epoch21: train: loss:0.5183822511148847 	 acc:0.795702479338843 | test: loss:0.5521472327756566 	 acc:0.7509933774834437 	 lr:0.0001
epoch22: train: loss:0.5260143520418278 	 acc:0.7867768595041322 | test: loss:0.5498864834671778 	 acc:0.7456953642384105 	 lr:0.0001
epoch23: train: loss:0.5345503386387155 	 acc:0.7090909090909091 | test: loss:0.5756767140319016 	 acc:0.6503311258278146 	 lr:0.0001
epoch24: train: loss:0.507367061839616 	 acc:0.8085950413223141 | test: loss:0.5356990506317442 	 acc:0.7456953642384105 	 lr:0.0001
epoch25: train: loss:0.5125438033253693 	 acc:0.7520661157024794 | test: loss:0.5560132592719123 	 acc:0.6913907284768211 	 lr:0.0001
epoch26: train: loss:0.5037545667601026 	 acc:0.8003305785123966 | test: loss:0.551744004037996 	 acc:0.7245033112582782 	 lr:0.0001
epoch27: train: loss:0.4939869902744766 	 acc:0.8066115702479338 | test: loss:0.5340309870164126 	 acc:0.743046357615894 	 lr:0.0001
epoch28: train: loss:0.5616115083970314 	 acc:0.6614876033057852 | test: loss:0.6169119471745775 	 acc:0.5973509933774834 	 lr:0.0001
epoch29: train: loss:0.505198250506535 	 acc:0.7570247933884298 | test: loss:0.543238084284675 	 acc:0.713907284768212 	 lr:0.0001
epoch30: train: loss:0.5216434649790614 	 acc:0.7973553719008264 | test: loss:0.5502841611571659 	 acc:0.7456953642384105 	 lr:0.0001
epoch31: train: loss:0.48662445711695457 	 acc:0.8135537190082645 | test: loss:0.5379317340471886 	 acc:0.7456953642384105 	 lr:0.0001
epoch32: train: loss:0.5054436273220156 	 acc:0.8168595041322314 | test: loss:0.5614658535711023 	 acc:0.7483443708609272 	 lr:0.0001
epoch33: train: loss:0.5099177140440823 	 acc:0.7490909090909091 | test: loss:0.579276137162518 	 acc:0.6476821192052981 	 lr:0.0001
epoch34: train: loss:0.4707413721478675 	 acc:0.8386776859504133 | test: loss:0.5393450379371643 	 acc:0.7218543046357616 	 lr:5e-05
epoch35: train: loss:0.4976399020419633 	 acc:0.7662809917355372 | test: loss:0.5586785861987942 	 acc:0.6781456953642384 	 lr:5e-05
epoch36: train: loss:0.5172706590408136 	 acc:0.807603305785124 | test: loss:0.600205248870597 	 acc:0.6993377483443709 	 lr:5e-05
epoch37: train: loss:0.4814839048129468 	 acc:0.7785123966942149 | test: loss:0.5490416674424481 	 acc:0.7019867549668874 	 lr:5e-05
epoch38: train: loss:0.4575722966115337 	 acc:0.8641322314049587 | test: loss:0.5463758200209662 	 acc:0.7403973509933774 	 lr:5e-05
epoch39: train: loss:0.47712807674053287 	 acc:0.8452892561983472 | test: loss:0.5496389438774412 	 acc:0.743046357615894 	 lr:5e-05
epoch40: train: loss:0.45840986157251784 	 acc:0.8671074380165289 | test: loss:0.5546979276549737 	 acc:0.7284768211920529 	 lr:2.5e-05
epoch41: train: loss:0.4788730090314692 	 acc:0.8492561983471074 | test: loss:0.5857084852970199 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch42: train: loss:0.46299533634146384 	 acc:0.819504132231405 | test: loss:0.5469087429393996 	 acc:0.7033112582781457 	 lr:2.5e-05
epoch43: train: loss:0.45548198966940573 	 acc:0.872396694214876 | test: loss:0.5566530851338873 	 acc:0.7403973509933774 	 lr:2.5e-05
epoch44: train: loss:0.4440635153183267 	 acc:0.8816528925619834 | test: loss:0.5454729194672692 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch45: train: loss:0.4380793290197357 	 acc:0.8859504132231405 | test: loss:0.5407876013920007 	 acc:0.743046357615894 	 lr:2.5e-05
epoch46: train: loss:0.43055157990495035 	 acc:0.8852892561983471 | test: loss:0.5336282153792729 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch47: train: loss:0.4367181414217988 	 acc:0.8866115702479339 | test: loss:0.5466696818932792 	 acc:0.7417218543046358 	 lr:1.25e-05
epoch48: train: loss:0.43174575251981245 	 acc:0.8720661157024794 | test: loss:0.5339486882386618 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch49: train: loss:0.42761107078268507 	 acc:0.8981818181818182 | test: loss:0.5315063556298515 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch50: train: loss:0.4339781228077313 	 acc:0.8740495867768595 | test: loss:0.5302866121001591 	 acc:0.752317880794702 	 lr:1.25e-05
epoch51: train: loss:0.4328357762837213 	 acc:0.8902479338842976 | test: loss:0.5396606458733414 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch52: train: loss:0.4337302170115069 	 acc:0.8849586776859504 | test: loss:0.532956173956789 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch53: train: loss:0.4340033610005024 	 acc:0.8806611570247934 | test: loss:0.5270487999284504 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch54: train: loss:0.42667112145542113 	 acc:0.8895867768595042 | test: loss:0.531792794710753 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch55: train: loss:0.4320333941514827 	 acc:0.891900826446281 | test: loss:0.5372729105665194 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch56: train: loss:0.4395911454464778 	 acc:0.8899173553719009 | test: loss:0.5543249241563658 	 acc:0.7377483443708609 	 lr:1.25e-05
epoch57: train: loss:0.42511367095403435 	 acc:0.8819834710743801 | test: loss:0.531729885521314 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch58: train: loss:0.42593583463637297 	 acc:0.8971900826446281 | test: loss:0.5382009094914064 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch59: train: loss:0.42691715279886544 	 acc:0.8922314049586777 | test: loss:0.5370301846636841 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch60: train: loss:0.423434506162139 	 acc:0.8938842975206611 | test: loss:0.5312093481322787 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch61: train: loss:0.4226415727059703 	 acc:0.8961983471074381 | test: loss:0.5321386711486917 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch62: train: loss:0.4228321822517174 	 acc:0.8932231404958678 | test: loss:0.530823815737339 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch63: train: loss:0.4197406195313477 	 acc:0.8961983471074381 | test: loss:0.529521945454427 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch64: train: loss:0.4196134353472182 	 acc:0.8988429752066116 | test: loss:0.5305059444035916 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch65: train: loss:0.42484908292116214 	 acc:0.8988429752066116 | test: loss:0.5297177568176724 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch66: train: loss:0.42154198427830847 	 acc:0.8961983471074381 | test: loss:0.5302882848985937 	 acc:0.7536423841059603 	 lr:3.125e-06
epoch67: train: loss:0.4259472650141755 	 acc:0.8872727272727273 | test: loss:0.5335595920385904 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch68: train: loss:0.42211240789121834 	 acc:0.9011570247933884 | test: loss:0.5339769385508354 	 acc:0.7509933774834437 	 lr:3.125e-06
epoch69: train: loss:0.42078385590521755 	 acc:0.9008264462809917 | test: loss:0.5354824587209335 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch70: train: loss:0.41966792824839755 	 acc:0.8965289256198347 | test: loss:0.5348262902127197 	 acc:0.743046357615894 	 lr:3.125e-06
epoch71: train: loss:0.42129726746850765 	 acc:0.8938842975206611 | test: loss:0.5324094608130044 	 acc:0.7496688741721854 	 lr:3.125e-06
epoch72: train: loss:0.42002866259291155 	 acc:0.9008264462809917 | test: loss:0.5340849388514134 	 acc:0.7509933774834437 	 lr:1.5625e-06
epoch73: train: loss:0.4186617072357619 	 acc:0.9024793388429752 | test: loss:0.5364877856330367 	 acc:0.7417218543046358 	 lr:1.5625e-06
epoch74: train: loss:0.42416571154082117 	 acc:0.8856198347107438 | test: loss:0.5369868753761645 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch75: train: loss:0.41759949445724487 	 acc:0.9011570247933884 | test: loss:0.5320945136594456 	 acc:0.752317880794702 	 lr:1.5625e-06
epoch76: train: loss:0.4197609806159311 	 acc:0.9047933884297521 | test: loss:0.5307416871683487 	 acc:0.7509933774834437 	 lr:1.5625e-06
epoch77: train: loss:0.42170866307148264 	 acc:0.8981818181818182 | test: loss:0.5321252832349562 	 acc:0.7509933774834437 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_12_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_12_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6835456598888744 	 acc:0.5804958677685951 | test: loss:0.6766969342894902 	 acc:0.6211920529801325 	 lr:0.0001
epoch1: train: loss:0.6399566528422773 	 acc:0.5672727272727273 | test: loss:0.633942222042589 	 acc:0.5417218543046357 	 lr:0.0001
epoch2: train: loss:0.6279792524566336 	 acc:0.5497520661157025 | test: loss:0.6245301187433154 	 acc:0.543046357615894 	 lr:0.0001
epoch3: train: loss:0.6070435580340299 	 acc:0.6803305785123966 | test: loss:0.5964531514818305 	 acc:0.671523178807947 	 lr:0.0001
epoch4: train: loss:0.6379597810477264 	 acc:0.656198347107438 | test: loss:0.6534560187762937 	 acc:0.6582781456953642 	 lr:0.0001
epoch5: train: loss:0.5880613979426297 	 acc:0.6714049586776859 | test: loss:0.5873292652186969 	 acc:0.6662251655629139 	 lr:0.0001
epoch6: train: loss:0.580176790194078 	 acc:0.739504132231405 | test: loss:0.5758833662563602 	 acc:0.7311258278145696 	 lr:0.0001
epoch7: train: loss:0.573252891568113 	 acc:0.6651239669421488 | test: loss:0.581482413430877 	 acc:0.6437086092715232 	 lr:0.0001
epoch8: train: loss:0.5630146602953761 	 acc:0.752396694214876 | test: loss:0.5696533783382138 	 acc:0.7245033112582782 	 lr:0.0001
epoch9: train: loss:0.5895192099208674 	 acc:0.6185123966942149 | test: loss:0.6111969436241301 	 acc:0.6026490066225165 	 lr:0.0001
epoch10: train: loss:0.5940042422625644 	 acc:0.6115702479338843 | test: loss:0.6198132282061293 	 acc:0.5748344370860927 	 lr:0.0001
epoch11: train: loss:0.5539462614453529 	 acc:0.6978512396694215 | test: loss:0.5668258876200544 	 acc:0.6688741721854304 	 lr:0.0001
epoch12: train: loss:0.5582202581925826 	 acc:0.7636363636363637 | test: loss:0.5893402431974348 	 acc:0.7245033112582782 	 lr:0.0001
epoch13: train: loss:0.5447847457168516 	 acc:0.7487603305785124 | test: loss:0.5477631712591411 	 acc:0.7470198675496689 	 lr:0.0001
epoch14: train: loss:0.5446688118847933 	 acc:0.7345454545454545 | test: loss:0.5569870699320408 	 acc:0.7258278145695364 	 lr:0.0001
epoch15: train: loss:0.5959356979496223 	 acc:0.7173553719008264 | test: loss:0.6341216027341932 	 acc:0.6834437086092715 	 lr:0.0001
epoch16: train: loss:0.5366946199314654 	 acc:0.7170247933884297 | test: loss:0.5530895344468931 	 acc:0.6940397350993377 	 lr:0.0001
epoch17: train: loss:0.5089255323685891 	 acc:0.7970247933884298 | test: loss:0.5362596988677979 	 acc:0.7549668874172185 	 lr:0.0001
epoch18: train: loss:0.5272887228343113 	 acc:0.7775206611570248 | test: loss:0.5563855801197077 	 acc:0.7470198675496689 	 lr:0.0001
epoch19: train: loss:0.49309549138565695 	 acc:0.7983471074380165 | test: loss:0.531352935484703 	 acc:0.7390728476821192 	 lr:0.0001
epoch20: train: loss:0.5137098849706413 	 acc:0.8175206611570248 | test: loss:0.5608991165824284 	 acc:0.743046357615894 	 lr:0.0001
epoch21: train: loss:0.5128775790604678 	 acc:0.8095867768595041 | test: loss:0.5344893971815804 	 acc:0.7536423841059603 	 lr:0.0001
epoch22: train: loss:0.5166538821961268 	 acc:0.8046280991735537 | test: loss:0.5510604736820751 	 acc:0.7589403973509934 	 lr:0.0001
epoch23: train: loss:0.5156817568235161 	 acc:0.731900826446281 | test: loss:0.5652019181788362 	 acc:0.6662251655629139 	 lr:0.0001
epoch24: train: loss:0.49220788976377694 	 acc:0.827107438016529 | test: loss:0.5295342256691282 	 acc:0.752317880794702 	 lr:0.0001
epoch25: train: loss:0.49127211836743945 	 acc:0.7980165289256198 | test: loss:0.5549087919936275 	 acc:0.7152317880794702 	 lr:0.0001
epoch26: train: loss:0.49702091081083316 	 acc:0.8102479338842975 | test: loss:0.5506692321884711 	 acc:0.7337748344370861 	 lr:0.0001
epoch27: train: loss:0.4730282869417805 	 acc:0.8300826446280992 | test: loss:0.529248122900527 	 acc:0.7284768211920529 	 lr:0.0001
epoch28: train: loss:0.5217990406288588 	 acc:0.7110743801652892 | test: loss:0.5917404951638733 	 acc:0.633112582781457 	 lr:0.0001
epoch29: train: loss:0.5238264715967099 	 acc:0.7170247933884297 | test: loss:0.5629095612772254 	 acc:0.6887417218543046 	 lr:0.0001
epoch30: train: loss:0.4919551267013077 	 acc:0.8373553719008264 | test: loss:0.5631157380066171 	 acc:0.7390728476821192 	 lr:0.0001
epoch31: train: loss:0.5021666693490399 	 acc:0.8145454545454546 | test: loss:0.5907832727526987 	 acc:0.713907284768212 	 lr:0.0001
epoch32: train: loss:0.5091256220084577 	 acc:0.8049586776859504 | test: loss:0.5723111925535644 	 acc:0.7403973509933774 	 lr:0.0001
epoch33: train: loss:0.49201241038062354 	 acc:0.7705785123966942 | test: loss:0.5842409207331424 	 acc:0.6463576158940397 	 lr:0.0001
epoch34: train: loss:0.4459368728507649 	 acc:0.8671074380165289 | test: loss:0.5332130575811627 	 acc:0.7403973509933774 	 lr:5e-05
epoch35: train: loss:0.45733268490507584 	 acc:0.8284297520661157 | test: loss:0.5346010955753705 	 acc:0.7258278145695364 	 lr:5e-05
epoch36: train: loss:0.47313623211600564 	 acc:0.8538842975206612 | test: loss:0.5900028040077513 	 acc:0.7231788079470198 	 lr:5e-05
epoch37: train: loss:0.44721928663490235 	 acc:0.8436363636363636 | test: loss:0.5322142937325484 	 acc:0.7337748344370861 	 lr:5e-05
epoch38: train: loss:0.4463447603903526 	 acc:0.8575206611570247 | test: loss:0.5284987098333851 	 acc:0.7417218543046358 	 lr:5e-05
epoch39: train: loss:0.4427326919815757 	 acc:0.8581818181818182 | test: loss:0.5346214963900332 	 acc:0.7231788079470198 	 lr:5e-05
epoch40: train: loss:0.4481731256177603 	 acc:0.8538842975206612 | test: loss:0.5448730475065724 	 acc:0.7377483443708609 	 lr:5e-05
epoch41: train: loss:0.5521017218621309 	 acc:0.7738842975206611 | test: loss:0.7096579336962163 	 acc:0.6344370860927152 	 lr:5e-05
epoch42: train: loss:0.43068272267491364 	 acc:0.8806611570247934 | test: loss:0.5338059288776473 	 acc:0.743046357615894 	 lr:5e-05
epoch43: train: loss:0.4295202579380067 	 acc:0.8882644628099173 | test: loss:0.5393346614395546 	 acc:0.7549668874172185 	 lr:5e-05
epoch44: train: loss:0.4201818228162025 	 acc:0.8932231404958678 | test: loss:0.5437969007239436 	 acc:0.7417218543046358 	 lr:5e-05
epoch45: train: loss:0.4146006125162456 	 acc:0.9074380165289256 | test: loss:0.5454466497661262 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch46: train: loss:0.40785677498037165 	 acc:0.9067768595041322 | test: loss:0.5267368654541622 	 acc:0.7562913907284768 	 lr:2.5e-05
epoch47: train: loss:0.40700216379047427 	 acc:0.9110743801652893 | test: loss:0.5422186027299489 	 acc:0.7496688741721854 	 lr:2.5e-05
epoch48: train: loss:0.41596183275388293 	 acc:0.8829752066115703 | test: loss:0.5355026254590773 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch49: train: loss:0.401603084044023 	 acc:0.9213223140495868 | test: loss:0.5366134202243477 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch50: train: loss:0.4021037080071189 	 acc:0.9133884297520661 | test: loss:0.5297595879100017 	 acc:0.7536423841059603 	 lr:2.5e-05
epoch51: train: loss:0.40038763990086956 	 acc:0.9150413223140496 | test: loss:0.5388503425168675 	 acc:0.752317880794702 	 lr:2.5e-05
epoch52: train: loss:0.4227938854398806 	 acc:0.9001652892561983 | test: loss:0.5537603023825892 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch53: train: loss:0.4046623768195633 	 acc:0.9057851239669421 | test: loss:0.5248942011239512 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch54: train: loss:0.3937819615080337 	 acc:0.927603305785124 | test: loss:0.5320038338370671 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch55: train: loss:0.4035241445135479 	 acc:0.92099173553719 | test: loss:0.5433930258087765 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch56: train: loss:0.398983681477791 	 acc:0.9219834710743802 | test: loss:0.5423242934492251 	 acc:0.743046357615894 	 lr:1.25e-05
epoch57: train: loss:0.3932935549602036 	 acc:0.9193388429752066 | test: loss:0.5259874094400975 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch58: train: loss:0.3973231938161141 	 acc:0.92099173553719 | test: loss:0.5356088377782051 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch59: train: loss:0.3962527549365335 	 acc:0.9223140495867769 | test: loss:0.535692123387823 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch60: train: loss:0.3924907741664855 	 acc:0.9302479338842975 | test: loss:0.5343748705276591 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch61: train: loss:0.3888678321858083 	 acc:0.9371900826446281 | test: loss:0.5333370152688184 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch62: train: loss:0.3938360228420289 	 acc:0.9252892561983471 | test: loss:0.5371238203238178 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch63: train: loss:0.39156610628790106 	 acc:0.9256198347107438 | test: loss:0.5323234269161098 	 acc:0.7417218543046358 	 lr:6.25e-06
epoch64: train: loss:0.39033909449892595 	 acc:0.9236363636363636 | test: loss:0.5307455699964865 	 acc:0.7417218543046358 	 lr:6.25e-06
epoch65: train: loss:0.391268468967154 	 acc:0.9249586776859504 | test: loss:0.5299119360399562 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch66: train: loss:0.39236983023399163 	 acc:0.9216528925619835 | test: loss:0.5289575136260481 	 acc:0.7536423841059603 	 lr:3.125e-06
epoch67: train: loss:0.3888004678832598 	 acc:0.931900826446281 | test: loss:0.5301786263257462 	 acc:0.7536423841059603 	 lr:3.125e-06
epoch68: train: loss:0.3895522890997327 	 acc:0.9322314049586777 | test: loss:0.5313628976708217 	 acc:0.7496688741721854 	 lr:3.125e-06
epoch69: train: loss:0.3915082041586726 	 acc:0.931900826446281 | test: loss:0.5352152350722559 	 acc:0.752317880794702 	 lr:3.125e-06
epoch70: train: loss:0.3881600051970521 	 acc:0.9305785123966942 | test: loss:0.5322029831393665 	 acc:0.7536423841059603 	 lr:3.125e-06
epoch71: train: loss:0.39174699500572585 	 acc:0.92099173553719 | test: loss:0.5295594378812424 	 acc:0.7536423841059603 	 lr:3.125e-06
epoch72: train: loss:0.38837185622246795 	 acc:0.9282644628099174 | test: loss:0.531616264384314 	 acc:0.7509933774834437 	 lr:1.5625e-06
epoch73: train: loss:0.3879347143882562 	 acc:0.9299173553719008 | test: loss:0.5338501981552074 	 acc:0.7483443708609272 	 lr:1.5625e-06
epoch74: train: loss:0.39588135054288814 	 acc:0.9180165289256198 | test: loss:0.5329694975290867 	 acc:0.7483443708609272 	 lr:1.5625e-06
epoch75: train: loss:0.38892721273682335 	 acc:0.9305785123966942 | test: loss:0.5297788933412918 	 acc:0.7509933774834437 	 lr:1.5625e-06
epoch76: train: loss:0.3876553865799234 	 acc:0.9269421487603305 | test: loss:0.5291739221440246 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch77: train: loss:0.3886262356348274 	 acc:0.9355371900826446 | test: loss:0.5314140615873779 	 acc:0.752317880794702 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_13_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_13_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6722355993326046 	 acc:0.5960330578512397 | test: loss:0.6678140503681259 	 acc:0.6132450331125828 	 lr:0.0001
epoch1: train: loss:0.6403536975482279 	 acc:0.5920661157024794 | test: loss:0.6331600448153666 	 acc:0.5801324503311258 	 lr:0.0001
epoch2: train: loss:0.6250650263423762 	 acc:0.5566942148760331 | test: loss:0.6227132752241678 	 acc:0.5509933774834437 	 lr:0.0001
epoch3: train: loss:0.5993587584731993 	 acc:0.6783471074380165 | test: loss:0.592654556391255 	 acc:0.6754966887417219 	 lr:0.0001
epoch4: train: loss:0.6001470286392968 	 acc:0.7011570247933885 | test: loss:0.6107405199120376 	 acc:0.7019867549668874 	 lr:0.0001
epoch5: train: loss:0.5836560278096475 	 acc:0.7024793388429752 | test: loss:0.5849898497000435 	 acc:0.6821192052980133 	 lr:0.0001
epoch6: train: loss:0.5650309745142282 	 acc:0.7557024793388429 | test: loss:0.5643999264729733 	 acc:0.7271523178807947 	 lr:0.0001
epoch7: train: loss:0.5728238744381046 	 acc:0.6733884297520661 | test: loss:0.5715525290823931 	 acc:0.671523178807947 	 lr:0.0001
epoch8: train: loss:0.5576414704322815 	 acc:0.7626446280991735 | test: loss:0.5629241839939395 	 acc:0.7271523178807947 	 lr:0.0001
epoch9: train: loss:0.5703283548946223 	 acc:0.656198347107438 | test: loss:0.5970170527893975 	 acc:0.6185430463576159 	 lr:0.0001
epoch10: train: loss:0.5745336878004152 	 acc:0.6568595041322314 | test: loss:0.6019585279439459 	 acc:0.6158940397350994 	 lr:0.0001
epoch11: train: loss:0.5405076495083896 	 acc:0.7269421487603306 | test: loss:0.559325496171484 	 acc:0.6847682119205298 	 lr:0.0001
epoch12: train: loss:0.5458601126789061 	 acc:0.7500826446280991 | test: loss:0.5784710776726931 	 acc:0.7271523178807947 	 lr:0.0001
epoch13: train: loss:0.5253004908364667 	 acc:0.7563636363636363 | test: loss:0.5448090911701026 	 acc:0.7033112582781457 	 lr:0.0001
epoch14: train: loss:0.5198657552269864 	 acc:0.7927272727272727 | test: loss:0.5431489424989713 	 acc:0.7615894039735099 	 lr:0.0001
epoch15: train: loss:0.551296596940884 	 acc:0.7636363636363637 | test: loss:0.5655484820043803 	 acc:0.7364238410596027 	 lr:0.0001
epoch16: train: loss:0.5205488299929406 	 acc:0.7358677685950413 | test: loss:0.5448243343277482 	 acc:0.7152317880794702 	 lr:0.0001
epoch17: train: loss:0.5081711465859217 	 acc:0.8082644628099174 | test: loss:0.5413776381126303 	 acc:0.7642384105960265 	 lr:0.0001
epoch18: train: loss:0.49430363091555507 	 acc:0.8072727272727273 | test: loss:0.536944099530479 	 acc:0.7483443708609272 	 lr:0.0001
epoch19: train: loss:0.48266711002539014 	 acc:0.8089256198347108 | test: loss:0.5287277820094531 	 acc:0.7417218543046358 	 lr:0.0001
epoch20: train: loss:0.5091714460396569 	 acc:0.8251239669421487 | test: loss:0.5833616298555538 	 acc:0.7311258278145696 	 lr:0.0001
epoch21: train: loss:0.4941844245520505 	 acc:0.7953719008264463 | test: loss:0.538819448205809 	 acc:0.7099337748344371 	 lr:0.0001
epoch22: train: loss:0.5069771837301491 	 acc:0.8155371900826446 | test: loss:0.5392046044993875 	 acc:0.7549668874172185 	 lr:0.0001
epoch23: train: loss:0.48253278585504894 	 acc:0.792396694214876 | test: loss:0.5356172057966523 	 acc:0.7271523178807947 	 lr:0.0001
epoch24: train: loss:0.48604591514453416 	 acc:0.8310743801652892 | test: loss:0.5289115443924405 	 acc:0.7589403973509934 	 lr:0.0001
epoch25: train: loss:0.4653840737007866 	 acc:0.8297520661157025 | test: loss:0.5466783992502073 	 acc:0.7456953642384105 	 lr:0.0001
epoch26: train: loss:0.4604528215995505 	 acc:0.8363636363636363 | test: loss:0.5304458719215646 	 acc:0.7403973509933774 	 lr:5e-05
epoch27: train: loss:0.4620107180619043 	 acc:0.8148760330578513 | test: loss:0.5460904085083513 	 acc:0.7019867549668874 	 lr:5e-05
epoch28: train: loss:0.4772396083016041 	 acc:0.7947107438016529 | test: loss:0.5585952741420822 	 acc:0.6887417218543046 	 lr:5e-05
epoch29: train: loss:0.452891399761862 	 acc:0.871404958677686 | test: loss:0.557787088921528 	 acc:0.7496688741721854 	 lr:5e-05
epoch30: train: loss:0.43388187408447265 	 acc:0.8895867768595042 | test: loss:0.530981001553946 	 acc:0.7576158940397351 	 lr:5e-05
epoch31: train: loss:0.42763033905305153 	 acc:0.8955371900826447 | test: loss:0.5290531865018883 	 acc:0.7549668874172185 	 lr:5e-05
epoch32: train: loss:0.42894561316356183 	 acc:0.8849586776859504 | test: loss:0.5293703496061414 	 acc:0.7615894039735099 	 lr:2.5e-05
epoch33: train: loss:0.4222579097747803 	 acc:0.8876033057851239 | test: loss:0.5270732017542352 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch34: train: loss:0.4136280287001744 	 acc:0.9044628099173554 | test: loss:0.5255404252090201 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch35: train: loss:0.4206848586492302 	 acc:0.8975206611570248 | test: loss:0.530895211759782 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch36: train: loss:0.42703141431177943 	 acc:0.8991735537190083 | test: loss:0.5515690859579883 	 acc:0.7403973509933774 	 lr:2.5e-05
epoch37: train: loss:0.4182277011969858 	 acc:0.891900826446281 | test: loss:0.531336621969741 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch38: train: loss:0.40787303178763584 	 acc:0.912396694214876 | test: loss:0.5301828798079333 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch39: train: loss:0.41438639016190837 	 acc:0.9028099173553719 | test: loss:0.5267850268755527 	 acc:0.7536423841059603 	 lr:2.5e-05
epoch40: train: loss:0.41815485033121974 	 acc:0.9057851239669421 | test: loss:0.5547900049102228 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch41: train: loss:0.4054772871683452 	 acc:0.9143801652892563 | test: loss:0.53422779768508 	 acc:0.7377483443708609 	 lr:1.25e-05
epoch42: train: loss:0.4129423008772953 	 acc:0.9074380165289256 | test: loss:0.5442294353680894 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch43: train: loss:0.4013371563548884 	 acc:0.9090909090909091 | test: loss:0.5310269373931632 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch44: train: loss:0.3991560879817679 	 acc:0.9166942148760331 | test: loss:0.5336871270312379 	 acc:0.7364238410596027 	 lr:1.25e-05
epoch45: train: loss:0.40311894164597695 	 acc:0.9196694214876033 | test: loss:0.5339708837452314 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch46: train: loss:0.40085183778085 	 acc:0.9213223140495868 | test: loss:0.5339829955669428 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch47: train: loss:0.39712231787768276 	 acc:0.9216528925619835 | test: loss:0.5397310779584165 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch48: train: loss:0.39585667767800575 	 acc:0.9190082644628099 | test: loss:0.5307518322736222 	 acc:0.7602649006622516 	 lr:6.25e-06
epoch49: train: loss:0.39137349090300316 	 acc:0.9295867768595041 | test: loss:0.530996458735687 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch50: train: loss:0.3980062777641391 	 acc:0.9183471074380165 | test: loss:0.5316562694429562 	 acc:0.7470198675496689 	 lr:6.25e-06
epoch51: train: loss:0.39942584482106297 	 acc:0.9176859504132231 | test: loss:0.5339857631171776 	 acc:0.752317880794702 	 lr:6.25e-06
epoch52: train: loss:0.40227631279259674 	 acc:0.9107438016528926 | test: loss:0.537668622171642 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch53: train: loss:0.4029171472935637 	 acc:0.9133884297520661 | test: loss:0.5369676611281389 	 acc:0.7509933774834437 	 lr:3.125e-06
epoch54: train: loss:0.3960636394674128 	 acc:0.9203305785123967 | test: loss:0.5333808847610524 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch55: train: loss:0.3995598996276698 	 acc:0.9180165289256198 | test: loss:0.5364118064476164 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch56: train: loss:0.39284461029304946 	 acc:0.9292561983471075 | test: loss:0.5384385063158755 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch57: train: loss:0.3911866391197709 	 acc:0.9295867768595041 | test: loss:0.5393473597551813 	 acc:0.7390728476821192 	 lr:3.125e-06
epoch58: train: loss:0.3971725735585552 	 acc:0.9193388429752066 | test: loss:0.5398848022056731 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch59: train: loss:0.39425044720823116 	 acc:0.924297520661157 | test: loss:0.5371284498284195 	 acc:0.7456953642384105 	 lr:1.5625e-06
epoch60: train: loss:0.39571793356217627 	 acc:0.9236363636363636 | test: loss:0.541300927171644 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch61: train: loss:0.39049724918751677 	 acc:0.9335537190082644 | test: loss:0.539212706862696 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch62: train: loss:0.39639266948069424 	 acc:0.9256198347107438 | test: loss:0.5375398507970848 	 acc:0.7456953642384105 	 lr:1.5625e-06
epoch63: train: loss:0.395273950011277 	 acc:0.9213223140495868 | test: loss:0.535516781049059 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch64: train: loss:0.3945148182604924 	 acc:0.9193388429752066 | test: loss:0.5334105748214469 	 acc:0.7549668874172185 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_14_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_14_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6616420477875008 	 acc:0.5742148760330579 | test: loss:0.6556453225628429 	 acc:0.5682119205298013 	 lr:0.0001
epoch1: train: loss:0.6339148023502886 	 acc:0.5884297520661157 | test: loss:0.6288693252778211 	 acc:0.5602649006622517 	 lr:0.0001
epoch2: train: loss:0.6203513946809059 	 acc:0.5623140495867769 | test: loss:0.6202883698292916 	 acc:0.552317880794702 	 lr:0.0001
epoch3: train: loss:0.5877026402063606 	 acc:0.6846280991735537 | test: loss:0.5823469501457467 	 acc:0.6834437086092715 	 lr:0.0001
epoch4: train: loss:0.5914065858746363 	 acc:0.7180165289256198 | test: loss:0.6008845022972057 	 acc:0.7059602649006622 	 lr:0.0001
epoch5: train: loss:0.5712044535392572 	 acc:0.6773553719008264 | test: loss:0.5874202078541383 	 acc:0.6476821192052981 	 lr:0.0001
epoch6: train: loss:0.5516674160563256 	 acc:0.7249586776859505 | test: loss:0.558737280116176 	 acc:0.713907284768212 	 lr:0.0001
epoch7: train: loss:0.547603221964245 	 acc:0.7322314049586777 | test: loss:0.5529972110363032 	 acc:0.7231788079470198 	 lr:0.0001
epoch8: train: loss:0.5354431703662084 	 acc:0.7659504132231405 | test: loss:0.5576094806589038 	 acc:0.7059602649006622 	 lr:0.0001
epoch9: train: loss:0.5656725267142304 	 acc:0.6694214876033058 | test: loss:0.5928490930835143 	 acc:0.6278145695364239 	 lr:0.0001
epoch10: train: loss:0.572445860322842 	 acc:0.6618181818181819 | test: loss:0.6014172602173509 	 acc:0.6132450331125828 	 lr:0.0001
epoch11: train: loss:0.5201507265114588 	 acc:0.7735537190082644 | test: loss:0.5404643901926003 	 acc:0.7350993377483444 	 lr:0.0001
epoch12: train: loss:0.5323217468616391 	 acc:0.7874380165289256 | test: loss:0.5806485519503916 	 acc:0.7165562913907285 	 lr:0.0001
epoch13: train: loss:0.5186585315594003 	 acc:0.7990082644628099 | test: loss:0.5453527718979791 	 acc:0.743046357615894 	 lr:0.0001
epoch14: train: loss:0.50339703179588 	 acc:0.819504132231405 | test: loss:0.5397279842010397 	 acc:0.752317880794702 	 lr:0.0001
epoch15: train: loss:0.5188942748850042 	 acc:0.7834710743801653 | test: loss:0.5470789073318835 	 acc:0.7324503311258278 	 lr:0.0001
epoch16: train: loss:0.5356307406267844 	 acc:0.703801652892562 | test: loss:0.5556704931701256 	 acc:0.7019867549668874 	 lr:0.0001
epoch17: train: loss:0.49541713604257126 	 acc:0.812892561983471 | test: loss:0.5421007651366935 	 acc:0.7456953642384105 	 lr:0.0001
epoch18: train: loss:0.4903943360837038 	 acc:0.7851239669421488 | test: loss:0.5373880378457884 	 acc:0.7377483443708609 	 lr:0.0001
epoch19: train: loss:0.4826290156821574 	 acc:0.7970247933884298 | test: loss:0.5368211592270049 	 acc:0.7125827814569536 	 lr:0.0001
epoch20: train: loss:0.46863394990440244 	 acc:0.8552066115702479 | test: loss:0.5442842035104107 	 acc:0.7602649006622516 	 lr:0.0001
epoch21: train: loss:0.5444272352447195 	 acc:0.7880991735537191 | test: loss:0.6429933521131806 	 acc:0.6768211920529801 	 lr:0.0001
epoch22: train: loss:0.4675903570454968 	 acc:0.8413223140495868 | test: loss:0.5387550166111119 	 acc:0.7231788079470198 	 lr:0.0001
epoch23: train: loss:0.44953121683814307 	 acc:0.8522314049586777 | test: loss:0.5387536391517185 	 acc:0.7337748344370861 	 lr:0.0001
epoch24: train: loss:0.4568982713872736 	 acc:0.8522314049586777 | test: loss:0.5307318710333464 	 acc:0.743046357615894 	 lr:0.0001
epoch25: train: loss:0.458847453860212 	 acc:0.819504132231405 | test: loss:0.5448186486762091 	 acc:0.7218543046357616 	 lr:0.0001
epoch26: train: loss:0.4548045071787085 	 acc:0.8462809917355372 | test: loss:0.5471954067811271 	 acc:0.7178807947019867 	 lr:0.0001
epoch27: train: loss:0.4488130742853338 	 acc:0.8575206611570247 | test: loss:0.5421688225095636 	 acc:0.7496688741721854 	 lr:0.0001
epoch28: train: loss:0.4406088674265491 	 acc:0.8717355371900827 | test: loss:0.5227438085916026 	 acc:0.7549668874172185 	 lr:0.0001
epoch29: train: loss:0.4546713189251167 	 acc:0.8429752066115702 | test: loss:0.5450139561236299 	 acc:0.7337748344370861 	 lr:0.0001
epoch30: train: loss:0.4580186582793874 	 acc:0.8614876033057851 | test: loss:0.5567789274335697 	 acc:0.7483443708609272 	 lr:0.0001
epoch31: train: loss:0.43369865662795454 	 acc:0.8859504132231405 | test: loss:0.5585628901885835 	 acc:0.7324503311258278 	 lr:0.0001
epoch32: train: loss:0.4352567400715568 	 acc:0.8763636363636363 | test: loss:0.5479002499422491 	 acc:0.7443708609271523 	 lr:0.0001
epoch33: train: loss:0.46059673358586206 	 acc:0.811900826446281 | test: loss:0.5575728780386464 	 acc:0.6900662251655629 	 lr:0.0001
epoch34: train: loss:0.42988261890805457 	 acc:0.8869421487603306 | test: loss:0.5307291235355351 	 acc:0.7576158940397351 	 lr:0.0001
epoch35: train: loss:0.420587931664522 	 acc:0.8869421487603306 | test: loss:0.552269044617154 	 acc:0.7417218543046358 	 lr:5e-05
epoch36: train: loss:0.4073052290057348 	 acc:0.9094214876033058 | test: loss:0.5475569428197595 	 acc:0.7443708609271523 	 lr:5e-05
epoch37: train: loss:0.40520099901955975 	 acc:0.9044628099173554 | test: loss:0.5306906674081916 	 acc:0.7509933774834437 	 lr:5e-05
epoch38: train: loss:0.39986744622553677 	 acc:0.9077685950413223 | test: loss:0.5369360517981826 	 acc:0.7470198675496689 	 lr:5e-05
epoch39: train: loss:0.3976533742581517 	 acc:0.9130578512396694 | test: loss:0.5305874561631916 	 acc:0.7509933774834437 	 lr:5e-05
epoch40: train: loss:0.400790499103956 	 acc:0.9206611570247933 | test: loss:0.5655846482870595 	 acc:0.7390728476821192 	 lr:5e-05
epoch41: train: loss:0.391767804415758 	 acc:0.9176859504132231 | test: loss:0.5325037840975831 	 acc:0.7403973509933774 	 lr:2.5e-05
epoch42: train: loss:0.3896697744948805 	 acc:0.9266115702479338 | test: loss:0.536794818552914 	 acc:0.743046357615894 	 lr:2.5e-05
epoch43: train: loss:0.39001630533825266 	 acc:0.9285950413223141 | test: loss:0.5659197347053629 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch44: train: loss:0.3798591445595765 	 acc:0.9302479338842975 | test: loss:0.5250028829700899 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch45: train: loss:0.38560966036536476 	 acc:0.9371900826446281 | test: loss:0.5451024383109137 	 acc:0.743046357615894 	 lr:2.5e-05
epoch46: train: loss:0.38328726002007474 	 acc:0.9325619834710743 | test: loss:0.5303536936147324 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch47: train: loss:0.3772113489809115 	 acc:0.9401652892561984 | test: loss:0.5364839731462744 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch48: train: loss:0.37776091554933344 	 acc:0.9348760330578513 | test: loss:0.5324926047135663 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch49: train: loss:0.3734760679587845 	 acc:0.9461157024793388 | test: loss:0.5366923381161216 	 acc:0.752317880794702 	 lr:1.25e-05
epoch50: train: loss:0.37655494543146495 	 acc:0.9401652892561984 | test: loss:0.5377759264794406 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch51: train: loss:0.3767426966635649 	 acc:0.9398347107438016 | test: loss:0.5478562522408189 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch52: train: loss:0.37764154227311947 	 acc:0.9431404958677686 | test: loss:0.5479538982277674 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch53: train: loss:0.37857314470385717 	 acc:0.9368595041322314 | test: loss:0.5406370067438543 	 acc:0.743046357615894 	 lr:6.25e-06
epoch54: train: loss:0.3729417210866597 	 acc:0.9411570247933885 | test: loss:0.5321423187950589 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch55: train: loss:0.37507032843660715 	 acc:0.9467768595041323 | test: loss:0.5384511392637594 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch56: train: loss:0.371755650654312 	 acc:0.9510743801652892 | test: loss:0.5451055459628832 	 acc:0.7470198675496689 	 lr:6.25e-06
epoch57: train: loss:0.37168983975717845 	 acc:0.9457851239669421 | test: loss:0.5465911700236087 	 acc:0.7390728476821192 	 lr:6.25e-06
epoch58: train: loss:0.37548081986175097 	 acc:0.9411570247933885 | test: loss:0.5405868774218275 	 acc:0.743046357615894 	 lr:6.25e-06
epoch59: train: loss:0.37288468379619694 	 acc:0.943801652892562 | test: loss:0.5401064463798574 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch60: train: loss:0.37224556106181184 	 acc:0.9487603305785124 | test: loss:0.5461050510406494 	 acc:0.7364238410596027 	 lr:3.125e-06
epoch61: train: loss:0.3700137288412772 	 acc:0.9454545454545454 | test: loss:0.5446831747396103 	 acc:0.7350993377483444 	 lr:3.125e-06
epoch62: train: loss:0.37731626562835757 	 acc:0.9358677685950413 | test: loss:0.5434454568174502 	 acc:0.7364238410596027 	 lr:3.125e-06
epoch63: train: loss:0.3724848447949433 	 acc:0.9414876033057851 | test: loss:0.5411670383238635 	 acc:0.7377483443708609 	 lr:3.125e-06
epoch64: train: loss:0.372892746925354 	 acc:0.9388429752066115 | test: loss:0.5382185729134161 	 acc:0.7456953642384105 	 lr:3.125e-06
epoch65: train: loss:0.3722520367665724 	 acc:0.9461157024793388 | test: loss:0.5396631600051526 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch66: train: loss:0.37155334199755646 	 acc:0.944793388429752 | test: loss:0.5402812605662062 	 acc:0.7390728476821192 	 lr:1.5625e-06
epoch67: train: loss:0.370908047433727 	 acc:0.9434710743801653 | test: loss:0.5395500768099399 	 acc:0.7390728476821192 	 lr:1.5625e-06
epoch68: train: loss:0.3716604205300985 	 acc:0.943801652892562 | test: loss:0.539991500835545 	 acc:0.7403973509933774 	 lr:1.5625e-06
epoch69: train: loss:0.3704866803973174 	 acc:0.9487603305785124 | test: loss:0.541370211058105 	 acc:0.7403973509933774 	 lr:1.5625e-06
epoch70: train: loss:0.3734811134968907 	 acc:0.9418181818181818 | test: loss:0.5407116487326211 	 acc:0.7417218543046358 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_15_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_15_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6849183888868852 	 acc:0.5695867768595041 | test: loss:0.6836442510023811 	 acc:0.5695364238410596 	 lr:0.0001
epoch1: train: loss:0.6341217757059523 	 acc:0.6009917355371901 | test: loss:0.620437585360167 	 acc:0.6039735099337749 	 lr:0.0001
epoch2: train: loss:0.61632134118356 	 acc:0.5801652892561984 | test: loss:0.6153173913229381 	 acc:0.5735099337748344 	 lr:0.0001
epoch3: train: loss:0.6017565264189539 	 acc:0.6968595041322314 | test: loss:0.5920757025282904 	 acc:0.6900662251655629 	 lr:0.0001
epoch4: train: loss:0.5841525236437143 	 acc:0.724297520661157 | test: loss:0.5837695462024765 	 acc:0.7165562913907285 	 lr:0.0001
epoch5: train: loss:0.5696286579005975 	 acc:0.7272727272727273 | test: loss:0.5723913076697595 	 acc:0.7364238410596027 	 lr:0.0001
epoch6: train: loss:0.562081825969633 	 acc:0.7418181818181818 | test: loss:0.5784504801232294 	 acc:0.7099337748344371 	 lr:0.0001
epoch7: train: loss:0.5585018148304017 	 acc:0.6955371900826446 | test: loss:0.5752418221227381 	 acc:0.6609271523178808 	 lr:0.0001
epoch8: train: loss:0.5456786810268055 	 acc:0.7087603305785124 | test: loss:0.5589689534231527 	 acc:0.6887417218543046 	 lr:0.0001
epoch9: train: loss:0.5475709835162833 	 acc:0.7649586776859504 | test: loss:0.5644538418346683 	 acc:0.7311258278145696 	 lr:0.0001
epoch10: train: loss:0.5389894647243594 	 acc:0.7133884297520661 | test: loss:0.5627681751124907 	 acc:0.680794701986755 	 lr:0.0001
epoch11: train: loss:0.557144981809884 	 acc:0.7573553719008265 | test: loss:0.5831682282567814 	 acc:0.7284768211920529 	 lr:0.0001
epoch12: train: loss:0.5266390192410177 	 acc:0.7424793388429752 | test: loss:0.5472210842252567 	 acc:0.7086092715231788 	 lr:0.0001
epoch13: train: loss:0.5148423585812908 	 acc:0.7781818181818182 | test: loss:0.5470210170903743 	 acc:0.7403973509933774 	 lr:0.0001
epoch14: train: loss:0.5039245828419677 	 acc:0.8099173553719008 | test: loss:0.5484078330709444 	 acc:0.7245033112582782 	 lr:0.0001
epoch15: train: loss:0.5043775263798138 	 acc:0.8165289256198347 | test: loss:0.5460286289650873 	 acc:0.7456953642384105 	 lr:0.0001
epoch16: train: loss:0.5315985855189237 	 acc:0.7034710743801653 | test: loss:0.5729705535812883 	 acc:0.6543046357615894 	 lr:0.0001
epoch17: train: loss:0.49350553435727584 	 acc:0.8023140495867769 | test: loss:0.5421239978430287 	 acc:0.7311258278145696 	 lr:0.0001
epoch18: train: loss:0.6030703984016229 	 acc:0.708099173553719 | test: loss:0.657986704482148 	 acc:0.6662251655629139 	 lr:0.0001
epoch19: train: loss:0.5232248829218967 	 acc:0.743801652892562 | test: loss:0.5821625024277642 	 acc:0.6728476821192053 	 lr:0.0001
epoch20: train: loss:0.49715330880535535 	 acc:0.7798347107438016 | test: loss:0.5587473788798251 	 acc:0.7099337748344371 	 lr:0.0001
epoch21: train: loss:0.48101490237496114 	 acc:0.8469421487603306 | test: loss:0.5720415137461479 	 acc:0.7258278145695364 	 lr:0.0001
epoch22: train: loss:0.5001681327228704 	 acc:0.7570247933884298 | test: loss:0.5651389978579338 	 acc:0.6821192052980133 	 lr:0.0001
epoch23: train: loss:0.4814072491216265 	 acc:0.83900826446281 | test: loss:0.5693017047762081 	 acc:0.7218543046357616 	 lr:0.0001
epoch24: train: loss:0.4540380635241832 	 acc:0.8492561983471074 | test: loss:0.545737407460118 	 acc:0.7178807947019867 	 lr:5e-05
epoch25: train: loss:0.4524791837526747 	 acc:0.8674380165289256 | test: loss:0.5448524093785823 	 acc:0.7350993377483444 	 lr:5e-05
epoch26: train: loss:0.44474759246692186 	 acc:0.871404958677686 | test: loss:0.5577692276594655 	 acc:0.7271523178807947 	 lr:5e-05
epoch27: train: loss:0.4451643554238248 	 acc:0.8671074380165289 | test: loss:0.5482442924518459 	 acc:0.7245033112582782 	 lr:5e-05
epoch28: train: loss:0.43825188528407705 	 acc:0.8687603305785124 | test: loss:0.5347223495016035 	 acc:0.7337748344370861 	 lr:5e-05
epoch29: train: loss:0.4404714595089274 	 acc:0.8717355371900827 | test: loss:0.5333150569176832 	 acc:0.7470198675496689 	 lr:5e-05
epoch30: train: loss:0.44185703943583593 	 acc:0.8677685950413223 | test: loss:0.5367742914237723 	 acc:0.7364238410596027 	 lr:5e-05
epoch31: train: loss:0.4434793536426607 	 acc:0.8783471074380166 | test: loss:0.5632041950888981 	 acc:0.7258278145695364 	 lr:5e-05
epoch32: train: loss:0.43337643698227307 	 acc:0.8928925619834711 | test: loss:0.559429526250094 	 acc:0.7258278145695364 	 lr:5e-05
epoch33: train: loss:0.42539510053051405 	 acc:0.8935537190082644 | test: loss:0.559747131376077 	 acc:0.7324503311258278 	 lr:5e-05
epoch34: train: loss:0.42676405029848585 	 acc:0.8905785123966942 | test: loss:0.5504020273290723 	 acc:0.7258278145695364 	 lr:5e-05
epoch35: train: loss:0.42176449687027734 	 acc:0.8902479338842976 | test: loss:0.5430681166269922 	 acc:0.7377483443708609 	 lr:5e-05
epoch36: train: loss:0.4208529003198482 	 acc:0.9077685950413223 | test: loss:0.5615558701635196 	 acc:0.7311258278145696 	 lr:2.5e-05
epoch37: train: loss:0.4275268776259146 	 acc:0.8664462809917355 | test: loss:0.5441527751107879 	 acc:0.7165562913907285 	 lr:2.5e-05
epoch38: train: loss:0.41282979838119066 	 acc:0.8955371900826447 | test: loss:0.5386530582478505 	 acc:0.7350993377483444 	 lr:2.5e-05
epoch39: train: loss:0.4110740120844408 	 acc:0.9077685950413223 | test: loss:0.5520727062067449 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch40: train: loss:0.4068908597221059 	 acc:0.9067768595041322 | test: loss:0.5532564352679726 	 acc:0.7284768211920529 	 lr:2.5e-05
epoch41: train: loss:0.40672973084055686 	 acc:0.908099173553719 | test: loss:0.5455209547320738 	 acc:0.7231788079470198 	 lr:2.5e-05
epoch42: train: loss:0.3994252357404094 	 acc:0.9186776859504132 | test: loss:0.5474736793151754 	 acc:0.7337748344370861 	 lr:1.25e-05
epoch43: train: loss:0.4032650729939957 	 acc:0.9120661157024793 | test: loss:0.5417005970778055 	 acc:0.7377483443708609 	 lr:1.25e-05
epoch44: train: loss:0.3987051178108562 	 acc:0.9223140495867769 | test: loss:0.5553594819757323 	 acc:0.7337748344370861 	 lr:1.25e-05
epoch45: train: loss:0.40424216575859007 	 acc:0.9107438016528926 | test: loss:0.5428363518209647 	 acc:0.7350993377483444 	 lr:1.25e-05
epoch46: train: loss:0.4000950983437625 	 acc:0.9173553719008265 | test: loss:0.542410074480322 	 acc:0.7350993377483444 	 lr:1.25e-05
epoch47: train: loss:0.3975708471152408 	 acc:0.9163636363636364 | test: loss:0.5480449974142163 	 acc:0.7377483443708609 	 lr:1.25e-05
epoch48: train: loss:0.39795141543238616 	 acc:0.9226446280991736 | test: loss:0.5484694165109799 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch49: train: loss:0.39678690310352105 	 acc:0.9262809917355372 | test: loss:0.5399183831467533 	 acc:0.7284768211920529 	 lr:6.25e-06
epoch50: train: loss:0.39936712380283135 	 acc:0.9213223140495868 | test: loss:0.5459007167658269 	 acc:0.7390728476821192 	 lr:6.25e-06
epoch51: train: loss:0.3966580913973249 	 acc:0.9252892561983471 | test: loss:0.5487070098618009 	 acc:0.7417218543046358 	 lr:6.25e-06
epoch52: train: loss:0.39286407406665075 	 acc:0.9246280991735537 | test: loss:0.5479773825367555 	 acc:0.7390728476821192 	 lr:6.25e-06
epoch53: train: loss:0.39647270329727613 	 acc:0.9180165289256198 | test: loss:0.5429277512411408 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch54: train: loss:0.3969125075281159 	 acc:0.9196694214876033 | test: loss:0.5453511831776195 	 acc:0.7417218543046358 	 lr:3.125e-06
epoch55: train: loss:0.39115875178132176 	 acc:0.9262809917355372 | test: loss:0.5461684133043352 	 acc:0.7337748344370861 	 lr:3.125e-06
epoch56: train: loss:0.38828637672849925 	 acc:0.9292561983471075 | test: loss:0.5448920511252043 	 acc:0.7390728476821192 	 lr:3.125e-06
epoch57: train: loss:0.39595682483074096 	 acc:0.9130578512396694 | test: loss:0.5424021274048761 	 acc:0.7350993377483444 	 lr:3.125e-06
epoch58: train: loss:0.39305771509477916 	 acc:0.9256198347107438 | test: loss:0.5433670455256835 	 acc:0.7390728476821192 	 lr:3.125e-06
epoch59: train: loss:0.39507643845455703 	 acc:0.9256198347107438 | test: loss:0.5482783705982941 	 acc:0.7377483443708609 	 lr:3.125e-06
epoch60: train: loss:0.39034092408566434 	 acc:0.9282644628099174 | test: loss:0.5499970768461164 	 acc:0.7364238410596027 	 lr:1.5625e-06
epoch61: train: loss:0.3954699305266388 	 acc:0.9223140495867769 | test: loss:0.553764374761392 	 acc:0.7364238410596027 	 lr:1.5625e-06
epoch62: train: loss:0.3925772769096469 	 acc:0.9239669421487603 | test: loss:0.5507170719026729 	 acc:0.7324503311258278 	 lr:1.5625e-06
epoch63: train: loss:0.391691540459956 	 acc:0.9252892561983471 | test: loss:0.5499515610814883 	 acc:0.7364238410596027 	 lr:1.5625e-06
epoch64: train: loss:0.3963398666224204 	 acc:0.92099173553719 | test: loss:0.5494659843034302 	 acc:0.7324503311258278 	 lr:1.5625e-06
epoch65: train: loss:0.39909995966706396 	 acc:0.915702479338843 | test: loss:0.5495730726924164 	 acc:0.7324503311258278 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_16_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_16_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6790819918813784 	 acc:0.5864462809917356 | test: loss:0.6832460513967552 	 acc:0.5880794701986755 	 lr:0.0001
epoch1: train: loss:0.629982904777054 	 acc:0.6446280991735537 | test: loss:0.6281967850710383 	 acc:0.6463576158940397 	 lr:0.0001
epoch2: train: loss:0.595084557296816 	 acc:0.6535537190082644 | test: loss:0.5943538540246471 	 acc:0.6662251655629139 	 lr:0.0001
epoch3: train: loss:0.5899635153565524 	 acc:0.7114049586776859 | test: loss:0.5933554402250327 	 acc:0.6821192052980133 	 lr:0.0001
epoch4: train: loss:0.5752807591966361 	 acc:0.7008264462809918 | test: loss:0.5788769252252894 	 acc:0.7086092715231788 	 lr:0.0001
epoch5: train: loss:0.5806071704084222 	 acc:0.7051239669421487 | test: loss:0.597530297964614 	 acc:0.7125827814569536 	 lr:0.0001
epoch6: train: loss:0.5644095864965896 	 acc:0.7490909090909091 | test: loss:0.5866832547629905 	 acc:0.7006622516556291 	 lr:0.0001
epoch7: train: loss:0.5453070237419823 	 acc:0.7424793388429752 | test: loss:0.5700545391499602 	 acc:0.6940397350993377 	 lr:0.0001
epoch8: train: loss:0.5382333583674155 	 acc:0.7259504132231405 | test: loss:0.5674408064772751 	 acc:0.6927152317880795 	 lr:0.0001
epoch9: train: loss:0.5287807183029238 	 acc:0.7742148760330578 | test: loss:0.550912768240796 	 acc:0.7112582781456953 	 lr:0.0001
epoch10: train: loss:0.5560952671500277 	 acc:0.6786776859504132 | test: loss:0.5831167121596684 	 acc:0.6423841059602649 	 lr:0.0001
epoch11: train: loss:0.5199588494458475 	 acc:0.7897520661157025 | test: loss:0.5495580856373768 	 acc:0.7245033112582782 	 lr:0.0001
epoch12: train: loss:0.5293427299862066 	 acc:0.7375206611570247 | test: loss:0.5632063972239463 	 acc:0.6940397350993377 	 lr:0.0001
epoch13: train: loss:0.50544277043382 	 acc:0.7725619834710744 | test: loss:0.550479175476049 	 acc:0.7231788079470198 	 lr:0.0001
epoch14: train: loss:0.5035767101453356 	 acc:0.7874380165289256 | test: loss:0.5536032863800099 	 acc:0.7072847682119205 	 lr:0.0001
epoch15: train: loss:0.4906023039699586 	 acc:0.8241322314049587 | test: loss:0.5489946256410208 	 acc:0.7324503311258278 	 lr:0.0001
epoch16: train: loss:0.4910167713303211 	 acc:0.7871074380165289 | test: loss:0.5505918130179904 	 acc:0.6927152317880795 	 lr:0.0001
epoch17: train: loss:0.49049250706168246 	 acc:0.8168595041322314 | test: loss:0.5527388372958101 	 acc:0.7192052980132451 	 lr:0.0001
epoch18: train: loss:0.4904857355898077 	 acc:0.8171900826446281 | test: loss:0.5542092544353561 	 acc:0.7284768211920529 	 lr:0.0001
epoch19: train: loss:0.4895229514374221 	 acc:0.8175206611570248 | test: loss:0.5680350801013163 	 acc:0.7192052980132451 	 lr:0.0001
epoch20: train: loss:0.47846580345768575 	 acc:0.8290909090909091 | test: loss:0.5688928859123331 	 acc:0.704635761589404 	 lr:0.0001
epoch21: train: loss:0.46274357068637184 	 acc:0.8565289256198347 | test: loss:0.5517361092251658 	 acc:0.7324503311258278 	 lr:0.0001
epoch22: train: loss:0.4712210304382419 	 acc:0.8099173553719008 | test: loss:0.5452887554831852 	 acc:0.7218543046357616 	 lr:5e-05
epoch23: train: loss:0.4583555311604965 	 acc:0.8542148760330579 | test: loss:0.5649372449773826 	 acc:0.7258278145695364 	 lr:5e-05
epoch24: train: loss:0.4529760506527483 	 acc:0.8743801652892562 | test: loss:0.5576609161515899 	 acc:0.7350993377483444 	 lr:5e-05
epoch25: train: loss:0.4434620953886962 	 acc:0.8528925619834711 | test: loss:0.5452934063033552 	 acc:0.7112582781456953 	 lr:5e-05
epoch26: train: loss:0.438097903580705 	 acc:0.856198347107438 | test: loss:0.5428338768466419 	 acc:0.7284768211920529 	 lr:5e-05
epoch27: train: loss:0.4291924560070038 	 acc:0.8826446280991735 | test: loss:0.5377878900395324 	 acc:0.743046357615894 	 lr:5e-05
epoch28: train: loss:0.4331832009307609 	 acc:0.8856198347107438 | test: loss:0.5416463688509354 	 acc:0.743046357615894 	 lr:5e-05
epoch29: train: loss:0.4418936418600319 	 acc:0.8902479338842976 | test: loss:0.5690687837190186 	 acc:0.7311258278145696 	 lr:5e-05
epoch30: train: loss:0.43482349399692755 	 acc:0.8631404958677686 | test: loss:0.534751991562496 	 acc:0.7496688741721854 	 lr:5e-05
epoch31: train: loss:0.4241789685497599 	 acc:0.8836363636363637 | test: loss:0.5394865102325843 	 acc:0.7377483443708609 	 lr:5e-05
epoch32: train: loss:0.41788663453307034 	 acc:0.9044628099173554 | test: loss:0.5430382066215111 	 acc:0.7364238410596027 	 lr:5e-05
epoch33: train: loss:0.4208366188333054 	 acc:0.888595041322314 | test: loss:0.5580564180746773 	 acc:0.7165562913907285 	 lr:5e-05
epoch34: train: loss:0.4250193290671041 	 acc:0.8783471074380166 | test: loss:0.541238779895353 	 acc:0.7152317880794702 	 lr:5e-05
epoch35: train: loss:0.4139641261593369 	 acc:0.8895867768595042 | test: loss:0.5482522201064407 	 acc:0.7218543046357616 	 lr:5e-05
epoch36: train: loss:0.41952590161118625 	 acc:0.8971900826446281 | test: loss:0.5593421733142524 	 acc:0.743046357615894 	 lr:5e-05
epoch37: train: loss:0.41674257720797514 	 acc:0.8786776859504132 | test: loss:0.5428699749984489 	 acc:0.7245033112582782 	 lr:2.5e-05
epoch38: train: loss:0.4054823982814127 	 acc:0.9011570247933884 | test: loss:0.5403001463176399 	 acc:0.7350993377483444 	 lr:2.5e-05
epoch39: train: loss:0.40824235857025654 	 acc:0.9153719008264463 | test: loss:0.5576702543441823 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch40: train: loss:0.4005236220852403 	 acc:0.9090909090909091 | test: loss:0.5467815316276045 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch41: train: loss:0.40666444329190843 	 acc:0.9084297520661156 | test: loss:0.5467318043014071 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch42: train: loss:0.39769936027605673 	 acc:0.9127272727272727 | test: loss:0.5425563770414188 	 acc:0.7245033112582782 	 lr:2.5e-05
epoch43: train: loss:0.39811285058328927 	 acc:0.9147107438016528 | test: loss:0.544510265849284 	 acc:0.7245033112582782 	 lr:1.25e-05
epoch44: train: loss:0.39529141919671995 	 acc:0.9256198347107438 | test: loss:0.5630549434794495 	 acc:0.7337748344370861 	 lr:1.25e-05
epoch45: train: loss:0.3998459950754465 	 acc:0.9143801652892563 | test: loss:0.5430127492014146 	 acc:0.7284768211920529 	 lr:1.25e-05
epoch46: train: loss:0.39648911959868816 	 acc:0.9163636363636364 | test: loss:0.5430985430218526 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch47: train: loss:0.387286525994293 	 acc:0.9312396694214876 | test: loss:0.5495711144232592 	 acc:0.7377483443708609 	 lr:1.25e-05
epoch48: train: loss:0.3906626898789209 	 acc:0.9272727272727272 | test: loss:0.5512123846849858 	 acc:0.7364238410596027 	 lr:1.25e-05
epoch49: train: loss:0.38933699717206405 	 acc:0.9249586776859504 | test: loss:0.5455286606258114 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch50: train: loss:0.39041441839588575 	 acc:0.9269421487603305 | test: loss:0.5502845993894615 	 acc:0.7350993377483444 	 lr:6.25e-06
epoch51: train: loss:0.39033723054838576 	 acc:0.9292561983471075 | test: loss:0.5490942823965818 	 acc:0.7364238410596027 	 lr:6.25e-06
epoch52: train: loss:0.38408214156292686 	 acc:0.9322314049586777 | test: loss:0.5499074594863993 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch53: train: loss:0.3931193634793778 	 acc:0.92 | test: loss:0.5430999721912358 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch54: train: loss:0.3921508720985129 	 acc:0.9249586776859504 | test: loss:0.5461113700803542 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch55: train: loss:0.3817226602912934 	 acc:0.935206611570248 | test: loss:0.543976545570702 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch56: train: loss:0.38105432360625463 	 acc:0.9368595041322314 | test: loss:0.5429856832453747 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch57: train: loss:0.3886283054726183 	 acc:0.9206611570247933 | test: loss:0.5413471907179876 	 acc:0.7443708609271523 	 lr:3.125e-06
epoch58: train: loss:0.38729274447299233 	 acc:0.9315702479338843 | test: loss:0.5451334232526109 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch59: train: loss:0.3880204956196556 	 acc:0.9315702479338843 | test: loss:0.5479687407316751 	 acc:0.7390728476821192 	 lr:3.125e-06
epoch60: train: loss:0.38376762892589095 	 acc:0.9342148760330579 | test: loss:0.5488522619601117 	 acc:0.7390728476821192 	 lr:3.125e-06
epoch61: train: loss:0.38379124174433304 	 acc:0.9418181818181818 | test: loss:0.548025495958644 	 acc:0.7390728476821192 	 lr:1.5625e-06
epoch62: train: loss:0.3827456188201904 	 acc:0.931900826446281 | test: loss:0.5457400031437147 	 acc:0.7390728476821192 	 lr:1.5625e-06
epoch63: train: loss:0.3831317081431712 	 acc:0.932892561983471 | test: loss:0.5459535331915546 	 acc:0.7390728476821192 	 lr:1.5625e-06
epoch64: train: loss:0.38857520657137407 	 acc:0.9282644628099174 | test: loss:0.5457727379356788 	 acc:0.7403973509933774 	 lr:1.5625e-06
epoch65: train: loss:0.3942944407660114 	 acc:0.9193388429752066 | test: loss:0.5446836593924769 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch66: train: loss:0.3839432552333706 	 acc:0.9345454545454546 | test: loss:0.5458510267024008 	 acc:0.7443708609271523 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_17_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_17_2/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6311378056943909 	 acc:0.6059504132231405 | test: loss:0.6401278518683073 	 acc:0.5894039735099338 	 lr:0.0001
epoch1: train: loss:0.6203658069460846 	 acc:0.6006611570247934 | test: loss:0.6312882190508559 	 acc:0.5801324503311258 	 lr:0.0001
epoch2: train: loss:0.6131668783613473 	 acc:0.5963636363636363 | test: loss:0.6213118430005004 	 acc:0.5827814569536424 	 lr:0.0001
epoch3: train: loss:0.5774720514904369 	 acc:0.6783471074380165 | test: loss:0.5931545396514286 	 acc:0.6596026490066225 	 lr:0.0001
epoch4: train: loss:0.5636606508444163 	 acc:0.7127272727272728 | test: loss:0.592859769499065 	 acc:0.6582781456953642 	 lr:0.0001
epoch5: train: loss:0.5672351136680477 	 acc:0.7147107438016529 | test: loss:0.5848998015290064 	 acc:0.7006622516556291 	 lr:0.0001
epoch6: train: loss:0.5480524346060004 	 acc:0.7328925619834711 | test: loss:0.5818423345388956 	 acc:0.6675496688741722 	 lr:0.0001
epoch7: train: loss:0.5504473951828381 	 acc:0.7510743801652893 | test: loss:0.5835499817172423 	 acc:0.680794701986755 	 lr:0.0001
epoch8: train: loss:0.5257096431472085 	 acc:0.7613223140495867 | test: loss:0.5689637751768757 	 acc:0.7019867549668874 	 lr:0.0001
epoch9: train: loss:0.5294293331705834 	 acc:0.7596694214876033 | test: loss:0.5831260665363034 	 acc:0.7086092715231788 	 lr:0.0001
epoch10: train: loss:0.5199483559249847 	 acc:0.76 | test: loss:0.5583334704108586 	 acc:0.7059602649006622 	 lr:0.0001
epoch11: train: loss:0.5066401692658417 	 acc:0.7874380165289256 | test: loss:0.5616938938368236 	 acc:0.7112582781456953 	 lr:0.0001
epoch12: train: loss:0.5149026889249313 	 acc:0.780495867768595 | test: loss:0.5660869711282237 	 acc:0.713907284768212 	 lr:0.0001
epoch13: train: loss:0.5020985953078783 	 acc:0.7947107438016529 | test: loss:0.5680673999502169 	 acc:0.7125827814569536 	 lr:0.0001
epoch14: train: loss:0.49728302211800884 	 acc:0.792396694214876 | test: loss:0.5547301880571226 	 acc:0.7231788079470198 	 lr:0.0001
epoch15: train: loss:0.5378504396864205 	 acc:0.7742148760330578 | test: loss:0.607302347240069 	 acc:0.6940397350993377 	 lr:0.0001
epoch16: train: loss:0.4904007081157905 	 acc:0.8135537190082645 | test: loss:0.552599556477654 	 acc:0.7205298013245033 	 lr:0.0001
epoch17: train: loss:0.49401541086267836 	 acc:0.8036363636363636 | test: loss:0.5661508367551084 	 acc:0.7311258278145696 	 lr:0.0001
epoch18: train: loss:0.4869259480602485 	 acc:0.819504132231405 | test: loss:0.5872603981700164 	 acc:0.6966887417218544 	 lr:0.0001
epoch19: train: loss:0.48944579285038403 	 acc:0.7947107438016529 | test: loss:0.5544083941851231 	 acc:0.7086092715231788 	 lr:0.0001
epoch20: train: loss:0.48796614556273155 	 acc:0.815206611570248 | test: loss:0.5855045803335329 	 acc:0.6980132450331126 	 lr:0.0001
epoch21: train: loss:0.4860887549534317 	 acc:0.8327272727272728 | test: loss:0.575087130543412 	 acc:0.7205298013245033 	 lr:0.0001
epoch22: train: loss:0.4782506289757973 	 acc:0.7980165289256198 | test: loss:0.5594808538228471 	 acc:0.7033112582781457 	 lr:0.0001
epoch23: train: loss:0.45977419805920816 	 acc:0.8426446280991735 | test: loss:0.5507083984400263 	 acc:0.7337748344370861 	 lr:5e-05
epoch24: train: loss:0.45872453836370103 	 acc:0.8585123966942149 | test: loss:0.5557765494119252 	 acc:0.7231788079470198 	 lr:5e-05
epoch25: train: loss:0.44148497888864563 	 acc:0.8700826446280991 | test: loss:0.5503108972745225 	 acc:0.7350993377483444 	 lr:5e-05
epoch26: train: loss:0.43521947356294993 	 acc:0.8760330578512396 | test: loss:0.5450961844810587 	 acc:0.7350993377483444 	 lr:5e-05
epoch27: train: loss:0.46034315861946296 	 acc:0.8651239669421488 | test: loss:0.5788335997537272 	 acc:0.7205298013245033 	 lr:5e-05
epoch28: train: loss:0.4401985618004129 	 acc:0.8733884297520661 | test: loss:0.5435640714026445 	 acc:0.7403973509933774 	 lr:5e-05
epoch29: train: loss:0.438179770599712 	 acc:0.8826446280991735 | test: loss:0.5485724058372295 	 acc:0.7403973509933774 	 lr:5e-05
epoch30: train: loss:0.43528228029731875 	 acc:0.8667768595041322 | test: loss:0.5399084691180299 	 acc:0.7271523178807947 	 lr:5e-05
epoch31: train: loss:0.42988403024752275 	 acc:0.8737190082644628 | test: loss:0.5481400256914808 	 acc:0.7258278145695364 	 lr:5e-05
epoch32: train: loss:0.41920382674075357 	 acc:0.8968595041322314 | test: loss:0.5532826243646887 	 acc:0.7311258278145696 	 lr:5e-05
epoch33: train: loss:0.42283185136219686 	 acc:0.8899173553719009 | test: loss:0.5497382333736546 	 acc:0.7311258278145696 	 lr:5e-05
epoch34: train: loss:0.42888412076579635 	 acc:0.8882644628099173 | test: loss:0.554152243105781 	 acc:0.7337748344370861 	 lr:5e-05
epoch35: train: loss:0.4211826077078985 	 acc:0.8852892561983471 | test: loss:0.5553258641666134 	 acc:0.7311258278145696 	 lr:5e-05
epoch36: train: loss:0.42525017579724966 	 acc:0.8750413223140496 | test: loss:0.5543522593201391 	 acc:0.7218543046357616 	 lr:5e-05
epoch37: train: loss:0.4164484784918383 	 acc:0.8995041322314049 | test: loss:0.5448486452071083 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch38: train: loss:0.4135265777229278 	 acc:0.8942148760330578 | test: loss:0.547466989858261 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch39: train: loss:0.40889851985884107 	 acc:0.9064462809917355 | test: loss:0.5613371467748225 	 acc:0.7218543046357616 	 lr:2.5e-05
epoch40: train: loss:0.4162545988973507 	 acc:0.9014876033057851 | test: loss:0.5760046504191215 	 acc:0.7192052980132451 	 lr:2.5e-05
epoch41: train: loss:0.40950881291026914 	 acc:0.8942148760330578 | test: loss:0.5446772138804 	 acc:0.7337748344370861 	 lr:2.5e-05
epoch42: train: loss:0.41513955145827997 	 acc:0.9094214876033058 | test: loss:0.5754821916289677 	 acc:0.7178807947019867 	 lr:2.5e-05
epoch43: train: loss:0.40532713015217425 	 acc:0.9028099173553719 | test: loss:0.5481314143597685 	 acc:0.7205298013245033 	 lr:1.25e-05
epoch44: train: loss:0.3967636069088928 	 acc:0.9289256198347108 | test: loss:0.5577230713225358 	 acc:0.7364238410596027 	 lr:1.25e-05
epoch45: train: loss:0.40630788266166185 	 acc:0.915702479338843 | test: loss:0.5580632522406167 	 acc:0.7298013245033113 	 lr:1.25e-05
epoch46: train: loss:0.40055253676146513 	 acc:0.9120661157024793 | test: loss:0.5463404310460123 	 acc:0.7337748344370861 	 lr:1.25e-05
epoch47: train: loss:0.39542184844489925 	 acc:0.9193388429752066 | test: loss:0.5487849995000473 	 acc:0.7284768211920529 	 lr:1.25e-05
epoch48: train: loss:0.40180083573357134 	 acc:0.9196694214876033 | test: loss:0.5577835913525512 	 acc:0.7337748344370861 	 lr:1.25e-05
epoch49: train: loss:0.40185943701050497 	 acc:0.9064462809917355 | test: loss:0.5463633763869077 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch50: train: loss:0.4027749822553524 	 acc:0.9133884297520661 | test: loss:0.5502256038962611 	 acc:0.7350993377483444 	 lr:6.25e-06
epoch51: train: loss:0.40059044767017205 	 acc:0.9094214876033058 | test: loss:0.550471032376321 	 acc:0.7350993377483444 	 lr:6.25e-06
epoch52: train: loss:0.39112764835357666 	 acc:0.9266115702479338 | test: loss:0.5543315447718892 	 acc:0.7350993377483444 	 lr:6.25e-06
epoch53: train: loss:0.39752171700650996 	 acc:0.9190082644628099 | test: loss:0.5496836587293259 	 acc:0.7377483443708609 	 lr:6.25e-06
epoch54: train: loss:0.400674999568088 	 acc:0.9120661157024793 | test: loss:0.5541755349430817 	 acc:0.7311258278145696 	 lr:6.25e-06
epoch55: train: loss:0.3938447866755084 	 acc:0.9223140495867769 | test: loss:0.5522737701207596 	 acc:0.7350993377483444 	 lr:3.125e-06
epoch56: train: loss:0.38997836178984524 	 acc:0.9292561983471075 | test: loss:0.5493151434999428 	 acc:0.7364238410596027 	 lr:3.125e-06
epoch57: train: loss:0.39349003077538547 	 acc:0.9173553719008265 | test: loss:0.548959877475208 	 acc:0.7364238410596027 	 lr:3.125e-06
epoch58: train: loss:0.39713313027846914 	 acc:0.9163636363636364 | test: loss:0.5510329999671078 	 acc:0.7337748344370861 	 lr:3.125e-06
epoch59: train: loss:0.3969508121328906 	 acc:0.9190082644628099 | test: loss:0.5552783626594291 	 acc:0.7311258278145696 	 lr:3.125e-06
epoch60: train: loss:0.3968129765396276 	 acc:0.9170247933884298 | test: loss:0.5576376792610875 	 acc:0.7350993377483444 	 lr:3.125e-06
epoch61: train: loss:0.3923859375961556 	 acc:0.924297520661157 | test: loss:0.5581962124401371 	 acc:0.7350993377483444 	 lr:1.5625e-06
epoch62: train: loss:0.38922967909781403 	 acc:0.9266115702479338 | test: loss:0.5549633482434102 	 acc:0.7350993377483444 	 lr:1.5625e-06
epoch63: train: loss:0.39586087575628737 	 acc:0.92 | test: loss:0.5533766300472992 	 acc:0.7337748344370861 	 lr:1.5625e-06
epoch64: train: loss:0.3977912296933576 	 acc:0.92099173553719 | test: loss:0.5546612173516229 	 acc:0.7324503311258278 	 lr:1.5625e-06
epoch65: train: loss:0.39677945682825133 	 acc:0.9190082644628099 | test: loss:0.555867525283864 	 acc:0.7337748344370861 	 lr:1.5625e-06
epoch66: train: loss:0.39320570826530454 	 acc:0.9229752066115703 | test: loss:0.5550485073335912 	 acc:0.7337748344370861 	 lr:1.5625e-06
