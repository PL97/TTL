
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/resnet50_imagenet_-1_3/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/resnet50_imagenet_1_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/resnet50_imagenet_1_3/
pooling!! 256
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6917180109820338 	 acc:0.4027504911591356 | test: loss:0.6939893650633882 	 acc:0.4027504911591356 	 lr:0.0001
epoch1: train: loss:0.6915618242121399 	 acc:0.40962671905697445 | test: loss:0.692928783317446 	 acc:0.4027504911591356 	 lr:0.0001
epoch2: train: loss:0.6877653715886863 	 acc:0.45284872298624756 | test: loss:0.6903094949094626 	 acc:0.40471512770137524 	 lr:0.0001
epoch3: train: loss:0.6865819498223266 	 acc:0.4818271119842829 | test: loss:0.6890199545090231 	 acc:0.4106090373280943 	 lr:0.0001
epoch4: train: loss:0.6849252708065955 	 acc:0.48379174852652257 | test: loss:0.6883577586157149 	 acc:0.4223968565815324 	 lr:0.0001
epoch5: train: loss:0.6838380446134241 	 acc:0.4715127701375246 | test: loss:0.6883129495066135 	 acc:0.4106090373280943 	 lr:0.0001
epoch6: train: loss:0.6823839549933998 	 acc:0.4931237721021611 | test: loss:0.6871288665853924 	 acc:0.4223968565815324 	 lr:0.0001
epoch7: train: loss:0.6784673854269541 	 acc:0.5260314341846758 | test: loss:0.6842075374365322 	 acc:0.4518664047151277 	 lr:0.0001
epoch8: train: loss:0.6761072306127117 	 acc:0.568762278978389 | test: loss:0.6828783903936975 	 acc:0.49705304518664045 	 lr:0.0001
epoch9: train: loss:0.6741830105865868 	 acc:0.5741650294695482 | test: loss:0.6817749656722213 	 acc:0.5422396856581533 	 lr:0.0001
epoch10: train: loss:0.6720967270536367 	 acc:0.6016699410609038 | test: loss:0.6741168836245134 	 acc:0.550098231827112 	 lr:0.0001
epoch11: train: loss:0.6722678283342912 	 acc:0.537819253438114 | test: loss:0.681406974675379 	 acc:0.4577603143418468 	 lr:0.0001
epoch12: train: loss:0.671424013458207 	 acc:0.6537328094302554 | test: loss:0.6715371714125448 	 acc:0.6385068762278978 	 lr:0.0001
epoch13: train: loss:0.6797210598741392 	 acc:0.6232809430255403 | test: loss:0.6710680313335187 	 acc:0.6522593320235757 	 lr:0.0001
epoch14: train: loss:0.6671088891788175 	 acc:0.5987229862475442 | test: loss:0.6692014664470095 	 acc:0.5972495088408645 	 lr:0.0001
epoch15: train: loss:0.669714476481402 	 acc:0.6365422396856582 | test: loss:0.6661774366215779 	 acc:0.6463654223968566 	 lr:0.0001
epoch16: train: loss:0.6649015261288476 	 acc:0.649803536345776 | test: loss:0.6638011562332423 	 acc:0.6719056974459725 	 lr:0.0001
epoch17: train: loss:0.6783682688748672 	 acc:0.5879174852652259 | test: loss:0.6826693690830223 	 acc:0.5658153241650294 	 lr:0.0001
epoch18: train: loss:0.6675157757779704 	 acc:0.6350687622789783 | test: loss:0.6643061244417502 	 acc:0.656188605108055 	 lr:0.0001
epoch19: train: loss:0.6644437496929131 	 acc:0.6070726915520629 | test: loss:0.6665126315971255 	 acc:0.5736738703339882 	 lr:0.0001
epoch20: train: loss:0.6581296678376338 	 acc:0.6350687622789783 | test: loss:0.6612348845759168 	 acc:0.6070726915520629 	 lr:0.0001
epoch21: train: loss:0.665985115501876 	 acc:0.6527504911591355 | test: loss:0.6631177868730661 	 acc:0.6660117878192534 	 lr:0.0001
epoch22: train: loss:0.6611107512869394 	 acc:0.6537328094302554 | test: loss:0.6690564981143929 	 acc:0.6365422396856582 	 lr:0.0001
epoch23: train: loss:0.6641525545382546 	 acc:0.6340864440078585 | test: loss:0.6661417295750092 	 acc:0.6168958742632613 	 lr:0.0001
epoch24: train: loss:0.6588580499463559 	 acc:0.5800589390962672 | test: loss:0.6615353793432764 	 acc:0.518664047151277 	 lr:0.0001
epoch25: train: loss:0.6581884663559364 	 acc:0.6335952848722987 | test: loss:0.6596559200633486 	 acc:0.6149312377210217 	 lr:0.0001
epoch26: train: loss:0.6758004690434469 	 acc:0.6213163064833006 | test: loss:0.6662823532091376 	 acc:0.6581532416502947 	 lr:0.0001
epoch27: train: loss:0.6643269440279963 	 acc:0.6060903732809431 | test: loss:0.6735069177942332 	 acc:0.581532416502947 	 lr:0.0001
epoch28: train: loss:0.655710465781581 	 acc:0.6286836935166994 | test: loss:0.6590202108815986 	 acc:0.6168958742632613 	 lr:0.0001
epoch29: train: loss:0.6568645514064781 	 acc:0.6389980353634578 | test: loss:0.6523501194998885 	 acc:0.6601178781925344 	 lr:0.0001
epoch30: train: loss:0.663863669444161 	 acc:0.6424361493123772 | test: loss:0.6613035693852513 	 acc:0.6660117878192534 	 lr:0.0001
epoch31: train: loss:0.6514856944149744 	 acc:0.6723968565815324 | test: loss:0.6508439389333743 	 acc:0.6895874263261297 	 lr:0.0001
epoch32: train: loss:0.6566470706861005 	 acc:0.5903732809430255 | test: loss:0.6636797319695149 	 acc:0.5461689587426326 	 lr:0.0001
epoch33: train: loss:0.653645440496958 	 acc:0.6586444007858546 | test: loss:0.6497349610263098 	 acc:0.7092337917485265 	 lr:0.0001
epoch34: train: loss:0.6674116971684814 	 acc:0.5648330058939096 | test: loss:0.6716118299188221 	 acc:0.5343811394891945 	 lr:0.0001
epoch35: train: loss:0.653166343046312 	 acc:0.6208251473477406 | test: loss:0.6597773558030194 	 acc:0.5952848722986247 	 lr:0.0001
epoch36: train: loss:0.6618808723386004 	 acc:0.5702357563850687 | test: loss:0.6641821878832309 	 acc:0.5402750491159135 	 lr:0.0001
epoch37: train: loss:0.6564809928240148 	 acc:0.5884086444007859 | test: loss:0.6538624088984342 	 acc:0.581532416502947 	 lr:0.0001
epoch38: train: loss:0.653609852425476 	 acc:0.6517681728880157 | test: loss:0.658618959087754 	 acc:0.630648330058939 	 lr:0.0001
epoch39: train: loss:0.6467156035492603 	 acc:0.6365422396856582 | test: loss:0.6542790908007818 	 acc:0.6070726915520629 	 lr:0.0001
epoch40: train: loss:0.6492799081362066 	 acc:0.6620825147347741 | test: loss:0.646338877485871 	 acc:0.6758349705304518 	 lr:5e-05
epoch41: train: loss:0.648609220747395 	 acc:0.6409626719056974 | test: loss:0.6531961752295729 	 acc:0.650294695481336 	 lr:5e-05
epoch42: train: loss:0.6468548377745278 	 acc:0.6606090373280943 | test: loss:0.6435022358810035 	 acc:0.6777996070726916 	 lr:5e-05
epoch43: train: loss:0.6495031667598807 	 acc:0.6389980353634578 | test: loss:0.6559302646425244 	 acc:0.587426326129666 	 lr:5e-05
epoch44: train: loss:0.6569434720547119 	 acc:0.6773084479371316 | test: loss:0.6459141897077879 	 acc:0.6915520628683693 	 lr:5e-05
epoch45: train: loss:0.6433506535406431 	 acc:0.6424361493123772 | test: loss:0.6535591062721897 	 acc:0.6090373280943026 	 lr:5e-05
epoch46: train: loss:0.6431140331716106 	 acc:0.6782907662082515 | test: loss:0.6455827872973294 	 acc:0.6601178781925344 	 lr:5e-05
epoch47: train: loss:0.6412996517417473 	 acc:0.6915520628683693 | test: loss:0.6404249970243113 	 acc:0.7013752455795678 	 lr:5e-05
epoch48: train: loss:0.6426358107030977 	 acc:0.6448919449901768 | test: loss:0.653799513585441 	 acc:0.593320235756385 	 lr:5e-05
epoch49: train: loss:0.6477758923073414 	 acc:0.674852652259332 | test: loss:0.6405624173479136 	 acc:0.6738703339882122 	 lr:5e-05
epoch50: train: loss:0.648164465057358 	 acc:0.6262278978388998 | test: loss:0.6501425658087365 	 acc:0.630648330058939 	 lr:5e-05
epoch51: train: loss:0.6532562754477406 	 acc:0.668958742632613 | test: loss:0.6413910915903117 	 acc:0.7151277013752456 	 lr:5e-05
epoch52: train: loss:0.6481453523420397 	 acc:0.6129666011787819 | test: loss:0.6547107255060687 	 acc:0.587426326129666 	 lr:5e-05
epoch53: train: loss:0.6435969815038745 	 acc:0.6773084479371316 | test: loss:0.6430457063414493 	 acc:0.6640471512770137 	 lr:5e-05
epoch54: train: loss:0.6442229103714169 	 acc:0.6866404715127702 | test: loss:0.6426900958968051 	 acc:0.68762278978389 	 lr:2.5e-05
epoch55: train: loss:0.6403958666301195 	 acc:0.6723968565815324 | test: loss:0.6455582740967306 	 acc:0.6581532416502947 	 lr:2.5e-05
epoch56: train: loss:0.6405395112477961 	 acc:0.6601178781925344 | test: loss:0.6401712255065708 	 acc:0.6719056974459725 	 lr:2.5e-05
epoch57: train: loss:0.6395578951170496 	 acc:0.6674852652259332 | test: loss:0.6405020052885494 	 acc:0.656188605108055 	 lr:2.5e-05
epoch58: train: loss:0.6394381144894598 	 acc:0.6679764243614931 | test: loss:0.638085570808478 	 acc:0.6640471512770137 	 lr:2.5e-05
epoch59: train: loss:0.6401706410531679 	 acc:0.6782907662082515 | test: loss:0.6386054167578872 	 acc:0.6758349705304518 	 lr:2.5e-05
epoch60: train: loss:0.6380628131930158 	 acc:0.668467583497053 | test: loss:0.640897880251609 	 acc:0.6699410609037328 	 lr:2.5e-05
epoch61: train: loss:0.641625388900517 	 acc:0.656679764243615 | test: loss:0.6365139630782347 	 acc:0.6797642436149313 	 lr:2.5e-05
epoch62: train: loss:0.6427985457868145 	 acc:0.6669941060903732 | test: loss:0.6353120043844511 	 acc:0.6836935166994106 	 lr:2.5e-05
epoch63: train: loss:0.6397760120496768 	 acc:0.6773084479371316 | test: loss:0.6391285717604437 	 acc:0.6758349705304518 	 lr:2.5e-05
epoch64: train: loss:0.6390869195896891 	 acc:0.6777996070726916 | test: loss:0.6407419809658073 	 acc:0.6640471512770137 	 lr:2.5e-05
epoch65: train: loss:0.6372311081305468 	 acc:0.6709233791748527 | test: loss:0.6405456392844674 	 acc:0.6719056974459725 	 lr:2.5e-05
epoch66: train: loss:0.6395794517399052 	 acc:0.6699410609037328 | test: loss:0.6432910534628248 	 acc:0.6601178781925344 	 lr:2.5e-05
epoch67: train: loss:0.6508017709306743 	 acc:0.6635559921414538 | test: loss:0.6383967654878357 	 acc:0.68762278978389 	 lr:2.5e-05
epoch68: train: loss:0.6344327480479167 	 acc:0.6704322200392927 | test: loss:0.6456191430860043 	 acc:0.618860510805501 	 lr:2.5e-05
epoch69: train: loss:0.6387359906273413 	 acc:0.6488212180746562 | test: loss:0.6485845830679409 	 acc:0.6149312377210217 	 lr:1.25e-05
epoch70: train: loss:0.6359707786901058 	 acc:0.6802554027504911 | test: loss:0.6387036119556614 	 acc:0.6777996070726916 	 lr:1.25e-05
epoch71: train: loss:0.6406653132326242 	 acc:0.6944990176817288 | test: loss:0.6367652397493014 	 acc:0.7111984282907662 	 lr:1.25e-05
epoch72: train: loss:0.6388051370506437 	 acc:0.6655206286836935 | test: loss:0.6399990197249152 	 acc:0.6817288801571709 	 lr:1.25e-05
epoch73: train: loss:0.6369408780090466 	 acc:0.6861493123772102 | test: loss:0.6378026697162561 	 acc:0.6797642436149313 	 lr:1.25e-05
epoch74: train: loss:0.6337243962615787 	 acc:0.6925343811394892 | test: loss:0.63737456761081 	 acc:0.6620825147347741 	 lr:1.25e-05
epoch75: train: loss:0.6367591226030661 	 acc:0.6817288801571709 | test: loss:0.6378392832452515 	 acc:0.6719056974459725 	 lr:6.25e-06
epoch76: train: loss:0.6360625710140745 	 acc:0.6949901768172888 | test: loss:0.6367975282528545 	 acc:0.6915520628683693 	 lr:6.25e-06
epoch77: train: loss:0.6394484709428664 	 acc:0.674852652259332 | test: loss:0.6363348202058984 	 acc:0.6994106090373281 	 lr:6.25e-06
epoch78: train: loss:0.6343327250134031 	 acc:0.6822200392927309 | test: loss:0.6367779576708151 	 acc:0.6994106090373281 	 lr:6.25e-06
epoch79: train: loss:0.6336973529667658 	 acc:0.681237721021611 | test: loss:0.6378119482506938 	 acc:0.6817288801571709 	 lr:6.25e-06
epoch80: train: loss:0.6356877959781639 	 acc:0.675343811394892 | test: loss:0.6393876332196833 	 acc:0.6699410609037328 	 lr:6.25e-06
epoch81: train: loss:0.6374309666030299 	 acc:0.6674852652259332 | test: loss:0.6385107854260442 	 acc:0.6679764243614931 	 lr:3.125e-06
epoch82: train: loss:0.6352890886820135 	 acc:0.6890962671905697 | test: loss:0.6371650516635543 	 acc:0.6797642436149313 	 lr:3.125e-06
epoch83: train: loss:0.6378332377651117 	 acc:0.6709233791748527 | test: loss:0.6368738159917895 	 acc:0.6817288801571709 	 lr:3.125e-06
epoch84: train: loss:0.6383014863506984 	 acc:0.6763261296660118 | test: loss:0.63604975379052 	 acc:0.6817288801571709 	 lr:3.125e-06
epoch85: train: loss:0.6380881299438551 	 acc:0.6846758349705304 | test: loss:0.6355987985148177 	 acc:0.6915520628683693 	 lr:3.125e-06
epoch86: train: loss:0.6361006542835348 	 acc:0.6827111984282908 | test: loss:0.6361510403498217 	 acc:0.6856581532416502 	 lr:3.125e-06
epoch87: train: loss:0.6382851461296232 	 acc:0.6723968565815324 | test: loss:0.6361597050383891 	 acc:0.6836935166994106 	 lr:1.5625e-06
epoch88: train: loss:0.63731687453032 	 acc:0.668467583497053 | test: loss:0.6361054504081636 	 acc:0.6895874263261297 	 lr:1.5625e-06
epoch89: train: loss:0.6382917828550507 	 acc:0.6709233791748527 | test: loss:0.6361944129752738 	 acc:0.68762278978389 	 lr:1.5625e-06
epoch90: train: loss:0.6345480313938119 	 acc:0.6851669941060904 | test: loss:0.636361329518039 	 acc:0.68762278978389 	 lr:1.5625e-06
epoch91: train: loss:0.6374101396393916 	 acc:0.681237721021611 | test: loss:0.6363478002004866 	 acc:0.6895874263261297 	 lr:1.5625e-06
epoch92: train: loss:0.6357840611096215 	 acc:0.6817288801571709 | test: loss:0.6362854035055239 	 acc:0.68762278978389 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/resnet50_imagenet_2_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/resnet50_imagenet_2_3/
pooling!! 512
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6904980208878433 	 acc:0.4174852652259332 | test: loss:0.6915578817571779 	 acc:0.4027504911591356 	 lr:0.0001
epoch1: train: loss:0.6879007071549391 	 acc:0.4336935166994106 | test: loss:0.6944935654376017 	 acc:0.4066797642436149 	 lr:0.0001
epoch2: train: loss:0.6773826851123442 	 acc:0.5235756385068763 | test: loss:0.6862885451972601 	 acc:0.4538310412573674 	 lr:0.0001
epoch3: train: loss:0.6749102015392251 	 acc:0.56286836935167 | test: loss:0.6817687396216252 	 acc:0.47544204322200395 	 lr:0.0001
epoch4: train: loss:0.6671618222019293 	 acc:0.6571709233791748 | test: loss:0.6696003283170914 	 acc:0.6483300589390962 	 lr:0.0001
epoch5: train: loss:0.6828627972799575 	 acc:0.612475442043222 | test: loss:0.6789521035136314 	 acc:0.6247544204322201 	 lr:0.0001
epoch6: train: loss:0.6595579871728519 	 acc:0.630648330058939 | test: loss:0.6665549286456379 	 acc:0.6149312377210217 	 lr:0.0001
epoch7: train: loss:0.7254393686245841 	 acc:0.6021611001964636 | test: loss:0.7098330892842036 	 acc:0.6051080550098232 	 lr:0.0001
epoch8: train: loss:0.6471394579162297 	 acc:0.6650294695481336 | test: loss:0.6432490220004309 	 acc:0.6660117878192534 	 lr:0.0001
epoch9: train: loss:0.6955983264038745 	 acc:0.6154223968565815 | test: loss:0.6686052073429984 	 acc:0.630648330058939 	 lr:0.0001
epoch10: train: loss:0.6358353693499312 	 acc:0.6768172888015717 | test: loss:0.6264487983904091 	 acc:0.7013752455795678 	 lr:0.0001
epoch11: train: loss:0.6443428054071363 	 acc:0.6763261296660118 | test: loss:0.6281748464393241 	 acc:0.7170923379174853 	 lr:0.0001
epoch12: train: loss:0.627545495984128 	 acc:0.7028487229862476 | test: loss:0.6281636549353834 	 acc:0.68762278978389 	 lr:0.0001
epoch13: train: loss:0.6309285765780915 	 acc:0.6281925343811395 | test: loss:0.6337515826543797 	 acc:0.6286836935166994 	 lr:0.0001
epoch14: train: loss:0.6196198329944274 	 acc:0.6954813359528488 | test: loss:0.6090215528877873 	 acc:0.7210216110019646 	 lr:0.0001
epoch15: train: loss:0.6229899526813409 	 acc:0.6787819253438114 | test: loss:0.6176591406636716 	 acc:0.7111984282907662 	 lr:0.0001
epoch16: train: loss:0.6386140457305084 	 acc:0.6011787819253438 | test: loss:0.64317271458845 	 acc:0.5717092337917485 	 lr:0.0001
epoch17: train: loss:0.6163952341248338 	 acc:0.68762278978389 | test: loss:0.6076894861541703 	 acc:0.7053045186640472 	 lr:0.0001
epoch18: train: loss:0.6077774706898598 	 acc:0.7107072691552063 | test: loss:0.6068262113569294 	 acc:0.7033398821218074 	 lr:0.0001
epoch19: train: loss:0.5972160098594861 	 acc:0.712180746561886 | test: loss:0.5975730628770555 	 acc:0.7210216110019646 	 lr:0.0001
epoch20: train: loss:0.6355354558742351 	 acc:0.5766208251473477 | test: loss:0.6344940973873925 	 acc:0.5776031434184676 	 lr:0.0001
epoch21: train: loss:0.6416193959989341 	 acc:0.6640471512770137 | test: loss:0.6464274862894609 	 acc:0.6542239685658153 	 lr:0.0001
epoch22: train: loss:0.6092391694458622 	 acc:0.7170923379174853 | test: loss:0.6004495692159432 	 acc:0.7406679764243614 	 lr:0.0001
epoch23: train: loss:0.6094282228492333 	 acc:0.6704322200392927 | test: loss:0.6014568905230824 	 acc:0.6974459724950884 	 lr:0.0001
epoch24: train: loss:0.5913678988492325 	 acc:0.730844793713163 | test: loss:0.5963240706147754 	 acc:0.7170923379174853 	 lr:0.0001
epoch25: train: loss:0.5912022694857509 	 acc:0.7288801571709234 | test: loss:0.589042985954547 	 acc:0.730844793713163 	 lr:0.0001
epoch26: train: loss:0.5944592296960077 	 acc:0.7288801571709234 | test: loss:0.5920213318058456 	 acc:0.7347740667976425 	 lr:0.0001
epoch27: train: loss:0.6058761800670437 	 acc:0.6512770137524558 | test: loss:0.6118653825317713 	 acc:0.6522593320235757 	 lr:0.0001
epoch28: train: loss:0.587443854462187 	 acc:0.7185658153241651 | test: loss:0.5941375611104759 	 acc:0.6699410609037328 	 lr:0.0001
epoch29: train: loss:0.5899091761098161 	 acc:0.7013752455795678 | test: loss:0.5935510425061749 	 acc:0.6817288801571709 	 lr:0.0001
epoch30: train: loss:0.6220472346120358 	 acc:0.6841846758349706 | test: loss:0.6307524295124886 	 acc:0.6699410609037328 	 lr:0.0001
epoch31: train: loss:0.578719484665539 	 acc:0.7303536345776032 | test: loss:0.5794302207547696 	 acc:0.7367387033398821 	 lr:0.0001
epoch32: train: loss:0.6076239216304247 	 acc:0.638015717092338 | test: loss:0.6106585333295796 	 acc:0.6404715127701375 	 lr:0.0001
epoch33: train: loss:0.6368119192029732 	 acc:0.6768172888015717 | test: loss:0.6249806523791465 	 acc:0.693516699410609 	 lr:0.0001
epoch34: train: loss:0.5980241346687137 	 acc:0.6773084479371316 | test: loss:0.5938756005703115 	 acc:0.7367387033398821 	 lr:0.0001
epoch35: train: loss:0.6134739252345267 	 acc:0.6267190569744597 | test: loss:0.607256842385575 	 acc:0.6345776031434185 	 lr:0.0001
epoch36: train: loss:0.5791495789010773 	 acc:0.7298624754420432 | test: loss:0.5831659355425881 	 acc:0.7190569744597249 	 lr:0.0001
epoch37: train: loss:0.5756831725360368 	 acc:0.7519646365422397 | test: loss:0.5832144800712645 	 acc:0.7465618860510805 	 lr:0.0001
epoch38: train: loss:0.5928823966642728 	 acc:0.7401768172888016 | test: loss:0.588531323757996 	 acc:0.7465618860510805 	 lr:5e-05
epoch39: train: loss:0.5751178697893334 	 acc:0.7082514734774067 | test: loss:0.585965826024241 	 acc:0.6954813359528488 	 lr:5e-05
epoch40: train: loss:0.5766119394883191 	 acc:0.7141453831041258 | test: loss:0.5822205061060038 	 acc:0.7013752455795678 	 lr:5e-05
epoch41: train: loss:0.5694246608288208 	 acc:0.7637524557956779 | test: loss:0.5683746121252918 	 acc:0.7603143418467584 	 lr:5e-05
epoch42: train: loss:0.5695308902408848 	 acc:0.7411591355599214 | test: loss:0.5843667653782663 	 acc:0.6777996070726916 	 lr:5e-05
epoch43: train: loss:0.5677420313091316 	 acc:0.7671905697445972 | test: loss:0.5744298071196131 	 acc:0.7603143418467584 	 lr:5e-05
epoch44: train: loss:0.5730480619638514 	 acc:0.7549115913555993 | test: loss:0.5728844528348132 	 acc:0.7445972495088409 	 lr:5e-05
epoch45: train: loss:0.5799659484730254 	 acc:0.762278978388998 | test: loss:0.5920218333514126 	 acc:0.7210216110019646 	 lr:5e-05
epoch46: train: loss:0.5648247187404595 	 acc:0.7426326129666012 | test: loss:0.580312942240702 	 acc:0.7151277013752456 	 lr:5e-05
epoch47: train: loss:0.5700313512140734 	 acc:0.731335952848723 | test: loss:0.5826202622799602 	 acc:0.6954813359528488 	 lr:5e-05
epoch48: train: loss:0.5655547653293796 	 acc:0.7303536345776032 | test: loss:0.5812585616158596 	 acc:0.7072691552062869 	 lr:2.5e-05
epoch49: train: loss:0.5581642420914881 	 acc:0.7789783889980354 | test: loss:0.5685265335914896 	 acc:0.7485265225933202 	 lr:2.5e-05
epoch50: train: loss:0.5676890219358658 	 acc:0.7755402750491159 | test: loss:0.57471624514443 	 acc:0.7701375245579568 	 lr:2.5e-05
epoch51: train: loss:0.5679745304092677 	 acc:0.7337917485265226 | test: loss:0.5765009468571376 	 acc:0.7229862475442044 	 lr:2.5e-05
epoch52: train: loss:0.5567533055316722 	 acc:0.7696463654223968 | test: loss:0.5669561802521201 	 acc:0.756385068762279 	 lr:2.5e-05
epoch53: train: loss:0.5557180393655549 	 acc:0.7711198428290766 | test: loss:0.5694971126751254 	 acc:0.724950884086444 	 lr:2.5e-05
epoch54: train: loss:0.553496173299366 	 acc:0.7696463654223968 | test: loss:0.567824874856383 	 acc:0.7387033398821218 	 lr:2.5e-05
epoch55: train: loss:0.5532737489065159 	 acc:0.7755402750491159 | test: loss:0.5620938935307951 	 acc:0.762278978388998 	 lr:2.5e-05
epoch56: train: loss:0.5548405394338671 	 acc:0.7539292730844793 | test: loss:0.5689884665438609 	 acc:0.7328094302554028 	 lr:2.5e-05
epoch57: train: loss:0.5576470757748617 	 acc:0.7902750491159135 | test: loss:0.5705312305442944 	 acc:0.7328094302554028 	 lr:2.5e-05
epoch58: train: loss:0.5552517632370145 	 acc:0.787328094302554 | test: loss:0.5691776145652141 	 acc:0.768172888015717 	 lr:2.5e-05
epoch59: train: loss:0.5515044337405671 	 acc:0.787819253438114 | test: loss:0.5606752397503506 	 acc:0.762278978388998 	 lr:2.5e-05
epoch60: train: loss:0.551432862731469 	 acc:0.7666994106090373 | test: loss:0.5632791685449117 	 acc:0.7406679764243614 	 lr:2.5e-05
epoch61: train: loss:0.5479612287229078 	 acc:0.7740667976424361 | test: loss:0.5626547555558106 	 acc:0.7485265225933202 	 lr:2.5e-05
epoch62: train: loss:0.5575436768222651 	 acc:0.762278978388998 | test: loss:0.5628732949905171 	 acc:0.7524557956777996 	 lr:2.5e-05
epoch63: train: loss:0.553320911520825 	 acc:0.7730844793713163 | test: loss:0.5598026306783754 	 acc:0.756385068762279 	 lr:2.5e-05
epoch64: train: loss:0.5485690621355428 	 acc:0.7755402750491159 | test: loss:0.5630069546006283 	 acc:0.7426326129666012 	 lr:2.5e-05
epoch65: train: loss:0.5476685768260469 	 acc:0.7853634577603144 | test: loss:0.5565125174044626 	 acc:0.7524557956777996 	 lr:2.5e-05
epoch66: train: loss:0.5456030687320912 	 acc:0.793713163064833 | test: loss:0.5566026694882361 	 acc:0.7544204322200393 	 lr:2.5e-05
epoch67: train: loss:0.5505939902397411 	 acc:0.7721021611001965 | test: loss:0.5602363589236217 	 acc:0.7445972495088409 	 lr:2.5e-05
epoch68: train: loss:0.5510482945001898 	 acc:0.7897838899803536 | test: loss:0.5572639509362182 	 acc:0.7583497053045186 	 lr:2.5e-05
epoch69: train: loss:0.547149249167714 	 acc:0.7799607072691552 | test: loss:0.5567464130566491 	 acc:0.7583497053045186 	 lr:2.5e-05
epoch70: train: loss:0.5451364588643807 	 acc:0.7721021611001965 | test: loss:0.5577618928227303 	 acc:0.75049115913556 	 lr:2.5e-05
epoch71: train: loss:0.543359841251186 	 acc:0.7892927308447937 | test: loss:0.5599701758217952 	 acc:0.75049115913556 	 lr:2.5e-05
epoch72: train: loss:0.5428337147990003 	 acc:0.787819253438114 | test: loss:0.5590412498457259 	 acc:0.75049115913556 	 lr:1.25e-05
epoch73: train: loss:0.5484648913672022 	 acc:0.7588408644400786 | test: loss:0.5653879643657587 	 acc:0.7210216110019646 	 lr:1.25e-05
epoch74: train: loss:0.5380957838594328 	 acc:0.7927308447937131 | test: loss:0.5570973315969666 	 acc:0.756385068762279 	 lr:1.25e-05
epoch75: train: loss:0.546321645818197 	 acc:0.7927308447937131 | test: loss:0.5567902546734613 	 acc:0.762278978388998 	 lr:1.25e-05
epoch76: train: loss:0.5523269500854208 	 acc:0.7603143418467584 | test: loss:0.5633874033195331 	 acc:0.7288801571709234 	 lr:1.25e-05
epoch77: train: loss:0.5450228546363666 	 acc:0.7775049115913556 | test: loss:0.5573499655910934 	 acc:0.75049115913556 	 lr:1.25e-05
epoch78: train: loss:0.5449584796292842 	 acc:0.7897838899803536 | test: loss:0.5561381532775864 	 acc:0.768172888015717 	 lr:6.25e-06
epoch79: train: loss:0.5414621530206827 	 acc:0.7951866404715128 | test: loss:0.5570287166972058 	 acc:0.7603143418467584 	 lr:6.25e-06
epoch80: train: loss:0.5453556866214879 	 acc:0.7907662082514735 | test: loss:0.5578804917794548 	 acc:0.7662082514734774 	 lr:6.25e-06
epoch81: train: loss:0.548999164451082 	 acc:0.7868369351669942 | test: loss:0.5574502180273734 	 acc:0.7603143418467584 	 lr:6.25e-06
epoch82: train: loss:0.5451851925119201 	 acc:0.7888015717092338 | test: loss:0.5568172433989924 	 acc:0.7603143418467584 	 lr:6.25e-06
epoch83: train: loss:0.5428554093673796 	 acc:0.7996070726915521 | test: loss:0.5563586193826672 	 acc:0.7642436149312377 	 lr:6.25e-06
epoch84: train: loss:0.5469815935976144 	 acc:0.7809430255402751 | test: loss:0.5558421026278573 	 acc:0.768172888015717 	 lr:6.25e-06
epoch85: train: loss:0.5436342935899386 	 acc:0.8005893909626719 | test: loss:0.5565217750948398 	 acc:0.768172888015717 	 lr:6.25e-06
epoch86: train: loss:0.5420616502143545 	 acc:0.7986247544204322 | test: loss:0.5578242559095731 	 acc:0.768172888015717 	 lr:6.25e-06
epoch87: train: loss:0.54474016046243 	 acc:0.7892927308447937 | test: loss:0.5557219773706612 	 acc:0.7662082514734774 	 lr:6.25e-06
epoch88: train: loss:0.5335230746531533 	 acc:0.8104125736738703 | test: loss:0.5556081964365391 	 acc:0.762278978388998 	 lr:6.25e-06
epoch89: train: loss:0.5386981298975017 	 acc:0.7996070726915521 | test: loss:0.5572221595318237 	 acc:0.7583497053045186 	 lr:6.25e-06
epoch90: train: loss:0.5409550118774235 	 acc:0.7897838899803536 | test: loss:0.5559707676263596 	 acc:0.7524557956777996 	 lr:6.25e-06
epoch91: train: loss:0.5417704777773799 	 acc:0.7868369351669942 | test: loss:0.5545614558493459 	 acc:0.762278978388998 	 lr:6.25e-06
epoch92: train: loss:0.5342506738214924 	 acc:0.8005893909626719 | test: loss:0.5556760201988146 	 acc:0.7524557956777996 	 lr:6.25e-06
epoch93: train: loss:0.5330916478029637 	 acc:0.8133595284872298 | test: loss:0.5560297629922218 	 acc:0.7701375245579568 	 lr:6.25e-06
epoch94: train: loss:0.5404270184766099 	 acc:0.787328094302554 | test: loss:0.5552084006822882 	 acc:0.7642436149312377 	 lr:6.25e-06
epoch95: train: loss:0.5382815046722622 	 acc:0.7888015717092338 | test: loss:0.5550617492503408 	 acc:0.756385068762279 	 lr:6.25e-06
epoch96: train: loss:0.5439198948076994 	 acc:0.7907662082514735 | test: loss:0.5537813182898261 	 acc:0.7642436149312377 	 lr:6.25e-06
epoch97: train: loss:0.5373184954956145 	 acc:0.8109037328094303 | test: loss:0.5554591797892378 	 acc:0.7721021611001965 	 lr:6.25e-06
epoch98: train: loss:0.5408286214577426 	 acc:0.787819253438114 | test: loss:0.5555104798324451 	 acc:0.756385068762279 	 lr:6.25e-06
epoch99: train: loss:0.5435197957606587 	 acc:0.7883104125736738 | test: loss:0.5554570885913077 	 acc:0.7662082514734774 	 lr:6.25e-06
epoch100: train: loss:0.5449148946988793 	 acc:0.775049115913556 | test: loss:0.5553894774852896 	 acc:0.7544204322200393 	 lr:6.25e-06
epoch101: train: loss:0.5394219581645223 	 acc:0.7996070726915521 | test: loss:0.5541290916019432 	 acc:0.7583497053045186 	 lr:6.25e-06
epoch102: train: loss:0.5347390666691868 	 acc:0.805992141453831 | test: loss:0.5539759055570441 	 acc:0.762278978388998 	 lr:6.25e-06
epoch103: train: loss:0.5373705415922438 	 acc:0.7976424361493124 | test: loss:0.5546338807623138 	 acc:0.7642436149312377 	 lr:3.125e-06
epoch104: train: loss:0.5357125516256321 	 acc:0.7956777996070727 | test: loss:0.5548594050650981 	 acc:0.762278978388998 	 lr:3.125e-06
epoch105: train: loss:0.5344640936973287 	 acc:0.7996070726915521 | test: loss:0.5545210244613452 	 acc:0.756385068762279 	 lr:3.125e-06
epoch106: train: loss:0.5351800340097873 	 acc:0.7961689587426326 | test: loss:0.5536790802811359 	 acc:0.7603143418467584 	 lr:3.125e-06
epoch107: train: loss:0.5367268704478071 	 acc:0.800098231827112 | test: loss:0.554170531467745 	 acc:0.7603143418467584 	 lr:3.125e-06
epoch108: train: loss:0.5395993692233192 	 acc:0.7927308447937131 | test: loss:0.5546661606003587 	 acc:0.762278978388998 	 lr:3.125e-06
epoch109: train: loss:0.5408816071765128 	 acc:0.7912573673870335 | test: loss:0.5545506045251558 	 acc:0.762278978388998 	 lr:3.125e-06
epoch110: train: loss:0.5358084280973333 	 acc:0.7956777996070727 | test: loss:0.5553885082364785 	 acc:0.7544204322200393 	 lr:3.125e-06
epoch111: train: loss:0.5413552294311449 	 acc:0.7848722986247544 | test: loss:0.5547430097237083 	 acc:0.7642436149312377 	 lr:3.125e-06
epoch112: train: loss:0.535323813411248 	 acc:0.8133595284872298 | test: loss:0.5544742758007087 	 acc:0.7642436149312377 	 lr:3.125e-06
epoch113: train: loss:0.5368436053834402 	 acc:0.81237721021611 | test: loss:0.5542705881103785 	 acc:0.7662082514734774 	 lr:1.5625e-06
epoch114: train: loss:0.5383042690796094 	 acc:0.7961689587426326 | test: loss:0.5544030640588996 	 acc:0.7662082514734774 	 lr:1.5625e-06
epoch115: train: loss:0.5387448216468271 	 acc:0.7951866404715128 | test: loss:0.5546387314562244 	 acc:0.7662082514734774 	 lr:1.5625e-06
epoch116: train: loss:0.5375120238143006 	 acc:0.8005893909626719 | test: loss:0.5549713554691472 	 acc:0.7583497053045186 	 lr:1.5625e-06
epoch117: train: loss:0.5362490510425305 	 acc:0.8010805500982319 | test: loss:0.555213220812717 	 acc:0.756385068762279 	 lr:1.5625e-06
epoch118: train: loss:0.5351624253456625 	 acc:0.7912573673870335 | test: loss:0.5550177046966928 	 acc:0.756385068762279 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/resnet50_imagenet_3_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/resnet50_imagenet_3_3/
pooling!! 1024
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
Traceback (most recent call last):
  File "main.py", line 429, in <module>
    train(model=model, trainloader=train_dl, valloader=val_dl, args=args)
  File "main.py", line 312, in train
    train_acc, train_loss = evaluate(model, trainloader, criterion, args=args)
  File "main.py", line 205, in evaluate
    return evaluate_single(model, valloader, criterion, args)
  File "main.py", line 90, in evaluate_single
    output = m(model(input))
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py", line 125, in forward
    out = self.bn1(out)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 178, in forward
    self.eps,
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 2282, in batch_norm
    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 39.41 GiB total capacity; 37.35 GiB already allocated; 38.50 MiB free; 37.53 GiB reserved in total by PyTorch)

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/slim_resnet50_imagenet_1_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/slim_resnet50_imagenet_1_3/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6940197109707681 	 acc:0.5972495088408645 | test: loss:0.6944305887390916 	 acc:0.5972495088408645 	 lr:0.0001
epoch1: train: loss:0.6957739397210082 	 acc:0.5972495088408645 | test: loss:0.6962347897190476 	 acc:0.5972495088408645 	 lr:0.0001
epoch2: train: loss:0.7007683830551642 	 acc:0.593811394891945 | test: loss:0.6993678215678877 	 acc:0.5972495088408645 	 lr:0.0001
epoch3: train: loss:0.7146631584654622 	 acc:0.5972495088408645 | test: loss:0.7131238391216705 	 acc:0.5972495088408645 	 lr:0.0001
epoch4: train: loss:0.6851367724433629 	 acc:0.5913555992141454 | test: loss:0.681664091899025 	 acc:0.6051080550098232 	 lr:0.0001
epoch5: train: loss:0.6949019060856233 	 acc:0.5987229862475442 | test: loss:0.6885447961173966 	 acc:0.5972495088408645 	 lr:0.0001
epoch6: train: loss:0.6822824739285677 	 acc:0.600196463654224 | test: loss:0.6814040380048846 	 acc:0.593320235756385 	 lr:0.0001
epoch7: train: loss:0.6878216781878518 	 acc:0.6041257367387033 | test: loss:0.6821372162382354 	 acc:0.6247544204322201 	 lr:0.0001
epoch8: train: loss:0.6774340120888881 	 acc:0.5093320235756386 | test: loss:0.6883573840081341 	 acc:0.4400785854616896 	 lr:0.0001
epoch9: train: loss:0.6831790687761513 	 acc:0.5903732809430255 | test: loss:0.6869701898870861 	 acc:0.587426326129666 	 lr:0.0001
epoch10: train: loss:0.6768479384701472 	 acc:0.5388015717092338 | test: loss:0.6845923473652313 	 acc:0.45579567779960706 	 lr:0.0001
epoch11: train: loss:0.6754745383159585 	 acc:0.6154223968565815 | test: loss:0.6797764591711678 	 acc:0.5677799607072691 	 lr:0.0001
epoch12: train: loss:0.6765579274454847 	 acc:0.6060903732809431 | test: loss:0.6737786039622219 	 acc:0.5992141453831041 	 lr:0.0001
epoch13: train: loss:0.6819414793859531 	 acc:0.5977406679764243 | test: loss:0.6744715531589006 	 acc:0.6267190569744597 	 lr:0.0001
epoch14: train: loss:0.6758475474618039 	 acc:0.6203339882121808 | test: loss:0.6759202083342202 	 acc:0.5893909626719057 	 lr:0.0001
epoch15: train: loss:0.6756527038594828 	 acc:0.611984282907662 | test: loss:0.6727735144918701 	 acc:0.6031434184675835 	 lr:0.0001
epoch16: train: loss:0.6743260470963649 	 acc:0.5741650294695482 | test: loss:0.6770682052685376 	 acc:0.5461689587426326 	 lr:0.0001
epoch17: train: loss:0.6725450525817797 	 acc:0.5545186640471512 | test: loss:0.6796480235744319 	 acc:0.48330058939096265 	 lr:0.0001
epoch18: train: loss:0.6684678974460759 	 acc:0.5795677799607073 | test: loss:0.6750522882390351 	 acc:0.5383104125736738 	 lr:0.0001
epoch19: train: loss:0.6797459848034827 	 acc:0.6168958742632613 | test: loss:0.6755910583704066 	 acc:0.6110019646365422 	 lr:0.0001
epoch20: train: loss:0.6836629573160632 	 acc:0.48772102161100195 | test: loss:0.6976476832785166 	 acc:0.4204322200392927 	 lr:0.0001
epoch21: train: loss:0.6983472949629448 	 acc:0.6021611001964636 | test: loss:0.6926654138124761 	 acc:0.6031434184675835 	 lr:0.0001
epoch22: train: loss:0.6726023704691814 	 acc:0.5658153241650294 | test: loss:0.6709910250600054 	 acc:0.5461689587426326 	 lr:5e-05
epoch23: train: loss:0.6684168406226078 	 acc:0.6208251473477406 | test: loss:0.6669551666921155 	 acc:0.6110019646365422 	 lr:5e-05
epoch24: train: loss:0.6665886878264676 	 acc:0.6321218074656189 | test: loss:0.6672467763859538 	 acc:0.6247544204322201 	 lr:5e-05
epoch25: train: loss:0.6677414281897555 	 acc:0.5839882121807466 | test: loss:0.669158707784529 	 acc:0.5442043222003929 	 lr:5e-05
epoch26: train: loss:0.6678405641806852 	 acc:0.6006876227897839 | test: loss:0.6693763970626126 	 acc:0.5893909626719057 	 lr:5e-05
epoch27: train: loss:0.6660298484950262 	 acc:0.611984282907662 | test: loss:0.6661711898440228 	 acc:0.6110019646365422 	 lr:5e-05
epoch28: train: loss:0.6629086434255649 	 acc:0.630648330058939 | test: loss:0.6650976718994396 	 acc:0.6227897838899804 	 lr:5e-05
epoch29: train: loss:0.6650454817680104 	 acc:0.5947937131630648 | test: loss:0.6658267648140433 	 acc:0.5834970530451866 	 lr:5e-05
epoch30: train: loss:0.6701954306928488 	 acc:0.6218074656188605 | test: loss:0.6632644234331277 	 acc:0.618860510805501 	 lr:5e-05
epoch31: train: loss:0.6654519691682752 	 acc:0.5952848722986247 | test: loss:0.6651327058467977 	 acc:0.5736738703339882 	 lr:5e-05
epoch32: train: loss:0.6710706650625278 	 acc:0.637524557956778 | test: loss:0.6703306475182179 	 acc:0.6404715127701375 	 lr:5e-05
epoch33: train: loss:0.6635136631242419 	 acc:0.5972495088408645 | test: loss:0.6674767456260786 	 acc:0.5736738703339882 	 lr:5e-05
epoch34: train: loss:0.6677798740053458 	 acc:0.5800589390962672 | test: loss:0.6709590502712722 	 acc:0.5363457760314342 	 lr:5e-05
epoch35: train: loss:0.6606330478121115 	 acc:0.6168958742632613 | test: loss:0.6601332417874065 	 acc:0.6110019646365422 	 lr:5e-05
epoch36: train: loss:0.6644412562514569 	 acc:0.6444007858546169 | test: loss:0.6605228280505872 	 acc:0.6444007858546169 	 lr:5e-05
epoch37: train: loss:0.6674229109217001 	 acc:0.5884086444007859 | test: loss:0.6641880482259575 	 acc:0.5913555992141454 	 lr:5e-05
epoch38: train: loss:0.6717252700876861 	 acc:0.631139489194499 | test: loss:0.6641623080362271 	 acc:0.6385068762278978 	 lr:5e-05
epoch39: train: loss:0.6701674598373458 	 acc:0.6281925343811395 | test: loss:0.664674167900048 	 acc:0.6345776031434185 	 lr:5e-05
epoch40: train: loss:0.6660327687713158 	 acc:0.5731827111984283 | test: loss:0.663289132427373 	 acc:0.5736738703339882 	 lr:5e-05
epoch41: train: loss:0.664040292293009 	 acc:0.5569744597249509 | test: loss:0.6699766508488384 	 acc:0.49901768172888017 	 lr:5e-05
epoch42: train: loss:0.6660134479666974 	 acc:0.6488212180746562 | test: loss:0.6629250753370859 	 acc:0.6385068762278978 	 lr:2.5e-05
epoch43: train: loss:0.6662781477442893 	 acc:0.6277013752455796 | test: loss:0.6602342919188539 	 acc:0.6463654223968566 	 lr:2.5e-05
epoch44: train: loss:0.6608886039561513 	 acc:0.5785854616895875 | test: loss:0.6621411354930555 	 acc:0.555992141453831 	 lr:2.5e-05
epoch45: train: loss:0.6566155744910475 	 acc:0.6085461689587426 | test: loss:0.6579423463649038 	 acc:0.5854616895874263 	 lr:2.5e-05
epoch46: train: loss:0.6612395919376366 	 acc:0.6385068762278978 | test: loss:0.6550731137834036 	 acc:0.6522593320235757 	 lr:2.5e-05
epoch47: train: loss:0.6608230851019764 	 acc:0.6168958742632613 | test: loss:0.6562430880861339 	 acc:0.6247544204322201 	 lr:2.5e-05
epoch48: train: loss:0.6596993528789528 	 acc:0.5805500982318271 | test: loss:0.6593207738020097 	 acc:0.5579567779960707 	 lr:2.5e-05
epoch49: train: loss:0.6573861666418949 	 acc:0.6439096267190569 | test: loss:0.6550121936207904 	 acc:0.6542239685658153 	 lr:2.5e-05
epoch50: train: loss:0.6588037316129344 	 acc:0.6355599214145383 | test: loss:0.6558780471093528 	 acc:0.6345776031434185 	 lr:2.5e-05
epoch51: train: loss:0.6567941199117184 	 acc:0.6031434184675835 | test: loss:0.6558648960295267 	 acc:0.6031434184675835 	 lr:2.5e-05
epoch52: train: loss:0.655578726284162 	 acc:0.6060903732809431 | test: loss:0.6565553369128634 	 acc:0.6149312377210217 	 lr:2.5e-05
epoch53: train: loss:0.6561627962968672 	 acc:0.6340864440078585 | test: loss:0.654875742780202 	 acc:0.656188605108055 	 lr:1.25e-05
epoch54: train: loss:0.659495713317324 	 acc:0.638015717092338 | test: loss:0.6588002599058544 	 acc:0.6463654223968566 	 lr:1.25e-05
epoch55: train: loss:0.6571693447343493 	 acc:0.6291748526522594 | test: loss:0.6576938852814186 	 acc:0.618860510805501 	 lr:1.25e-05
epoch56: train: loss:0.6550988652148744 	 acc:0.6296660117878192 | test: loss:0.6551435677616442 	 acc:0.6404715127701375 	 lr:1.25e-05
epoch57: train: loss:0.6543491880879655 	 acc:0.6404715127701375 | test: loss:0.6536634209582286 	 acc:0.6444007858546169 	 lr:1.25e-05
epoch58: train: loss:0.657854456798501 	 acc:0.6326129666011788 | test: loss:0.6537797230868536 	 acc:0.6404715127701375 	 lr:1.25e-05
epoch59: train: loss:0.6580088879832584 	 acc:0.6365422396856582 | test: loss:0.6530769802264006 	 acc:0.6385068762278978 	 lr:1.25e-05
epoch60: train: loss:0.6566278087835181 	 acc:0.6547151277013753 | test: loss:0.653580111464255 	 acc:0.6640471512770137 	 lr:1.25e-05
epoch61: train: loss:0.6566813658403273 	 acc:0.6222986247544204 | test: loss:0.6520778341237127 	 acc:0.6620825147347741 	 lr:1.25e-05
epoch62: train: loss:0.6595640430750219 	 acc:0.6237721021611002 | test: loss:0.6521167362837051 	 acc:0.6699410609037328 	 lr:1.25e-05
epoch63: train: loss:0.6572772975286473 	 acc:0.6389980353634578 | test: loss:0.6523270537904765 	 acc:0.6601178781925344 	 lr:1.25e-05
epoch64: train: loss:0.6517597282330976 	 acc:0.6389980353634578 | test: loss:0.6534194463831035 	 acc:0.6522593320235757 	 lr:1.25e-05
epoch65: train: loss:0.6509813104256665 	 acc:0.6095284872298625 | test: loss:0.6530614505816537 	 acc:0.6051080550098232 	 lr:1.25e-05
epoch66: train: loss:0.6499043573096599 	 acc:0.6335952848722987 | test: loss:0.6495544244123582 	 acc:0.6601178781925344 	 lr:1.25e-05
epoch67: train: loss:0.6514791852130403 	 acc:0.6552062868369352 | test: loss:0.6511477562205497 	 acc:0.6601178781925344 	 lr:1.25e-05
epoch68: train: loss:0.6557640210818683 	 acc:0.6399803536345776 | test: loss:0.6524918098112454 	 acc:0.6581532416502947 	 lr:1.25e-05
epoch69: train: loss:0.65466972605887 	 acc:0.6389980353634578 | test: loss:0.6524809745299559 	 acc:0.6444007858546169 	 lr:1.25e-05
epoch70: train: loss:0.6550107353562691 	 acc:0.6090373280943026 | test: loss:0.6525385467617357 	 acc:0.6286836935166994 	 lr:1.25e-05
epoch71: train: loss:0.6565306494652874 	 acc:0.6198428290766208 | test: loss:0.6519693010916644 	 acc:0.6483300589390962 	 lr:1.25e-05
epoch72: train: loss:0.6532487769726452 	 acc:0.631139489194499 | test: loss:0.6491337084817044 	 acc:0.6542239685658153 	 lr:1.25e-05
epoch73: train: loss:0.6527938470390785 	 acc:0.6453831041257367 | test: loss:0.6486599553076364 	 acc:0.6620825147347741 	 lr:1.25e-05
epoch74: train: loss:0.6492211572781995 	 acc:0.6389980353634578 | test: loss:0.6481771231400241 	 acc:0.6463654223968566 	 lr:1.25e-05
epoch75: train: loss:0.6547560896761057 	 acc:0.6257367387033399 | test: loss:0.6480715679981853 	 acc:0.6542239685658153 	 lr:1.25e-05
epoch76: train: loss:0.6558839404746919 	 acc:0.6360510805500982 | test: loss:0.6488114388143618 	 acc:0.6758349705304518 	 lr:1.25e-05
epoch77: train: loss:0.6549559595074307 	 acc:0.6385068762278978 | test: loss:0.6485921179615912 	 acc:0.650294695481336 	 lr:1.25e-05
epoch78: train: loss:0.6496636270305731 	 acc:0.6615913555992141 | test: loss:0.6496766270495117 	 acc:0.6620825147347741 	 lr:1.25e-05
epoch79: train: loss:0.6555286418712444 	 acc:0.6247544204322201 | test: loss:0.6526204330045254 	 acc:0.6424361493123772 	 lr:1.25e-05
epoch80: train: loss:0.6493063005810871 	 acc:0.6110019646365422 | test: loss:0.6556659730806332 	 acc:0.5756385068762279 	 lr:1.25e-05
epoch81: train: loss:0.6528033799178943 	 acc:0.6006876227897839 | test: loss:0.6565738572118793 	 acc:0.5540275049115914 	 lr:1.25e-05
epoch82: train: loss:0.6536623462009992 	 acc:0.6095284872298625 | test: loss:0.6528355336376632 	 acc:0.6149312377210217 	 lr:6.25e-06
epoch83: train: loss:0.6511033583717871 	 acc:0.6301571709233792 | test: loss:0.6509315898469951 	 acc:0.6483300589390962 	 lr:6.25e-06
epoch84: train: loss:0.6512479989374083 	 acc:0.6439096267190569 | test: loss:0.6505552113407487 	 acc:0.656188605108055 	 lr:6.25e-06
epoch85: train: loss:0.6504895270222062 	 acc:0.6414538310412574 | test: loss:0.650310735571361 	 acc:0.6463654223968566 	 lr:6.25e-06
epoch86: train: loss:0.654538747720962 	 acc:0.6370333988212181 | test: loss:0.6505471590928338 	 acc:0.6483300589390962 	 lr:6.25e-06
epoch87: train: loss:0.654333183011278 	 acc:0.6370333988212181 | test: loss:0.6511456232173972 	 acc:0.6483300589390962 	 lr:6.25e-06
epoch88: train: loss:0.6511830830386673 	 acc:0.6463654223968566 | test: loss:0.6514528590475881 	 acc:0.6483300589390962 	 lr:3.125e-06
epoch89: train: loss:0.6508164076767642 	 acc:0.6468565815324165 | test: loss:0.6524359570973515 	 acc:0.6404715127701375 	 lr:3.125e-06
epoch90: train: loss:0.6538312438663191 	 acc:0.6478388998035364 | test: loss:0.6525391848476088 	 acc:0.6483300589390962 	 lr:3.125e-06
epoch91: train: loss:0.6509015194793581 	 acc:0.6419449901768173 | test: loss:0.6522066940961512 	 acc:0.6463654223968566 	 lr:3.125e-06
epoch92: train: loss:0.65038566551883 	 acc:0.6414538310412574 | test: loss:0.6518818176331361 	 acc:0.6542239685658153 	 lr:3.125e-06
epoch93: train: loss:0.6478921439652359 	 acc:0.6394891944990176 | test: loss:0.6516315048242131 	 acc:0.650294695481336 	 lr:3.125e-06
epoch94: train: loss:0.6485872856526104 	 acc:0.6517681728880157 | test: loss:0.6517709707932763 	 acc:0.650294695481336 	 lr:1.5625e-06
epoch95: train: loss:0.6529890582228924 	 acc:0.6286836935166994 | test: loss:0.6516356388571689 	 acc:0.6483300589390962 	 lr:1.5625e-06
epoch96: train: loss:0.6487487074434172 	 acc:0.6444007858546169 | test: loss:0.6519697040377993 	 acc:0.6463654223968566 	 lr:1.5625e-06
epoch97: train: loss:0.6539418574400642 	 acc:0.6272102161100196 | test: loss:0.6523636962435334 	 acc:0.6444007858546169 	 lr:1.5625e-06
epoch98: train: loss:0.6507500424132132 	 acc:0.6458742632612967 | test: loss:0.6521424214357477 	 acc:0.6444007858546169 	 lr:1.5625e-06
epoch99: train: loss:0.6540539456725355 	 acc:0.6316306483300589 | test: loss:0.6520689705968605 	 acc:0.6463654223968566 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/slim_resnet50_imagenet_2_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/slim_resnet50_imagenet_2_3/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6929743723222924 	 acc:0.5613948919449901 | test: loss:0.6929268561786659 	 acc:0.593320235756385 	 lr:0.0001
epoch1: train: loss:0.6915235300429443 	 acc:0.4901768172888016 | test: loss:0.6934335273937533 	 acc:0.4931237721021611 	 lr:0.0001
epoch2: train: loss:0.6894826898406203 	 acc:0.5545186640471512 | test: loss:0.6910128615694102 	 acc:0.5461689587426326 	 lr:0.0001
epoch3: train: loss:0.6829690905123188 	 acc:0.6031434184675835 | test: loss:0.6863445818775529 	 acc:0.6031434184675835 	 lr:0.0001
epoch4: train: loss:0.6782773815110062 	 acc:0.6095284872298625 | test: loss:0.6844303599743572 	 acc:0.581532416502947 	 lr:0.0001
epoch5: train: loss:0.6803074007184191 	 acc:0.6090373280943026 | test: loss:0.6767051971731579 	 acc:0.6267190569744597 	 lr:0.0001
epoch6: train: loss:0.6754470002674635 	 acc:0.6409626719056974 | test: loss:0.6753334446365566 	 acc:0.6129666011787819 	 lr:0.0001
epoch7: train: loss:0.6681862507447278 	 acc:0.6394891944990176 | test: loss:0.6702815905535385 	 acc:0.6070726915520629 	 lr:0.0001
epoch8: train: loss:0.6686215517797264 	 acc:0.6095284872298625 | test: loss:0.6674112134223369 	 acc:0.5776031434184676 	 lr:0.0001
epoch9: train: loss:0.6699061271952271 	 acc:0.5299607072691552 | test: loss:0.6862790138407634 	 acc:0.4675834970530452 	 lr:0.0001
epoch10: train: loss:0.6631334388888421 	 acc:0.6478388998035364 | test: loss:0.6636420320902452 	 acc:0.6620825147347741 	 lr:0.0001
epoch11: train: loss:0.6725612542718239 	 acc:0.519155206286837 | test: loss:0.6787401234002853 	 acc:0.4774066797642436 	 lr:0.0001
epoch12: train: loss:0.6835711989515189 	 acc:0.6203339882121808 | test: loss:0.6800606308142656 	 acc:0.6345776031434185 	 lr:0.0001
epoch13: train: loss:0.6599445898078514 	 acc:0.5722003929273084 | test: loss:0.656459945007017 	 acc:0.5756385068762279 	 lr:0.0001
epoch14: train: loss:0.6553917510804587 	 acc:0.6070726915520629 | test: loss:0.6478292849536963 	 acc:0.6031434184675835 	 lr:0.0001
epoch15: train: loss:0.662902182586535 	 acc:0.6517681728880157 | test: loss:0.6591733599224353 	 acc:0.656188605108055 	 lr:0.0001
epoch16: train: loss:0.643518866396606 	 acc:0.6360510805500982 | test: loss:0.6403930346483332 	 acc:0.6227897838899804 	 lr:0.0001
epoch17: train: loss:0.6417376059445978 	 acc:0.630648330058939 | test: loss:0.6398988935240125 	 acc:0.6345776031434185 	 lr:0.0001
epoch18: train: loss:0.6421973864549738 	 acc:0.6512770137524558 | test: loss:0.6339872622302567 	 acc:0.6836935166994106 	 lr:0.0001
epoch19: train: loss:0.6459988001990178 	 acc:0.6650294695481336 | test: loss:0.6308327892206038 	 acc:0.7053045186640472 	 lr:0.0001
epoch20: train: loss:0.6402059895350563 	 acc:0.6168958742632613 | test: loss:0.6417746385797302 	 acc:0.5893909626719057 	 lr:0.0001
epoch21: train: loss:0.6478456702588349 	 acc:0.6439096267190569 | test: loss:0.6398997322046921 	 acc:0.6404715127701375 	 lr:0.0001
epoch22: train: loss:0.6432696251831729 	 acc:0.6149312377210217 | test: loss:0.6365831840951691 	 acc:0.6051080550098232 	 lr:0.0001
epoch23: train: loss:0.7185720900889698 	 acc:0.6021611001964636 | test: loss:0.7120779755073352 	 acc:0.6051080550098232 	 lr:0.0001
epoch24: train: loss:0.6611116259177682 	 acc:0.525049115913556 | test: loss:0.6721904441743562 	 acc:0.4950884086444008 	 lr:0.0001
epoch25: train: loss:0.6463166265684401 	 acc:0.6674852652259332 | test: loss:0.6432636711592759 	 acc:0.6817288801571709 	 lr:0.0001
epoch26: train: loss:0.6329710366449562 	 acc:0.6429273084479371 | test: loss:0.63215299299049 	 acc:0.6031434184675835 	 lr:5e-05
epoch27: train: loss:0.636993844288974 	 acc:0.6886051080550099 | test: loss:0.6331771708190558 	 acc:0.693516699410609 	 lr:5e-05
epoch28: train: loss:0.6379093136674997 	 acc:0.5982318271119843 | test: loss:0.6342972910474466 	 acc:0.5952848722986247 	 lr:5e-05
epoch29: train: loss:0.6232169860706817 	 acc:0.6434184675834971 | test: loss:0.6187812615705613 	 acc:0.6424361493123772 	 lr:5e-05
epoch30: train: loss:0.6309620330516388 	 acc:0.6105108055009824 | test: loss:0.6165123670883403 	 acc:0.6463654223968566 	 lr:5e-05
epoch31: train: loss:0.6166990607801262 	 acc:0.6679764243614931 | test: loss:0.609477665429031 	 acc:0.693516699410609 	 lr:5e-05
epoch32: train: loss:0.6276559384726353 	 acc:0.6237721021611002 | test: loss:0.6185959462800991 	 acc:0.6444007858546169 	 lr:5e-05
epoch33: train: loss:0.6331879120678705 	 acc:0.6090373280943026 | test: loss:0.6423052193373734 	 acc:0.5618860510805501 	 lr:5e-05
epoch34: train: loss:0.6150377817847171 	 acc:0.7092337917485265 | test: loss:0.6114906701451435 	 acc:0.693516699410609 	 lr:5e-05
epoch35: train: loss:0.6144823777418006 	 acc:0.7013752455795678 | test: loss:0.6116355253577467 	 acc:0.68762278978389 	 lr:5e-05
epoch36: train: loss:0.6129209832498742 	 acc:0.6542239685658153 | test: loss:0.6198007286414651 	 acc:0.6267190569744597 	 lr:5e-05
epoch37: train: loss:0.6250634996736448 	 acc:0.6915520628683693 | test: loss:0.6152864767198244 	 acc:0.6974459724950884 	 lr:5e-05
epoch38: train: loss:0.610084639551129 	 acc:0.6792730844793713 | test: loss:0.6054888002296328 	 acc:0.6856581532416502 	 lr:2.5e-05
epoch39: train: loss:0.6153402880039103 	 acc:0.668467583497053 | test: loss:0.6126017161811499 	 acc:0.6601178781925344 	 lr:2.5e-05
epoch40: train: loss:0.6081265074096635 	 acc:0.7146365422396856 | test: loss:0.6042586598977124 	 acc:0.6974459724950884 	 lr:2.5e-05
epoch41: train: loss:0.6088593982760236 	 acc:0.6925343811394892 | test: loss:0.6051206909603126 	 acc:0.6915520628683693 	 lr:2.5e-05
epoch42: train: loss:0.607991121841086 	 acc:0.6827111984282908 | test: loss:0.605460798576445 	 acc:0.6797642436149313 	 lr:2.5e-05
epoch43: train: loss:0.6168610772121632 	 acc:0.7003929273084479 | test: loss:0.6005947745619213 	 acc:0.7269155206286837 	 lr:2.5e-05
epoch44: train: loss:0.6096194202632942 	 acc:0.7102161100196464 | test: loss:0.5980787610960849 	 acc:0.7151277013752456 	 lr:2.5e-05
epoch45: train: loss:0.6080324704848479 	 acc:0.6832023575638507 | test: loss:0.6007569614936888 	 acc:0.6817288801571709 	 lr:2.5e-05
epoch46: train: loss:0.6000075051030382 	 acc:0.7008840864440079 | test: loss:0.5966628390585744 	 acc:0.6954813359528488 	 lr:2.5e-05
epoch47: train: loss:0.6107584749785763 	 acc:0.6758349705304518 | test: loss:0.5986552092087526 	 acc:0.6856581532416502 	 lr:2.5e-05
epoch48: train: loss:0.6075289104214352 	 acc:0.6792730844793713 | test: loss:0.6061986386190463 	 acc:0.6640471512770137 	 lr:2.5e-05
epoch49: train: loss:0.6008362122509475 	 acc:0.7151277013752456 | test: loss:0.5934879166204007 	 acc:0.7072691552062869 	 lr:2.5e-05
epoch50: train: loss:0.5994649029433844 	 acc:0.6925343811394892 | test: loss:0.5974975676808235 	 acc:0.6994106090373281 	 lr:2.5e-05
epoch51: train: loss:0.6051162956739924 	 acc:0.6910609037328095 | test: loss:0.6046109598839915 	 acc:0.6758349705304518 	 lr:2.5e-05
epoch52: train: loss:0.6094036688973252 	 acc:0.712671905697446 | test: loss:0.5995355810538257 	 acc:0.7131630648330058 	 lr:2.5e-05
epoch53: train: loss:0.605514438775293 	 acc:0.7161100196463654 | test: loss:0.5912851656115594 	 acc:0.7210216110019646 	 lr:2.5e-05
epoch54: train: loss:0.6046415566227057 	 acc:0.6861493123772102 | test: loss:0.5963579198466303 	 acc:0.6856581532416502 	 lr:2.5e-05
epoch55: train: loss:0.6040014761604354 	 acc:0.6900785854616895 | test: loss:0.59802275544768 	 acc:0.6994106090373281 	 lr:2.5e-05
epoch56: train: loss:0.599220540762416 	 acc:0.7229862475442044 | test: loss:0.597682263387444 	 acc:0.7033398821218074 	 lr:2.5e-05
epoch57: train: loss:0.5984367871097123 	 acc:0.7131630648330058 | test: loss:0.5885181983233904 	 acc:0.7170923379174853 	 lr:2.5e-05
epoch58: train: loss:0.598580892404545 	 acc:0.7190569744597249 | test: loss:0.5859034805026176 	 acc:0.730844793713163 	 lr:2.5e-05
epoch59: train: loss:0.6029279165277313 	 acc:0.68713163064833 | test: loss:0.58980719373362 	 acc:0.7072691552062869 	 lr:2.5e-05
epoch60: train: loss:0.6024576406816134 	 acc:0.712671905697446 | test: loss:0.5880445000699086 	 acc:0.7387033398821218 	 lr:2.5e-05
epoch61: train: loss:0.6047688580917703 	 acc:0.668958742632613 | test: loss:0.5870732441398155 	 acc:0.7269155206286837 	 lr:2.5e-05
epoch62: train: loss:0.6237894256831621 	 acc:0.6021611001964636 | test: loss:0.6242240415574993 	 acc:0.587426326129666 	 lr:2.5e-05
epoch63: train: loss:0.5935446968949848 	 acc:0.712180746561886 | test: loss:0.5900919705570798 	 acc:0.7111984282907662 	 lr:2.5e-05
epoch64: train: loss:0.596535990186665 	 acc:0.7166011787819253 | test: loss:0.5860009264852303 	 acc:0.7269155206286837 	 lr:2.5e-05
epoch65: train: loss:0.5970818964812048 	 acc:0.706286836935167 | test: loss:0.5849886985096809 	 acc:0.730844793713163 	 lr:1.25e-05
epoch66: train: loss:0.5925372095379707 	 acc:0.7234774066797642 | test: loss:0.5837504866081042 	 acc:0.7387033398821218 	 lr:1.25e-05
epoch67: train: loss:0.5898488378243737 	 acc:0.7269155206286837 | test: loss:0.584493566824083 	 acc:0.7347740667976425 	 lr:1.25e-05
epoch68: train: loss:0.5911014116349061 	 acc:0.6979371316306483 | test: loss:0.5868792085375908 	 acc:0.7033398821218074 	 lr:1.25e-05
epoch69: train: loss:0.59410612311485 	 acc:0.6949901768172888 | test: loss:0.5886750702539455 	 acc:0.7033398821218074 	 lr:1.25e-05
epoch70: train: loss:0.5957643874501901 	 acc:0.7048133595284872 | test: loss:0.5865896006932662 	 acc:0.7013752455795678 	 lr:1.25e-05
epoch71: train: loss:0.5922873107998217 	 acc:0.7244597249508841 | test: loss:0.5813509155584459 	 acc:0.7347740667976425 	 lr:1.25e-05
epoch72: train: loss:0.5933932601117432 	 acc:0.7244597249508841 | test: loss:0.5818598833674298 	 acc:0.7426326129666012 	 lr:1.25e-05
epoch73: train: loss:0.5927324752910667 	 acc:0.712180746561886 | test: loss:0.5835575759996365 	 acc:0.7190569744597249 	 lr:1.25e-05
epoch74: train: loss:0.589659075371643 	 acc:0.7107072691552063 | test: loss:0.5885824824597372 	 acc:0.7053045186640472 	 lr:1.25e-05
epoch75: train: loss:0.5888913628628774 	 acc:0.7185658153241651 | test: loss:0.5856741753448906 	 acc:0.7111984282907662 	 lr:1.25e-05
epoch76: train: loss:0.5939688694266299 	 acc:0.7013752455795678 | test: loss:0.5884206684961301 	 acc:0.7033398821218074 	 lr:1.25e-05
epoch77: train: loss:0.5871459216406397 	 acc:0.7190569744597249 | test: loss:0.5895554952630828 	 acc:0.6974459724950884 	 lr:1.25e-05
epoch78: train: loss:0.5915572219372732 	 acc:0.7082514734774067 | test: loss:0.5844683723271713 	 acc:0.7072691552062869 	 lr:6.25e-06
epoch79: train: loss:0.5947154718204191 	 acc:0.712671905697446 | test: loss:0.5833297870716552 	 acc:0.730844793713163 	 lr:6.25e-06
epoch80: train: loss:0.5905861187074882 	 acc:0.7185658153241651 | test: loss:0.5813753342581638 	 acc:0.7406679764243614 	 lr:6.25e-06
epoch81: train: loss:0.5915109290355315 	 acc:0.7195481335952849 | test: loss:0.5810864509205921 	 acc:0.7387033398821218 	 lr:6.25e-06
epoch82: train: loss:0.5886180558935364 	 acc:0.7298624754420432 | test: loss:0.5809469294454354 	 acc:0.7445972495088409 	 lr:6.25e-06
epoch83: train: loss:0.5885576989889614 	 acc:0.7293713163064833 | test: loss:0.5807264073658551 	 acc:0.7347740667976425 	 lr:6.25e-06
epoch84: train: loss:0.5852567554691217 	 acc:0.7195481335952849 | test: loss:0.5808374559715361 	 acc:0.7269155206286837 	 lr:6.25e-06
epoch85: train: loss:0.5890712698222612 	 acc:0.7269155206286837 | test: loss:0.5802704445973829 	 acc:0.7387033398821218 	 lr:6.25e-06
epoch86: train: loss:0.5909981633918927 	 acc:0.7239685658153242 | test: loss:0.5798145325338442 	 acc:0.75049115913556 	 lr:6.25e-06
epoch87: train: loss:0.5855943304148077 	 acc:0.7323182711198428 | test: loss:0.5795135098730886 	 acc:0.7465618860510805 	 lr:6.25e-06
epoch88: train: loss:0.5876884260205717 	 acc:0.7220039292730844 | test: loss:0.5809743644446427 	 acc:0.724950884086444 	 lr:6.25e-06
epoch89: train: loss:0.5877094323836516 	 acc:0.7151277013752456 | test: loss:0.5834225334212447 	 acc:0.7151277013752456 	 lr:6.25e-06
epoch90: train: loss:0.5886845278365205 	 acc:0.7200392927308448 | test: loss:0.5794888190295701 	 acc:0.7190569744597249 	 lr:6.25e-06
epoch91: train: loss:0.5854458327611912 	 acc:0.7239685658153242 | test: loss:0.5771201833760574 	 acc:0.7426326129666012 	 lr:6.25e-06
epoch92: train: loss:0.5895753496990691 	 acc:0.7254420432220039 | test: loss:0.5776114161684377 	 acc:0.7426326129666012 	 lr:6.25e-06
epoch93: train: loss:0.5877583035083088 	 acc:0.7239685658153242 | test: loss:0.5767023713274883 	 acc:0.7485265225933202 	 lr:6.25e-06
epoch94: train: loss:0.5881965489424985 	 acc:0.7082514734774067 | test: loss:0.5775788651234041 	 acc:0.730844793713163 	 lr:6.25e-06
epoch95: train: loss:0.5868373027250668 	 acc:0.7254420432220039 | test: loss:0.5768759770571366 	 acc:0.7367387033398821 	 lr:6.25e-06
epoch96: train: loss:0.5865339594646146 	 acc:0.7151277013752456 | test: loss:0.577514481333712 	 acc:0.7288801571709234 	 lr:6.25e-06
epoch97: train: loss:0.5834800093956218 	 acc:0.7323182711198428 | test: loss:0.5774700309063925 	 acc:0.7367387033398821 	 lr:6.25e-06
epoch98: train: loss:0.5858946829038894 	 acc:0.7210216110019646 | test: loss:0.5789001052412397 	 acc:0.7328094302554028 	 lr:6.25e-06
epoch99: train: loss:0.5880051864153744 	 acc:0.7234774066797642 | test: loss:0.5782395109446438 	 acc:0.7465618860510805 	 lr:6.25e-06
epoch100: train: loss:0.5862293353483579 	 acc:0.7239685658153242 | test: loss:0.5778707152265462 	 acc:0.7347740667976425 	 lr:3.125e-06
epoch101: train: loss:0.578402483275925 	 acc:0.7352652259332023 | test: loss:0.5782085530650171 	 acc:0.7328094302554028 	 lr:3.125e-06
epoch102: train: loss:0.58701261509144 	 acc:0.7185658153241651 | test: loss:0.5782373867241011 	 acc:0.724950884086444 	 lr:3.125e-06
epoch103: train: loss:0.5903885017912139 	 acc:0.7048133595284872 | test: loss:0.5784609085918644 	 acc:0.724950884086444 	 lr:3.125e-06
epoch104: train: loss:0.5831637014808729 	 acc:0.7293713163064833 | test: loss:0.5774286453756696 	 acc:0.7288801571709234 	 lr:3.125e-06
epoch105: train: loss:0.5847060050383532 	 acc:0.7220039292730844 | test: loss:0.5761951064079824 	 acc:0.7328094302554028 	 lr:3.125e-06
epoch106: train: loss:0.590388909070103 	 acc:0.7190569744597249 | test: loss:0.5761076727175759 	 acc:0.7387033398821218 	 lr:3.125e-06
epoch107: train: loss:0.5861811724533501 	 acc:0.7328094302554028 | test: loss:0.5760430611655379 	 acc:0.7387033398821218 	 lr:3.125e-06
epoch108: train: loss:0.5882583837846408 	 acc:0.7264243614931237 | test: loss:0.5757123642913953 	 acc:0.7406679764243614 	 lr:3.125e-06
epoch109: train: loss:0.585935727083847 	 acc:0.7274066797642437 | test: loss:0.5755475446845318 	 acc:0.7367387033398821 	 lr:3.125e-06
epoch110: train: loss:0.5827068661893984 	 acc:0.7220039292730844 | test: loss:0.5756471295253701 	 acc:0.7387033398821218 	 lr:3.125e-06
epoch111: train: loss:0.5840469852880317 	 acc:0.7229862475442044 | test: loss:0.5761704720073693 	 acc:0.7269155206286837 	 lr:3.125e-06
epoch112: train: loss:0.5865667766812742 	 acc:0.7161100196463654 | test: loss:0.576564766217777 	 acc:0.7229862475442044 	 lr:3.125e-06
epoch113: train: loss:0.5812858551564994 	 acc:0.7254420432220039 | test: loss:0.5765744896206734 	 acc:0.7269155206286837 	 lr:3.125e-06
epoch114: train: loss:0.5897331450917632 	 acc:0.7136542239685658 | test: loss:0.5767716908267533 	 acc:0.7328094302554028 	 lr:3.125e-06
epoch115: train: loss:0.5865933165568968 	 acc:0.7180746561886051 | test: loss:0.5755839332616165 	 acc:0.7367387033398821 	 lr:3.125e-06
epoch116: train: loss:0.5856934459129813 	 acc:0.7357563850687623 | test: loss:0.5752501333158002 	 acc:0.7426326129666012 	 lr:1.5625e-06
epoch117: train: loss:0.5871063192138972 	 acc:0.7278978388998035 | test: loss:0.5754827962408834 	 acc:0.7387033398821218 	 lr:1.5625e-06
epoch118: train: loss:0.5874048120146416 	 acc:0.724950884086444 | test: loss:0.5750604499065571 	 acc:0.7347740667976425 	 lr:1.5625e-06
epoch119: train: loss:0.5795355036357063 	 acc:0.7357563850687623 | test: loss:0.5753610887087164 	 acc:0.7328094302554028 	 lr:1.5625e-06
epoch120: train: loss:0.5853358130089661 	 acc:0.7220039292730844 | test: loss:0.5749538578312617 	 acc:0.7328094302554028 	 lr:1.5625e-06
epoch121: train: loss:0.5842954800967384 	 acc:0.7234774066797642 | test: loss:0.5749940334696667 	 acc:0.7347740667976425 	 lr:1.5625e-06
epoch122: train: loss:0.5823739769416614 	 acc:0.7352652259332023 | test: loss:0.5745959007903962 	 acc:0.7387033398821218 	 lr:1.5625e-06
epoch123: train: loss:0.5864026753747862 	 acc:0.7097249508840865 | test: loss:0.5753669882804331 	 acc:0.7406679764243614 	 lr:1.5625e-06
epoch124: train: loss:0.5831505352949581 	 acc:0.7166011787819253 | test: loss:0.5751568542716545 	 acc:0.7367387033398821 	 lr:1.5625e-06
epoch125: train: loss:0.5831842506798405 	 acc:0.7220039292730844 | test: loss:0.5748484574975574 	 acc:0.7387033398821218 	 lr:1.5625e-06
epoch126: train: loss:0.5837589588989678 	 acc:0.7293713163064833 | test: loss:0.5745619517880948 	 acc:0.7288801571709234 	 lr:1.5625e-06
epoch127: train: loss:0.582717962606951 	 acc:0.7274066797642437 | test: loss:0.5746800884282425 	 acc:0.7328094302554028 	 lr:1.5625e-06
epoch128: train: loss:0.5810283269067176 	 acc:0.7406679764243614 | test: loss:0.5751153377043944 	 acc:0.7347740667976425 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/slim_resnet50_imagenet_3_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/slim_resnet50_imagenet_3_3/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
Traceback (most recent call last):
  File "main.py", line 429, in <module>
    train(model=model, trainloader=train_dl, valloader=val_dl, args=args)
  File "main.py", line 312, in train
    train_acc, train_loss = evaluate(model, trainloader, criterion, args=args)
  File "main.py", line 205, in evaluate
    return evaluate_single(model, valloader, criterion, args)
  File "main.py", line 90, in evaluate_single
    output = m(model(input))
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/slim_resnet.py", line 257, in forward
    return self._forward_impl(x)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/slim_resnet.py", line 247, in _forward_impl
    x = self.layer3(x)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/slim_resnet.py", line 133, in forward
    out = self.bn3(out)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 178, in forward
    self.eps,
  File "/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 2282, in batch_norm
    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 39.41 GiB total capacity; 37.16 GiB already allocated; 130.50 MiB free; 37.44 GiB reserved in total by PyTorch)

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/freeze_resnet50_imagenet_1_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/freeze_resnet50_imagenet_1_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/freeze_resnet50_imagenet_2_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/freeze_resnet50_imagenet_2_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/freeze_resnet50_imagenet_3_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/freeze_resnet50_imagenet_3_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/freeze_resnet50_imagenet_4_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/freeze_resnet50_imagenet_4_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints/BIMCV/freeze_resnet50_imagenet_5_3/
Training on BIMCV, create new exp container at ../checkpoints/BIMCV/freeze_resnet50_imagenet_5_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'
