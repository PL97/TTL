
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/resnet50_imagenet_-1_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/resnet50_imagenet_-1_1/
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6289001437061089 	 acc:0.5950413223140496 | test: loss:0.6318502432463184 	 acc:0.5841059602649007 	 lr:0.0001
epoch1: train: loss:0.606024896783277 	 acc:0.6 | test: loss:0.6082705774844087 	 acc:0.5880794701986755 	 lr:0.0001
epoch2: train: loss:0.5926741802396853 	 acc:0.610909090909091 | test: loss:0.6015133324837842 	 acc:0.5973509933774834 	 lr:0.0001
epoch3: train: loss:0.5522652740912004 	 acc:0.7137190082644628 | test: loss:0.5672716877318376 	 acc:0.6900662251655629 	 lr:0.0001
epoch4: train: loss:0.5543351009069395 	 acc:0.7464462809917355 | test: loss:0.5582925378092077 	 acc:0.7192052980132451 	 lr:0.0001
epoch5: train: loss:0.541278098831492 	 acc:0.7193388429752066 | test: loss:0.5497886173772496 	 acc:0.704635761589404 	 lr:0.0001
epoch6: train: loss:0.5283231890694169 	 acc:0.7728925619834711 | test: loss:0.5361035840400797 	 acc:0.752317880794702 	 lr:0.0001
epoch7: train: loss:0.5155909003699122 	 acc:0.7990082644628099 | test: loss:0.5363641374158543 	 acc:0.7615894039735099 	 lr:0.0001
epoch8: train: loss:0.560061281298803 	 acc:0.7636363636363637 | test: loss:0.6133889664087864 	 acc:0.7006622516556291 	 lr:0.0001
epoch9: train: loss:0.5046432266550616 	 acc:0.7861157024793388 | test: loss:0.5509896122067179 	 acc:0.7443708609271523 	 lr:0.0001
epoch10: train: loss:0.4842833283814517 	 acc:0.8175206611570248 | test: loss:0.5324963185960884 	 acc:0.7443708609271523 	 lr:0.0001
epoch11: train: loss:0.4763418098126561 	 acc:0.8327272727272728 | test: loss:0.5277126705409675 	 acc:0.7377483443708609 	 lr:0.0001
epoch12: train: loss:0.48037103102226886 	 acc:0.8370247933884297 | test: loss:0.5585494013022113 	 acc:0.7271523178807947 	 lr:0.0001
epoch13: train: loss:0.48048413223471526 	 acc:0.8006611570247933 | test: loss:0.5344658880833758 	 acc:0.7165562913907285 	 lr:0.0001
epoch14: train: loss:0.502451938656736 	 acc:0.7553719008264462 | test: loss:0.559953063134326 	 acc:0.6794701986754967 	 lr:0.0001
epoch15: train: loss:0.4707715463835346 	 acc:0.816198347107438 | test: loss:0.5262903049292154 	 acc:0.7576158940397351 	 lr:0.0001
epoch16: train: loss:0.46923090185015653 	 acc:0.8158677685950413 | test: loss:0.5260855743427151 	 acc:0.7456953642384105 	 lr:0.0001
epoch17: train: loss:0.4612504891029074 	 acc:0.8373553719008264 | test: loss:0.5258926296865704 	 acc:0.7311258278145696 	 lr:0.0001
epoch18: train: loss:0.45215032999180566 	 acc:0.850909090909091 | test: loss:0.5362660689069735 	 acc:0.7377483443708609 	 lr:0.0001
epoch19: train: loss:0.45665609329199985 	 acc:0.8697520661157024 | test: loss:0.5396225412160356 	 acc:0.7549668874172185 	 lr:0.0001
epoch20: train: loss:0.5260134651050095 	 acc:0.7996694214876033 | test: loss:0.6559395541418467 	 acc:0.6622516556291391 	 lr:0.0001
epoch21: train: loss:0.42927573668070074 	 acc:0.8905785123966942 | test: loss:0.5461986360960449 	 acc:0.7562913907284768 	 lr:0.0001
epoch22: train: loss:0.4418671705703105 	 acc:0.856198347107438 | test: loss:0.5221434616095183 	 acc:0.7562913907284768 	 lr:0.0001
epoch23: train: loss:0.4366810909480103 	 acc:0.8614876033057851 | test: loss:0.507037779590152 	 acc:0.7814569536423841 	 lr:0.0001
epoch24: train: loss:0.4209319371526892 	 acc:0.8935537190082644 | test: loss:0.5161258223830469 	 acc:0.7774834437086092 	 lr:0.0001
epoch25: train: loss:0.41833250919649423 	 acc:0.8955371900826447 | test: loss:0.5088068617100747 	 acc:0.7629139072847683 	 lr:0.0001
epoch26: train: loss:0.4197969660975716 	 acc:0.8740495867768595 | test: loss:0.5165670278056568 	 acc:0.7708609271523179 	 lr:0.0001
epoch27: train: loss:0.4071607378494641 	 acc:0.9028099173553719 | test: loss:0.5194373488426208 	 acc:0.766887417218543 	 lr:0.0001
epoch28: train: loss:0.4092656618405965 	 acc:0.8912396694214876 | test: loss:0.523319690353823 	 acc:0.743046357615894 	 lr:0.0001
epoch29: train: loss:0.41801134807019197 	 acc:0.891900826446281 | test: loss:0.5152247110739449 	 acc:0.776158940397351 	 lr:0.0001
epoch30: train: loss:0.3967971011725339 	 acc:0.92 | test: loss:0.4986379682622998 	 acc:0.8013245033112583 	 lr:5e-05
epoch31: train: loss:0.3990011428210361 	 acc:0.9054545454545454 | test: loss:0.49755064454299724 	 acc:0.7894039735099337 	 lr:5e-05
epoch32: train: loss:0.3980535698725172 	 acc:0.9216528925619835 | test: loss:0.538147297284461 	 acc:0.7735099337748345 	 lr:5e-05
epoch33: train: loss:0.37944978110061206 	 acc:0.9395041322314049 | test: loss:0.5040817423372079 	 acc:0.7788079470198676 	 lr:5e-05
epoch34: train: loss:0.38065649063133994 	 acc:0.9292561983471075 | test: loss:0.5110784843267984 	 acc:0.776158940397351 	 lr:5e-05
epoch35: train: loss:0.37711522139793585 	 acc:0.936198347107438 | test: loss:0.5070717401851882 	 acc:0.7841059602649006 	 lr:5e-05
epoch36: train: loss:0.38458975450066496 	 acc:0.9219834710743802 | test: loss:0.5155053632148844 	 acc:0.7509933774834437 	 lr:5e-05
epoch37: train: loss:0.3745093698343955 	 acc:0.9461157024793388 | test: loss:0.5083042940556608 	 acc:0.7841059602649006 	 lr:5e-05
epoch38: train: loss:0.3683129060465442 	 acc:0.9537190082644628 | test: loss:0.5159216923429476 	 acc:0.7867549668874172 	 lr:2.5e-05
epoch39: train: loss:0.36976039253975734 	 acc:0.9504132231404959 | test: loss:0.5063669919178185 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch40: train: loss:0.36977774265383884 	 acc:0.9484297520661157 | test: loss:0.5228568429978478 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch41: train: loss:0.3654124509303038 	 acc:0.9484297520661157 | test: loss:0.5002284628665999 	 acc:0.7960264900662252 	 lr:2.5e-05
epoch42: train: loss:0.3657292404056581 	 acc:0.9477685950413223 | test: loss:0.5008025404633276 	 acc:0.7920529801324503 	 lr:2.5e-05
epoch43: train: loss:0.3735411628612802 	 acc:0.9474380165289257 | test: loss:0.5189403946826 	 acc:0.7774834437086092 	 lr:2.5e-05
epoch44: train: loss:0.3662392056382392 	 acc:0.9514049586776859 | test: loss:0.5019630513443852 	 acc:0.7986754966887417 	 lr:1.25e-05
epoch45: train: loss:0.36519802021586206 	 acc:0.9507438016528925 | test: loss:0.5013174644369163 	 acc:0.7907284768211921 	 lr:1.25e-05
epoch46: train: loss:0.3661669147408698 	 acc:0.9490909090909091 | test: loss:0.49924678944594025 	 acc:0.7880794701986755 	 lr:1.25e-05
epoch47: train: loss:0.36071728959556454 	 acc:0.952396694214876 | test: loss:0.4991722727453472 	 acc:0.7880794701986755 	 lr:1.25e-05
epoch48: train: loss:0.3632175896088939 	 acc:0.9507438016528925 | test: loss:0.5022665623797486 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch49: train: loss:0.3616630421197119 	 acc:0.952396694214876 | test: loss:0.5085756359510863 	 acc:0.7827814569536424 	 lr:1.25e-05
epoch50: train: loss:0.3563568687734525 	 acc:0.9573553719008264 | test: loss:0.5030247648030717 	 acc:0.7933774834437086 	 lr:6.25e-06
epoch51: train: loss:0.3576866640236752 	 acc:0.9560330578512397 | test: loss:0.5021340774384555 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch52: train: loss:0.3585410525187973 	 acc:0.9586776859504132 | test: loss:0.5059178473144178 	 acc:0.785430463576159 	 lr:6.25e-06
epoch53: train: loss:0.35965741860965067 	 acc:0.9583471074380165 | test: loss:0.5075834365869989 	 acc:0.7880794701986755 	 lr:6.25e-06
epoch54: train: loss:0.3579810516991891 	 acc:0.9603305785123967 | test: loss:0.5069504488382908 	 acc:0.7920529801324503 	 lr:6.25e-06
epoch55: train: loss:0.36213213442770903 	 acc:0.9474380165289257 | test: loss:0.5014577464552115 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch56: train: loss:0.3576726900546019 	 acc:0.9583471074380165 | test: loss:0.5047032974413689 	 acc:0.7920529801324503 	 lr:3.125e-06
epoch57: train: loss:0.359597392949191 	 acc:0.9560330578512397 | test: loss:0.506141544572565 	 acc:0.7907284768211921 	 lr:3.125e-06
epoch58: train: loss:0.35700604043716244 	 acc:0.9603305785123967 | test: loss:0.5071437290962169 	 acc:0.7880794701986755 	 lr:3.125e-06
epoch59: train: loss:0.3612066877873476 	 acc:0.95900826446281 | test: loss:0.5058514666083632 	 acc:0.7880794701986755 	 lr:3.125e-06
epoch60: train: loss:0.3580612217788854 	 acc:0.9583471074380165 | test: loss:0.5017633864421718 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch61: train: loss:0.35932130846110255 	 acc:0.9570247933884297 | test: loss:0.5026326559237297 	 acc:0.7907284768211921 	 lr:3.125e-06
epoch62: train: loss:0.3613791106850648 	 acc:0.9547107438016529 | test: loss:0.5039421582853557 	 acc:0.7933774834437086 	 lr:1.5625e-06
epoch63: train: loss:0.35669271729209207 	 acc:0.96 | test: loss:0.5043437128035438 	 acc:0.7920529801324503 	 lr:1.5625e-06
epoch64: train: loss:0.35671705294246514 	 acc:0.9603305785123967 | test: loss:0.5040836931064429 	 acc:0.7907284768211921 	 lr:1.5625e-06
epoch65: train: loss:0.35605925780682524 	 acc:0.9629752066115702 | test: loss:0.5076579457876698 	 acc:0.7880794701986755 	 lr:1.5625e-06
epoch66: train: loss:0.3573588642404099 	 acc:0.9656198347107438 | test: loss:0.5070038306002586 	 acc:0.7920529801324503 	 lr:1.5625e-06
epoch67: train: loss:0.35775732292616663 	 acc:0.9613223140495868 | test: loss:0.5056507550327983 	 acc:0.7894039735099337 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_1_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_1_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.8679858764932176 	 acc:0.4786776859504132 | test: loss:0.8762140119312615 	 acc:0.4781456953642384 	 lr:0.0001
epoch1: train: loss:0.6910575672811713 	 acc:0.5213223140495867 | test: loss:0.6910699196998646 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.7456414799059718 	 acc:0.5008264462809917 | test: loss:0.7596685044023375 	 acc:0.49801324503311256 	 lr:0.0001
epoch3: train: loss:0.6601122237434072 	 acc:0.5209917355371901 | test: loss:0.6597596391147336 	 acc:0.5245033112582781 	 lr:0.0001
epoch4: train: loss:0.6576134328802755 	 acc:0.5213223140495867 | test: loss:0.6601652221963895 	 acc:0.5324503311258278 	 lr:0.0001
epoch5: train: loss:0.6591827960250791 	 acc:0.5325619834710744 | test: loss:0.6617582970107628 	 acc:0.5311258278145695 	 lr:0.0001
epoch6: train: loss:0.6572865163590297 	 acc:0.5213223140495867 | test: loss:0.657493872279363 	 acc:0.5231788079470199 	 lr:0.0001
epoch7: train: loss:0.6835727226044521 	 acc:0.5666115702479338 | test: loss:0.6830638671552898 	 acc:0.5801324503311258 	 lr:0.0001
epoch8: train: loss:0.6670532725665195 	 acc:0.5656198347107438 | test: loss:0.6679814307105463 	 acc:0.56158940397351 	 lr:0.0001
epoch9: train: loss:0.6647817155940473 	 acc:0.5500826446280992 | test: loss:0.6577946427642115 	 acc:0.5589403973509933 	 lr:0.0001
epoch10: train: loss:0.6555950616805022 	 acc:0.5219834710743801 | test: loss:0.6535795206265733 	 acc:0.5231788079470199 	 lr:0.0001
epoch11: train: loss:0.6827584228042729 	 acc:0.5636363636363636 | test: loss:0.6813443014163845 	 acc:0.5735099337748344 	 lr:0.0001
epoch12: train: loss:0.6545899125170117 	 acc:0.5418181818181819 | test: loss:0.6530719216296215 	 acc:0.5470198675496689 	 lr:0.0001
epoch13: train: loss:0.6541229478583849 	 acc:0.523305785123967 | test: loss:0.6521034075724368 	 acc:0.5231788079470199 	 lr:0.0001
epoch14: train: loss:0.7125155759842928 	 acc:0.5193388429752066 | test: loss:0.7044878688079632 	 acc:0.543046357615894 	 lr:0.0001
epoch15: train: loss:0.6514915400891265 	 acc:0.5282644628099173 | test: loss:0.6493641437284204 	 acc:0.5298013245033113 	 lr:0.0001
epoch16: train: loss:0.6945524751056324 	 acc:0.5570247933884298 | test: loss:0.6864955190001734 	 acc:0.5814569536423841 	 lr:0.0001
epoch17: train: loss:0.6568607634749294 	 acc:0.5685950413223141 | test: loss:0.6531832277380079 	 acc:0.5695364238410596 	 lr:0.0001
epoch18: train: loss:0.6538261251410177 	 acc:0.5249586776859504 | test: loss:0.6484366330879413 	 acc:0.5245033112582781 	 lr:0.0001
epoch19: train: loss:0.6664883416152197 	 acc:0.5920661157024794 | test: loss:0.660165062171734 	 acc:0.6172185430463576 	 lr:0.0001
epoch20: train: loss:0.6512484460428727 	 acc:0.5299173553719009 | test: loss:0.6445358011106782 	 acc:0.5443708609271524 	 lr:0.0001
epoch21: train: loss:0.6701720010938723 	 acc:0.5811570247933884 | test: loss:0.659219451534827 	 acc:0.623841059602649 	 lr:0.0001
epoch22: train: loss:0.6512290732328557 	 acc:0.5312396694214876 | test: loss:0.6446659884705449 	 acc:0.5443708609271524 	 lr:0.0001
epoch23: train: loss:0.6715138117538011 	 acc:0.588099173553719 | test: loss:0.6612742611904018 	 acc:0.6013245033112583 	 lr:0.0001
epoch24: train: loss:0.7094520551902204 	 acc:0.5342148760330578 | test: loss:0.6959391482618471 	 acc:0.5589403973509933 	 lr:0.0001
epoch25: train: loss:0.6697198140522665 	 acc:0.5986776859504133 | test: loss:0.6631871324501291 	 acc:0.6291390728476821 	 lr:0.0001
epoch26: train: loss:0.6594356882079574 	 acc:0.5877685950413223 | test: loss:0.6501915105920754 	 acc:0.6185430463576159 	 lr:0.0001
epoch27: train: loss:0.6593609240035381 	 acc:0.5692561983471074 | test: loss:0.6481085575969014 	 acc:0.6052980132450331 	 lr:5e-05
epoch28: train: loss:0.6504188810104181 	 acc:0.5219834710743801 | test: loss:0.644319355408877 	 acc:0.5245033112582781 	 lr:5e-05
epoch29: train: loss:0.6535393331661697 	 acc:0.5709090909090909 | test: loss:0.6430598705809637 	 acc:0.5841059602649007 	 lr:5e-05
epoch30: train: loss:0.6558689163933116 	 acc:0.5983471074380166 | test: loss:0.6498216131665059 	 acc:0.6039735099337749 	 lr:5e-05
epoch31: train: loss:0.6882140621863121 	 acc:0.5685950413223141 | test: loss:0.6783368176182374 	 acc:0.5933774834437087 	 lr:5e-05
epoch32: train: loss:0.6571363190185925 	 acc:0.5897520661157025 | test: loss:0.6488788841575976 	 acc:0.6198675496688741 	 lr:5e-05
epoch33: train: loss:0.6579560219945987 	 acc:0.5841322314049586 | test: loss:0.6494786033567214 	 acc:0.6291390728476821 	 lr:5e-05
epoch34: train: loss:0.6618629626203174 	 acc:0.5940495867768595 | test: loss:0.6486499394012603 	 acc:0.6198675496688741 	 lr:5e-05
epoch35: train: loss:0.6581674944270741 	 acc:0.5871074380165289 | test: loss:0.6473468872885041 	 acc:0.6185430463576159 	 lr:5e-05
epoch36: train: loss:0.648161780913014 	 acc:0.5342148760330578 | test: loss:0.6400835495121432 	 acc:0.5364238410596026 	 lr:2.5e-05
epoch37: train: loss:0.651696058403362 	 acc:0.564297520661157 | test: loss:0.6413240177741903 	 acc:0.5880794701986755 	 lr:2.5e-05
epoch38: train: loss:0.6482800981624067 	 acc:0.5533884297520661 | test: loss:0.6408408864444455 	 acc:0.5708609271523178 	 lr:2.5e-05
epoch39: train: loss:0.653040743524378 	 acc:0.5844628099173553 | test: loss:0.6438024548505316 	 acc:0.6026490066225165 	 lr:2.5e-05
epoch40: train: loss:0.6561971998608802 	 acc:0.5940495867768595 | test: loss:0.6468410801413833 	 acc:0.6172185430463576 	 lr:2.5e-05
epoch41: train: loss:0.6513974862256326 	 acc:0.5897520661157025 | test: loss:0.6446719591191273 	 acc:0.6105960264900663 	 lr:2.5e-05
epoch42: train: loss:0.6446765594443014 	 acc:0.5441322314049587 | test: loss:0.6393220803595536 	 acc:0.56158940397351 	 lr:2.5e-05
epoch43: train: loss:0.6537063863651812 	 acc:0.5923966942148761 | test: loss:0.6447334403233812 	 acc:0.609271523178808 	 lr:2.5e-05
epoch44: train: loss:0.6508972523035097 	 acc:0.5715702479338843 | test: loss:0.6415484036041411 	 acc:0.5920529801324503 	 lr:2.5e-05
epoch45: train: loss:0.6587435880180232 	 acc:0.5864462809917356 | test: loss:0.6468438866912134 	 acc:0.6397350993377483 	 lr:2.5e-05
epoch46: train: loss:0.6464917420749822 	 acc:0.5428099173553719 | test: loss:0.6386035599455928 	 acc:0.5589403973509933 	 lr:2.5e-05
epoch47: train: loss:0.6475540505558991 	 acc:0.5457851239669421 | test: loss:0.638259974141784 	 acc:0.5668874172185431 	 lr:2.5e-05
epoch48: train: loss:0.6515090981987882 	 acc:0.5785123966942148 | test: loss:0.6406969355431613 	 acc:0.5933774834437087 	 lr:2.5e-05
epoch49: train: loss:0.6602143988727538 	 acc:0.6033057851239669 | test: loss:0.6520171395200767 	 acc:0.6463576158940397 	 lr:2.5e-05
epoch50: train: loss:0.6489436678452926 	 acc:0.5593388429752066 | test: loss:0.6385688462004756 	 acc:0.5841059602649007 	 lr:2.5e-05
epoch51: train: loss:0.6598841776729615 	 acc:0.6052892561983471 | test: loss:0.6462732939530682 	 acc:0.6397350993377483 	 lr:2.5e-05
epoch52: train: loss:0.64764775977647 	 acc:0.5517355371900826 | test: loss:0.6375725727207613 	 acc:0.5668874172185431 	 lr:2.5e-05
epoch53: train: loss:0.6458645325849864 	 acc:0.5447933884297521 | test: loss:0.6386016802282523 	 acc:0.5496688741721855 	 lr:2.5e-05
epoch54: train: loss:0.6528710363719089 	 acc:0.5828099173553719 | test: loss:0.6425341722191564 	 acc:0.5973509933774834 	 lr:2.5e-05
epoch55: train: loss:0.6495578889807394 	 acc:0.5709090909090909 | test: loss:0.6389936579773757 	 acc:0.5735099337748344 	 lr:2.5e-05
epoch56: train: loss:0.6647827131886127 	 acc:0.6033057851239669 | test: loss:0.6518907257263235 	 acc:0.6463576158940397 	 lr:2.5e-05
epoch57: train: loss:0.651070921342235 	 acc:0.5755371900826446 | test: loss:0.6393063377070901 	 acc:0.5761589403973509 	 lr:2.5e-05
epoch58: train: loss:0.6575300399725102 	 acc:0.610909090909091 | test: loss:0.6458698040602223 	 acc:0.6264900662251656 	 lr:2.5e-05
epoch59: train: loss:0.6532138878254851 	 acc:0.5914049586776859 | test: loss:0.6422346360636073 	 acc:0.6198675496688741 	 lr:1.25e-05
epoch60: train: loss:0.646841105271962 	 acc:0.5613223140495868 | test: loss:0.6387179699954607 	 acc:0.5774834437086093 	 lr:1.25e-05
epoch61: train: loss:0.6534690534772952 	 acc:0.5890909090909091 | test: loss:0.6431353425348042 	 acc:0.6158940397350994 	 lr:1.25e-05
epoch62: train: loss:0.6466578192356204 	 acc:0.5504132231404959 | test: loss:0.6368535283385524 	 acc:0.5735099337748344 	 lr:1.25e-05
epoch63: train: loss:0.6514182818428544 	 acc:0.5768595041322314 | test: loss:0.6390238521904346 	 acc:0.5960264900662252 	 lr:1.25e-05
epoch64: train: loss:0.6477251354721952 	 acc:0.5824793388429752 | test: loss:0.6387956781103121 	 acc:0.590728476821192 	 lr:1.25e-05
epoch65: train: loss:0.6466463223173599 	 acc:0.5547107438016529 | test: loss:0.6370866984721051 	 acc:0.5642384105960265 	 lr:1.25e-05
epoch66: train: loss:0.6489535312416139 	 acc:0.56 | test: loss:0.6374973177120385 	 acc:0.5735099337748344 	 lr:1.25e-05
epoch67: train: loss:0.650597305888972 	 acc:0.5732231404958678 | test: loss:0.6384373240912987 	 acc:0.5933774834437087 	 lr:1.25e-05
epoch68: train: loss:0.6655905662686371 	 acc:0.6099173553719008 | test: loss:0.654424497149638 	 acc:0.6476821192052981 	 lr:1.25e-05
epoch69: train: loss:0.6481995103181887 	 acc:0.5801652892561984 | test: loss:0.6390569851887936 	 acc:0.5960264900662252 	 lr:6.25e-06
epoch70: train: loss:0.6505327013504406 	 acc:0.5818181818181818 | test: loss:0.6406783043154028 	 acc:0.5960264900662252 	 lr:6.25e-06
epoch71: train: loss:0.6484365760393379 	 acc:0.5890909090909091 | test: loss:0.6411657869421094 	 acc:0.6013245033112583 	 lr:6.25e-06
epoch72: train: loss:0.6533420501661694 	 acc:0.5970247933884297 | test: loss:0.6430351488637608 	 acc:0.6225165562913907 	 lr:6.25e-06
epoch73: train: loss:0.648973125051861 	 acc:0.5801652892561984 | test: loss:0.6389263308600874 	 acc:0.5827814569536424 	 lr:6.25e-06
epoch74: train: loss:0.6477977882535004 	 acc:0.5580165289256198 | test: loss:0.6369493514496759 	 acc:0.5682119205298013 	 lr:6.25e-06
epoch75: train: loss:0.6504817569354349 	 acc:0.5725619834710743 | test: loss:0.6385133699076065 	 acc:0.5867549668874172 	 lr:3.125e-06
epoch76: train: loss:0.6531495796550404 	 acc:0.5910743801652892 | test: loss:0.6424535496345419 	 acc:0.6251655629139072 	 lr:3.125e-06
epoch77: train: loss:0.645611279503373 	 acc:0.5788429752066115 | test: loss:0.6379025295870193 	 acc:0.5774834437086093 	 lr:3.125e-06
epoch78: train: loss:0.6506478770902334 	 acc:0.5765289256198347 | test: loss:0.6392521911898985 	 acc:0.590728476821192 	 lr:3.125e-06
epoch79: train: loss:0.6489119834545231 	 acc:0.588099173553719 | test: loss:0.6394407565230565 	 acc:0.5973509933774834 	 lr:3.125e-06
epoch80: train: loss:0.646775099364194 	 acc:0.5738842975206612 | test: loss:0.6380219517164673 	 acc:0.5788079470198676 	 lr:3.125e-06
epoch81: train: loss:0.6505251173145515 	 acc:0.6023140495867768 | test: loss:0.6405533884534773 	 acc:0.6 	 lr:1.5625e-06
epoch82: train: loss:0.6493825151703575 	 acc:0.5775206611570248 | test: loss:0.6383457498834623 	 acc:0.5854304635761589 	 lr:1.5625e-06
epoch83: train: loss:0.6515724243211353 	 acc:0.583801652892562 | test: loss:0.6395242589198991 	 acc:0.5973509933774834 	 lr:1.5625e-06
epoch84: train: loss:0.6476912598373477 	 acc:0.5821487603305785 | test: loss:0.6381969517429933 	 acc:0.5841059602649007 	 lr:1.5625e-06
epoch85: train: loss:0.6489501369689121 	 acc:0.579504132231405 | test: loss:0.6386954412555063 	 acc:0.5854304635761589 	 lr:1.5625e-06
epoch86: train: loss:0.650279221515025 	 acc:0.5771900826446281 | test: loss:0.6400197939367483 	 acc:0.5947019867549669 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_2_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_2_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7791547804627537 	 acc:0.49553719008264463 | test: loss:0.7993937846840612 	 acc:0.5033112582781457 	 lr:0.0001
epoch1: train: loss:0.7936524961802585 	 acc:0.48760330578512395 | test: loss:0.7847062758262584 	 acc:0.5046357615894039 	 lr:0.0001
epoch2: train: loss:0.7147076526949229 	 acc:0.5338842975206611 | test: loss:0.698903236483896 	 acc:0.5562913907284768 	 lr:0.0001
epoch3: train: loss:0.6589341736628004 	 acc:0.5937190082644628 | test: loss:0.6529725809760442 	 acc:0.5801324503311258 	 lr:0.0001
epoch4: train: loss:0.6541615264868933 	 acc:0.5213223140495867 | test: loss:0.6534651896811479 	 acc:0.5218543046357615 	 lr:0.0001
epoch5: train: loss:0.6675026232151946 	 acc:0.6132231404958678 | test: loss:0.6574209114573649 	 acc:0.6264900662251656 	 lr:0.0001
epoch6: train: loss:0.6427519566559594 	 acc:0.5309090909090909 | test: loss:0.6373295776101927 	 acc:0.5377483443708609 	 lr:0.0001
epoch7: train: loss:0.6421860603655666 	 acc:0.5385123966942149 | test: loss:0.6384482396359475 	 acc:0.5337748344370861 	 lr:0.0001
epoch8: train: loss:0.653320596080181 	 acc:0.6036363636363636 | test: loss:0.6388938339340766 	 acc:0.6423841059602649 	 lr:0.0001
epoch9: train: loss:0.6690938661906345 	 acc:0.6092561983471074 | test: loss:0.6601156058690406 	 acc:0.6370860927152318 	 lr:0.0001
epoch10: train: loss:0.6388547744238672 	 acc:0.6062809917355372 | test: loss:0.6301163136564343 	 acc:0.5827814569536424 	 lr:0.0001
epoch11: train: loss:0.6360935177881856 	 acc:0.5464462809917355 | test: loss:0.6292498302775503 	 acc:0.5589403973509933 	 lr:0.0001
epoch12: train: loss:0.6341903411652431 	 acc:0.5814876033057851 | test: loss:0.6272421878694698 	 acc:0.5933774834437087 	 lr:0.0001
epoch13: train: loss:0.6442309774446093 	 acc:0.5249586776859504 | test: loss:0.6371475532354898 	 acc:0.5258278145695364 	 lr:0.0001
epoch14: train: loss:0.6328821078607859 	 acc:0.5659504132231405 | test: loss:0.6244336057972434 	 acc:0.5788079470198676 	 lr:0.0001
epoch15: train: loss:0.6447014444721632 	 acc:0.6201652892561983 | test: loss:0.6398661415308516 	 acc:0.6264900662251656 	 lr:0.0001
epoch16: train: loss:0.6374140197974592 	 acc:0.5375206611570248 | test: loss:0.6350622557646391 	 acc:0.528476821192053 	 lr:0.0001
epoch17: train: loss:0.6321066562597417 	 acc:0.5811570247933884 | test: loss:0.6204284514022979 	 acc:0.6198675496688741 	 lr:0.0001
epoch18: train: loss:0.6342668209785273 	 acc:0.5441322314049587 | test: loss:0.6279418069005802 	 acc:0.5602649006622517 	 lr:0.0001
epoch19: train: loss:0.62748204495296 	 acc:0.5894214876033058 | test: loss:0.6191058654658842 	 acc:0.5920529801324503 	 lr:0.0001
epoch20: train: loss:0.6348790341172337 	 acc:0.6366942148760331 | test: loss:0.6273608647434916 	 acc:0.6516556291390728 	 lr:0.0001
epoch21: train: loss:0.6388441786096116 	 acc:0.619504132231405 | test: loss:0.6252016813549774 	 acc:0.6463576158940397 	 lr:0.0001
epoch22: train: loss:0.6314196766506542 	 acc:0.6380165289256199 | test: loss:0.6169547862564491 	 acc:0.6516556291390728 	 lr:0.0001
epoch23: train: loss:0.6380104507493579 	 acc:0.6006611570247934 | test: loss:0.6180777619216615 	 acc:0.6225165562913907 	 lr:0.0001
epoch24: train: loss:0.6336798284861667 	 acc:0.5418181818181819 | test: loss:0.6287375846445955 	 acc:0.5456953642384106 	 lr:0.0001
epoch25: train: loss:0.6223693143828841 	 acc:0.607603305785124 | test: loss:0.6157125519600926 	 acc:0.609271523178808 	 lr:0.0001
epoch26: train: loss:0.6526280542444591 	 acc:0.6145454545454545 | test: loss:0.6410251214014774 	 acc:0.6397350993377483 	 lr:0.0001
epoch27: train: loss:0.6621236578492093 	 acc:0.6254545454545455 | test: loss:0.6396159995470615 	 acc:0.6516556291390728 	 lr:0.0001
epoch28: train: loss:0.6505924552925362 	 acc:0.5256198347107438 | test: loss:0.6422341829893605 	 acc:0.5311258278145695 	 lr:0.0001
epoch29: train: loss:0.6166556713600789 	 acc:0.6459504132231405 | test: loss:0.6083690339366332 	 acc:0.6450331125827815 	 lr:0.0001
epoch30: train: loss:0.7143492567834775 	 acc:0.5672727272727273 | test: loss:0.6990040792534683 	 acc:0.5642384105960265 	 lr:0.0001
epoch31: train: loss:0.6234011176203893 	 acc:0.6436363636363637 | test: loss:0.6156687654406819 	 acc:0.6529801324503312 	 lr:0.0001
epoch32: train: loss:0.61840990442875 	 acc:0.5887603305785124 | test: loss:0.6130750420867213 	 acc:0.5947019867549669 	 lr:0.0001
epoch33: train: loss:0.6185186082863611 	 acc:0.6327272727272727 | test: loss:0.6077268410202683 	 acc:0.6556291390728477 	 lr:0.0001
epoch34: train: loss:0.6320919628970879 	 acc:0.540495867768595 | test: loss:0.6241824783236776 	 acc:0.5589403973509933 	 lr:0.0001
epoch35: train: loss:0.6237763141797594 	 acc:0.5636363636363636 | test: loss:0.6163474486363645 	 acc:0.5748344370860927 	 lr:0.0001
epoch36: train: loss:0.6160837970883393 	 acc:0.6290909090909091 | test: loss:0.6090056294637011 	 acc:0.6543046357615894 	 lr:0.0001
epoch37: train: loss:0.6267274219339544 	 acc:0.5596694214876033 | test: loss:0.6230671105795349 	 acc:0.5456953642384106 	 lr:0.0001
epoch38: train: loss:0.6226818680369164 	 acc:0.5593388429752066 | test: loss:0.6223023802239374 	 acc:0.5483443708609271 	 lr:0.0001
epoch39: train: loss:0.6185996048312542 	 acc:0.5831404958677686 | test: loss:0.6169155664791335 	 acc:0.5708609271523178 	 lr:0.0001
epoch40: train: loss:0.6148468572837262 	 acc:0.6046280991735538 | test: loss:0.6097600879258668 	 acc:0.6039735099337749 	 lr:5e-05
epoch41: train: loss:0.6141427182166045 	 acc:0.6489256198347108 | test: loss:0.6067323341274893 	 acc:0.6529801324503312 	 lr:5e-05
epoch42: train: loss:0.618153901494239 	 acc:0.6515702479338843 | test: loss:0.6058695894992904 	 acc:0.6503311258278146 	 lr:5e-05
epoch43: train: loss:0.6142918060830802 	 acc:0.6512396694214876 | test: loss:0.6056733577456695 	 acc:0.6609271523178808 	 lr:5e-05
epoch44: train: loss:0.6394286251068115 	 acc:0.6585123966942149 | test: loss:0.6210661652861842 	 acc:0.6768211920529801 	 lr:5e-05
epoch45: train: loss:0.6142280579204402 	 acc:0.6211570247933884 | test: loss:0.6043749140587864 	 acc:0.6463576158940397 	 lr:5e-05
epoch46: train: loss:0.6119417168877341 	 acc:0.6327272727272727 | test: loss:0.6063375416180945 	 acc:0.6317880794701987 	 lr:5e-05
epoch47: train: loss:0.6125764428485524 	 acc:0.6363636363636364 | test: loss:0.6057321877669025 	 acc:0.6423841059602649 	 lr:5e-05
epoch48: train: loss:0.6148144653414892 	 acc:0.6198347107438017 | test: loss:0.6047582527659586 	 acc:0.6344370860927152 	 lr:5e-05
epoch49: train: loss:0.6228483137808556 	 acc:0.663801652892562 | test: loss:0.6082267862282051 	 acc:0.6847682119205298 	 lr:5e-05
epoch50: train: loss:0.6234419712350389 	 acc:0.6704132231404959 | test: loss:0.616577956771219 	 acc:0.6622516556291391 	 lr:5e-05
epoch51: train: loss:0.6103848948163435 	 acc:0.6535537190082644 | test: loss:0.6047318992235803 	 acc:0.6635761589403973 	 lr:5e-05
epoch52: train: loss:0.6186635641027088 	 acc:0.6747107438016529 | test: loss:0.6112143767590554 	 acc:0.6768211920529801 	 lr:2.5e-05
epoch53: train: loss:0.6137497845760062 	 acc:0.6241322314049587 | test: loss:0.6045549958746954 	 acc:0.6463576158940397 	 lr:2.5e-05
epoch54: train: loss:0.6193747305278936 	 acc:0.6740495867768596 | test: loss:0.6082244584891969 	 acc:0.6874172185430464 	 lr:2.5e-05
epoch55: train: loss:0.6098035417903553 	 acc:0.6208264462809917 | test: loss:0.6027925320018995 	 acc:0.6437086092715232 	 lr:2.5e-05
epoch56: train: loss:0.6151830880109929 	 acc:0.675702479338843 | test: loss:0.6066045814792052 	 acc:0.6794701986754967 	 lr:2.5e-05
epoch57: train: loss:0.610661864694485 	 acc:0.6353719008264462 | test: loss:0.6032808201991959 	 acc:0.633112582781457 	 lr:2.5e-05
epoch58: train: loss:0.6183802816099372 	 acc:0.6819834710743802 | test: loss:0.610806783777199 	 acc:0.6927152317880795 	 lr:2.5e-05
epoch59: train: loss:0.6150450160089603 	 acc:0.6555371900826447 | test: loss:0.6030883020123109 	 acc:0.6662251655629139 	 lr:2.5e-05
epoch60: train: loss:0.6242673819912367 	 acc:0.6753719008264463 | test: loss:0.6111099898420422 	 acc:0.6940397350993377 	 lr:2.5e-05
epoch61: train: loss:0.6134742646572019 	 acc:0.6618181818181819 | test: loss:0.6030692904200775 	 acc:0.680794701986755 	 lr:2.5e-05
epoch62: train: loss:0.6158289349768773 	 acc:0.6558677685950414 | test: loss:0.6021499839839556 	 acc:0.6887417218543046 	 lr:1.25e-05
epoch63: train: loss:0.6109291703641907 	 acc:0.6657851239669421 | test: loss:0.6009347250919468 	 acc:0.6741721854304635 	 lr:1.25e-05
epoch64: train: loss:0.6100951573277308 	 acc:0.6578512396694215 | test: loss:0.6013364491083764 	 acc:0.6622516556291391 	 lr:1.25e-05
epoch65: train: loss:0.6089767718512165 	 acc:0.6730578512396694 | test: loss:0.6019122643976023 	 acc:0.6834437086092715 	 lr:1.25e-05
epoch66: train: loss:0.6157281244096677 	 acc:0.6575206611570248 | test: loss:0.6030208856854218 	 acc:0.6874172185430464 	 lr:1.25e-05
epoch67: train: loss:0.6133972460967451 	 acc:0.6631404958677686 | test: loss:0.6015401196006118 	 acc:0.6781456953642384 	 lr:1.25e-05
epoch68: train: loss:0.6111901078145366 	 acc:0.6694214876033058 | test: loss:0.601334758389075 	 acc:0.6754966887417219 	 lr:1.25e-05
epoch69: train: loss:0.6090547212490366 	 acc:0.656198347107438 | test: loss:0.6012435519932121 	 acc:0.6741721854304635 	 lr:1.25e-05
epoch70: train: loss:0.6114585716665284 	 acc:0.6621487603305786 | test: loss:0.6015590381148636 	 acc:0.6834437086092715 	 lr:6.25e-06
epoch71: train: loss:0.612054866680429 	 acc:0.6608264462809917 | test: loss:0.6018416823930298 	 acc:0.6847682119205298 	 lr:6.25e-06
epoch72: train: loss:0.6141676132541057 	 acc:0.6624793388429752 | test: loss:0.6023281878193483 	 acc:0.686092715231788 	 lr:6.25e-06
epoch73: train: loss:0.6102393546380287 	 acc:0.6680991735537191 | test: loss:0.6015830290238589 	 acc:0.6754966887417219 	 lr:6.25e-06
epoch74: train: loss:0.61319694028413 	 acc:0.6786776859504132 | test: loss:0.6031099103933928 	 acc:0.6847682119205298 	 lr:6.25e-06
epoch75: train: loss:0.6092126527896597 	 acc:0.663801652892562 | test: loss:0.6010987965476434 	 acc:0.6675496688741722 	 lr:6.25e-06
epoch76: train: loss:0.6135471710488816 	 acc:0.6743801652892562 | test: loss:0.6030246795407984 	 acc:0.6794701986754967 	 lr:3.125e-06
epoch77: train: loss:0.6109772025061048 	 acc:0.6704132231404959 | test: loss:0.6014814853668213 	 acc:0.6768211920529801 	 lr:3.125e-06
epoch78: train: loss:0.6146717394284965 	 acc:0.6773553719008264 | test: loss:0.6042577952738629 	 acc:0.6834437086092715 	 lr:3.125e-06
epoch79: train: loss:0.6118197346324763 	 acc:0.6667768595041322 | test: loss:0.6015752562623939 	 acc:0.6794701986754967 	 lr:3.125e-06
epoch80: train: loss:0.6108133601551213 	 acc:0.6674380165289256 | test: loss:0.6021107871800858 	 acc:0.6781456953642384 	 lr:3.125e-06
epoch81: train: loss:0.6098614865098118 	 acc:0.6581818181818182 | test: loss:0.6013131930338625 	 acc:0.6794701986754967 	 lr:3.125e-06
epoch82: train: loss:0.6135739026581946 	 acc:0.6634710743801653 | test: loss:0.6027705187829125 	 acc:0.6794701986754967 	 lr:1.5625e-06
epoch83: train: loss:0.6110100195821652 	 acc:0.6700826446280992 | test: loss:0.6016310822884768 	 acc:0.6768211920529801 	 lr:1.5625e-06
epoch84: train: loss:0.6101278941690429 	 acc:0.6753719008264463 | test: loss:0.6017563533309279 	 acc:0.680794701986755 	 lr:1.5625e-06
epoch85: train: loss:0.6125841875706822 	 acc:0.6690909090909091 | test: loss:0.6024674571902546 	 acc:0.6768211920529801 	 lr:1.5625e-06
epoch86: train: loss:0.6099763737040118 	 acc:0.6733884297520661 | test: loss:0.602415240442516 	 acc:0.680794701986755 	 lr:1.5625e-06
epoch87: train: loss:0.6131993539471272 	 acc:0.6608264462809917 | test: loss:0.6015978502121982 	 acc:0.6781456953642384 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_3_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_3_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6914857679950305 	 acc:0.5213223140495867 | test: loss:0.6902190335539004 	 acc:0.5218543046357615 	 lr:0.0001
epoch1: train: loss:0.6742849945430913 	 acc:0.5213223140495867 | test: loss:0.6750695639098717 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6521890503315886 	 acc:0.5282644628099173 | test: loss:0.6500916729699697 	 acc:0.5271523178807948 	 lr:0.0001
epoch3: train: loss:0.6813894165251866 	 acc:0.5785123966942148 | test: loss:0.6763329926705518 	 acc:0.5854304635761589 	 lr:0.0001
epoch4: train: loss:0.6503517388312284 	 acc:0.5203305785123967 | test: loss:0.6503726194236452 	 acc:0.5218543046357615 	 lr:0.0001
epoch5: train: loss:0.6399731820082861 	 acc:0.5593388429752066 | test: loss:0.6353793505801271 	 acc:0.5589403973509933 	 lr:0.0001
epoch6: train: loss:0.6368734122504873 	 acc:0.5613223140495868 | test: loss:0.6328468367753439 	 acc:0.5788079470198676 	 lr:0.0001
epoch7: train: loss:0.6367643653853866 	 acc:0.5623140495867769 | test: loss:0.6347956383465142 	 acc:0.5549668874172186 	 lr:0.0001
epoch8: train: loss:0.6387456867911598 	 acc:0.5877685950413223 | test: loss:0.6286503279445976 	 acc:0.590728476821192 	 lr:0.0001
epoch9: train: loss:0.637869879588608 	 acc:0.623801652892562 | test: loss:0.6312063352161685 	 acc:0.6370860927152318 	 lr:0.0001
epoch10: train: loss:0.6303920073942705 	 acc:0.5715702479338843 | test: loss:0.6276025703411229 	 acc:0.5629139072847682 	 lr:0.0001
epoch11: train: loss:0.6278623083603284 	 acc:0.5745454545454546 | test: loss:0.6228872692348152 	 acc:0.5894039735099338 	 lr:0.0001
epoch12: train: loss:0.6289161664986413 	 acc:0.6221487603305785 | test: loss:0.6250081801256596 	 acc:0.6463576158940397 	 lr:0.0001
epoch13: train: loss:0.649754351249411 	 acc:0.5223140495867769 | test: loss:0.6497673969395114 	 acc:0.5245033112582781 	 lr:0.0001
epoch14: train: loss:0.6376208211567777 	 acc:0.5375206611570248 | test: loss:0.6344507876610914 	 acc:0.528476821192053 	 lr:0.0001
epoch15: train: loss:0.6260423573580656 	 acc:0.6168595041322315 | test: loss:0.6187308290146833 	 acc:0.6476821192052981 	 lr:0.0001
epoch16: train: loss:0.6378255532201657 	 acc:0.5606611570247934 | test: loss:0.6346741137125634 	 acc:0.5456953642384106 	 lr:0.0001
epoch17: train: loss:0.6418127599432448 	 acc:0.6376859504132232 | test: loss:0.6360739428475992 	 acc:0.6357615894039735 	 lr:0.0001
epoch18: train: loss:0.6246303287419406 	 acc:0.6095867768595041 | test: loss:0.617501880712067 	 acc:0.6397350993377483 	 lr:0.0001
epoch19: train: loss:0.6276888140568063 	 acc:0.6542148760330578 | test: loss:0.6154276337844646 	 acc:0.6596026490066225 	 lr:0.0001
epoch20: train: loss:0.6332511855353994 	 acc:0.6571900826446281 | test: loss:0.6243906932161344 	 acc:0.6556291390728477 	 lr:0.0001
epoch21: train: loss:0.6207108652493185 	 acc:0.6287603305785124 | test: loss:0.6104603570028646 	 acc:0.6569536423841059 	 lr:0.0001
epoch22: train: loss:0.6889264060249013 	 acc:0.5874380165289256 | test: loss:0.6765609072533665 	 acc:0.6105960264900663 	 lr:0.0001
epoch23: train: loss:0.6272937496437514 	 acc:0.5732231404958678 | test: loss:0.6185065695781582 	 acc:0.5801324503311258 	 lr:0.0001
epoch24: train: loss:0.6186636203379671 	 acc:0.6066115702479339 | test: loss:0.6123262842759392 	 acc:0.6052980132450331 	 lr:0.0001
epoch25: train: loss:0.6389105693761967 	 acc:0.6472727272727272 | test: loss:0.6272505328355246 	 acc:0.6768211920529801 	 lr:0.0001
epoch26: train: loss:0.685508452900185 	 acc:0.5943801652892562 | test: loss:0.6831411042750276 	 acc:0.5960264900662252 	 lr:0.0001
epoch27: train: loss:0.6471889021968054 	 acc:0.6416528925619834 | test: loss:0.6252449328536229 	 acc:0.6701986754966888 	 lr:0.0001
epoch28: train: loss:0.6587867637705211 	 acc:0.6393388429752066 | test: loss:0.6486299255825826 	 acc:0.6357615894039735 	 lr:5e-05
epoch29: train: loss:0.6664901214591727 	 acc:0.628099173553719 | test: loss:0.6645682763579666 	 acc:0.6185430463576159 	 lr:5e-05
epoch30: train: loss:0.637711759500267 	 acc:0.6727272727272727 | test: loss:0.6284305694876917 	 acc:0.6675496688741722 	 lr:5e-05
epoch31: train: loss:0.612304087098965 	 acc:0.6509090909090909 | test: loss:0.6050564540143044 	 acc:0.671523178807947 	 lr:5e-05
epoch32: train: loss:0.6136763523432834 	 acc:0.6667768595041322 | test: loss:0.6099565188616317 	 acc:0.680794701986755 	 lr:5e-05
epoch33: train: loss:0.6085496519616813 	 acc:0.6340495867768595 | test: loss:0.6027116568672736 	 acc:0.6370860927152318 	 lr:5e-05
epoch34: train: loss:0.6091617947767589 	 acc:0.6168595041322315 | test: loss:0.6038605273954126 	 acc:0.633112582781457 	 lr:5e-05
epoch35: train: loss:0.609906680840106 	 acc:0.628099173553719 | test: loss:0.6017665977509606 	 acc:0.6543046357615894 	 lr:5e-05
epoch36: train: loss:0.6197978984817001 	 acc:0.6780165289256198 | test: loss:0.6145973628719911 	 acc:0.6993377483443709 	 lr:5e-05
epoch37: train: loss:0.6136361505965556 	 acc:0.6680991735537191 | test: loss:0.6041954275788061 	 acc:0.6741721854304635 	 lr:5e-05
epoch38: train: loss:0.6071735161001032 	 acc:0.6618181818181819 | test: loss:0.5987464397158844 	 acc:0.6662251655629139 	 lr:5e-05
epoch39: train: loss:0.6140112657980485 	 acc:0.5910743801652892 | test: loss:0.6103090992037035 	 acc:0.5880794701986755 	 lr:5e-05
epoch40: train: loss:0.6082338470860946 	 acc:0.6466115702479339 | test: loss:0.6020973790560337 	 acc:0.6317880794701987 	 lr:5e-05
epoch41: train: loss:0.6225281883074233 	 acc:0.6819834710743802 | test: loss:0.613996189477428 	 acc:0.7019867549668874 	 lr:5e-05
epoch42: train: loss:0.6367852050213775 	 acc:0.6720661157024793 | test: loss:0.6200882669316222 	 acc:0.6768211920529801 	 lr:5e-05
epoch43: train: loss:0.6063674569918105 	 acc:0.6459504132231405 | test: loss:0.6007742585725342 	 acc:0.6476821192052981 	 lr:5e-05
epoch44: train: loss:0.6260359711883482 	 acc:0.6661157024793388 | test: loss:0.6119557679094226 	 acc:0.695364238410596 	 lr:5e-05
epoch45: train: loss:0.6055284252836685 	 acc:0.6419834710743801 | test: loss:0.5970212118515116 	 acc:0.6688741721854304 	 lr:2.5e-05
epoch46: train: loss:0.6068097833956568 	 acc:0.687603305785124 | test: loss:0.6006766393484658 	 acc:0.6874172185430464 	 lr:2.5e-05
epoch47: train: loss:0.6026059178281421 	 acc:0.6548760330578512 | test: loss:0.5978031764756765 	 acc:0.6688741721854304 	 lr:2.5e-05
epoch48: train: loss:0.6078022808476913 	 acc:0.6469421487603306 | test: loss:0.5986539894381896 	 acc:0.6635761589403973 	 lr:2.5e-05
epoch49: train: loss:0.6104817763832975 	 acc:0.6680991735537191 | test: loss:0.5983467375995307 	 acc:0.6927152317880795 	 lr:2.5e-05
epoch50: train: loss:0.6020342619754067 	 acc:0.6565289256198347 | test: loss:0.5963838101222815 	 acc:0.6649006622516557 	 lr:2.5e-05
epoch51: train: loss:0.5992521620978993 	 acc:0.6542148760330578 | test: loss:0.5996348170255194 	 acc:0.6397350993377483 	 lr:2.5e-05
epoch52: train: loss:0.6036656972002392 	 acc:0.6760330578512397 | test: loss:0.5978931541474449 	 acc:0.6887417218543046 	 lr:2.5e-05
epoch53: train: loss:0.6064195880022916 	 acc:0.6680991735537191 | test: loss:0.5986759629470623 	 acc:0.6940397350993377 	 lr:2.5e-05
epoch54: train: loss:0.607242958860949 	 acc:0.6006611570247934 | test: loss:0.6054519803318756 	 acc:0.5947019867549669 	 lr:2.5e-05
epoch55: train: loss:0.6029344281480332 	 acc:0.6690909090909091 | test: loss:0.5968286548229242 	 acc:0.6754966887417219 	 lr:2.5e-05
epoch56: train: loss:0.6034969699284262 	 acc:0.6409917355371901 | test: loss:0.5997316847573843 	 acc:0.6476821192052981 	 lr:2.5e-05
epoch57: train: loss:0.60435934506172 	 acc:0.6565289256198347 | test: loss:0.5980290262904389 	 acc:0.6476821192052981 	 lr:1.25e-05
epoch58: train: loss:0.6047039115527445 	 acc:0.6902479338842975 | test: loss:0.5985597117057699 	 acc:0.6980132450331126 	 lr:1.25e-05
epoch59: train: loss:0.6039791866964546 	 acc:0.675702479338843 | test: loss:0.5965795066182976 	 acc:0.6847682119205298 	 lr:1.25e-05
epoch60: train: loss:0.6049052926528552 	 acc:0.6687603305785124 | test: loss:0.5960886783157753 	 acc:0.6847682119205298 	 lr:1.25e-05
epoch61: train: loss:0.6046438468586315 	 acc:0.6833057851239669 | test: loss:0.5969872432039274 	 acc:0.695364238410596 	 lr:1.25e-05
epoch62: train: loss:0.6063736319738972 	 acc:0.6743801652892562 | test: loss:0.59546125258831 	 acc:0.6927152317880795 	 lr:1.25e-05
epoch63: train: loss:0.6057041647611571 	 acc:0.6872727272727273 | test: loss:0.5968148965709257 	 acc:0.7019867549668874 	 lr:1.25e-05
epoch64: train: loss:0.6028488744388927 	 acc:0.687603305785124 | test: loss:0.5971144324896351 	 acc:0.6980132450331126 	 lr:1.25e-05
epoch65: train: loss:0.5983427920223268 	 acc:0.6813223140495868 | test: loss:0.5953663515728831 	 acc:0.6900662251655629 	 lr:1.25e-05
epoch66: train: loss:0.6070662170993395 	 acc:0.692892561983471 | test: loss:0.5984854413184109 	 acc:0.6993377483443709 	 lr:1.25e-05
epoch67: train: loss:0.6034070022047059 	 acc:0.6770247933884298 | test: loss:0.5950687117923964 	 acc:0.6847682119205298 	 lr:1.25e-05
epoch68: train: loss:0.6046790437658957 	 acc:0.6786776859504132 | test: loss:0.5952086658667255 	 acc:0.6927152317880795 	 lr:1.25e-05
epoch69: train: loss:0.6029831006704283 	 acc:0.6899173553719008 | test: loss:0.5958257980694045 	 acc:0.6940397350993377 	 lr:1.25e-05
epoch70: train: loss:0.6018330678072843 	 acc:0.6856198347107438 | test: loss:0.5950177538473874 	 acc:0.6966887417218544 	 lr:1.25e-05
epoch71: train: loss:0.6030024081025241 	 acc:0.6647933884297521 | test: loss:0.5946462681751378 	 acc:0.6768211920529801 	 lr:1.25e-05
epoch72: train: loss:0.6136812686132005 	 acc:0.6909090909090909 | test: loss:0.6006025908009106 	 acc:0.7086092715231788 	 lr:1.25e-05
epoch73: train: loss:0.6015744296775376 	 acc:0.6912396694214876 | test: loss:0.5959288220531893 	 acc:0.7033112582781457 	 lr:1.25e-05
epoch74: train: loss:0.6046902425033002 	 acc:0.6902479338842975 | test: loss:0.5965674376645625 	 acc:0.6913907284768211 	 lr:1.25e-05
epoch75: train: loss:0.5993337495859005 	 acc:0.6829752066115703 | test: loss:0.5944136437201342 	 acc:0.6741721854304635 	 lr:1.25e-05
epoch76: train: loss:0.5994088124637761 	 acc:0.6565289256198347 | test: loss:0.5951278623366199 	 acc:0.6476821192052981 	 lr:1.25e-05
epoch77: train: loss:0.6017662433159253 	 acc:0.6816528925619835 | test: loss:0.5944506029419552 	 acc:0.6821192052980133 	 lr:1.25e-05
epoch78: train: loss:0.6083355086499994 	 acc:0.6958677685950413 | test: loss:0.599269903969291 	 acc:0.7086092715231788 	 lr:1.25e-05
epoch79: train: loss:0.6003004271530908 	 acc:0.6803305785123966 | test: loss:0.5943754781160923 	 acc:0.6913907284768211 	 lr:1.25e-05
epoch80: train: loss:0.6033452963632 	 acc:0.6912396694214876 | test: loss:0.5972680101331496 	 acc:0.695364238410596 	 lr:1.25e-05
epoch81: train: loss:0.6069682841064516 	 acc:0.6942148760330579 | test: loss:0.5977747261918933 	 acc:0.7086092715231788 	 lr:1.25e-05
epoch82: train: loss:0.6032573004005369 	 acc:0.6733884297520661 | test: loss:0.5941130355493912 	 acc:0.686092715231788 	 lr:6.25e-06
epoch83: train: loss:0.6012473421057394 	 acc:0.6747107438016529 | test: loss:0.5932787348103049 	 acc:0.6781456953642384 	 lr:6.25e-06
epoch84: train: loss:0.604567779233633 	 acc:0.6935537190082645 | test: loss:0.5969059489420707 	 acc:0.7086092715231788 	 lr:6.25e-06
epoch85: train: loss:0.6118697575301179 	 acc:0.6922314049586776 | test: loss:0.6031649531907594 	 acc:0.6940397350993377 	 lr:6.25e-06
epoch86: train: loss:0.6042259510883615 	 acc:0.7034710743801653 | test: loss:0.5991575256878177 	 acc:0.7086092715231788 	 lr:6.25e-06
epoch87: train: loss:0.602496558792335 	 acc:0.6766942148760331 | test: loss:0.5941671724351036 	 acc:0.6913907284768211 	 lr:6.25e-06
epoch88: train: loss:0.6041912085556786 	 acc:0.7057851239669422 | test: loss:0.5964953829910582 	 acc:0.7099337748344371 	 lr:6.25e-06
epoch89: train: loss:0.6044292856839077 	 acc:0.6942148760330579 | test: loss:0.5950301330610617 	 acc:0.7072847682119205 	 lr:6.25e-06
epoch90: train: loss:0.6016618679377659 	 acc:0.6644628099173554 | test: loss:0.592830007202578 	 acc:0.6794701986754967 	 lr:3.125e-06
epoch91: train: loss:0.6014256650160167 	 acc:0.6836363636363636 | test: loss:0.5940691602940591 	 acc:0.6847682119205298 	 lr:3.125e-06
epoch92: train: loss:0.6023898375329892 	 acc:0.6965289256198347 | test: loss:0.5963857764439867 	 acc:0.7072847682119205 	 lr:3.125e-06
epoch93: train: loss:0.5996220488981767 	 acc:0.6829752066115703 | test: loss:0.5941872857264335 	 acc:0.686092715231788 	 lr:3.125e-06
epoch94: train: loss:0.6068650295714701 	 acc:0.6895867768595041 | test: loss:0.5974159147565729 	 acc:0.7125827814569536 	 lr:3.125e-06
epoch95: train: loss:0.5984923992866327 	 acc:0.6905785123966942 | test: loss:0.5937892128300193 	 acc:0.6821192052980133 	 lr:3.125e-06
epoch96: train: loss:0.6049251751860312 	 acc:0.708099173553719 | test: loss:0.5995707609795576 	 acc:0.7112582781456953 	 lr:3.125e-06
epoch97: train: loss:0.6020191233217224 	 acc:0.6747107438016529 | test: loss:0.5935169019446468 	 acc:0.686092715231788 	 lr:1.5625e-06
epoch98: train: loss:0.6016795475620869 	 acc:0.6872727272727273 | test: loss:0.5946098927630494 	 acc:0.6927152317880795 	 lr:1.5625e-06
epoch99: train: loss:0.601363235268711 	 acc:0.6942148760330579 | test: loss:0.5937442553753884 	 acc:0.686092715231788 	 lr:1.5625e-06
epoch100: train: loss:0.6024760299871775 	 acc:0.6816528925619835 | test: loss:0.5939814268358495 	 acc:0.6887417218543046 	 lr:1.5625e-06
epoch101: train: loss:0.600529415036036 	 acc:0.6866115702479338 | test: loss:0.5943723842008224 	 acc:0.6940397350993377 	 lr:1.5625e-06
epoch102: train: loss:0.6001662106750425 	 acc:0.6816528925619835 | test: loss:0.593845998925089 	 acc:0.6887417218543046 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_4_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_4_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6892945581625316 	 acc:0.5213223140495867 | test: loss:0.6876135627165535 	 acc:0.5218543046357615 	 lr:0.0001
epoch1: train: loss:0.6757037998033949 	 acc:0.5213223140495867 | test: loss:0.6763356415641228 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6828205273171102 	 acc:0.5791735537190082 | test: loss:0.668122879400948 	 acc:0.5721854304635762 	 lr:0.0001
epoch3: train: loss:0.6547957474535162 	 acc:0.5884297520661157 | test: loss:0.6500360439155275 	 acc:0.6052980132450331 	 lr:0.0001
epoch4: train: loss:0.6563851846545196 	 acc:0.6155371900826446 | test: loss:0.6484706258931697 	 acc:0.6 	 lr:0.0001
epoch5: train: loss:0.6391383946237486 	 acc:0.5474380165289257 | test: loss:0.6377022909012852 	 acc:0.5324503311258278 	 lr:0.0001
epoch6: train: loss:0.6452071085843173 	 acc:0.5236363636363637 | test: loss:0.6457340569685627 	 acc:0.5258278145695364 	 lr:0.0001
epoch7: train: loss:0.6367873179617007 	 acc:0.5365289256198347 | test: loss:0.6392463648556084 	 acc:0.528476821192053 	 lr:0.0001
epoch8: train: loss:0.6392117765521215 	 acc:0.6039669421487603 | test: loss:0.6276961919487707 	 acc:0.614569536423841 	 lr:0.0001
epoch9: train: loss:0.6328505768263636 	 acc:0.5947107438016529 | test: loss:0.6259836522159198 	 acc:0.6198675496688741 	 lr:0.0001
epoch10: train: loss:0.6279232753406871 	 acc:0.5791735537190082 | test: loss:0.6262019334249939 	 acc:0.5735099337748344 	 lr:0.0001
epoch11: train: loss:0.6270889906055671 	 acc:0.6218181818181818 | test: loss:0.622874979546528 	 acc:0.6264900662251656 	 lr:0.0001
epoch12: train: loss:0.6353228855330096 	 acc:0.6492561983471075 | test: loss:0.633351062070455 	 acc:0.6516556291390728 	 lr:0.0001
epoch13: train: loss:0.6496220032439745 	 acc:0.523305785123967 | test: loss:0.6503764720152545 	 acc:0.5245033112582781 	 lr:0.0001
epoch14: train: loss:0.6394762722322763 	 acc:0.5368595041322314 | test: loss:0.637515075633068 	 acc:0.5311258278145695 	 lr:0.0001
epoch15: train: loss:0.6195852297790779 	 acc:0.6009917355371901 | test: loss:0.6140452817575821 	 acc:0.6278145695364239 	 lr:0.0001
epoch16: train: loss:0.6287533033583775 	 acc:0.5937190082644628 | test: loss:0.6253162741661071 	 acc:0.5761589403973509 	 lr:0.0001
epoch17: train: loss:0.634810944056708 	 acc:0.6495867768595042 | test: loss:0.6286191563732577 	 acc:0.6529801324503312 	 lr:0.0001
epoch18: train: loss:0.6244462582887697 	 acc:0.6459504132231405 | test: loss:0.6207501204598029 	 acc:0.6596026490066225 	 lr:0.0001
epoch19: train: loss:0.62637207291343 	 acc:0.6621487603305786 | test: loss:0.6146563126551394 	 acc:0.6781456953642384 	 lr:0.0001
epoch20: train: loss:0.6285896288856002 	 acc:0.6565289256198347 | test: loss:0.622054250192958 	 acc:0.6754966887417219 	 lr:0.0001
epoch21: train: loss:0.6196902453209743 	 acc:0.6651239669421488 | test: loss:0.6123157484641928 	 acc:0.680794701986755 	 lr:0.0001
epoch22: train: loss:0.703248971296736 	 acc:0.5715702479338843 | test: loss:0.6979381375754906 	 acc:0.5761589403973509 	 lr:0.0001
epoch23: train: loss:0.6232058452574675 	 acc:0.5669421487603306 | test: loss:0.6148355932425189 	 acc:0.5814569536423841 	 lr:0.0001
epoch24: train: loss:0.6165847659702144 	 acc:0.6641322314049587 | test: loss:0.6045793156750154 	 acc:0.6582781456953642 	 lr:0.0001
epoch25: train: loss:0.6135203025754818 	 acc:0.6694214876033058 | test: loss:0.6075216573595211 	 acc:0.6635761589403973 	 lr:0.0001
epoch26: train: loss:0.6232727348902994 	 acc:0.6598347107438016 | test: loss:0.6176604809350525 	 acc:0.6781456953642384 	 lr:0.0001
epoch27: train: loss:0.6384892306249004 	 acc:0.6446280991735537 | test: loss:0.628628289778501 	 acc:0.6556291390728477 	 lr:0.0001
epoch28: train: loss:0.6246310153086323 	 acc:0.6796694214876033 | test: loss:0.6162358778991447 	 acc:0.680794701986755 	 lr:0.0001
epoch29: train: loss:0.606880592334369 	 acc:0.6694214876033058 | test: loss:0.6029333210149348 	 acc:0.6662251655629139 	 lr:0.0001
epoch30: train: loss:0.6073882833197097 	 acc:0.6099173553719008 | test: loss:0.6064577107397926 	 acc:0.6119205298013245 	 lr:0.0001
epoch31: train: loss:0.6249019126852682 	 acc:0.6386776859504132 | test: loss:0.604566044602173 	 acc:0.6662251655629139 	 lr:0.0001
epoch32: train: loss:0.603427011612033 	 acc:0.6300826446280992 | test: loss:0.5991894178043138 	 acc:0.6437086092715232 	 lr:0.0001
epoch33: train: loss:0.6235848034905993 	 acc:0.6694214876033058 | test: loss:0.6135915891224185 	 acc:0.6821192052980133 	 lr:0.0001
epoch34: train: loss:0.6144056783037737 	 acc:0.5722314049586776 | test: loss:0.6073593987534378 	 acc:0.5973509933774834 	 lr:0.0001
epoch35: train: loss:0.6133620563026302 	 acc:0.6925619834710743 | test: loss:0.6054292185417074 	 acc:0.6834437086092715 	 lr:0.0001
epoch36: train: loss:0.6061899465568795 	 acc:0.6790082644628099 | test: loss:0.5963225628366533 	 acc:0.7072847682119205 	 lr:0.0001
epoch37: train: loss:0.6138322071004505 	 acc:0.6852892561983471 | test: loss:0.6032673385759063 	 acc:0.6940397350993377 	 lr:0.0001
epoch38: train: loss:0.6084814919716071 	 acc:0.6023140495867768 | test: loss:0.601540378150561 	 acc:0.6079470198675496 	 lr:0.0001
epoch39: train: loss:0.6111179496237069 	 acc:0.7004958677685951 | test: loss:0.601020272204418 	 acc:0.704635761589404 	 lr:0.0001
epoch40: train: loss:0.60676005960496 	 acc:0.6852892561983471 | test: loss:0.5933323291753302 	 acc:0.7019867549668874 	 lr:0.0001
epoch41: train: loss:0.599634028742136 	 acc:0.6234710743801652 | test: loss:0.595109616210129 	 acc:0.6251655629139072 	 lr:0.0001
epoch42: train: loss:0.6402019819740421 	 acc:0.6598347107438016 | test: loss:0.6237706665171693 	 acc:0.6662251655629139 	 lr:0.0001
epoch43: train: loss:0.6392792454829886 	 acc:0.5325619834710744 | test: loss:0.6347145705823077 	 acc:0.5390728476821192 	 lr:0.0001
epoch44: train: loss:0.6642232625740618 	 acc:0.6393388429752066 | test: loss:0.6541318960537185 	 acc:0.6344370860927152 	 lr:0.0001
epoch45: train: loss:0.601771647673993 	 acc:0.6958677685950413 | test: loss:0.5928913985656586 	 acc:0.7059602649006622 	 lr:0.0001
epoch46: train: loss:0.5889207615734132 	 acc:0.675702479338843 | test: loss:0.5853027624010251 	 acc:0.6821192052980133 	 lr:0.0001
epoch47: train: loss:0.595936645377766 	 acc:0.6671074380165289 | test: loss:0.5884437775769771 	 acc:0.6556291390728477 	 lr:0.0001
epoch48: train: loss:0.603183544411147 	 acc:0.616198347107438 | test: loss:0.5890545110039364 	 acc:0.6516556291390728 	 lr:0.0001
epoch49: train: loss:0.6481020917774232 	 acc:0.6509090909090909 | test: loss:0.6300130041229803 	 acc:0.6701986754966888 	 lr:0.0001
epoch50: train: loss:0.5912184202572531 	 acc:0.6942148760330579 | test: loss:0.5833526215805913 	 acc:0.704635761589404 	 lr:0.0001
epoch51: train: loss:0.5912455364495269 	 acc:0.691900826446281 | test: loss:0.5827722721541955 	 acc:0.7006622516556291 	 lr:0.0001
epoch52: train: loss:0.6728508330967801 	 acc:0.6251239669421488 | test: loss:0.6683199812244895 	 acc:0.6198675496688741 	 lr:0.0001
epoch53: train: loss:0.6368067571348395 	 acc:0.6694214876033058 | test: loss:0.6327444806793667 	 acc:0.6622516556291391 	 lr:0.0001
epoch54: train: loss:0.6550344486867101 	 acc:0.6363636363636364 | test: loss:0.6352638024368034 	 acc:0.6476821192052981 	 lr:0.0001
epoch55: train: loss:0.587456700289545 	 acc:0.6406611570247934 | test: loss:0.5847180567829814 	 acc:0.6476821192052981 	 lr:0.0001
epoch56: train: loss:0.5938469337234812 	 acc:0.6945454545454546 | test: loss:0.582086193719447 	 acc:0.7205298013245033 	 lr:0.0001
epoch57: train: loss:0.7817259775311494 	 acc:0.5239669421487604 | test: loss:0.7640421785266194 	 acc:0.5324503311258278 	 lr:0.0001
epoch58: train: loss:0.622595599663159 	 acc:0.6796694214876033 | test: loss:0.5986137165928519 	 acc:0.6993377483443709 	 lr:0.0001
epoch59: train: loss:0.6072211936682709 	 acc:0.7011570247933885 | test: loss:0.5832556563497379 	 acc:0.7178807947019867 	 lr:0.0001
epoch60: train: loss:0.6522013183467644 	 acc:0.6383471074380165 | test: loss:0.645933293191013 	 acc:0.6503311258278146 	 lr:0.0001
epoch61: train: loss:0.6354458927911175 	 acc:0.6671074380165289 | test: loss:0.6186673503837838 	 acc:0.6847682119205298 	 lr:0.0001
epoch62: train: loss:0.6114237322689088 	 acc:0.6965289256198347 | test: loss:0.5957100450597852 	 acc:0.7072847682119205 	 lr:0.0001
epoch63: train: loss:0.606111252465524 	 acc:0.715702479338843 | test: loss:0.5860211857107301 	 acc:0.7192052980132451 	 lr:5e-05
epoch64: train: loss:0.6117600839788263 	 acc:0.7041322314049587 | test: loss:0.6038160392780177 	 acc:0.695364238410596 	 lr:5e-05
epoch65: train: loss:0.6241605547242913 	 acc:0.691900826446281 | test: loss:0.6147038254516803 	 acc:0.6940397350993377 	 lr:5e-05
epoch66: train: loss:0.5821222988633085 	 acc:0.6578512396694215 | test: loss:0.5763964349860388 	 acc:0.6582781456953642 	 lr:5e-05
epoch67: train: loss:0.5772264145622569 	 acc:0.6932231404958678 | test: loss:0.5709092419668539 	 acc:0.6993377483443709 	 lr:5e-05
epoch68: train: loss:0.6004737510759969 	 acc:0.7150413223140496 | test: loss:0.5821445915872687 	 acc:0.7258278145695364 	 lr:5e-05
epoch69: train: loss:0.5844581798679572 	 acc:0.6426446280991736 | test: loss:0.5810663520105627 	 acc:0.6410596026490066 	 lr:5e-05
epoch70: train: loss:0.6294113198587717 	 acc:0.6856198347107438 | test: loss:0.6242110262643422 	 acc:0.6556291390728477 	 lr:5e-05
epoch71: train: loss:0.579471331746125 	 acc:0.6952066115702479 | test: loss:0.5698970699941875 	 acc:0.7086092715231788 	 lr:5e-05
epoch72: train: loss:0.5825724361947745 	 acc:0.6971900826446281 | test: loss:0.5737351089123859 	 acc:0.7086092715231788 	 lr:5e-05
epoch73: train: loss:0.5836217478090081 	 acc:0.7203305785123967 | test: loss:0.5743996459916727 	 acc:0.7218543046357616 	 lr:5e-05
epoch74: train: loss:0.5837609082017063 	 acc:0.730909090909091 | test: loss:0.5739761238066566 	 acc:0.7390728476821192 	 lr:5e-05
epoch75: train: loss:0.5818348865075544 	 acc:0.715702479338843 | test: loss:0.5699243292113803 	 acc:0.7337748344370861 	 lr:5e-05
epoch76: train: loss:0.6232071659584676 	 acc:0.6809917355371901 | test: loss:0.6108453376403707 	 acc:0.6834437086092715 	 lr:5e-05
epoch77: train: loss:0.5841953222219609 	 acc:0.7107438016528925 | test: loss:0.5723731055954434 	 acc:0.7364238410596027 	 lr:5e-05
epoch78: train: loss:0.5901065014807646 	 acc:0.7315702479338843 | test: loss:0.5821778684262409 	 acc:0.7311258278145696 	 lr:2.5e-05
epoch79: train: loss:0.573227408602218 	 acc:0.6955371900826446 | test: loss:0.5698436396800919 	 acc:0.7099337748344371 	 lr:2.5e-05
epoch80: train: loss:0.5841086986439287 	 acc:0.7325619834710744 | test: loss:0.5798451921008281 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch81: train: loss:0.5768187453333011 	 acc:0.725289256198347 | test: loss:0.5685078607489731 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch82: train: loss:0.5780864561490776 	 acc:0.6677685950413224 | test: loss:0.5722770634865919 	 acc:0.671523178807947 	 lr:2.5e-05
epoch83: train: loss:0.5885303043333953 	 acc:0.7335537190082645 | test: loss:0.5774587207282615 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch84: train: loss:0.5751381176956429 	 acc:0.724297520661157 | test: loss:0.5697468268950254 	 acc:0.7231788079470198 	 lr:2.5e-05
epoch85: train: loss:0.5917664127507486 	 acc:0.7233057851239669 | test: loss:0.583420012407745 	 acc:0.7218543046357616 	 lr:2.5e-05
epoch86: train: loss:0.5745972457207924 	 acc:0.6879338842975207 | test: loss:0.5696275223959361 	 acc:0.6900662251655629 	 lr:2.5e-05
epoch87: train: loss:0.5787927343825663 	 acc:0.72 | test: loss:0.5675303188380816 	 acc:0.7364238410596027 	 lr:2.5e-05
epoch88: train: loss:0.5802503753693635 	 acc:0.7371900826446282 | test: loss:0.5721684489029133 	 acc:0.7298013245033113 	 lr:2.5e-05
epoch89: train: loss:0.5777226443330119 	 acc:0.7233057851239669 | test: loss:0.5691641815450807 	 acc:0.7231788079470198 	 lr:2.5e-05
epoch90: train: loss:0.5783769626065719 	 acc:0.7143801652892562 | test: loss:0.5695320365444714 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch91: train: loss:0.5755179331519387 	 acc:0.7180165289256198 | test: loss:0.5686933077723775 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch92: train: loss:0.579726771520189 	 acc:0.7411570247933884 | test: loss:0.5752581048485459 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch93: train: loss:0.5828609137101607 	 acc:0.6393388429752066 | test: loss:0.5828017502431049 	 acc:0.6370860927152318 	 lr:2.5e-05
epoch94: train: loss:0.5746490548070797 	 acc:0.7233057851239669 | test: loss:0.5673327545456539 	 acc:0.7350993377483444 	 lr:1.25e-05
epoch95: train: loss:0.5709469742026211 	 acc:0.7180165289256198 | test: loss:0.5659284642200596 	 acc:0.7245033112582782 	 lr:1.25e-05
epoch96: train: loss:0.5824466348482558 	 acc:0.739504132231405 | test: loss:0.5744877861825046 	 acc:0.7311258278145696 	 lr:1.25e-05
epoch97: train: loss:0.5740093595528406 	 acc:0.7163636363636363 | test: loss:0.5656226103668971 	 acc:0.7284768211920529 	 lr:1.25e-05
epoch98: train: loss:0.570504121110459 	 acc:0.6912396694214876 | test: loss:0.5679284406024099 	 acc:0.6980132450331126 	 lr:1.25e-05
epoch99: train: loss:0.57401036506842 	 acc:0.7348760330578512 | test: loss:0.5676762246927678 	 acc:0.7324503311258278 	 lr:1.25e-05
epoch100: train: loss:0.5746808752737754 	 acc:0.7226446280991735 | test: loss:0.5653714783933779 	 acc:0.7298013245033113 	 lr:1.25e-05
epoch101: train: loss:0.5709869221025262 	 acc:0.7110743801652892 | test: loss:0.565623240360361 	 acc:0.7337748344370861 	 lr:1.25e-05
epoch102: train: loss:0.5763922642085177 	 acc:0.7368595041322314 | test: loss:0.5705463339950865 	 acc:0.7364238410596027 	 lr:1.25e-05
epoch103: train: loss:0.5700068192836667 	 acc:0.7305785123966942 | test: loss:0.5658072287673193 	 acc:0.7258278145695364 	 lr:1.25e-05
epoch104: train: loss:0.5742754061754085 	 acc:0.7209917355371901 | test: loss:0.5655229883478178 	 acc:0.7258278145695364 	 lr:1.25e-05
epoch105: train: loss:0.5709793518397434 	 acc:0.7295867768595041 | test: loss:0.5653727439855109 	 acc:0.7311258278145696 	 lr:1.25e-05
epoch106: train: loss:0.5680194183814624 	 acc:0.7110743801652892 | test: loss:0.5664751366274247 	 acc:0.713907284768212 	 lr:1.25e-05
epoch107: train: loss:0.5712288396811682 	 acc:0.7289256198347107 | test: loss:0.5667199107195368 	 acc:0.7284768211920529 	 lr:6.25e-06
epoch108: train: loss:0.5811119895730137 	 acc:0.7348760330578512 | test: loss:0.5717147949515589 	 acc:0.7390728476821192 	 lr:6.25e-06
epoch109: train: loss:0.5699009786952626 	 acc:0.7338842975206612 | test: loss:0.5651235348341481 	 acc:0.7271523178807947 	 lr:6.25e-06
epoch110: train: loss:0.5712344605272467 	 acc:0.7322314049586777 | test: loss:0.565206326238367 	 acc:0.7284768211920529 	 lr:6.25e-06
epoch111: train: loss:0.571178967637464 	 acc:0.7100826446280992 | test: loss:0.5650417957874323 	 acc:0.7218543046357616 	 lr:6.25e-06
epoch112: train: loss:0.5735354094071822 	 acc:0.7338842975206612 | test: loss:0.5704466735290376 	 acc:0.7377483443708609 	 lr:6.25e-06
epoch113: train: loss:0.573310859715643 	 acc:0.7315702479338843 | test: loss:0.5671678587300888 	 acc:0.7337748344370861 	 lr:6.25e-06
epoch114: train: loss:0.571671025575685 	 acc:0.7289256198347107 | test: loss:0.5652397233129337 	 acc:0.7245033112582782 	 lr:6.25e-06
epoch115: train: loss:0.571854872289768 	 acc:0.7269421487603306 | test: loss:0.5646288155719934 	 acc:0.7350993377483444 	 lr:6.25e-06
epoch116: train: loss:0.5688286280434979 	 acc:0.7282644628099173 | test: loss:0.5640647119244203 	 acc:0.7337748344370861 	 lr:6.25e-06
epoch117: train: loss:0.5697337532831618 	 acc:0.730909090909091 | test: loss:0.5645751077607768 	 acc:0.7324503311258278 	 lr:6.25e-06
epoch118: train: loss:0.5703606969857019 	 acc:0.736198347107438 | test: loss:0.566066783706084 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch119: train: loss:0.5686071278635135 	 acc:0.7289256198347107 | test: loss:0.5644423326909147 	 acc:0.7350993377483444 	 lr:6.25e-06
epoch120: train: loss:0.5722127577490058 	 acc:0.716694214876033 | test: loss:0.5642463091193446 	 acc:0.7218543046357616 	 lr:6.25e-06
epoch121: train: loss:0.5711326156371881 	 acc:0.7328925619834711 | test: loss:0.5647134870882855 	 acc:0.7337748344370861 	 lr:6.25e-06
epoch122: train: loss:0.5709752202033996 	 acc:0.7256198347107438 | test: loss:0.5657255784565249 	 acc:0.7271523178807947 	 lr:6.25e-06
epoch123: train: loss:0.5739665308274513 	 acc:0.7457851239669422 | test: loss:0.5667721972560251 	 acc:0.7417218543046358 	 lr:3.125e-06
epoch124: train: loss:0.5684497534342049 	 acc:0.715702479338843 | test: loss:0.5642601947121273 	 acc:0.7218543046357616 	 lr:3.125e-06
epoch125: train: loss:0.5708204051482776 	 acc:0.7282644628099173 | test: loss:0.5652866421945837 	 acc:0.7403973509933774 	 lr:3.125e-06
epoch126: train: loss:0.5717837345895689 	 acc:0.7385123966942149 | test: loss:0.5658467339364108 	 acc:0.7364238410596027 	 lr:3.125e-06
epoch127: train: loss:0.5715518123256274 	 acc:0.7209917355371901 | test: loss:0.5648748425458441 	 acc:0.7364238410596027 	 lr:3.125e-06
epoch128: train: loss:0.571056749406925 	 acc:0.7295867768595041 | test: loss:0.5640429663342356 	 acc:0.7298013245033113 	 lr:3.125e-06
epoch129: train: loss:0.5710443690985687 	 acc:0.7391735537190083 | test: loss:0.5648407327418296 	 acc:0.7337748344370861 	 lr:1.5625e-06
epoch130: train: loss:0.569301578111885 	 acc:0.7338842975206612 | test: loss:0.5647499735781688 	 acc:0.7364238410596027 	 lr:1.5625e-06
epoch131: train: loss:0.5686738962181344 	 acc:0.7375206611570247 | test: loss:0.565709669779468 	 acc:0.7337748344370861 	 lr:1.5625e-06
epoch132: train: loss:0.5696826732257181 	 acc:0.7322314049586777 | test: loss:0.5644813729437771 	 acc:0.7284768211920529 	 lr:1.5625e-06
epoch133: train: loss:0.5699082784810342 	 acc:0.7272727272727273 | test: loss:0.564252150295586 	 acc:0.7311258278145696 	 lr:1.5625e-06
epoch134: train: loss:0.5728491393199637 	 acc:0.7289256198347107 | test: loss:0.5645455913038443 	 acc:0.7350993377483444 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_5_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_5_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7292153961205285 	 acc:0.4909090909090909 | test: loss:0.7218996443495845 	 acc:0.5072847682119205 	 lr:0.0001
epoch1: train: loss:0.6517513160469118 	 acc:0.524297520661157 | test: loss:0.6490821558908122 	 acc:0.5231788079470199 	 lr:0.0001
epoch2: train: loss:0.6547918365021382 	 acc:0.5603305785123966 | test: loss:0.6472706115008979 	 acc:0.5801324503311258 	 lr:0.0001
epoch3: train: loss:0.6535178552186194 	 acc:0.6052892561983471 | test: loss:0.645433548586258 	 acc:0.6105960264900663 	 lr:0.0001
epoch4: train: loss:0.6405902096259692 	 acc:0.5808264462809918 | test: loss:0.6355471821810236 	 acc:0.5933774834437087 	 lr:0.0001
epoch5: train: loss:0.6328463796938747 	 acc:0.5963636363636363 | test: loss:0.6275436466103358 	 acc:0.5933774834437087 	 lr:0.0001
epoch6: train: loss:0.6448031921229087 	 acc:0.6360330578512396 | test: loss:0.625739540090624 	 acc:0.6450331125827815 	 lr:0.0001
epoch7: train: loss:0.6483743207710834 	 acc:0.6528925619834711 | test: loss:0.6386766919237099 	 acc:0.6781456953642384 	 lr:0.0001
epoch8: train: loss:0.6271432478762855 	 acc:0.6224793388429752 | test: loss:0.6174532286378721 	 acc:0.6410596026490066 	 lr:0.0001
epoch9: train: loss:0.6218506282420198 	 acc:0.595702479338843 | test: loss:0.6203995238076773 	 acc:0.5986754966887418 	 lr:0.0001
epoch10: train: loss:0.6196711440913933 	 acc:0.6218181818181818 | test: loss:0.6098301895406862 	 acc:0.6278145695364239 	 lr:0.0001
epoch11: train: loss:0.6144961363027903 	 acc:0.660495867768595 | test: loss:0.6028861774514053 	 acc:0.6622516556291391 	 lr:0.0001
epoch12: train: loss:0.6351703794140461 	 acc:0.6588429752066116 | test: loss:0.6171718214521345 	 acc:0.686092715231788 	 lr:0.0001
epoch13: train: loss:0.6104499808224765 	 acc:0.596694214876033 | test: loss:0.6022908345752994 	 acc:0.6066225165562914 	 lr:0.0001
epoch14: train: loss:0.6343070605175555 	 acc:0.6720661157024793 | test: loss:0.6202167589143412 	 acc:0.6966887417218544 	 lr:0.0001
epoch15: train: loss:0.6097831957793433 	 acc:0.6261157024793389 | test: loss:0.5937247119202519 	 acc:0.6596026490066225 	 lr:0.0001
epoch16: train: loss:0.6191089060089805 	 acc:0.6740495867768596 | test: loss:0.6041348611282197 	 acc:0.7019867549668874 	 lr:0.0001
epoch17: train: loss:0.6018923433556045 	 acc:0.6545454545454545 | test: loss:0.5920019876877993 	 acc:0.6476821192052981 	 lr:0.0001
epoch18: train: loss:0.601011136208684 	 acc:0.6780165289256198 | test: loss:0.58904647448205 	 acc:0.6993377483443709 	 lr:0.0001
epoch19: train: loss:0.6190587208487771 	 acc:0.6773553719008264 | test: loss:0.6085734496842946 	 acc:0.7178807947019867 	 lr:0.0001
epoch20: train: loss:0.5993108115905572 	 acc:0.7011570247933885 | test: loss:0.5818084180749805 	 acc:0.713907284768212 	 lr:0.0001
epoch21: train: loss:0.6084284639752601 	 acc:0.7004958677685951 | test: loss:0.5882827340372351 	 acc:0.7311258278145696 	 lr:0.0001
epoch22: train: loss:0.6266908816463691 	 acc:0.6816528925619835 | test: loss:0.6150544954451505 	 acc:0.7033112582781457 	 lr:0.0001
epoch23: train: loss:0.6083824998682196 	 acc:0.6846280991735537 | test: loss:0.5968118248396362 	 acc:0.7258278145695364 	 lr:0.0001
epoch24: train: loss:0.6001082774824348 	 acc:0.6370247933884298 | test: loss:0.5881427187793302 	 acc:0.6357615894039735 	 lr:0.0001
epoch25: train: loss:0.5994033762837244 	 acc:0.6737190082644628 | test: loss:0.5863156519188786 	 acc:0.704635761589404 	 lr:0.0001
epoch26: train: loss:0.5876755107532848 	 acc:0.6720661157024793 | test: loss:0.5739242097399883 	 acc:0.6940397350993377 	 lr:0.0001
epoch27: train: loss:0.6927287855818252 	 acc:0.5970247933884297 | test: loss:0.7077340107879891 	 acc:0.590728476821192 	 lr:0.0001
epoch28: train: loss:0.5994586830690872 	 acc:0.708099173553719 | test: loss:0.579313835166148 	 acc:0.7311258278145696 	 lr:0.0001
epoch29: train: loss:0.5829689979159143 	 acc:0.696198347107438 | test: loss:0.5652251617008487 	 acc:0.7218543046357616 	 lr:0.0001
epoch30: train: loss:0.5827481825883724 	 acc:0.6879338842975207 | test: loss:0.5709110463691863 	 acc:0.7019867549668874 	 lr:0.0001
epoch31: train: loss:0.5890842388681143 	 acc:0.708099173553719 | test: loss:0.5683105514538999 	 acc:0.7496688741721854 	 lr:0.0001
epoch32: train: loss:0.613304440758445 	 acc:0.6796694214876033 | test: loss:0.6167875945173352 	 acc:0.6874172185430464 	 lr:0.0001
epoch33: train: loss:0.583985506443938 	 acc:0.6882644628099174 | test: loss:0.5698308549969402 	 acc:0.7125827814569536 	 lr:0.0001
epoch34: train: loss:0.5884502378574088 	 acc:0.6320661157024794 | test: loss:0.5814423321888147 	 acc:0.6410596026490066 	 lr:0.0001
epoch35: train: loss:0.5977384096925908 	 acc:0.7127272727272728 | test: loss:0.5797984056914879 	 acc:0.7549668874172185 	 lr:0.0001
epoch36: train: loss:0.5765167678289177 	 acc:0.7084297520661157 | test: loss:0.5629087690486024 	 acc:0.7258278145695364 	 lr:5e-05
epoch37: train: loss:0.571057716026779 	 acc:0.7067768595041323 | test: loss:0.5616262332493106 	 acc:0.7152317880794702 	 lr:5e-05
epoch38: train: loss:0.5771078307569519 	 acc:0.7352066115702479 | test: loss:0.5646157838650887 	 acc:0.7456953642384105 	 lr:5e-05
epoch39: train: loss:0.5773105526561579 	 acc:0.6704132231404959 | test: loss:0.5649649111640375 	 acc:0.686092715231788 	 lr:5e-05
epoch40: train: loss:0.5771278179972625 	 acc:0.7249586776859505 | test: loss:0.5644209948596576 	 acc:0.7496688741721854 	 lr:5e-05
epoch41: train: loss:0.5852188304633148 	 acc:0.7322314049586777 | test: loss:0.5770069926779792 	 acc:0.7390728476821192 	 lr:5e-05
epoch42: train: loss:0.5721275945143266 	 acc:0.7133884297520661 | test: loss:0.5570900301270137 	 acc:0.7284768211920529 	 lr:5e-05
epoch43: train: loss:0.5755901697647473 	 acc:0.7269421487603306 | test: loss:0.56665437063634 	 acc:0.7377483443708609 	 lr:5e-05
epoch44: train: loss:0.5728141348224041 	 acc:0.6737190082644628 | test: loss:0.5664018426509883 	 acc:0.6781456953642384 	 lr:5e-05
epoch45: train: loss:0.5728057506458818 	 acc:0.7434710743801652 | test: loss:0.5599419304866664 	 acc:0.7509933774834437 	 lr:5e-05
epoch46: train: loss:0.5730010172552313 	 acc:0.7345454545454545 | test: loss:0.5624877755215626 	 acc:0.7337748344370861 	 lr:5e-05
epoch47: train: loss:0.5629491570764337 	 acc:0.7150413223140496 | test: loss:0.5584886382747171 	 acc:0.713907284768212 	 lr:5e-05
epoch48: train: loss:0.5785859199988941 	 acc:0.7345454545454545 | test: loss:0.5635850782425987 	 acc:0.7470198675496689 	 lr:5e-05
epoch49: train: loss:0.5682066594667671 	 acc:0.7216528925619835 | test: loss:0.5541184612457326 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch50: train: loss:0.5628726719233615 	 acc:0.7272727272727273 | test: loss:0.5528451588769622 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch51: train: loss:0.5676164005807609 	 acc:0.7431404958677686 | test: loss:0.5550994457788025 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch52: train: loss:0.5633642519998157 	 acc:0.7206611570247934 | test: loss:0.5514545670408286 	 acc:0.7417218543046358 	 lr:2.5e-05
epoch53: train: loss:0.5568958953786487 	 acc:0.7315702479338843 | test: loss:0.5508810787800922 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch54: train: loss:0.5629406174943467 	 acc:0.7355371900826446 | test: loss:0.5533118280353925 	 acc:0.7311258278145696 	 lr:2.5e-05
epoch55: train: loss:0.5621950655535233 	 acc:0.7444628099173554 | test: loss:0.5539350073858602 	 acc:0.7536423841059603 	 lr:2.5e-05
epoch56: train: loss:0.5689001233321576 	 acc:0.7474380165289256 | test: loss:0.5605873193961896 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch57: train: loss:0.5655060903296983 	 acc:0.7381818181818182 | test: loss:0.5518066978612483 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch58: train: loss:0.5697026042110663 	 acc:0.7414876033057851 | test: loss:0.555573215784616 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch59: train: loss:0.5733892652219977 	 acc:0.7342148760330579 | test: loss:0.5576679629205868 	 acc:0.7496688741721854 	 lr:2.5e-05
epoch60: train: loss:0.5631920156794146 	 acc:0.7484297520661157 | test: loss:0.5523769111033306 	 acc:0.7483443708609272 	 lr:1.25e-05
epoch61: train: loss:0.5613779866990964 	 acc:0.7418181818181818 | test: loss:0.5506756474640195 	 acc:0.7483443708609272 	 lr:1.25e-05
epoch62: train: loss:0.5618944198040923 	 acc:0.7563636363636363 | test: loss:0.552814297249775 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch63: train: loss:0.5629484942333758 	 acc:0.743801652892562 | test: loss:0.5526495665114447 	 acc:0.7483443708609272 	 lr:1.25e-05
epoch64: train: loss:0.562429947380192 	 acc:0.7355371900826446 | test: loss:0.5490589507368226 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch65: train: loss:0.5566437204022053 	 acc:0.7378512396694215 | test: loss:0.5492689613474916 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch66: train: loss:0.5579171907212124 	 acc:0.7428099173553719 | test: loss:0.5511137129455213 	 acc:0.752317880794702 	 lr:1.25e-05
epoch67: train: loss:0.5689216962924674 	 acc:0.748099173553719 | test: loss:0.5591850966017767 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch68: train: loss:0.56780150697251 	 acc:0.7520661157024794 | test: loss:0.5576548082149582 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch69: train: loss:0.5608526243848249 	 acc:0.7461157024793389 | test: loss:0.5487647581574143 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch70: train: loss:0.5625817251796564 	 acc:0.7471074380165289 | test: loss:0.5524089285869472 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch71: train: loss:0.5586675592099339 	 acc:0.7477685950413223 | test: loss:0.5510970862495979 	 acc:0.752317880794702 	 lr:1.25e-05
epoch72: train: loss:0.559566731551462 	 acc:0.7322314049586777 | test: loss:0.5503930875797145 	 acc:0.7483443708609272 	 lr:1.25e-05
epoch73: train: loss:0.5626460175080733 	 acc:0.7550413223140496 | test: loss:0.5530099322464292 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch74: train: loss:0.5553185585904713 	 acc:0.7477685950413223 | test: loss:0.5485551103061398 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch75: train: loss:0.5593158871477301 	 acc:0.7530578512396694 | test: loss:0.5493790853891941 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch76: train: loss:0.5707445642179694 	 acc:0.7494214876033057 | test: loss:0.5597306294157015 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch77: train: loss:0.5591406230295985 	 acc:0.7500826446280991 | test: loss:0.5490105009236873 	 acc:0.7602649006622516 	 lr:1.25e-05
epoch78: train: loss:0.557582959872632 	 acc:0.7378512396694215 | test: loss:0.5491110324859619 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch79: train: loss:0.5595649583083538 	 acc:0.7447933884297521 | test: loss:0.548796020359393 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch80: train: loss:0.5575689604262675 	 acc:0.7530578512396694 | test: loss:0.5506444080775936 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch81: train: loss:0.5567210762559875 	 acc:0.7533884297520661 | test: loss:0.5533188732254584 	 acc:0.7496688741721854 	 lr:6.25e-06
epoch82: train: loss:0.5560115785638163 	 acc:0.7315702479338843 | test: loss:0.5474746744364303 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch83: train: loss:0.5571907786495429 	 acc:0.7676033057851239 | test: loss:0.5535605268762601 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch84: train: loss:0.5536224221986188 	 acc:0.7500826446280991 | test: loss:0.5475346442879431 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch85: train: loss:0.5546497601122895 	 acc:0.7434710743801652 | test: loss:0.547062419581887 	 acc:0.7496688741721854 	 lr:6.25e-06
epoch86: train: loss:0.5593173082209816 	 acc:0.7570247933884298 | test: loss:0.5491916965175149 	 acc:0.7602649006622516 	 lr:6.25e-06
epoch87: train: loss:0.5581211604362677 	 acc:0.739504132231405 | test: loss:0.5463107444592659 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch88: train: loss:0.5617430756308815 	 acc:0.7613223140495867 | test: loss:0.5505933230286403 	 acc:0.7602649006622516 	 lr:6.25e-06
epoch89: train: loss:0.5591716384296574 	 acc:0.7431404958677686 | test: loss:0.5484556800482289 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch90: train: loss:0.5549974285866603 	 acc:0.7381818181818182 | test: loss:0.5471285996847595 	 acc:0.7390728476821192 	 lr:6.25e-06
epoch91: train: loss:0.5584645786758297 	 acc:0.7583471074380165 | test: loss:0.5501312358489889 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch92: train: loss:0.5586379022046554 	 acc:0.7510743801652893 | test: loss:0.5493288135686457 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch93: train: loss:0.5557572713568191 	 acc:0.7388429752066116 | test: loss:0.5463884182323683 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch94: train: loss:0.5564471290131245 	 acc:0.7593388429752066 | test: loss:0.5486043825844266 	 acc:0.7549668874172185 	 lr:3.125e-06
epoch95: train: loss:0.5590708790928864 	 acc:0.7474380165289256 | test: loss:0.5476128578975501 	 acc:0.752317880794702 	 lr:3.125e-06
epoch96: train: loss:0.557305104200505 	 acc:0.7517355371900827 | test: loss:0.5470666218277634 	 acc:0.7615894039735099 	 lr:3.125e-06
epoch97: train: loss:0.5562264727560942 	 acc:0.7527272727272727 | test: loss:0.5472563384384509 	 acc:0.7576158940397351 	 lr:3.125e-06
epoch98: train: loss:0.5583932613932396 	 acc:0.7454545454545455 | test: loss:0.5482211667180851 	 acc:0.7576158940397351 	 lr:3.125e-06
epoch99: train: loss:0.5570806148032511 	 acc:0.7510743801652893 | test: loss:0.5485424632268237 	 acc:0.7562913907284768 	 lr:3.125e-06
epoch100: train: loss:0.5568028823403287 	 acc:0.7454545454545455 | test: loss:0.5474689331275738 	 acc:0.7549668874172185 	 lr:1.5625e-06
epoch101: train: loss:0.5589173207007164 	 acc:0.7428099173553719 | test: loss:0.5483606273764806 	 acc:0.7549668874172185 	 lr:1.5625e-06
epoch102: train: loss:0.5561987866251922 	 acc:0.7500826446280991 | test: loss:0.5482152286744275 	 acc:0.7562913907284768 	 lr:1.5625e-06
epoch103: train: loss:0.5570640637263778 	 acc:0.7477685950413223 | test: loss:0.5476348475115189 	 acc:0.7549668874172185 	 lr:1.5625e-06
epoch104: train: loss:0.5560747929841033 	 acc:0.7527272727272727 | test: loss:0.5475277005441931 	 acc:0.7536423841059603 	 lr:1.5625e-06
epoch105: train: loss:0.5590876115058079 	 acc:0.7441322314049587 | test: loss:0.5474152235005865 	 acc:0.752317880794702 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_6_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_6_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7497790201242305 	 acc:0.484297520661157 | test: loss:0.7431991806093431 	 acc:0.4966887417218543 	 lr:0.0001
epoch1: train: loss:0.6556257538953103 	 acc:0.5219834710743801 | test: loss:0.6529317257420116 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6547603963032241 	 acc:0.5639669421487603 | test: loss:0.6449325599417781 	 acc:0.5894039735099338 	 lr:0.0001
epoch3: train: loss:0.6559610465538404 	 acc:0.6056198347107438 | test: loss:0.6455187240973214 	 acc:0.6172185430463576 	 lr:0.0001
epoch4: train: loss:0.641109291797827 	 acc:0.5672727272727273 | test: loss:0.6325815099754081 	 acc:0.5761589403973509 	 lr:0.0001
epoch5: train: loss:0.6341851855112501 	 acc:0.5818181818181818 | test: loss:0.6257580502933224 	 acc:0.5761589403973509 	 lr:0.0001
epoch6: train: loss:0.6434301613973192 	 acc:0.6261157024793389 | test: loss:0.6249311680035875 	 acc:0.6463576158940397 	 lr:0.0001
epoch7: train: loss:0.6535446733482613 	 acc:0.6558677685950414 | test: loss:0.6406144066362192 	 acc:0.6781456953642384 	 lr:0.0001
epoch8: train: loss:0.6273579140340001 	 acc:0.6016528925619835 | test: loss:0.6151464992801086 	 acc:0.6278145695364239 	 lr:0.0001
epoch9: train: loss:0.6209748867523571 	 acc:0.6039669421487603 | test: loss:0.6150351424880375 	 acc:0.6172185430463576 	 lr:0.0001
epoch10: train: loss:0.6263841938578393 	 acc:0.6621487603305786 | test: loss:0.6109213127205704 	 acc:0.6768211920529801 	 lr:0.0001
epoch11: train: loss:0.6192574238974201 	 acc:0.6532231404958677 | test: loss:0.6065992097980929 	 acc:0.6768211920529801 	 lr:0.0001
epoch12: train: loss:0.617729448365771 	 acc:0.6766942148760331 | test: loss:0.6031166059291916 	 acc:0.6874172185430464 	 lr:0.0001
epoch13: train: loss:0.6169115271844154 	 acc:0.5785123966942148 | test: loss:0.6070053758210694 	 acc:0.5920529801324503 	 lr:0.0001
epoch14: train: loss:0.6436982074453811 	 acc:0.6509090909090909 | test: loss:0.6362722872898279 	 acc:0.6635761589403973 	 lr:0.0001
epoch15: train: loss:0.6059516482116762 	 acc:0.6423140495867768 | test: loss:0.5893745782359546 	 acc:0.6874172185430464 	 lr:0.0001
epoch16: train: loss:0.610536393252286 	 acc:0.6727272727272727 | test: loss:0.590607894414308 	 acc:0.7311258278145696 	 lr:0.0001
epoch17: train: loss:0.602673502303352 	 acc:0.6727272727272727 | test: loss:0.5875229466040403 	 acc:0.6927152317880795 	 lr:0.0001
epoch18: train: loss:0.6013254410570318 	 acc:0.6862809917355371 | test: loss:0.5854937347355268 	 acc:0.7152317880794702 	 lr:0.0001
epoch19: train: loss:0.6225714907370323 	 acc:0.6720661157024793 | test: loss:0.615969445768571 	 acc:0.6874172185430464 	 lr:0.0001
epoch20: train: loss:0.6087352896524855 	 acc:0.7074380165289256 | test: loss:0.579023311232889 	 acc:0.7417218543046358 	 lr:0.0001
epoch21: train: loss:0.622332574670965 	 acc:0.6819834710743802 | test: loss:0.5951817993296693 	 acc:0.7298013245033113 	 lr:0.0001
epoch22: train: loss:0.6063243971383276 	 acc:0.7104132231404958 | test: loss:0.5908972811225234 	 acc:0.7364238410596027 	 lr:0.0001
epoch23: train: loss:0.6016806667304236 	 acc:0.6945454545454546 | test: loss:0.5882728944551077 	 acc:0.7284768211920529 	 lr:0.0001
epoch24: train: loss:0.5981434764152715 	 acc:0.6211570247933884 | test: loss:0.5889848990945626 	 acc:0.6264900662251656 	 lr:0.0001
epoch25: train: loss:0.6096262802565393 	 acc:0.6856198347107438 | test: loss:0.5947412536633725 	 acc:0.7258278145695364 	 lr:0.0001
epoch26: train: loss:0.5861351839569975 	 acc:0.6611570247933884 | test: loss:0.5714241157304373 	 acc:0.6927152317880795 	 lr:0.0001
epoch27: train: loss:0.6867778182226765 	 acc:0.6079338842975207 | test: loss:0.6944441651666401 	 acc:0.6105960264900663 	 lr:0.0001
epoch28: train: loss:0.5992380679737438 	 acc:0.708099173553719 | test: loss:0.5778774074371288 	 acc:0.7417218543046358 	 lr:0.0001
epoch29: train: loss:0.5849901433030436 	 acc:0.7041322314049587 | test: loss:0.5663006425693335 	 acc:0.7298013245033113 	 lr:0.0001
epoch30: train: loss:0.5819011295137326 	 acc:0.6628099173553719 | test: loss:0.572687005838811 	 acc:0.671523178807947 	 lr:0.0001
epoch31: train: loss:0.5852412241746572 	 acc:0.7183471074380166 | test: loss:0.5606548888793844 	 acc:0.7615894039735099 	 lr:0.0001
epoch32: train: loss:0.6223614738795383 	 acc:0.68 | test: loss:0.6253155057003956 	 acc:0.671523178807947 	 lr:0.0001
epoch33: train: loss:0.579649763343748 	 acc:0.6459504132231405 | test: loss:0.564678065982086 	 acc:0.6900662251655629 	 lr:0.0001
epoch34: train: loss:0.5942885411672356 	 acc:0.6191735537190083 | test: loss:0.5888831359661179 	 acc:0.6158940397350994 	 lr:0.0001
epoch35: train: loss:0.6020750134050353 	 acc:0.7024793388429752 | test: loss:0.5853640123708359 	 acc:0.7337748344370861 	 lr:0.0001
epoch36: train: loss:0.5764273994422155 	 acc:0.7143801652892562 | test: loss:0.5674978917008204 	 acc:0.7364238410596027 	 lr:0.0001
epoch37: train: loss:0.6269407027220923 	 acc:0.6770247933884298 | test: loss:0.6193253691622753 	 acc:0.7112582781456953 	 lr:0.0001
epoch38: train: loss:0.5698340877816697 	 acc:0.7451239669421488 | test: loss:0.5603083500009499 	 acc:0.7456953642384105 	 lr:5e-05
epoch39: train: loss:0.5736163991935982 	 acc:0.6674380165289256 | test: loss:0.5650447635461163 	 acc:0.6728476821192053 	 lr:5e-05
epoch40: train: loss:0.5850090471969163 	 acc:0.7236363636363636 | test: loss:0.5668223409463238 	 acc:0.7470198675496689 	 lr:5e-05
epoch41: train: loss:0.5723892798502583 	 acc:0.7414876033057851 | test: loss:0.5665098765827962 	 acc:0.743046357615894 	 lr:5e-05
epoch42: train: loss:0.5634297254262877 	 acc:0.7173553719008264 | test: loss:0.5498271621615681 	 acc:0.7337748344370861 	 lr:5e-05
epoch43: train: loss:0.5678128867701066 	 acc:0.736198347107438 | test: loss:0.5582735972688687 	 acc:0.7337748344370861 	 lr:5e-05
epoch44: train: loss:0.5726030385198672 	 acc:0.6700826446280992 | test: loss:0.5719266806217219 	 acc:0.6569536423841059 	 lr:5e-05
epoch45: train: loss:0.563129616591556 	 acc:0.7454545454545455 | test: loss:0.5532042762301616 	 acc:0.7483443708609272 	 lr:5e-05
epoch46: train: loss:0.5685706892092366 	 acc:0.7428099173553719 | test: loss:0.5569400742353983 	 acc:0.7470198675496689 	 lr:5e-05
epoch47: train: loss:0.5548392429430623 	 acc:0.7213223140495868 | test: loss:0.5502634667402861 	 acc:0.7192052980132451 	 lr:5e-05
epoch48: train: loss:0.5700406857758514 	 acc:0.7533884297520661 | test: loss:0.5566168132207252 	 acc:0.7536423841059603 	 lr:5e-05
epoch49: train: loss:0.5599081439026131 	 acc:0.7289256198347107 | test: loss:0.5471955766741013 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch50: train: loss:0.554927884054578 	 acc:0.7365289256198347 | test: loss:0.5461439485581505 	 acc:0.7417218543046358 	 lr:2.5e-05
epoch51: train: loss:0.5581517877263471 	 acc:0.7590082644628099 | test: loss:0.5484339491421024 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch52: train: loss:0.5566406637577971 	 acc:0.7388429752066116 | test: loss:0.5460507602881122 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch53: train: loss:0.5496840025373727 	 acc:0.7282644628099173 | test: loss:0.5437980399226511 	 acc:0.7350993377483444 	 lr:2.5e-05
epoch54: train: loss:0.5521023338885347 	 acc:0.7401652892561984 | test: loss:0.5453406723129828 	 acc:0.7337748344370861 	 lr:2.5e-05
epoch55: train: loss:0.5525874031082658 	 acc:0.7428099173553719 | test: loss:0.5466357341665306 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch56: train: loss:0.5636331862457528 	 acc:0.7467768595041322 | test: loss:0.5585055468887683 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch57: train: loss:0.560307546627423 	 acc:0.7580165289256199 | test: loss:0.5471878217545566 	 acc:0.752317880794702 	 lr:2.5e-05
epoch58: train: loss:0.5646106406676867 	 acc:0.7520661157024794 | test: loss:0.5513920290580648 	 acc:0.7562913907284768 	 lr:2.5e-05
epoch59: train: loss:0.5619354399964829 	 acc:0.7474380165289256 | test: loss:0.5480589601377778 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch60: train: loss:0.5550941101775682 	 acc:0.7570247933884298 | test: loss:0.5472896219089332 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch61: train: loss:0.5511478124571241 	 acc:0.7540495867768595 | test: loss:0.5435570198968547 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch62: train: loss:0.5524305707167003 	 acc:0.7616528925619834 | test: loss:0.5457009018651697 	 acc:0.752317880794702 	 lr:1.25e-05
epoch63: train: loss:0.5521308010471754 	 acc:0.7484297520661157 | test: loss:0.5446797516961761 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch64: train: loss:0.5539313916726546 	 acc:0.7408264462809917 | test: loss:0.5427175922899057 	 acc:0.743046357615894 	 lr:1.25e-05
epoch65: train: loss:0.5492130915192533 	 acc:0.7477685950413223 | test: loss:0.5428012093171378 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch66: train: loss:0.5485405534949185 	 acc:0.7557024793388429 | test: loss:0.5438330353490565 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch67: train: loss:0.5607625562100371 	 acc:0.7543801652892562 | test: loss:0.5534908619148052 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch68: train: loss:0.5559623786043529 	 acc:0.7557024793388429 | test: loss:0.5479287817778177 	 acc:0.752317880794702 	 lr:1.25e-05
epoch69: train: loss:0.5505521603458183 	 acc:0.7484297520661157 | test: loss:0.541593088456337 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch70: train: loss:0.5546310556821586 	 acc:0.7540495867768595 | test: loss:0.5470889800431713 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch71: train: loss:0.5495315874903656 	 acc:0.7543801652892562 | test: loss:0.5438197615920313 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch72: train: loss:0.549657781143819 	 acc:0.7391735537190083 | test: loss:0.541495530494791 	 acc:0.743046357615894 	 lr:1.25e-05
epoch73: train: loss:0.5536868924148812 	 acc:0.7609917355371901 | test: loss:0.545061288448359 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch74: train: loss:0.5466591234049522 	 acc:0.7461157024793389 | test: loss:0.5421500319676683 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch75: train: loss:0.5499888522959938 	 acc:0.7586776859504132 | test: loss:0.5435828114976946 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch76: train: loss:0.5595180372758345 	 acc:0.7679338842975206 | test: loss:0.5537475990933298 	 acc:0.766887417218543 	 lr:1.25e-05
epoch77: train: loss:0.548654725236341 	 acc:0.76 | test: loss:0.5412513177915914 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch78: train: loss:0.5491230243296663 	 acc:0.7540495867768595 | test: loss:0.5430526025247889 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch79: train: loss:0.5510941611833808 	 acc:0.7593388429752066 | test: loss:0.5414856693602556 	 acc:0.752317880794702 	 lr:1.25e-05
epoch80: train: loss:0.5465105708768545 	 acc:0.7662809917355372 | test: loss:0.5423917419863064 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch81: train: loss:0.5493776954698169 	 acc:0.7676033057851239 | test: loss:0.5484542567998368 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch82: train: loss:0.5475191860159566 	 acc:0.7352066115702479 | test: loss:0.5410700606984018 	 acc:0.7364238410596027 	 lr:1.25e-05
epoch83: train: loss:0.5482423013103895 	 acc:0.7728925619834711 | test: loss:0.5451402235504808 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch84: train: loss:0.5476475852580109 	 acc:0.7659504132231405 | test: loss:0.541191379597645 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch85: train: loss:0.551006659417113 	 acc:0.7163636363636363 | test: loss:0.550844711736338 	 acc:0.7072847682119205 	 lr:1.25e-05
epoch86: train: loss:0.5450747870807805 	 acc:0.7580165289256199 | test: loss:0.5396778295371706 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch87: train: loss:0.5518754755563973 	 acc:0.751404958677686 | test: loss:0.5419835812998134 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch88: train: loss:0.5516687135854044 	 acc:0.7669421487603306 | test: loss:0.5414288239763273 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch89: train: loss:0.5517489770424268 	 acc:0.7636363636363637 | test: loss:0.5440445378126687 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch90: train: loss:0.5469796974796894 	 acc:0.7504132231404959 | test: loss:0.5408264547783808 	 acc:0.7417218543046358 	 lr:1.25e-05
epoch91: train: loss:0.5447295984157846 	 acc:0.7566942148760331 | test: loss:0.5403440683882758 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch92: train: loss:0.5449253638322689 	 acc:0.7447933884297521 | test: loss:0.5394222110312507 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch93: train: loss:0.5470057894375698 	 acc:0.7332231404958678 | test: loss:0.5444207616989186 	 acc:0.7205298013245033 	 lr:1.25e-05
epoch94: train: loss:0.5460949249306987 	 acc:0.7735537190082644 | test: loss:0.5410401367193816 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch95: train: loss:0.5650275097405615 	 acc:0.7603305785123967 | test: loss:0.5535515346274471 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch96: train: loss:0.5476916815623765 	 acc:0.7609917355371901 | test: loss:0.5407248177275752 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch97: train: loss:0.5441039574638871 	 acc:0.7629752066115703 | test: loss:0.5391571039395616 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch98: train: loss:0.5543874711241604 	 acc:0.763305785123967 | test: loss:0.5484165659803428 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch99: train: loss:0.5426995636608974 	 acc:0.7471074380165289 | test: loss:0.5401677991380754 	 acc:0.7337748344370861 	 lr:1.25e-05
epoch100: train: loss:0.544035739031705 	 acc:0.7507438016528926 | test: loss:0.5381137338695147 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch101: train: loss:0.546559487571401 	 acc:0.7557024793388429 | test: loss:0.5408564177569964 	 acc:0.752317880794702 	 lr:1.25e-05
epoch102: train: loss:0.5415851839317763 	 acc:0.7451239669421488 | test: loss:0.5389212365971495 	 acc:0.7483443708609272 	 lr:1.25e-05
epoch103: train: loss:0.5470986092977287 	 acc:0.7441322314049587 | test: loss:0.5394532687616664 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch104: train: loss:0.5454563122347367 	 acc:0.7672727272727272 | test: loss:0.5383103733031166 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch105: train: loss:0.5507967886845928 	 acc:0.7619834710743801 | test: loss:0.5411581323636289 	 acc:0.766887417218543 	 lr:1.25e-05
epoch106: train: loss:0.5491394839405028 	 acc:0.7699173553719009 | test: loss:0.5437556770463653 	 acc:0.7695364238410596 	 lr:1.25e-05
epoch107: train: loss:0.5429490058481201 	 acc:0.7623140495867768 | test: loss:0.5373124468405515 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch108: train: loss:0.542445595500883 	 acc:0.7537190082644628 | test: loss:0.5375025662365339 	 acc:0.7350993377483444 	 lr:6.25e-06
epoch109: train: loss:0.5439153053543785 	 acc:0.7662809917355372 | test: loss:0.5384875042549032 	 acc:0.7496688741721854 	 lr:6.25e-06
epoch110: train: loss:0.5468429030071605 	 acc:0.7702479338842976 | test: loss:0.5391369680695186 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch111: train: loss:0.5413524913787842 	 acc:0.7676033057851239 | test: loss:0.5380577118980964 	 acc:0.7417218543046358 	 lr:6.25e-06
epoch112: train: loss:0.5432936856766377 	 acc:0.7781818181818182 | test: loss:0.5414306330365061 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch113: train: loss:0.5416048961631522 	 acc:0.7725619834710744 | test: loss:0.5414651400206105 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch114: train: loss:0.5437916382088149 	 acc:0.7520661157024794 | test: loss:0.5380708215252453 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch115: train: loss:0.5435880289195983 	 acc:0.7557024793388429 | test: loss:0.5376041293933692 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch116: train: loss:0.5381866924821838 	 acc:0.7755371900826447 | test: loss:0.5377784487427465 	 acc:0.743046357615894 	 lr:3.125e-06
epoch117: train: loss:0.5416359626754257 	 acc:0.7646280991735537 | test: loss:0.5378712919374176 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch118: train: loss:0.5413426220121462 	 acc:0.7646280991735537 | test: loss:0.5388705748595939 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch119: train: loss:0.5436816723090558 	 acc:0.7557024793388429 | test: loss:0.5377412357867158 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch120: train: loss:0.5407175474324503 	 acc:0.7646280991735537 | test: loss:0.5378524155806232 	 acc:0.752317880794702 	 lr:1.5625e-06
epoch121: train: loss:0.541653128773713 	 acc:0.7662809917355372 | test: loss:0.5384142581990223 	 acc:0.7456953642384105 	 lr:1.5625e-06
epoch122: train: loss:0.5420657980146487 	 acc:0.7748760330578512 | test: loss:0.5385475503687827 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch123: train: loss:0.5454343518343838 	 acc:0.7702479338842976 | test: loss:0.5380521054299462 	 acc:0.7483443708609272 	 lr:1.5625e-06
epoch124: train: loss:0.5466787239933802 	 acc:0.7547107438016529 | test: loss:0.5382490820442604 	 acc:0.7483443708609272 	 lr:1.5625e-06
epoch125: train: loss:0.5419986861993459 	 acc:0.7593388429752066 | test: loss:0.5369307082220418 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch126: train: loss:0.5436246438459916 	 acc:0.7695867768595042 | test: loss:0.5387890313634809 	 acc:0.7483443708609272 	 lr:1.5625e-06
epoch127: train: loss:0.5461767907773167 	 acc:0.7616528925619834 | test: loss:0.5387210583844722 	 acc:0.7483443708609272 	 lr:1.5625e-06
epoch128: train: loss:0.541695267878288 	 acc:0.7679338842975206 | test: loss:0.5376795143481122 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch129: train: loss:0.5425110185441891 	 acc:0.7725619834710744 | test: loss:0.5381683638553746 	 acc:0.7456953642384105 	 lr:1.5625e-06
epoch130: train: loss:0.545163431798131 	 acc:0.7540495867768595 | test: loss:0.537777883485453 	 acc:0.7470198675496689 	 lr:1.5625e-06
epoch131: train: loss:0.5417414742658946 	 acc:0.7619834710743801 | test: loss:0.5376437119300792 	 acc:0.7470198675496689 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_7_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_7_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.8180185455527187 	 acc:0.48231404958677687 | test: loss:0.8218531498056374 	 acc:0.4821192052980132 	 lr:0.0001
epoch1: train: loss:0.6605931108451086 	 acc:0.5213223140495867 | test: loss:0.6589255756889747 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.7013797026823375 	 acc:0.543801652892562 | test: loss:0.6922154883675228 	 acc:0.5456953642384106 	 lr:0.0001
epoch3: train: loss:0.6478593609352742 	 acc:0.5229752066115703 | test: loss:0.6415847242273243 	 acc:0.5258278145695364 	 lr:0.0001
epoch4: train: loss:0.6639621898556544 	 acc:0.6201652892561983 | test: loss:0.648996734540194 	 acc:0.6264900662251656 	 lr:0.0001
epoch5: train: loss:0.6407111709571082 	 acc:0.6089256198347107 | test: loss:0.6261812329292298 	 acc:0.6185430463576159 	 lr:0.0001
epoch6: train: loss:0.654632291517967 	 acc:0.6330578512396694 | test: loss:0.6323003836025466 	 acc:0.686092715231788 	 lr:0.0001
epoch7: train: loss:0.6517715788872774 	 acc:0.6423140495867768 | test: loss:0.636637167977971 	 acc:0.6847682119205298 	 lr:0.0001
epoch8: train: loss:0.6267758023837381 	 acc:0.5735537190082645 | test: loss:0.6175625505036866 	 acc:0.5814569536423841 	 lr:0.0001
epoch9: train: loss:0.6209362137613218 	 acc:0.6499173553719009 | test: loss:0.614010142964243 	 acc:0.6490066225165563 	 lr:0.0001
epoch10: train: loss:0.6162722289857786 	 acc:0.6373553719008265 | test: loss:0.6024369046388083 	 acc:0.6556291390728477 	 lr:0.0001
epoch11: train: loss:0.610018949607187 	 acc:0.6128925619834711 | test: loss:0.6001133605344406 	 acc:0.6211920529801325 	 lr:0.0001
epoch12: train: loss:0.6100046590537079 	 acc:0.6535537190082644 | test: loss:0.595351074310328 	 acc:0.6754966887417219 	 lr:0.0001
epoch13: train: loss:0.6023972338290254 	 acc:0.6208264462809917 | test: loss:0.5903284567870841 	 acc:0.6344370860927152 	 lr:0.0001
epoch14: train: loss:0.5964901834085953 	 acc:0.6608264462809917 | test: loss:0.579042107932615 	 acc:0.6980132450331126 	 lr:0.0001
epoch15: train: loss:0.5971241345287355 	 acc:0.6654545454545454 | test: loss:0.5774661454143903 	 acc:0.6966887417218544 	 lr:0.0001
epoch16: train: loss:0.600714191227905 	 acc:0.6948760330578513 | test: loss:0.5812169062380759 	 acc:0.7390728476821192 	 lr:0.0001
epoch17: train: loss:0.5937655089315305 	 acc:0.6380165289256199 | test: loss:0.5858100374802848 	 acc:0.6384105960264901 	 lr:0.0001
epoch18: train: loss:0.597209970616112 	 acc:0.6300826446280992 | test: loss:0.5839707897988377 	 acc:0.6556291390728477 	 lr:0.0001
epoch19: train: loss:0.5902193209750594 	 acc:0.6482644628099173 | test: loss:0.5688830808298477 	 acc:0.695364238410596 	 lr:0.0001
epoch20: train: loss:0.5796879447590221 	 acc:0.6935537190082645 | test: loss:0.5620019489566221 	 acc:0.7033112582781457 	 lr:0.0001
epoch21: train: loss:0.5809437015036906 	 acc:0.7061157024793389 | test: loss:0.5627116079362023 	 acc:0.7231788079470198 	 lr:0.0001
epoch22: train: loss:0.599354407826731 	 acc:0.7170247933884297 | test: loss:0.5730077149852222 	 acc:0.7496688741721854 	 lr:0.0001
epoch23: train: loss:0.6337396196491463 	 acc:0.6548760330578512 | test: loss:0.6292071588781496 	 acc:0.6794701986754967 	 lr:0.0001
epoch24: train: loss:0.5886446332537438 	 acc:0.6608264462809917 | test: loss:0.5727649790561752 	 acc:0.6728476821192053 	 lr:0.0001
epoch25: train: loss:0.6097293027570425 	 acc:0.7014876033057851 | test: loss:0.590599226793706 	 acc:0.7178807947019867 	 lr:0.0001
epoch26: train: loss:0.5757101058368841 	 acc:0.7193388429752066 | test: loss:0.557106293510917 	 acc:0.7403973509933774 	 lr:0.0001
epoch27: train: loss:0.5768882864959969 	 acc:0.7236363636363636 | test: loss:0.5579864833528633 	 acc:0.7337748344370861 	 lr:0.0001
epoch28: train: loss:0.5804808146894471 	 acc:0.7279338842975207 | test: loss:0.5621246176050199 	 acc:0.7443708609271523 	 lr:0.0001
epoch29: train: loss:0.5800669743600956 	 acc:0.7279338842975207 | test: loss:0.5575565312082404 	 acc:0.7509933774834437 	 lr:0.0001
epoch30: train: loss:0.5873136844910866 	 acc:0.6370247933884298 | test: loss:0.5880667148836402 	 acc:0.6185430463576159 	 lr:0.0001
epoch31: train: loss:0.5718616604607952 	 acc:0.7107438016528925 | test: loss:0.5452601059383114 	 acc:0.7483443708609272 	 lr:0.0001
epoch32: train: loss:0.6180485413685318 	 acc:0.687603305785124 | test: loss:0.6260789504114366 	 acc:0.6794701986754967 	 lr:0.0001
epoch33: train: loss:0.5679356267235496 	 acc:0.7173553719008264 | test: loss:0.5526041935611244 	 acc:0.7470198675496689 	 lr:0.0001
epoch34: train: loss:0.5769696911898526 	 acc:0.6661157024793388 | test: loss:0.5648773134149463 	 acc:0.680794701986755 	 lr:0.0001
epoch35: train: loss:0.5863983738717954 	 acc:0.7223140495867768 | test: loss:0.5549406312159355 	 acc:0.7549668874172185 	 lr:0.0001
epoch36: train: loss:0.5790212444431526 	 acc:0.6879338842975207 | test: loss:0.5662960905902433 	 acc:0.713907284768212 	 lr:0.0001
epoch37: train: loss:0.5850561475359704 	 acc:0.7180165289256198 | test: loss:0.5803671762643271 	 acc:0.7271523178807947 	 lr:0.0001
epoch38: train: loss:0.5585646544606232 	 acc:0.7510743801652893 | test: loss:0.5504137979437973 	 acc:0.7456953642384105 	 lr:5e-05
epoch39: train: loss:0.5570144539234066 	 acc:0.7338842975206612 | test: loss:0.5442524692870134 	 acc:0.7403973509933774 	 lr:5e-05
epoch40: train: loss:0.5603863627260381 	 acc:0.7487603305785124 | test: loss:0.5443307831587381 	 acc:0.7403973509933774 	 lr:5e-05
epoch41: train: loss:0.5584724392181586 	 acc:0.7613223140495867 | test: loss:0.5489667828509349 	 acc:0.7576158940397351 	 lr:5e-05
epoch42: train: loss:0.5544516871389279 	 acc:0.715702479338843 | test: loss:0.5437652139474225 	 acc:0.7258278145695364 	 lr:5e-05
epoch43: train: loss:0.5656735219049059 	 acc:0.7520661157024794 | test: loss:0.5533207972318132 	 acc:0.7721854304635761 	 lr:5e-05
epoch44: train: loss:0.5610330023056219 	 acc:0.6902479338842975 | test: loss:0.5634788352132633 	 acc:0.6728476821192053 	 lr:5e-05
epoch45: train: loss:0.5667649475996159 	 acc:0.7467768595041322 | test: loss:0.5544971601852519 	 acc:0.7589403973509934 	 lr:5e-05
epoch46: train: loss:0.5519887866855653 	 acc:0.7603305785123967 | test: loss:0.5425907967106396 	 acc:0.7483443708609272 	 lr:5e-05
epoch47: train: loss:0.5466270505889388 	 acc:0.7434710743801652 | test: loss:0.5386804894106277 	 acc:0.7350993377483444 	 lr:5e-05
epoch48: train: loss:0.5743756858770512 	 acc:0.7451239669421488 | test: loss:0.5594572263048184 	 acc:0.7470198675496689 	 lr:5e-05
epoch49: train: loss:0.5514507733691822 	 acc:0.7203305785123967 | test: loss:0.5432923556163611 	 acc:0.7258278145695364 	 lr:5e-05
epoch50: train: loss:0.5480323373975833 	 acc:0.7547107438016529 | test: loss:0.5415767372049243 	 acc:0.7298013245033113 	 lr:5e-05
epoch51: train: loss:0.5437080900334129 	 acc:0.7474380165289256 | test: loss:0.5370068715897617 	 acc:0.7403973509933774 	 lr:5e-05
epoch52: train: loss:0.5431212340110589 	 acc:0.7507438016528926 | test: loss:0.5354733238946523 	 acc:0.7456953642384105 	 lr:5e-05
epoch53: train: loss:0.5438606245458618 	 acc:0.7613223140495867 | test: loss:0.538109800594532 	 acc:0.7589403973509934 	 lr:5e-05
epoch54: train: loss:0.54355424289861 	 acc:0.7563636363636363 | test: loss:0.534409567851894 	 acc:0.7589403973509934 	 lr:5e-05
epoch55: train: loss:0.5481260854153593 	 acc:0.7709090909090909 | test: loss:0.5477493550603753 	 acc:0.7629139072847683 	 lr:5e-05
epoch56: train: loss:0.5458993096193991 	 acc:0.7553719008264462 | test: loss:0.540671878934696 	 acc:0.7589403973509934 	 lr:5e-05
epoch57: train: loss:0.5728527777057049 	 acc:0.7590082644628099 | test: loss:0.5606134151780842 	 acc:0.7562913907284768 	 lr:5e-05
epoch58: train: loss:0.5485992618040605 	 acc:0.715702479338843 | test: loss:0.5465910084200222 	 acc:0.704635761589404 	 lr:5e-05
epoch59: train: loss:0.5443784549610674 	 acc:0.7563636363636363 | test: loss:0.5334353869324489 	 acc:0.7549668874172185 	 lr:5e-05
epoch60: train: loss:0.6012670189290007 	 acc:0.7127272727272728 | test: loss:0.5851791591833759 	 acc:0.7271523178807947 	 lr:5e-05
epoch61: train: loss:0.5374101622833694 	 acc:0.7365289256198347 | test: loss:0.5347594328274 	 acc:0.7496688741721854 	 lr:5e-05
epoch62: train: loss:0.5432947812986768 	 acc:0.7385123966942149 | test: loss:0.5412062807588388 	 acc:0.7178807947019867 	 lr:5e-05
epoch63: train: loss:0.5409559552137517 	 acc:0.7451239669421488 | test: loss:0.5374023506972964 	 acc:0.7377483443708609 	 lr:5e-05
epoch64: train: loss:0.5437985784948365 	 acc:0.7606611570247934 | test: loss:0.5393839960856154 	 acc:0.7562913907284768 	 lr:5e-05
epoch65: train: loss:0.5635146802318983 	 acc:0.7563636363636363 | test: loss:0.5656529548152394 	 acc:0.7417218543046358 	 lr:5e-05
epoch66: train: loss:0.536283838906564 	 acc:0.7355371900826446 | test: loss:0.5373136706699598 	 acc:0.7337748344370861 	 lr:2.5e-05
epoch67: train: loss:0.534899047780628 	 acc:0.7758677685950414 | test: loss:0.5371742238272105 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch68: train: loss:0.5322342440116504 	 acc:0.7676033057851239 | test: loss:0.5300665249887682 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch69: train: loss:0.5369206034250495 	 acc:0.7702479338842976 | test: loss:0.5356661020525244 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch70: train: loss:0.5490784341441698 	 acc:0.7748760330578512 | test: loss:0.547612266982628 	 acc:0.766887417218543 	 lr:2.5e-05
epoch71: train: loss:0.5454007280759575 	 acc:0.7828099173553719 | test: loss:0.5452520396535759 	 acc:0.752317880794702 	 lr:2.5e-05
epoch72: train: loss:0.5311496537184912 	 acc:0.7785123966942149 | test: loss:0.5326674515837865 	 acc:0.7576158940397351 	 lr:2.5e-05
epoch73: train: loss:0.5405986911797327 	 acc:0.7795041322314049 | test: loss:0.5385702026600869 	 acc:0.7695364238410596 	 lr:2.5e-05
epoch74: train: loss:0.5301028610458058 	 acc:0.7563636363636363 | test: loss:0.5302893552559101 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch75: train: loss:0.5333517806983191 	 acc:0.7791735537190083 | test: loss:0.5347949024857275 	 acc:0.7682119205298014 	 lr:1.25e-05
epoch76: train: loss:0.5449283996692373 	 acc:0.7828099173553719 | test: loss:0.5455806965859521 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch77: train: loss:0.528717626697761 	 acc:0.7880991735537191 | test: loss:0.531304287120996 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch78: train: loss:0.5299979914121391 	 acc:0.7709090909090909 | test: loss:0.5309185881488371 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch79: train: loss:0.5322922691530433 	 acc:0.7715702479338843 | test: loss:0.5299809438503341 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch80: train: loss:0.527054846641446 	 acc:0.7910743801652893 | test: loss:0.5327185743691906 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch81: train: loss:0.5299730209082611 	 acc:0.7880991735537191 | test: loss:0.5366217723745383 	 acc:0.7682119205298014 	 lr:1.25e-05
epoch82: train: loss:0.5286386449869014 	 acc:0.7570247933884298 | test: loss:0.5320225272747066 	 acc:0.7377483443708609 	 lr:1.25e-05
epoch83: train: loss:0.5278584178420138 	 acc:0.7980165289256198 | test: loss:0.5340235511198739 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch84: train: loss:0.52928244226235 	 acc:0.771900826446281 | test: loss:0.529843166727104 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch85: train: loss:0.531841913294201 	 acc:0.7381818181818182 | test: loss:0.5377146139839627 	 acc:0.7218543046357616 	 lr:1.25e-05
epoch86: train: loss:0.5382656258393911 	 acc:0.7834710743801653 | test: loss:0.5371699642661392 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch87: train: loss:0.5291893947025961 	 acc:0.7705785123966942 | test: loss:0.5288034236194282 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch88: train: loss:0.5263794243631285 	 acc:0.7811570247933884 | test: loss:0.528279588396186 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch89: train: loss:0.5300519002173558 	 acc:0.7798347107438016 | test: loss:0.5312866209358569 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch90: train: loss:0.5277100276947021 	 acc:0.7596694214876033 | test: loss:0.5307675876364802 	 acc:0.7417218543046358 	 lr:1.25e-05
epoch91: train: loss:0.5242386503850133 	 acc:0.7824793388429752 | test: loss:0.528723816919011 	 acc:0.766887417218543 	 lr:1.25e-05
epoch92: train: loss:0.5313348020009758 	 acc:0.7874380165289256 | test: loss:0.5327584327451441 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch93: train: loss:0.5263108776029476 	 acc:0.7857851239669421 | test: loss:0.5289745250285066 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch94: train: loss:0.5294959731732518 	 acc:0.8003305785123966 | test: loss:0.5330042799577018 	 acc:0.7695364238410596 	 lr:1.25e-05
epoch95: train: loss:0.5271064031025595 	 acc:0.7854545454545454 | test: loss:0.5266022774557404 	 acc:0.766887417218543 	 lr:6.25e-06
epoch96: train: loss:0.5228578499328992 	 acc:0.7765289256198347 | test: loss:0.5270760750138996 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch97: train: loss:0.5242651523834417 	 acc:0.7976859504132231 | test: loss:0.5290613694696237 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch98: train: loss:0.5248198087353352 	 acc:0.7798347107438016 | test: loss:0.5273190724139182 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch99: train: loss:0.5243855002497838 	 acc:0.780495867768595 | test: loss:0.5277205539065482 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch100: train: loss:0.5214577608955793 	 acc:0.7861157024793388 | test: loss:0.5262240645111791 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch101: train: loss:0.5289322400684199 	 acc:0.7847933884297521 | test: loss:0.5329196229675748 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch102: train: loss:0.5209890121861923 	 acc:0.7808264462809917 | test: loss:0.5277559131975995 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch103: train: loss:0.5252608400731047 	 acc:0.7828099173553719 | test: loss:0.5273137289956705 	 acc:0.7589403973509934 	 lr:6.25e-06
epoch104: train: loss:0.5245900104656692 	 acc:0.7894214876033058 | test: loss:0.5271913885280786 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch105: train: loss:0.5245280898898101 	 acc:0.7818181818181819 | test: loss:0.526266987986912 	 acc:0.7602649006622516 	 lr:6.25e-06
epoch106: train: loss:0.5228722245831134 	 acc:0.7907438016528926 | test: loss:0.5281818494891489 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch107: train: loss:0.5241519506706679 	 acc:0.7818181818181819 | test: loss:0.5255143444269699 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch108: train: loss:0.5233170399981096 	 acc:0.7857851239669421 | test: loss:0.5253856546831447 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch109: train: loss:0.525649917303038 	 acc:0.7861157024793388 | test: loss:0.5273537464489211 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch110: train: loss:0.5220741295420434 	 acc:0.7877685950413224 | test: loss:0.5265894895357801 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch111: train: loss:0.5254225036132434 	 acc:0.7900826446280992 | test: loss:0.5275936947753098 	 acc:0.7629139072847683 	 lr:3.125e-06
epoch112: train: loss:0.520185117012213 	 acc:0.7874380165289256 | test: loss:0.5269775818515298 	 acc:0.7615894039735099 	 lr:3.125e-06
epoch113: train: loss:0.5213241974578416 	 acc:0.7920661157024793 | test: loss:0.5288532141818116 	 acc:0.7602649006622516 	 lr:3.125e-06
epoch114: train: loss:0.5228804194631655 	 acc:0.7818181818181819 | test: loss:0.5263163249224228 	 acc:0.7629139072847683 	 lr:3.125e-06
epoch115: train: loss:0.5238170189108731 	 acc:0.7831404958677686 | test: loss:0.5264424769294183 	 acc:0.7602649006622516 	 lr:1.5625e-06
epoch116: train: loss:0.5172281165950554 	 acc:0.8006611570247933 | test: loss:0.5271599653540857 	 acc:0.7642384105960265 	 lr:1.5625e-06
epoch117: train: loss:0.5217886365937793 	 acc:0.7966942148760331 | test: loss:0.5268534168502352 	 acc:0.7629139072847683 	 lr:1.5625e-06
epoch118: train: loss:0.5225477868860419 	 acc:0.7867768595041322 | test: loss:0.5268371093352109 	 acc:0.7602649006622516 	 lr:1.5625e-06
epoch119: train: loss:0.5237420424548063 	 acc:0.7867768595041322 | test: loss:0.5262707383427399 	 acc:0.7629139072847683 	 lr:1.5625e-06
epoch120: train: loss:0.5213413199511441 	 acc:0.7811570247933884 | test: loss:0.5264390972276397 	 acc:0.7615894039735099 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_8_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_8_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.8222007052563439 	 acc:0.48231404958677687 | test: loss:0.8260227543628769 	 acc:0.48079470198675495 	 lr:0.0001
epoch1: train: loss:0.6579304652371682 	 acc:0.5213223140495867 | test: loss:0.652442619816357 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6850849660763071 	 acc:0.5732231404958678 | test: loss:0.6732549266309927 	 acc:0.5867549668874172 	 lr:0.0001
epoch3: train: loss:0.6481521289407715 	 acc:0.5236363636363637 | test: loss:0.639583017099772 	 acc:0.528476821192053 	 lr:0.0001
epoch4: train: loss:0.6685029296047431 	 acc:0.6138842975206612 | test: loss:0.6522994689594042 	 acc:0.6410596026490066 	 lr:0.0001
epoch5: train: loss:0.6384543388146015 	 acc:0.5907438016528925 | test: loss:0.6246009576399595 	 acc:0.6105960264900663 	 lr:0.0001
epoch6: train: loss:0.6578770959278769 	 acc:0.6343801652892562 | test: loss:0.6331421888427229 	 acc:0.695364238410596 	 lr:0.0001
epoch7: train: loss:0.6491836384308239 	 acc:0.6347107438016529 | test: loss:0.6310257615632568 	 acc:0.6887417218543046 	 lr:0.0001
epoch8: train: loss:0.6282576090639288 	 acc:0.5613223140495868 | test: loss:0.6177248998983017 	 acc:0.5655629139072847 	 lr:0.0001
epoch9: train: loss:0.6205633895850379 	 acc:0.6492561983471075 | test: loss:0.6098562314810342 	 acc:0.6675496688741722 	 lr:0.0001
epoch10: train: loss:0.6153984289721024 	 acc:0.6393388429752066 | test: loss:0.5986893705184886 	 acc:0.6556291390728477 	 lr:0.0001
epoch11: train: loss:0.6141689796487162 	 acc:0.5831404958677686 | test: loss:0.6068404266376369 	 acc:0.5788079470198676 	 lr:0.0001
epoch12: train: loss:0.6188615986729457 	 acc:0.6509090909090909 | test: loss:0.5973877013124378 	 acc:0.6927152317880795 	 lr:0.0001
epoch13: train: loss:0.6021167212478385 	 acc:0.623801652892562 | test: loss:0.5911305227027034 	 acc:0.6344370860927152 	 lr:0.0001
epoch14: train: loss:0.6045024767197853 	 acc:0.6707438016528926 | test: loss:0.5853670956283216 	 acc:0.7231788079470198 	 lr:0.0001
epoch15: train: loss:0.5966320856346572 	 acc:0.6611570247933884 | test: loss:0.5739802481322889 	 acc:0.6980132450331126 	 lr:0.0001
epoch16: train: loss:0.5940264297713919 	 acc:0.6568595041322314 | test: loss:0.5763661203005456 	 acc:0.6754966887417219 	 lr:0.0001
epoch17: train: loss:0.5988016589613986 	 acc:0.6214876033057851 | test: loss:0.582022205725411 	 acc:0.6529801324503312 	 lr:0.0001
epoch18: train: loss:0.5949528030324573 	 acc:0.6201652892561983 | test: loss:0.584942575795761 	 acc:0.6344370860927152 	 lr:0.0001
epoch19: train: loss:0.5855607262721731 	 acc:0.6585123966942149 | test: loss:0.568340717561987 	 acc:0.6701986754966888 	 lr:0.0001
epoch20: train: loss:0.5747581058494315 	 acc:0.6945454545454546 | test: loss:0.5573182142333479 	 acc:0.7072847682119205 	 lr:0.0001
epoch21: train: loss:0.5805403744484767 	 acc:0.68 | test: loss:0.5592896641484949 	 acc:0.7086092715231788 	 lr:0.0001
epoch22: train: loss:0.5796353049120627 	 acc:0.7229752066115702 | test: loss:0.5596833232222803 	 acc:0.7417218543046358 	 lr:0.0001
epoch23: train: loss:0.6277042268524485 	 acc:0.6697520661157025 | test: loss:0.6145754035734973 	 acc:0.6900662251655629 	 lr:0.0001
epoch24: train: loss:0.5816567457017819 	 acc:0.6585123966942149 | test: loss:0.5722140215880034 	 acc:0.6635761589403973 	 lr:0.0001
epoch25: train: loss:0.5775585301257362 	 acc:0.7345454545454545 | test: loss:0.5532472084689614 	 acc:0.7562913907284768 	 lr:0.0001
epoch26: train: loss:0.5735525006302132 	 acc:0.716694214876033 | test: loss:0.5523358120034073 	 acc:0.752317880794702 	 lr:0.0001
epoch27: train: loss:0.5890992458989798 	 acc:0.7018181818181818 | test: loss:0.5761519946799373 	 acc:0.7218543046357616 	 lr:0.0001
epoch28: train: loss:0.593695109540766 	 acc:0.7170247933884297 | test: loss:0.5814458003107286 	 acc:0.7245033112582782 	 lr:0.0001
epoch29: train: loss:0.5729129858253416 	 acc:0.7328925619834711 | test: loss:0.5509375655098466 	 acc:0.7536423841059603 	 lr:0.0001
epoch30: train: loss:0.5847859588536349 	 acc:0.6383471074380165 | test: loss:0.5883477214156397 	 acc:0.6198675496688741 	 lr:0.0001
epoch31: train: loss:0.5684590955608148 	 acc:0.7375206611570247 | test: loss:0.5440478615413439 	 acc:0.776158940397351 	 lr:0.0001
epoch32: train: loss:0.6112933867627924 	 acc:0.696198347107438 | test: loss:0.6222977275879967 	 acc:0.6980132450331126 	 lr:0.0001
epoch33: train: loss:0.564969359922015 	 acc:0.7322314049586777 | test: loss:0.5512883192656056 	 acc:0.7456953642384105 	 lr:0.0001
epoch34: train: loss:0.558690679388598 	 acc:0.7084297520661157 | test: loss:0.5389410712071602 	 acc:0.743046357615894 	 lr:0.0001
epoch35: train: loss:0.5728621931115457 	 acc:0.7414876033057851 | test: loss:0.550471157980281 	 acc:0.7615894039735099 	 lr:0.0001
epoch36: train: loss:0.5608905092940842 	 acc:0.6922314049586776 | test: loss:0.5527202974092092 	 acc:0.6993377483443709 	 lr:0.0001
epoch37: train: loss:0.5705316735890286 	 acc:0.7388429752066116 | test: loss:0.5599815303916174 	 acc:0.7456953642384105 	 lr:0.0001
epoch38: train: loss:0.6398595913973721 	 acc:0.6690909090909091 | test: loss:0.654532277741969 	 acc:0.6529801324503312 	 lr:0.0001
epoch39: train: loss:0.5503556883827714 	 acc:0.7295867768595041 | test: loss:0.5407349644117797 	 acc:0.7205298013245033 	 lr:0.0001
epoch40: train: loss:0.5606986859810253 	 acc:0.6733884297520661 | test: loss:0.5567669032425281 	 acc:0.6754966887417219 	 lr:0.0001
epoch41: train: loss:0.5736070601999267 	 acc:0.7388429752066116 | test: loss:0.5637072308963498 	 acc:0.7589403973509934 	 lr:5e-05
epoch42: train: loss:0.5399714017111408 	 acc:0.7461157024793389 | test: loss:0.5313475184093248 	 acc:0.7417218543046358 	 lr:5e-05
epoch43: train: loss:0.5538117324616298 	 acc:0.7682644628099173 | test: loss:0.5428508901438176 	 acc:0.7655629139072848 	 lr:5e-05
epoch44: train: loss:0.5644501609250534 	 acc:0.6631404958677686 | test: loss:0.5647016970527093 	 acc:0.6596026490066225 	 lr:5e-05
epoch45: train: loss:0.553363236829269 	 acc:0.7732231404958678 | test: loss:0.5407087636309744 	 acc:0.7615894039735099 	 lr:5e-05
epoch46: train: loss:0.5441869569218848 	 acc:0.7692561983471075 | test: loss:0.5360204335869543 	 acc:0.7549668874172185 	 lr:5e-05
epoch47: train: loss:0.5377638840084233 	 acc:0.7636363636363637 | test: loss:0.5290472831947125 	 acc:0.7456953642384105 	 lr:5e-05
epoch48: train: loss:0.5719617785304046 	 acc:0.748099173553719 | test: loss:0.5617689544001951 	 acc:0.7589403973509934 	 lr:5e-05
epoch49: train: loss:0.5526262785974613 	 acc:0.6938842975206612 | test: loss:0.5456099714664434 	 acc:0.7112582781456953 	 lr:5e-05
epoch50: train: loss:0.5405513030044303 	 acc:0.7662809917355372 | test: loss:0.5373812505740994 	 acc:0.7443708609271523 	 lr:5e-05
epoch51: train: loss:0.5369816040204576 	 acc:0.7464462809917355 | test: loss:0.532091423375717 	 acc:0.7443708609271523 	 lr:5e-05
epoch52: train: loss:0.5360221249406988 	 acc:0.7540495867768595 | test: loss:0.5307001987040437 	 acc:0.7390728476821192 	 lr:5e-05
epoch53: train: loss:0.5463075339498599 	 acc:0.775206611570248 | test: loss:0.538695364045781 	 acc:0.776158940397351 	 lr:5e-05
epoch54: train: loss:0.5328438422896645 	 acc:0.7745454545454545 | test: loss:0.5265006050368808 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch55: train: loss:0.5347432105403301 	 acc:0.7851239669421488 | test: loss:0.5365714358178195 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch56: train: loss:0.5317899397779102 	 acc:0.7768595041322314 | test: loss:0.5274169670824973 	 acc:0.7708609271523179 	 lr:2.5e-05
epoch57: train: loss:0.5374567976864901 	 acc:0.7880991735537191 | test: loss:0.5288456811020706 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch58: train: loss:0.530912610794887 	 acc:0.7745454545454545 | test: loss:0.526774261092508 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch59: train: loss:0.5389502373214595 	 acc:0.7722314049586777 | test: loss:0.5296867196133594 	 acc:0.7708609271523179 	 lr:2.5e-05
epoch60: train: loss:0.5354433179098712 	 acc:0.7904132231404959 | test: loss:0.5325965132934368 	 acc:0.766887417218543 	 lr:2.5e-05
epoch61: train: loss:0.5253669944676486 	 acc:0.7748760330578512 | test: loss:0.52354723767729 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch62: train: loss:0.5258377455482798 	 acc:0.7771900826446281 | test: loss:0.5252567067841031 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch63: train: loss:0.5263374026550734 	 acc:0.7725619834710744 | test: loss:0.5256330453796892 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch64: train: loss:0.5299666902447535 	 acc:0.7844628099173554 | test: loss:0.5241008303812797 	 acc:0.766887417218543 	 lr:1.25e-05
epoch65: train: loss:0.5278363311192221 	 acc:0.7877685950413224 | test: loss:0.527639476274023 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch66: train: loss:0.5254003002820922 	 acc:0.792396694214876 | test: loss:0.5276581112122694 	 acc:0.7748344370860927 	 lr:1.25e-05
epoch67: train: loss:0.5324013363625393 	 acc:0.7847933884297521 | test: loss:0.5288665267015924 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch68: train: loss:0.524666363306282 	 acc:0.7914049586776859 | test: loss:0.5246404990455172 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch69: train: loss:0.5262753423974533 	 acc:0.7748760330578512 | test: loss:0.5227534369917105 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch70: train: loss:0.5253493096611717 	 acc:0.7864462809917355 | test: loss:0.5230565115316025 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch71: train: loss:0.5251140417146288 	 acc:0.7910743801652893 | test: loss:0.525467540965175 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch72: train: loss:0.5234480794599233 	 acc:0.7986776859504132 | test: loss:0.5249026536941528 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch73: train: loss:0.5241826514370185 	 acc:0.7904132231404959 | test: loss:0.5235364728416039 	 acc:0.766887417218543 	 lr:6.25e-06
epoch74: train: loss:0.5238724001577078 	 acc:0.7775206611570248 | test: loss:0.5230549524161989 	 acc:0.7642384105960265 	 lr:6.25e-06
epoch75: train: loss:0.5234729492565817 	 acc:0.7927272727272727 | test: loss:0.5248894476732671 	 acc:0.7774834437086092 	 lr:6.25e-06
epoch76: train: loss:0.5243092180874722 	 acc:0.7904132231404959 | test: loss:0.5250542789895013 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch77: train: loss:0.5254784082775273 	 acc:0.7960330578512397 | test: loss:0.52492550816757 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch78: train: loss:0.5248931793240477 	 acc:0.7844628099173554 | test: loss:0.5232111535324956 	 acc:0.766887417218543 	 lr:3.125e-06
epoch79: train: loss:0.5250025995211168 	 acc:0.7880991735537191 | test: loss:0.5225759180965802 	 acc:0.7708609271523179 	 lr:3.125e-06
epoch80: train: loss:0.5206532270455163 	 acc:0.7910743801652893 | test: loss:0.5231417130160806 	 acc:0.7682119205298014 	 lr:3.125e-06
epoch81: train: loss:0.5195866864180761 	 acc:0.7983471074380165 | test: loss:0.5235837172988235 	 acc:0.7721854304635761 	 lr:3.125e-06
epoch82: train: loss:0.5245166591573353 	 acc:0.7907438016528926 | test: loss:0.523721239977325 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch83: train: loss:0.5180027543217682 	 acc:0.7933884297520661 | test: loss:0.5230668727925282 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch84: train: loss:0.522391700823445 	 acc:0.7867768595041322 | test: loss:0.5235307879005836 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch85: train: loss:0.522695952821369 	 acc:0.7824793388429752 | test: loss:0.5222946692776206 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch86: train: loss:0.5227788178782817 	 acc:0.780495867768595 | test: loss:0.5219191227527644 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch87: train: loss:0.5283623161000653 	 acc:0.7851239669421488 | test: loss:0.5231648111974956 	 acc:0.7748344370860927 	 lr:3.125e-06
epoch88: train: loss:0.5255065036214087 	 acc:0.7890909090909091 | test: loss:0.5229960720270674 	 acc:0.7695364238410596 	 lr:3.125e-06
epoch89: train: loss:0.5263708092161447 	 acc:0.783801652892562 | test: loss:0.5250806147689061 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch90: train: loss:0.5249344924264703 	 acc:0.780495867768595 | test: loss:0.5225087489513371 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch91: train: loss:0.5226370909391356 	 acc:0.7871074380165289 | test: loss:0.5228726811756361 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch92: train: loss:0.5233978607437827 	 acc:0.7900826446280992 | test: loss:0.5231539753888617 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch93: train: loss:0.5238945747604055 	 acc:0.7867768595041322 | test: loss:0.5221779982775252 	 acc:0.7682119205298014 	 lr:1.5625e-06
epoch94: train: loss:0.5176320665532892 	 acc:0.7917355371900826 | test: loss:0.522376022433603 	 acc:0.7642384105960265 	 lr:1.5625e-06
epoch95: train: loss:0.5255503581181046 	 acc:0.7821487603305786 | test: loss:0.5228891896096286 	 acc:0.7642384105960265 	 lr:1.5625e-06
epoch96: train: loss:0.5215677905476783 	 acc:0.7943801652892561 | test: loss:0.5234607736795943 	 acc:0.7708609271523179 	 lr:1.5625e-06
epoch97: train: loss:0.5210338877252311 	 acc:0.7947107438016529 | test: loss:0.5231506216604977 	 acc:0.7655629139072848 	 lr:1.5625e-06
epoch98: train: loss:0.5246780864857445 	 acc:0.7857851239669421 | test: loss:0.5236412795963666 	 acc:0.7695364238410596 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_9_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_9_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.693814679906388 	 acc:0.5566942148760331 | test: loss:0.6860686542972034 	 acc:0.543046357615894 	 lr:0.0001
epoch1: train: loss:0.6520232777556112 	 acc:0.5226446280991736 | test: loss:0.6527321759438672 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6369627171114457 	 acc:0.5940495867768595 | test: loss:0.633785686903442 	 acc:0.5986754966887418 	 lr:0.0001
epoch3: train: loss:0.6348974092538692 	 acc:0.6426446280991736 | test: loss:0.6300556139440726 	 acc:0.6450331125827815 	 lr:0.0001
epoch4: train: loss:0.6222026536287355 	 acc:0.6753719008264463 | test: loss:0.6150406205890984 	 acc:0.7059602649006622 	 lr:0.0001
epoch5: train: loss:0.6128352041480954 	 acc:0.6849586776859504 | test: loss:0.6019162261722893 	 acc:0.7033112582781457 	 lr:0.0001
epoch6: train: loss:0.6008199065578871 	 acc:0.6373553719008265 | test: loss:0.594834626036764 	 acc:0.6198675496688741 	 lr:0.0001
epoch7: train: loss:0.6229199898932591 	 acc:0.6915702479338843 | test: loss:0.6178917579303514 	 acc:0.7006622516556291 	 lr:0.0001
epoch8: train: loss:0.6122197284580262 	 acc:0.7057851239669422 | test: loss:0.6008200103873449 	 acc:0.7337748344370861 	 lr:0.0001
epoch9: train: loss:0.5800657711147277 	 acc:0.7203305785123967 | test: loss:0.5657443221041698 	 acc:0.7284768211920529 	 lr:0.0001
epoch10: train: loss:0.5802610526597204 	 acc:0.6694214876033058 | test: loss:0.5683200560658184 	 acc:0.6675496688741722 	 lr:0.0001
epoch11: train: loss:0.5808378900575244 	 acc:0.6694214876033058 | test: loss:0.5652663597207985 	 acc:0.7086092715231788 	 lr:0.0001
epoch12: train: loss:0.5731010696119513 	 acc:0.6869421487603306 | test: loss:0.5608242607274593 	 acc:0.7006622516556291 	 lr:0.0001
epoch13: train: loss:0.5868105006414996 	 acc:0.6413223140495867 | test: loss:0.5800899910611033 	 acc:0.633112582781457 	 lr:0.0001
epoch14: train: loss:0.5732124971161204 	 acc:0.7259504132231405 | test: loss:0.5497546188089232 	 acc:0.7417218543046358 	 lr:0.0001
epoch15: train: loss:0.6397556834181478 	 acc:0.6565289256198347 | test: loss:0.646639778756148 	 acc:0.6569536423841059 	 lr:0.0001
epoch16: train: loss:0.5748317086204024 	 acc:0.7500826446280991 | test: loss:0.5699740233800269 	 acc:0.7390728476821192 	 lr:0.0001
epoch17: train: loss:0.5912479849492223 	 acc:0.7269421487603306 | test: loss:0.6088403836780826 	 acc:0.6768211920529801 	 lr:0.0001
epoch18: train: loss:0.5581401359739382 	 acc:0.7576859504132232 | test: loss:0.5509577692739221 	 acc:0.7470198675496689 	 lr:0.0001
epoch19: train: loss:0.5669429817672603 	 acc:0.6585123966942149 | test: loss:0.5674929890411579 	 acc:0.6543046357615894 	 lr:0.0001
epoch20: train: loss:0.5510252195350395 	 acc:0.7543801652892562 | test: loss:0.5460970948074038 	 acc:0.7496688741721854 	 lr:0.0001
epoch21: train: loss:0.5455791937812301 	 acc:0.7543801652892562 | test: loss:0.5390760592277476 	 acc:0.7576158940397351 	 lr:0.0001
epoch22: train: loss:0.5517756090085368 	 acc:0.7580165289256199 | test: loss:0.5399213797209279 	 acc:0.7629139072847683 	 lr:0.0001
epoch23: train: loss:0.5403112404602618 	 acc:0.7345454545454545 | test: loss:0.5393692570806339 	 acc:0.7324503311258278 	 lr:0.0001
epoch24: train: loss:0.5717369627755535 	 acc:0.7467768595041322 | test: loss:0.5869090693675919 	 acc:0.713907284768212 	 lr:0.0001
epoch25: train: loss:0.5309383286720465 	 acc:0.7451239669421488 | test: loss:0.5297292438563922 	 acc:0.7536423841059603 	 lr:0.0001
epoch26: train: loss:0.547679327480064 	 acc:0.7024793388429752 | test: loss:0.5422423669044545 	 acc:0.713907284768212 	 lr:0.0001
epoch27: train: loss:0.5641947331704384 	 acc:0.7639669421487604 | test: loss:0.5467444150653107 	 acc:0.7549668874172185 	 lr:0.0001
epoch28: train: loss:0.5258687088509236 	 acc:0.76 | test: loss:0.5244770098206223 	 acc:0.7456953642384105 	 lr:0.0001
epoch29: train: loss:0.5552917670218412 	 acc:0.7705785123966942 | test: loss:0.5655341802843359 	 acc:0.7509933774834437 	 lr:0.0001
epoch30: train: loss:0.5229023766123558 	 acc:0.7818181818181819 | test: loss:0.5312971070112772 	 acc:0.7549668874172185 	 lr:0.0001
epoch31: train: loss:0.5241313084492013 	 acc:0.7930578512396694 | test: loss:0.5333493179043397 	 acc:0.7602649006622516 	 lr:0.0001
epoch32: train: loss:0.5497582059655308 	 acc:0.6899173553719008 | test: loss:0.5491534631773336 	 acc:0.7072847682119205 	 lr:0.0001
epoch33: train: loss:0.5259993361638597 	 acc:0.7543801652892562 | test: loss:0.5397397342896619 	 acc:0.7231788079470198 	 lr:0.0001
epoch34: train: loss:0.5248476668428783 	 acc:0.771900826446281 | test: loss:0.5282455875384097 	 acc:0.743046357615894 	 lr:0.0001
epoch35: train: loss:0.514466669953559 	 acc:0.8109090909090909 | test: loss:0.5292439088916147 	 acc:0.7695364238410596 	 lr:5e-05
epoch36: train: loss:0.5177289326624437 	 acc:0.8115702479338843 | test: loss:0.5298213963477028 	 acc:0.766887417218543 	 lr:5e-05
epoch37: train: loss:0.5182315411252424 	 acc:0.8066115702479338 | test: loss:0.5371175690202523 	 acc:0.7708609271523179 	 lr:5e-05
epoch38: train: loss:0.5411043245733277 	 acc:0.6942148760330579 | test: loss:0.5570079587942717 	 acc:0.6701986754966888 	 lr:5e-05
epoch39: train: loss:0.5050315887671857 	 acc:0.811900826446281 | test: loss:0.5184784473962342 	 acc:0.7655629139072848 	 lr:5e-05
epoch40: train: loss:0.5067236852054754 	 acc:0.792396694214876 | test: loss:0.5160814822904322 	 acc:0.7589403973509934 	 lr:5e-05
epoch41: train: loss:0.4989692317552803 	 acc:0.8082644628099174 | test: loss:0.5181445717811585 	 acc:0.7708609271523179 	 lr:5e-05
epoch42: train: loss:0.4978669811280306 	 acc:0.8105785123966942 | test: loss:0.5168126080209846 	 acc:0.7748344370860927 	 lr:5e-05
epoch43: train: loss:0.5420124000557198 	 acc:0.6836363636363636 | test: loss:0.5672895387308488 	 acc:0.6569536423841059 	 lr:5e-05
epoch44: train: loss:0.5037168755117527 	 acc:0.8204958677685951 | test: loss:0.5282618182384415 	 acc:0.7642384105960265 	 lr:5e-05
epoch45: train: loss:0.5171754908364666 	 acc:0.7550413223140496 | test: loss:0.5250598697472881 	 acc:0.7456953642384105 	 lr:5e-05
epoch46: train: loss:0.5042712127866824 	 acc:0.811900826446281 | test: loss:0.5314259885952173 	 acc:0.7708609271523179 	 lr:5e-05
epoch47: train: loss:0.49858909914316224 	 acc:0.8317355371900826 | test: loss:0.5230330359067349 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch48: train: loss:0.4923717124403016 	 acc:0.8185123966942148 | test: loss:0.5159312359544616 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch49: train: loss:0.48840906363873443 	 acc:0.815206611570248 | test: loss:0.5164629742799216 	 acc:0.7615894039735099 	 lr:2.5e-05
epoch50: train: loss:0.48996065162430125 	 acc:0.8165289256198347 | test: loss:0.51331302282826 	 acc:0.7721854304635761 	 lr:2.5e-05
epoch51: train: loss:0.4925239013246268 	 acc:0.8320661157024793 | test: loss:0.520397941876721 	 acc:0.7814569536423841 	 lr:2.5e-05
epoch52: train: loss:0.49500808305976807 	 acc:0.7871074380165289 | test: loss:0.5229250478428721 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch53: train: loss:0.4939340926990036 	 acc:0.8138842975206612 | test: loss:0.5111681435281867 	 acc:0.7774834437086092 	 lr:2.5e-05
epoch54: train: loss:0.48503114982084794 	 acc:0.8317355371900826 | test: loss:0.514054670318073 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch55: train: loss:0.49170929647674244 	 acc:0.8314049586776859 | test: loss:0.5150474195448768 	 acc:0.7880794701986755 	 lr:2.5e-05
epoch56: train: loss:0.4909580508145419 	 acc:0.7976859504132231 | test: loss:0.514986043260587 	 acc:0.7576158940397351 	 lr:2.5e-05
epoch57: train: loss:0.4858333745475643 	 acc:0.823801652892562 | test: loss:0.515282241161296 	 acc:0.766887417218543 	 lr:2.5e-05
epoch58: train: loss:0.4844459280100736 	 acc:0.8343801652892562 | test: loss:0.5174827123319866 	 acc:0.7960264900662252 	 lr:2.5e-05
epoch59: train: loss:0.48949102287450114 	 acc:0.8317355371900826 | test: loss:0.523382520359873 	 acc:0.7695364238410596 	 lr:2.5e-05
epoch60: train: loss:0.4820434842030864 	 acc:0.8191735537190082 | test: loss:0.5122507055074174 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch61: train: loss:0.4796284116792285 	 acc:0.8459504132231405 | test: loss:0.5170928701659702 	 acc:0.7814569536423841 	 lr:1.25e-05
epoch62: train: loss:0.48720821113625834 	 acc:0.8383471074380165 | test: loss:0.5231044294818348 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch63: train: loss:0.48172159984092083 	 acc:0.8317355371900826 | test: loss:0.5121541118779719 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch64: train: loss:0.4806762516005965 	 acc:0.8502479338842975 | test: loss:0.5211115304997426 	 acc:0.7695364238410596 	 lr:1.25e-05
epoch65: train: loss:0.4768143147969049 	 acc:0.8347107438016529 | test: loss:0.5138091307602182 	 acc:0.7682119205298014 	 lr:1.25e-05
epoch66: train: loss:0.4748780026908748 	 acc:0.8426446280991735 | test: loss:0.5141253257429363 	 acc:0.776158940397351 	 lr:6.25e-06
epoch67: train: loss:0.4804576262462238 	 acc:0.83900826446281 | test: loss:0.5165705927160402 	 acc:0.7695364238410596 	 lr:6.25e-06
epoch68: train: loss:0.46950801173517526 	 acc:0.8386776859504133 | test: loss:0.5116358843070782 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch69: train: loss:0.4767209639135471 	 acc:0.83900826446281 | test: loss:0.5120014165410932 	 acc:0.7774834437086092 	 lr:6.25e-06
epoch70: train: loss:0.4775238044990981 	 acc:0.8393388429752067 | test: loss:0.5139636691042919 	 acc:0.7748344370860927 	 lr:6.25e-06
epoch71: train: loss:0.4774397519895853 	 acc:0.836694214876033 | test: loss:0.516510738047543 	 acc:0.7748344370860927 	 lr:6.25e-06
epoch72: train: loss:0.47108566385655365 	 acc:0.8535537190082645 | test: loss:0.5143768785015637 	 acc:0.7708609271523179 	 lr:3.125e-06
epoch73: train: loss:0.475552195803193 	 acc:0.8376859504132231 | test: loss:0.5137864579428111 	 acc:0.7708609271523179 	 lr:3.125e-06
epoch74: train: loss:0.4757783055404001 	 acc:0.844297520661157 | test: loss:0.5146563317602044 	 acc:0.766887417218543 	 lr:3.125e-06
epoch75: train: loss:0.47512926410052403 	 acc:0.8512396694214877 | test: loss:0.5143682665382789 	 acc:0.7788079470198676 	 lr:3.125e-06
epoch76: train: loss:0.4739540955744499 	 acc:0.8469421487603306 | test: loss:0.5143745603150879 	 acc:0.7748344370860927 	 lr:3.125e-06
epoch77: train: loss:0.47311706033619966 	 acc:0.8535537190082645 | test: loss:0.5171371472592385 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch78: train: loss:0.4797090390989603 	 acc:0.8433057851239669 | test: loss:0.5170836001831964 	 acc:0.7735099337748345 	 lr:1.5625e-06
epoch79: train: loss:0.4723286541729919 	 acc:0.8462809917355372 | test: loss:0.5137849251166084 	 acc:0.7682119205298014 	 lr:1.5625e-06
epoch80: train: loss:0.47427409047922814 	 acc:0.8429752066115702 | test: loss:0.5136828790437307 	 acc:0.7748344370860927 	 lr:1.5625e-06
epoch81: train: loss:0.4761934754474104 	 acc:0.844297520661157 | test: loss:0.5121060707711226 	 acc:0.7788079470198676 	 lr:1.5625e-06
epoch82: train: loss:0.47350548217119265 	 acc:0.8383471074380165 | test: loss:0.5119034272156014 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch83: train: loss:0.4746863699846031 	 acc:0.8419834710743802 | test: loss:0.5130552621866693 	 acc:0.7721854304635761 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_10_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_10_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6884163816901278 	 acc:0.5755371900826446 | test: loss:0.6853599161501752 	 acc:0.5443708609271524 	 lr:0.0001
epoch1: train: loss:0.6471290179126519 	 acc:0.5252892561983471 | test: loss:0.6482665385631536 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.632108013058497 	 acc:0.6062809917355372 | test: loss:0.629514885027677 	 acc:0.6066225165562914 	 lr:0.0001
epoch3: train: loss:0.6192035815341413 	 acc:0.6373553719008265 | test: loss:0.6173138334261661 	 acc:0.6370860927152318 	 lr:0.0001
epoch4: train: loss:0.6183752523966072 	 acc:0.6813223140495868 | test: loss:0.6119322809162518 	 acc:0.7245033112582782 	 lr:0.0001
epoch5: train: loss:0.6248760737072337 	 acc:0.6879338842975207 | test: loss:0.6197581910139678 	 acc:0.6834437086092715 	 lr:0.0001
epoch6: train: loss:0.586415395283502 	 acc:0.7094214876033058 | test: loss:0.5700488902085664 	 acc:0.7496688741721854 	 lr:0.0001
epoch7: train: loss:0.5850293798289024 	 acc:0.7004958677685951 | test: loss:0.5690946180299418 	 acc:0.7403973509933774 	 lr:0.0001
epoch8: train: loss:0.6040259504909358 	 acc:0.7107438016528925 | test: loss:0.5878698858204267 	 acc:0.7298013245033113 	 lr:0.0001
epoch9: train: loss:0.5959104531855622 	 acc:0.7130578512396695 | test: loss:0.5816286901764522 	 acc:0.7192052980132451 	 lr:0.0001
epoch10: train: loss:0.5699970313340179 	 acc:0.692892561983471 | test: loss:0.5530461627915995 	 acc:0.7284768211920529 	 lr:0.0001
epoch11: train: loss:0.5611701721593368 	 acc:0.7097520661157025 | test: loss:0.5438352774310585 	 acc:0.7483443708609272 	 lr:0.0001
epoch12: train: loss:0.5611326379815409 	 acc:0.6955371900826446 | test: loss:0.5482219317890951 	 acc:0.7165562913907285 	 lr:0.0001
epoch13: train: loss:0.5936268480947196 	 acc:0.5980165289256199 | test: loss:0.5978092996489923 	 acc:0.6013245033112583 	 lr:0.0001
epoch14: train: loss:0.5603028932484714 	 acc:0.7315702479338843 | test: loss:0.5421597269197174 	 acc:0.7271523178807947 	 lr:0.0001
epoch15: train: loss:0.596689510365163 	 acc:0.7180165289256198 | test: loss:0.5785911521374785 	 acc:0.7390728476821192 	 lr:0.0001
epoch16: train: loss:0.582694155716699 	 acc:0.7375206611570247 | test: loss:0.5733231850017775 	 acc:0.7390728476821192 	 lr:0.0001
epoch17: train: loss:0.5478197453238748 	 acc:0.7702479338842976 | test: loss:0.5613080120244563 	 acc:0.7470198675496689 	 lr:0.0001
epoch18: train: loss:0.5508773765682189 	 acc:0.7256198347107438 | test: loss:0.5434555619757696 	 acc:0.7311258278145696 	 lr:0.0001
epoch19: train: loss:0.5377742024886707 	 acc:0.7484297520661157 | test: loss:0.5416066236843337 	 acc:0.7178807947019867 	 lr:0.0001
epoch20: train: loss:0.5581538987356769 	 acc:0.7550413223140496 | test: loss:0.5767544922449731 	 acc:0.7231788079470198 	 lr:0.0001
epoch21: train: loss:0.5292605306294339 	 acc:0.7745454545454545 | test: loss:0.5266475621438185 	 acc:0.7682119205298014 	 lr:0.0001
epoch22: train: loss:0.5311015837054607 	 acc:0.7834710743801653 | test: loss:0.5351730000893802 	 acc:0.7615894039735099 	 lr:0.0001
epoch23: train: loss:0.5215060915434656 	 acc:0.771900826446281 | test: loss:0.5252928808824906 	 acc:0.7417218543046358 	 lr:0.0001
epoch24: train: loss:0.5457572628249807 	 acc:0.7791735537190083 | test: loss:0.5744243684193946 	 acc:0.7231788079470198 	 lr:0.0001
epoch25: train: loss:0.5287727267486004 	 acc:0.7186776859504133 | test: loss:0.5305921640617168 	 acc:0.7350993377483444 	 lr:0.0001
epoch26: train: loss:0.5076435095515133 	 acc:0.8003305785123966 | test: loss:0.5185205836959232 	 acc:0.7549668874172185 	 lr:0.0001
epoch27: train: loss:0.5730909697083403 	 acc:0.7444628099173554 | test: loss:0.5967601739807634 	 acc:0.7033112582781457 	 lr:0.0001
epoch28: train: loss:0.5067014571260815 	 acc:0.8033057851239669 | test: loss:0.5120129960262223 	 acc:0.7788079470198676 	 lr:0.0001
epoch29: train: loss:0.5143878682585787 	 acc:0.8185123966942148 | test: loss:0.5341466205799027 	 acc:0.7682119205298014 	 lr:0.0001
epoch30: train: loss:0.5188529660879088 	 acc:0.80099173553719 | test: loss:0.5470287802993067 	 acc:0.7576158940397351 	 lr:0.0001
epoch31: train: loss:0.5793357054064097 	 acc:0.7421487603305785 | test: loss:0.6215863440210456 	 acc:0.704635761589404 	 lr:0.0001
epoch32: train: loss:0.5168011059248743 	 acc:0.7897520661157025 | test: loss:0.542162674073352 	 acc:0.7417218543046358 	 lr:0.0001
epoch33: train: loss:0.5052377598147747 	 acc:0.7897520661157025 | test: loss:0.5291280436200022 	 acc:0.7324503311258278 	 lr:0.0001
epoch34: train: loss:0.5344936390947704 	 acc:0.7904132231404959 | test: loss:0.547830939766587 	 acc:0.7589403973509934 	 lr:0.0001
epoch35: train: loss:0.4873609519004822 	 acc:0.8386776859504133 | test: loss:0.5175407429404606 	 acc:0.7748344370860927 	 lr:5e-05
epoch36: train: loss:0.48468350013425526 	 acc:0.8208264462809918 | test: loss:0.5106881112452374 	 acc:0.766887417218543 	 lr:5e-05
epoch37: train: loss:0.557178900458596 	 acc:0.7662809917355372 | test: loss:0.6039850242090541 	 acc:0.7165562913907285 	 lr:5e-05
epoch38: train: loss:0.48919778913505807 	 acc:0.7917355371900826 | test: loss:0.5168528702874847 	 acc:0.7456953642384105 	 lr:5e-05
epoch39: train: loss:0.48559710737102285 	 acc:0.8469421487603306 | test: loss:0.5266845462338025 	 acc:0.7788079470198676 	 lr:5e-05
epoch40: train: loss:0.4775529530915347 	 acc:0.8284297520661157 | test: loss:0.5054737296325481 	 acc:0.7788079470198676 	 lr:5e-05
epoch41: train: loss:0.4791312581448516 	 acc:0.8201652892561984 | test: loss:0.5126955404976345 	 acc:0.7576158940397351 	 lr:5e-05
epoch42: train: loss:0.46779314972152397 	 acc:0.8416528925619835 | test: loss:0.5135564580658414 	 acc:0.7708609271523179 	 lr:5e-05
epoch43: train: loss:0.5304093818231063 	 acc:0.704793388429752 | test: loss:0.5635128970967224 	 acc:0.6596026490066225 	 lr:5e-05
epoch44: train: loss:0.4769374741010429 	 acc:0.8115702479338843 | test: loss:0.5191216361443728 	 acc:0.7536423841059603 	 lr:5e-05
epoch45: train: loss:0.5159578270557498 	 acc:0.8132231404958677 | test: loss:0.5585364528049697 	 acc:0.7470198675496689 	 lr:5e-05
epoch46: train: loss:0.4756439843158091 	 acc:0.8459504132231405 | test: loss:0.520427227651836 	 acc:0.7721854304635761 	 lr:5e-05
epoch47: train: loss:0.5000549206654887 	 acc:0.8284297520661157 | test: loss:0.5650497056790535 	 acc:0.7536423841059603 	 lr:2.5e-05
epoch48: train: loss:0.46111968022732697 	 acc:0.84 | test: loss:0.5068823179661833 	 acc:0.7695364238410596 	 lr:2.5e-05
epoch49: train: loss:0.45518936797607046 	 acc:0.8373553719008264 | test: loss:0.5046937733296527 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch50: train: loss:0.45467151867456673 	 acc:0.868099173553719 | test: loss:0.5130786122075769 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch51: train: loss:0.45713596512463467 	 acc:0.8690909090909091 | test: loss:0.5151843622820267 	 acc:0.7788079470198676 	 lr:2.5e-05
epoch52: train: loss:0.450618233513241 	 acc:0.8525619834710744 | test: loss:0.5069015864504883 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch53: train: loss:0.45983370159283155 	 acc:0.8657851239669422 | test: loss:0.5092127261572327 	 acc:0.776158940397351 	 lr:2.5e-05
epoch54: train: loss:0.44625611252035974 	 acc:0.8608264462809917 | test: loss:0.5057857543427423 	 acc:0.7721854304635761 	 lr:2.5e-05
epoch55: train: loss:0.4547905180868038 	 acc:0.8674380165289256 | test: loss:0.517552786789193 	 acc:0.7880794701986755 	 lr:2.5e-05
epoch56: train: loss:0.44566412714887255 	 acc:0.8657851239669422 | test: loss:0.5058226038288597 	 acc:0.7748344370860927 	 lr:1.25e-05
epoch57: train: loss:0.44571141690262095 	 acc:0.8677685950413223 | test: loss:0.5046551134412652 	 acc:0.7774834437086092 	 lr:1.25e-05
epoch58: train: loss:0.4418984900821339 	 acc:0.8803305785123967 | test: loss:0.5098497262853661 	 acc:0.7841059602649006 	 lr:1.25e-05
epoch59: train: loss:0.44758629428453683 	 acc:0.8740495867768595 | test: loss:0.5129159460794057 	 acc:0.7814569536423841 	 lr:1.25e-05
epoch60: train: loss:0.44371734529487356 	 acc:0.8684297520661157 | test: loss:0.5072468710261465 	 acc:0.7774834437086092 	 lr:1.25e-05
epoch61: train: loss:0.43899156641369025 	 acc:0.8922314049586777 | test: loss:0.5142115098751144 	 acc:0.7801324503311259 	 lr:1.25e-05
epoch62: train: loss:0.4445261975851926 	 acc:0.8796694214876033 | test: loss:0.5142103773868637 	 acc:0.7788079470198676 	 lr:6.25e-06
epoch63: train: loss:0.44160250781981414 	 acc:0.8816528925619834 | test: loss:0.5068981360915481 	 acc:0.7801324503311259 	 lr:6.25e-06
epoch64: train: loss:0.44600041329368084 	 acc:0.8730578512396694 | test: loss:0.5210947280688002 	 acc:0.776158940397351 	 lr:6.25e-06
epoch65: train: loss:0.44441320861666656 	 acc:0.8634710743801652 | test: loss:0.5045475802674199 	 acc:0.7748344370860927 	 lr:6.25e-06
epoch66: train: loss:0.4368617820542706 	 acc:0.8852892561983471 | test: loss:0.5059631339761596 	 acc:0.7880794701986755 	 lr:6.25e-06
epoch67: train: loss:0.44358924399722705 	 acc:0.8826446280991735 | test: loss:0.5138029353508097 	 acc:0.7894039735099337 	 lr:6.25e-06
epoch68: train: loss:0.4334727149758457 	 acc:0.8856198347107438 | test: loss:0.5071134274369044 	 acc:0.7788079470198676 	 lr:6.25e-06
epoch69: train: loss:0.4426598461107774 	 acc:0.8803305785123967 | test: loss:0.5110926388904748 	 acc:0.7814569536423841 	 lr:6.25e-06
epoch70: train: loss:0.43974018851587593 	 acc:0.8733884297520661 | test: loss:0.5086948238461223 	 acc:0.7748344370860927 	 lr:6.25e-06
epoch71: train: loss:0.4379208228410768 	 acc:0.8872727272727273 | test: loss:0.5133791942470121 	 acc:0.776158940397351 	 lr:6.25e-06
epoch72: train: loss:0.4330028066063715 	 acc:0.8945454545454545 | test: loss:0.5128834871266852 	 acc:0.776158940397351 	 lr:3.125e-06
epoch73: train: loss:0.43776481385073385 	 acc:0.8803305785123967 | test: loss:0.5080800455927059 	 acc:0.7801324503311259 	 lr:3.125e-06
epoch74: train: loss:0.43721942219852417 	 acc:0.8813223140495868 | test: loss:0.5097662330463233 	 acc:0.7814569536423841 	 lr:3.125e-06
epoch75: train: loss:0.4377204644581503 	 acc:0.888595041322314 | test: loss:0.513113368189098 	 acc:0.7801324503311259 	 lr:3.125e-06
epoch76: train: loss:0.43518747438083993 	 acc:0.8806611570247934 | test: loss:0.5114503734948619 	 acc:0.7721854304635761 	 lr:3.125e-06
epoch77: train: loss:0.43664749326784746 	 acc:0.8866115702479339 | test: loss:0.5117472230203893 	 acc:0.776158940397351 	 lr:3.125e-06
epoch78: train: loss:0.4392790528860959 	 acc:0.8839669421487604 | test: loss:0.5121291047689931 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch79: train: loss:0.43829571695367164 	 acc:0.8780165289256199 | test: loss:0.5106908687692604 	 acc:0.7801324503311259 	 lr:1.5625e-06
epoch80: train: loss:0.4329924930127199 	 acc:0.888595041322314 | test: loss:0.5092478993712671 	 acc:0.7801324503311259 	 lr:1.5625e-06
epoch81: train: loss:0.4396551729233797 	 acc:0.8773553719008265 | test: loss:0.5079865529837198 	 acc:0.7814569536423841 	 lr:1.5625e-06
epoch82: train: loss:0.4385728251835531 	 acc:0.876694214876033 | test: loss:0.5084366087881934 	 acc:0.7827814569536424 	 lr:1.5625e-06
epoch83: train: loss:0.43606575240773604 	 acc:0.8852892561983471 | test: loss:0.5089540006309156 	 acc:0.785430463576159 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_11_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_11_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6864465894383832 	 acc:0.5808264462809918 | test: loss:0.684210279524721 	 acc:0.5682119205298013 	 lr:0.0001
epoch1: train: loss:0.6466708395500813 	 acc:0.5249586776859504 | test: loss:0.6471840348464764 	 acc:0.5231788079470199 	 lr:0.0001
epoch2: train: loss:0.631662135695623 	 acc:0.6019834710743802 | test: loss:0.6283652766650876 	 acc:0.6119205298013245 	 lr:0.0001
epoch3: train: loss:0.6215605930257435 	 acc:0.6515702479338843 | test: loss:0.6156134857247207 	 acc:0.6649006622516557 	 lr:0.0001
epoch4: train: loss:0.6044559584964405 	 acc:0.6869421487603306 | test: loss:0.5929181661826886 	 acc:0.7284768211920529 	 lr:0.0001
epoch5: train: loss:0.6112571447545831 	 acc:0.6730578512396694 | test: loss:0.5959484550337129 	 acc:0.7311258278145696 	 lr:0.0001
epoch6: train: loss:0.5797570717433267 	 acc:0.6833057851239669 | test: loss:0.5648221192770446 	 acc:0.7072847682119205 	 lr:0.0001
epoch7: train: loss:0.5840278758884462 	 acc:0.7137190082644628 | test: loss:0.5659820116908345 	 acc:0.7364238410596027 	 lr:0.0001
epoch8: train: loss:0.6046547362627077 	 acc:0.7090909090909091 | test: loss:0.5882017997716437 	 acc:0.7192052980132451 	 lr:0.0001
epoch9: train: loss:0.5846466211050995 	 acc:0.7328925619834711 | test: loss:0.5714731827476957 	 acc:0.7549668874172185 	 lr:0.0001
epoch10: train: loss:0.5637944579321491 	 acc:0.6885950413223141 | test: loss:0.5469144990112608 	 acc:0.7245033112582782 	 lr:0.0001
epoch11: train: loss:0.557356719852479 	 acc:0.7123966942148761 | test: loss:0.5374612268233141 	 acc:0.7483443708609272 	 lr:0.0001
epoch12: train: loss:0.5609512130682134 	 acc:0.7140495867768595 | test: loss:0.548766815662384 	 acc:0.7245033112582782 	 lr:0.0001
epoch13: train: loss:0.5775164422318955 	 acc:0.6406611570247934 | test: loss:0.5848491316599562 	 acc:0.6278145695364239 	 lr:0.0001
epoch14: train: loss:0.5620568981249471 	 acc:0.7100826446280992 | test: loss:0.5544335115824314 	 acc:0.6874172185430464 	 lr:0.0001
epoch15: train: loss:0.5529545959756395 	 acc:0.7649586776859504 | test: loss:0.5386068729375372 	 acc:0.7615894039735099 	 lr:0.0001
epoch16: train: loss:0.5743865890148258 	 acc:0.7570247933884298 | test: loss:0.5760787681238541 	 acc:0.7443708609271523 	 lr:0.0001
epoch17: train: loss:0.5248110616699723 	 acc:0.7884297520661157 | test: loss:0.5338755488395691 	 acc:0.7682119205298014 	 lr:0.0001
epoch18: train: loss:0.5402578775547753 	 acc:0.7811570247933884 | test: loss:0.5475457317781764 	 acc:0.7735099337748345 	 lr:0.0001
epoch19: train: loss:0.5258555256433723 	 acc:0.7378512396694215 | test: loss:0.5289038058937781 	 acc:0.743046357615894 	 lr:0.0001
epoch20: train: loss:0.5336481205490995 	 acc:0.7808264462809917 | test: loss:0.544311026232132 	 acc:0.7629139072847683 	 lr:0.0001
epoch21: train: loss:0.5492779633821535 	 acc:0.7768595041322314 | test: loss:0.5438734095617636 	 acc:0.766887417218543 	 lr:0.0001
epoch22: train: loss:0.5245684859181239 	 acc:0.7993388429752066 | test: loss:0.5346791337657448 	 acc:0.7735099337748345 	 lr:0.0001
epoch23: train: loss:0.5167735217228409 	 acc:0.7652892561983471 | test: loss:0.5243642732797079 	 acc:0.7417218543046358 	 lr:0.0001
epoch24: train: loss:0.541029428095857 	 acc:0.7818181818181819 | test: loss:0.5730686760106624 	 acc:0.743046357615894 	 lr:0.0001
epoch25: train: loss:0.49954160537601505 	 acc:0.7874380165289256 | test: loss:0.5176680519091372 	 acc:0.7655629139072848 	 lr:0.0001
epoch26: train: loss:0.5188071993953925 	 acc:0.7398347107438017 | test: loss:0.5287164362061103 	 acc:0.7258278145695364 	 lr:0.0001
epoch27: train: loss:0.5476511507191933 	 acc:0.7785123966942149 | test: loss:0.5544702229910339 	 acc:0.7615894039735099 	 lr:0.0001
epoch28: train: loss:0.5288946798419164 	 acc:0.80099173553719 | test: loss:0.5499424197815901 	 acc:0.7642384105960265 	 lr:0.0001
epoch29: train: loss:0.4979423766392322 	 acc:0.823801652892562 | test: loss:0.5296016947323123 	 acc:0.7615894039735099 	 lr:0.0001
epoch30: train: loss:0.5679768731574382 	 acc:0.7527272727272727 | test: loss:0.6028403117956704 	 acc:0.7125827814569536 	 lr:0.0001
epoch31: train: loss:0.5082495847418289 	 acc:0.8095867768595041 | test: loss:0.5469154977640569 	 acc:0.743046357615894 	 lr:0.0001
epoch32: train: loss:0.48156902835388815 	 acc:0.8304132231404958 | test: loss:0.5079858664645265 	 acc:0.785430463576159 	 lr:5e-05
epoch33: train: loss:0.49141814883090246 	 acc:0.7847933884297521 | test: loss:0.519542991720288 	 acc:0.7470198675496689 	 lr:5e-05
epoch34: train: loss:0.49020212080853043 	 acc:0.7748760330578512 | test: loss:0.5235284768982439 	 acc:0.7390728476821192 	 lr:5e-05
epoch35: train: loss:0.4692387886874932 	 acc:0.8383471074380165 | test: loss:0.5092701638771209 	 acc:0.7721854304635761 	 lr:5e-05
epoch36: train: loss:0.4675406423383508 	 acc:0.8535537190082645 | test: loss:0.5109782698928126 	 acc:0.7788079470198676 	 lr:5e-05
epoch37: train: loss:0.541218771579837 	 acc:0.7801652892561983 | test: loss:0.6043774693217498 	 acc:0.7192052980132451 	 lr:5e-05
epoch38: train: loss:0.46075638636084626 	 acc:0.8542148760330579 | test: loss:0.5056468296524704 	 acc:0.7748344370860927 	 lr:5e-05
epoch39: train: loss:0.4850437930103176 	 acc:0.83900826446281 | test: loss:0.5362232437986412 	 acc:0.752317880794702 	 lr:5e-05
epoch40: train: loss:0.46901137599275133 	 acc:0.8251239669421487 | test: loss:0.5069457542027859 	 acc:0.7682119205298014 	 lr:5e-05
epoch41: train: loss:0.45759466082596584 	 acc:0.8489256198347107 | test: loss:0.5058898152105066 	 acc:0.7774834437086092 	 lr:5e-05
epoch42: train: loss:0.453388196633867 	 acc:0.8492561983471074 | test: loss:0.5123891674130168 	 acc:0.7642384105960265 	 lr:5e-05
epoch43: train: loss:0.48571815848350525 	 acc:0.7824793388429752 | test: loss:0.5258456076217803 	 acc:0.7311258278145696 	 lr:5e-05
epoch44: train: loss:0.45957745818067186 	 acc:0.8575206611570247 | test: loss:0.5124876026286195 	 acc:0.766887417218543 	 lr:5e-05
epoch45: train: loss:0.4740489967401363 	 acc:0.8558677685950413 | test: loss:0.5368733188174418 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch46: train: loss:0.45876565807121844 	 acc:0.8661157024793389 | test: loss:0.5243114624591853 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch47: train: loss:0.43949802001645744 	 acc:0.8839669421487604 | test: loss:0.5099948965950517 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch48: train: loss:0.44613060371934876 	 acc:0.8591735537190083 | test: loss:0.5046069650460553 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch49: train: loss:0.44362983539084755 	 acc:0.8558677685950413 | test: loss:0.5053046877810498 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch50: train: loss:0.44660980408841916 	 acc:0.8813223140495868 | test: loss:0.5267305681247585 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch51: train: loss:0.43782172228679184 	 acc:0.8876033057851239 | test: loss:0.5152912760412456 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch52: train: loss:0.43679476736005673 	 acc:0.8730578512396694 | test: loss:0.5139026121587943 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch53: train: loss:0.44768538497696236 	 acc:0.8733884297520661 | test: loss:0.5087620058596529 	 acc:0.7602649006622516 	 lr:2.5e-05
epoch54: train: loss:0.43792013217595 	 acc:0.8614876033057851 | test: loss:0.5068998301266044 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch55: train: loss:0.43558567121994396 	 acc:0.8895867768595042 | test: loss:0.5108534087408457 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch56: train: loss:0.4340767245844376 	 acc:0.8770247933884298 | test: loss:0.5060903647877523 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch57: train: loss:0.42613866724258614 	 acc:0.8975206611570248 | test: loss:0.5053943332457385 	 acc:0.7695364238410596 	 lr:1.25e-05
epoch58: train: loss:0.42883905175303627 	 acc:0.8882644628099173 | test: loss:0.5093515122173637 	 acc:0.7774834437086092 	 lr:1.25e-05
epoch59: train: loss:0.429152845361016 	 acc:0.8961983471074381 | test: loss:0.5146504448739109 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch60: train: loss:0.42685569422304137 	 acc:0.8829752066115703 | test: loss:0.5116754782910379 	 acc:0.7682119205298014 	 lr:1.25e-05
epoch61: train: loss:0.4245244454450844 	 acc:0.8975206611570248 | test: loss:0.5112122282287143 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch62: train: loss:0.42878927316547427 	 acc:0.8909090909090909 | test: loss:0.5145144866002317 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch63: train: loss:0.42572362183539336 	 acc:0.8905785123966942 | test: loss:0.5071056158337373 	 acc:0.7774834437086092 	 lr:6.25e-06
epoch64: train: loss:0.42868604776287866 	 acc:0.8882644628099173 | test: loss:0.5125009890423705 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch65: train: loss:0.43040983335045746 	 acc:0.8760330578512396 | test: loss:0.5082053663714832 	 acc:0.7682119205298014 	 lr:6.25e-06
epoch66: train: loss:0.4219801788980311 	 acc:0.9001652892561983 | test: loss:0.5099221803494637 	 acc:0.776158940397351 	 lr:6.25e-06
epoch67: train: loss:0.42504449489688084 	 acc:0.8879338842975206 | test: loss:0.5119381018821767 	 acc:0.7748344370860927 	 lr:3.125e-06
epoch68: train: loss:0.42022320646885014 	 acc:0.8998347107438016 | test: loss:0.5106619907530727 	 acc:0.7788079470198676 	 lr:3.125e-06
epoch69: train: loss:0.4291822579379909 	 acc:0.8899173553719009 | test: loss:0.5117997075548235 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch70: train: loss:0.4248649337961654 	 acc:0.8909090909090909 | test: loss:0.5130002425206418 	 acc:0.7788079470198676 	 lr:3.125e-06
epoch71: train: loss:0.42354523928697446 	 acc:0.8965289256198347 | test: loss:0.5143187802358968 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch72: train: loss:0.4183690043421816 	 acc:0.9044628099173554 | test: loss:0.5130756306332468 	 acc:0.7721854304635761 	 lr:3.125e-06
epoch73: train: loss:0.42553817839661906 	 acc:0.8915702479338843 | test: loss:0.5114452752845966 	 acc:0.766887417218543 	 lr:1.5625e-06
epoch74: train: loss:0.42312583441576684 	 acc:0.8958677685950414 | test: loss:0.5117364311849835 	 acc:0.7708609271523179 	 lr:1.5625e-06
epoch75: train: loss:0.422022039466653 	 acc:0.8955371900826447 | test: loss:0.5130613533076861 	 acc:0.7735099337748345 	 lr:1.5625e-06
epoch76: train: loss:0.4201310863770729 	 acc:0.8998347107438016 | test: loss:0.5112335175867901 	 acc:0.7708609271523179 	 lr:1.5625e-06
epoch77: train: loss:0.4213284459685491 	 acc:0.9008264462809917 | test: loss:0.5122140373615239 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch78: train: loss:0.42730772984914545 	 acc:0.8945454545454545 | test: loss:0.5146094119311958 	 acc:0.776158940397351 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_12_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_12_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6880044012424374 	 acc:0.5715702479338843 | test: loss:0.6844547842512068 	 acc:0.5682119205298013 	 lr:0.0001
epoch1: train: loss:0.6474543210888697 	 acc:0.5249586776859504 | test: loss:0.6483275068516763 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6261500756602641 	 acc:0.6155371900826446 | test: loss:0.6232508806203375 	 acc:0.6158940397350994 	 lr:0.0001
epoch3: train: loss:0.6118465139649131 	 acc:0.6766942148760331 | test: loss:0.6070103309801872 	 acc:0.6900662251655629 	 lr:0.0001
epoch4: train: loss:0.60399817413535 	 acc:0.692892561983471 | test: loss:0.5952352549856073 	 acc:0.7218543046357616 	 lr:0.0001
epoch5: train: loss:0.5947279677903357 	 acc:0.7011570247933885 | test: loss:0.579747481377709 	 acc:0.7350993377483444 	 lr:0.0001
epoch6: train: loss:0.5942098640016288 	 acc:0.7196694214876033 | test: loss:0.5766670910727899 	 acc:0.7311258278145696 	 lr:0.0001
epoch7: train: loss:0.5766452112079652 	 acc:0.7018181818181818 | test: loss:0.5641878287523787 	 acc:0.695364238410596 	 lr:0.0001
epoch8: train: loss:0.5853174917954058 	 acc:0.7302479338842975 | test: loss:0.5665246664293554 	 acc:0.7377483443708609 	 lr:0.0001
epoch9: train: loss:0.5830614691135312 	 acc:0.7411570247933884 | test: loss:0.5702570857591187 	 acc:0.7443708609271523 	 lr:0.0001
epoch10: train: loss:0.5528566334070253 	 acc:0.7388429752066116 | test: loss:0.5384319718310375 	 acc:0.7576158940397351 	 lr:0.0001
epoch11: train: loss:0.5491493841636279 	 acc:0.7338842975206612 | test: loss:0.5305081417229002 	 acc:0.766887417218543 	 lr:0.0001
epoch12: train: loss:0.5512389197428365 	 acc:0.7328925619834711 | test: loss:0.5418850455063068 	 acc:0.7337748344370861 	 lr:0.0001
epoch13: train: loss:0.6124853986353913 	 acc:0.5646280991735537 | test: loss:0.6285539252868552 	 acc:0.5562913907284768 	 lr:0.0001
epoch14: train: loss:0.5684640570317419 	 acc:0.7348760330578512 | test: loss:0.5432365160114718 	 acc:0.7509933774834437 	 lr:0.0001
epoch15: train: loss:0.5633274693331443 	 acc:0.7557024793388429 | test: loss:0.5436605242703925 	 acc:0.7549668874172185 	 lr:0.0001
epoch16: train: loss:0.5925936735957122 	 acc:0.7279338842975207 | test: loss:0.5949896091656969 	 acc:0.7271523178807947 	 lr:0.0001
epoch17: train: loss:0.5433858844662501 	 acc:0.7771900826446281 | test: loss:0.5680361525901896 	 acc:0.7377483443708609 	 lr:0.0001
epoch18: train: loss:0.5249312657758224 	 acc:0.7953719008264463 | test: loss:0.5288146251084789 	 acc:0.7708609271523179 	 lr:5e-05
epoch19: train: loss:0.5087678577683189 	 acc:0.7864462809917355 | test: loss:0.51911799228744 	 acc:0.752317880794702 	 lr:5e-05
epoch20: train: loss:0.5030267685208439 	 acc:0.8095867768595041 | test: loss:0.5179917359983685 	 acc:0.7589403973509934 	 lr:5e-05
epoch21: train: loss:0.49771901499141347 	 acc:0.8105785123966942 | test: loss:0.515958467540362 	 acc:0.7549668874172185 	 lr:5e-05
epoch22: train: loss:0.5016781682711987 	 acc:0.7920661157024793 | test: loss:0.5202986795381205 	 acc:0.7443708609271523 	 lr:5e-05
epoch23: train: loss:0.504462603104016 	 acc:0.7801652892561983 | test: loss:0.5250134740444209 	 acc:0.7284768211920529 	 lr:5e-05
epoch24: train: loss:0.5206862237039677 	 acc:0.8069421487603305 | test: loss:0.5513441613967845 	 acc:0.7576158940397351 	 lr:5e-05
epoch25: train: loss:0.5062169037279018 	 acc:0.8221487603305785 | test: loss:0.5395217293935106 	 acc:0.7615894039735099 	 lr:5e-05
epoch26: train: loss:0.49032820055307436 	 acc:0.8036363636363636 | test: loss:0.5143106192942487 	 acc:0.7615894039735099 	 lr:5e-05
epoch27: train: loss:0.49082219726783183 	 acc:0.8148760330578513 | test: loss:0.5056370336488383 	 acc:0.7841059602649006 	 lr:5e-05
epoch28: train: loss:0.4837650076022818 	 acc:0.8026446280991736 | test: loss:0.506957721710205 	 acc:0.7629139072847683 	 lr:5e-05
epoch29: train: loss:0.4843320603015994 	 acc:0.8456198347107438 | test: loss:0.5168824636383562 	 acc:0.776158940397351 	 lr:5e-05
epoch30: train: loss:0.5292508744996441 	 acc:0.8016528925619835 | test: loss:0.557023322898031 	 acc:0.7509933774834437 	 lr:5e-05
epoch31: train: loss:0.4784854100164303 	 acc:0.8287603305785124 | test: loss:0.5096298851714229 	 acc:0.7735099337748345 	 lr:5e-05
epoch32: train: loss:0.4669652819239404 	 acc:0.8376859504132231 | test: loss:0.5033649443001147 	 acc:0.7867549668874172 	 lr:5e-05
epoch33: train: loss:0.47420741738366684 	 acc:0.8439669421487603 | test: loss:0.5141814670815373 	 acc:0.7814569536423841 	 lr:5e-05
epoch34: train: loss:0.46728094082233335 	 acc:0.8264462809917356 | test: loss:0.512313862667968 	 acc:0.7562913907284768 	 lr:5e-05
epoch35: train: loss:0.46493164552144767 	 acc:0.8535537190082645 | test: loss:0.5104758231845123 	 acc:0.7708609271523179 	 lr:5e-05
epoch36: train: loss:0.4693190966952931 	 acc:0.8512396694214877 | test: loss:0.5177094785582941 	 acc:0.7721854304635761 	 lr:5e-05
epoch37: train: loss:0.516861007745601 	 acc:0.8089256198347108 | test: loss:0.5700116341477198 	 acc:0.7403973509933774 	 lr:5e-05
epoch38: train: loss:0.45403396808411467 	 acc:0.8585123966942149 | test: loss:0.5061597190155889 	 acc:0.7774834437086092 	 lr:5e-05
epoch39: train: loss:0.4529886070164767 	 acc:0.8687603305785124 | test: loss:0.5085850737742241 	 acc:0.7907284768211921 	 lr:2.5e-05
epoch40: train: loss:0.4535685433139486 	 acc:0.8456198347107438 | test: loss:0.5006215605514729 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch41: train: loss:0.4442678162677229 	 acc:0.871404958677686 | test: loss:0.5068041445403699 	 acc:0.785430463576159 	 lr:2.5e-05
epoch42: train: loss:0.44528951758195545 	 acc:0.8793388429752066 | test: loss:0.5122564351321846 	 acc:0.7788079470198676 	 lr:2.5e-05
epoch43: train: loss:0.4486582092608302 	 acc:0.8542148760330579 | test: loss:0.5084888124308049 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch44: train: loss:0.4474833954759866 	 acc:0.8737190082644628 | test: loss:0.5176012417338541 	 acc:0.7695364238410596 	 lr:2.5e-05
epoch45: train: loss:0.4429252493972621 	 acc:0.8717355371900827 | test: loss:0.504309119688754 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch46: train: loss:0.44321494871919803 	 acc:0.8720661157024794 | test: loss:0.5039867043495179 	 acc:0.7894039735099337 	 lr:2.5e-05
epoch47: train: loss:0.4326264487613331 	 acc:0.8902479338842976 | test: loss:0.5049044088022598 	 acc:0.7801324503311259 	 lr:1.25e-05
epoch48: train: loss:0.4371373917051583 	 acc:0.8720661157024794 | test: loss:0.501489839964355 	 acc:0.7907284768211921 	 lr:1.25e-05
epoch49: train: loss:0.4314611994629064 	 acc:0.8846280991735537 | test: loss:0.5030427357218913 	 acc:0.7841059602649006 	 lr:1.25e-05
epoch50: train: loss:0.4345926989898209 	 acc:0.8707438016528926 | test: loss:0.5001466289261319 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch51: train: loss:0.44014502691828516 	 acc:0.8928925619834711 | test: loss:0.5246105336195586 	 acc:0.776158940397351 	 lr:1.25e-05
epoch52: train: loss:0.4297367112794198 	 acc:0.8780165289256199 | test: loss:0.5044078378093164 	 acc:0.7748344370860927 	 lr:1.25e-05
epoch53: train: loss:0.43674989220524624 	 acc:0.8690909090909091 | test: loss:0.5050986092611653 	 acc:0.7774834437086092 	 lr:1.25e-05
epoch54: train: loss:0.4246228157784328 	 acc:0.8935537190082644 | test: loss:0.5044582038920447 	 acc:0.7880794701986755 	 lr:1.25e-05
epoch55: train: loss:0.42790030846911026 	 acc:0.8971900826446281 | test: loss:0.5076912694419456 	 acc:0.7880794701986755 	 lr:1.25e-05
epoch56: train: loss:0.4311499232891177 	 acc:0.891900826446281 | test: loss:0.5133929780776927 	 acc:0.7801324503311259 	 lr:1.25e-05
epoch57: train: loss:0.41872060078234713 	 acc:0.8985123966942149 | test: loss:0.5043525222121485 	 acc:0.7801324503311259 	 lr:6.25e-06
epoch58: train: loss:0.42426440609388116 	 acc:0.8985123966942149 | test: loss:0.5088477017468964 	 acc:0.7774834437086092 	 lr:6.25e-06
epoch59: train: loss:0.42539464792929405 	 acc:0.8995041322314049 | test: loss:0.5103146576328783 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch60: train: loss:0.42163140772787994 	 acc:0.8948760330578512 | test: loss:0.5048747339785494 	 acc:0.7827814569536424 	 lr:6.25e-06
epoch61: train: loss:0.42210397238573755 	 acc:0.9077685950413223 | test: loss:0.5100931062998361 	 acc:0.7801324503311259 	 lr:6.25e-06
epoch62: train: loss:0.422974060163025 	 acc:0.8932231404958678 | test: loss:0.5085686852600401 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch63: train: loss:0.42121026463745054 	 acc:0.8988429752066116 | test: loss:0.5035041047642562 	 acc:0.7880794701986755 	 lr:3.125e-06
epoch64: train: loss:0.4210258174139606 	 acc:0.8965289256198347 | test: loss:0.5058495231416841 	 acc:0.7827814569536424 	 lr:3.125e-06
epoch65: train: loss:0.42270890352154566 	 acc:0.888595041322314 | test: loss:0.5054591386523468 	 acc:0.7814569536423841 	 lr:3.125e-06
epoch66: train: loss:0.4172651167349382 	 acc:0.900495867768595 | test: loss:0.5040222949539589 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch67: train: loss:0.4197135072207648 	 acc:0.8932231404958678 | test: loss:0.5051865918746847 	 acc:0.7827814569536424 	 lr:3.125e-06
epoch68: train: loss:0.4141323018369596 	 acc:0.9084297520661156 | test: loss:0.5052530683034303 	 acc:0.7814569536423841 	 lr:3.125e-06
epoch69: train: loss:0.42364051619837106 	 acc:0.8925619834710744 | test: loss:0.5071100356168304 	 acc:0.7801324503311259 	 lr:1.5625e-06
epoch70: train: loss:0.4187873103598918 	 acc:0.9014876033057851 | test: loss:0.5061364293493182 	 acc:0.785430463576159 	 lr:1.5625e-06
epoch71: train: loss:0.4193084973737228 	 acc:0.9034710743801653 | test: loss:0.507002649994086 	 acc:0.7867549668874172 	 lr:1.5625e-06
epoch72: train: loss:0.4168948649374907 	 acc:0.9077685950413223 | test: loss:0.5082600588830102 	 acc:0.7827814569536424 	 lr:1.5625e-06
epoch73: train: loss:0.4200166229571193 	 acc:0.8995041322314049 | test: loss:0.5070825474151712 	 acc:0.7827814569536424 	 lr:1.5625e-06
epoch74: train: loss:0.4191249689582951 	 acc:0.9018181818181819 | test: loss:0.5069891898047845 	 acc:0.7827814569536424 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_13_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_13_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6814125365856265 	 acc:0.584793388429752 | test: loss:0.6766448563297853 	 acc:0.5986754966887418 	 lr:0.0001
epoch1: train: loss:0.6432546202801476 	 acc:0.5272727272727272 | test: loss:0.6411270736858544 	 acc:0.5245033112582781 	 lr:0.0001
epoch2: train: loss:0.629540101181377 	 acc:0.6489256198347108 | test: loss:0.6275035436579723 	 acc:0.6728476821192053 	 lr:0.0001
epoch3: train: loss:0.6198794849253884 	 acc:0.6955371900826446 | test: loss:0.6146237475982564 	 acc:0.695364238410596 	 lr:0.0001
epoch4: train: loss:0.5964123539688173 	 acc:0.6915702479338843 | test: loss:0.5906036721949546 	 acc:0.7298013245033113 	 lr:0.0001
epoch5: train: loss:0.5820532849012328 	 acc:0.7067768595041323 | test: loss:0.5634033485753647 	 acc:0.7536423841059603 	 lr:0.0001
epoch6: train: loss:0.5921374190543309 	 acc:0.7193388429752066 | test: loss:0.5830958156396222 	 acc:0.7324503311258278 	 lr:0.0001
epoch7: train: loss:0.5648843654325185 	 acc:0.7180165289256198 | test: loss:0.5468103194868328 	 acc:0.7311258278145696 	 lr:0.0001
epoch8: train: loss:0.5610241437943514 	 acc:0.7576859504132232 | test: loss:0.5400045283582826 	 acc:0.7562913907284768 	 lr:0.0001
epoch9: train: loss:0.5651619379382489 	 acc:0.7454545454545455 | test: loss:0.5573719625441443 	 acc:0.743046357615894 	 lr:0.0001
epoch10: train: loss:0.5446285838726138 	 acc:0.76 | test: loss:0.535720416725866 	 acc:0.7642384105960265 	 lr:0.0001
epoch11: train: loss:0.5349377110378801 	 acc:0.7563636363636363 | test: loss:0.5227699961883343 	 acc:0.7682119205298014 	 lr:0.0001
epoch12: train: loss:0.5433832588865737 	 acc:0.7457851239669422 | test: loss:0.5395763323796506 	 acc:0.7390728476821192 	 lr:0.0001
epoch13: train: loss:0.5855618546225808 	 acc:0.6142148760330578 | test: loss:0.598147116907385 	 acc:0.6052980132450331 	 lr:0.0001
epoch14: train: loss:0.5390947158277527 	 acc:0.771900826446281 | test: loss:0.5307022113673734 	 acc:0.7562913907284768 	 lr:0.0001
epoch15: train: loss:0.515251295822711 	 acc:0.7917355371900826 | test: loss:0.5207941122402419 	 acc:0.752317880794702 	 lr:0.0001
epoch16: train: loss:0.5305599556875623 	 acc:0.7970247933884298 | test: loss:0.5487698162628325 	 acc:0.7509933774834437 	 lr:0.0001
epoch17: train: loss:0.5667257225414938 	 acc:0.7580165289256199 | test: loss:0.608440064515499 	 acc:0.7006622516556291 	 lr:0.0001
epoch18: train: loss:0.5208386743561295 	 acc:0.8092561983471075 | test: loss:0.544687931427103 	 acc:0.7708609271523179 	 lr:0.0001
epoch19: train: loss:0.552504796351283 	 acc:0.6591735537190082 | test: loss:0.5844849158596519 	 acc:0.6304635761589404 	 lr:0.0001
epoch20: train: loss:0.49837038789898896 	 acc:0.7980165289256198 | test: loss:0.5342587506534248 	 acc:0.7284768211920529 	 lr:0.0001
epoch21: train: loss:0.488631243025961 	 acc:0.8330578512396695 | test: loss:0.5157137030007823 	 acc:0.7920529801324503 	 lr:0.0001
epoch22: train: loss:0.4946393060487164 	 acc:0.8171900826446281 | test: loss:0.5407522550481834 	 acc:0.7708609271523179 	 lr:0.0001
epoch23: train: loss:0.4933152921436247 	 acc:0.7864462809917355 | test: loss:0.5269166419048184 	 acc:0.7364238410596027 	 lr:0.0001
epoch24: train: loss:0.48429981464196825 	 acc:0.8138842975206612 | test: loss:0.526440263268174 	 acc:0.7629139072847683 	 lr:0.0001
epoch25: train: loss:0.4625721668507442 	 acc:0.8290909090909091 | test: loss:0.510898159039731 	 acc:0.7721854304635761 	 lr:0.0001
epoch26: train: loss:0.5110282317074862 	 acc:0.7322314049586777 | test: loss:0.5428190801317329 	 acc:0.7059602649006622 	 lr:0.0001
epoch27: train: loss:0.5021812351577538 	 acc:0.8069421487603305 | test: loss:0.5710359323893162 	 acc:0.7403973509933774 	 lr:0.0001
epoch28: train: loss:0.5097446535540021 	 acc:0.8204958677685951 | test: loss:0.5989620080057358 	 acc:0.7218543046357616 	 lr:0.0001
epoch29: train: loss:0.5302725767892255 	 acc:0.7993388429752066 | test: loss:0.6101330435039192 	 acc:0.7125827814569536 	 lr:0.0001
epoch30: train: loss:0.47457345536917694 	 acc:0.8499173553719008 | test: loss:0.5470085222989518 	 acc:0.7655629139072848 	 lr:0.0001
epoch31: train: loss:0.44607845836434484 	 acc:0.8733884297520661 | test: loss:0.5234483967553701 	 acc:0.766887417218543 	 lr:0.0001
epoch32: train: loss:0.4564345865505786 	 acc:0.812892561983471 | test: loss:0.524356908198224 	 acc:0.7403973509933774 	 lr:5e-05
epoch33: train: loss:0.4345512462056373 	 acc:0.8644628099173554 | test: loss:0.5262878979278716 	 acc:0.7390728476821192 	 lr:5e-05
epoch34: train: loss:0.4211124781439127 	 acc:0.8869421487603306 | test: loss:0.5160629546405464 	 acc:0.7629139072847683 	 lr:5e-05
epoch35: train: loss:0.4257714543066734 	 acc:0.8760330578512396 | test: loss:0.5143899417870882 	 acc:0.7496688741721854 	 lr:5e-05
epoch36: train: loss:0.41946503248096495 	 acc:0.9107438016528926 | test: loss:0.5347630752632949 	 acc:0.7602649006622516 	 lr:5e-05
epoch37: train: loss:0.46424578406594014 	 acc:0.8614876033057851 | test: loss:0.5813828867002828 	 acc:0.7350993377483444 	 lr:5e-05
epoch38: train: loss:0.40230771722872394 	 acc:0.9150413223140496 | test: loss:0.5088776635018406 	 acc:0.7708609271523179 	 lr:2.5e-05
epoch39: train: loss:0.404187711940324 	 acc:0.9133884297520661 | test: loss:0.5256718808452026 	 acc:0.766887417218543 	 lr:2.5e-05
epoch40: train: loss:0.4042461045320369 	 acc:0.9044628099173554 | test: loss:0.5047880731671062 	 acc:0.7721854304635761 	 lr:2.5e-05
epoch41: train: loss:0.3955534278361265 	 acc:0.9332231404958677 | test: loss:0.5149611197560039 	 acc:0.7814569536423841 	 lr:2.5e-05
epoch42: train: loss:0.39548789767194387 	 acc:0.9203305785123967 | test: loss:0.5120857582976487 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch43: train: loss:0.39606133850152825 	 acc:0.9190082644628099 | test: loss:0.5124899737882298 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch44: train: loss:0.39862962302097604 	 acc:0.9262809917355372 | test: loss:0.5270906472837689 	 acc:0.776158940397351 	 lr:2.5e-05
epoch45: train: loss:0.3996561767641178 	 acc:0.9190082644628099 | test: loss:0.5257439183873056 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch46: train: loss:0.3961159187506053 	 acc:0.9170247933884298 | test: loss:0.511808537331638 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch47: train: loss:0.3899025854985576 	 acc:0.9292561983471075 | test: loss:0.5131757413314668 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch48: train: loss:0.3949955069329128 	 acc:0.923305785123967 | test: loss:0.5165323278761857 	 acc:0.7708609271523179 	 lr:1.25e-05
epoch49: train: loss:0.3892716807077739 	 acc:0.9282644628099174 | test: loss:0.5110048856166814 	 acc:0.7774834437086092 	 lr:1.25e-05
epoch50: train: loss:0.3876196011925532 	 acc:0.9295867768595041 | test: loss:0.5089596339408925 	 acc:0.776158940397351 	 lr:1.25e-05
epoch51: train: loss:0.3869548779282688 	 acc:0.936198347107438 | test: loss:0.5191342690133101 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch52: train: loss:0.38997980604487015 	 acc:0.9219834710743802 | test: loss:0.5172390764122767 	 acc:0.7695364238410596 	 lr:1.25e-05
epoch53: train: loss:0.3942478730166254 	 acc:0.9176859504132231 | test: loss:0.5147700308963953 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch54: train: loss:0.3853296903834855 	 acc:0.931900826446281 | test: loss:0.5175159361188775 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch55: train: loss:0.38602041218891614 	 acc:0.9322314049586777 | test: loss:0.5106727298521838 	 acc:0.7774834437086092 	 lr:6.25e-06
epoch56: train: loss:0.3833364329259258 	 acc:0.9348760330578513 | test: loss:0.5149712998345988 	 acc:0.7748344370860927 	 lr:6.25e-06
epoch57: train: loss:0.382665300763343 	 acc:0.9414876033057851 | test: loss:0.5181862358225892 	 acc:0.7695364238410596 	 lr:6.25e-06
epoch58: train: loss:0.3884516083406023 	 acc:0.9285950413223141 | test: loss:0.5132717659141843 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch59: train: loss:0.3849541039801826 	 acc:0.932892561983471 | test: loss:0.5150017942813848 	 acc:0.7721854304635761 	 lr:3.125e-06
epoch60: train: loss:0.38192257712695227 	 acc:0.9371900826446281 | test: loss:0.5135573187411226 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch61: train: loss:0.3836394110789969 	 acc:0.9342148760330579 | test: loss:0.5134779008019049 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch62: train: loss:0.3874091638316793 	 acc:0.9216528925619835 | test: loss:0.510561431717399 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch63: train: loss:0.38341244356691345 	 acc:0.9381818181818182 | test: loss:0.5097628294237402 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch64: train: loss:0.38299589175823306 	 acc:0.935206611570248 | test: loss:0.5120585509483387 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch65: train: loss:0.3830229166026943 	 acc:0.9325619834710743 | test: loss:0.5140670918470976 	 acc:0.7721854304635761 	 lr:1.5625e-06
epoch66: train: loss:0.381386163668199 	 acc:0.9378512396694215 | test: loss:0.5125983466375742 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch67: train: loss:0.3820219864628532 	 acc:0.9368595041322314 | test: loss:0.512251127081991 	 acc:0.7788079470198676 	 lr:1.5625e-06
epoch68: train: loss:0.3797641965278909 	 acc:0.943801652892562 | test: loss:0.5136446247827139 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch69: train: loss:0.38856408018711186 	 acc:0.9269421487603305 | test: loss:0.5131863124323207 	 acc:0.7735099337748345 	 lr:1.5625e-06
epoch70: train: loss:0.3826627910925337 	 acc:0.9325619834710743 | test: loss:0.5105817810589115 	 acc:0.7721854304635761 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_14_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_14_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6714672817080474 	 acc:0.5705785123966942 | test: loss:0.6648736778474011 	 acc:0.6052980132450331 	 lr:0.0001
epoch1: train: loss:0.6379115986232915 	 acc:0.5328925619834711 | test: loss:0.6360612959261761 	 acc:0.5271523178807948 	 lr:0.0001
epoch2: train: loss:0.6215170121784053 	 acc:0.6809917355371901 | test: loss:0.6225718769016645 	 acc:0.6821192052980133 	 lr:0.0001
epoch3: train: loss:0.6021263379301907 	 acc:0.7031404958677686 | test: loss:0.602116585646244 	 acc:0.7099337748344371 	 lr:0.0001
epoch4: train: loss:0.5751280275455191 	 acc:0.6971900826446281 | test: loss:0.5595699737403567 	 acc:0.7403973509933774 	 lr:0.0001
epoch5: train: loss:0.5750403196358483 	 acc:0.7193388429752066 | test: loss:0.5574207490643128 	 acc:0.7390728476821192 	 lr:0.0001
epoch6: train: loss:0.5723620153852731 	 acc:0.7461157024793389 | test: loss:0.5597978464815001 	 acc:0.7509933774834437 	 lr:0.0001
epoch7: train: loss:0.5611939694861735 	 acc:0.728595041322314 | test: loss:0.5465412581203789 	 acc:0.7205298013245033 	 lr:0.0001
epoch8: train: loss:0.5607988479708838 	 acc:0.7557024793388429 | test: loss:0.5428970625858434 	 acc:0.7615894039735099 	 lr:0.0001
epoch9: train: loss:0.5656492073476808 	 acc:0.7464462809917355 | test: loss:0.5562078652792419 	 acc:0.7456953642384105 	 lr:0.0001
epoch10: train: loss:0.537790295683648 	 acc:0.7517355371900827 | test: loss:0.5294426992239541 	 acc:0.7695364238410596 	 lr:0.0001
epoch11: train: loss:0.5270200338048383 	 acc:0.7464462809917355 | test: loss:0.5222978236659473 	 acc:0.7496688741721854 	 lr:0.0001
epoch12: train: loss:0.5349485530538007 	 acc:0.7818181818181819 | test: loss:0.5397718376671241 	 acc:0.7814569536423841 	 lr:0.0001
epoch13: train: loss:0.5581701035933061 	 acc:0.6611570247933884 | test: loss:0.567338867850651 	 acc:0.6437086092715232 	 lr:0.0001
epoch14: train: loss:0.5282275953371662 	 acc:0.7619834710743801 | test: loss:0.5345146585773948 	 acc:0.7245033112582782 	 lr:0.0001
epoch15: train: loss:0.5093135829208311 	 acc:0.8039669421487603 | test: loss:0.5233353709542988 	 acc:0.7589403973509934 	 lr:0.0001
epoch16: train: loss:0.4995788370380717 	 acc:0.8089256198347108 | test: loss:0.5180571676090063 	 acc:0.7735099337748345 	 lr:0.0001
epoch17: train: loss:0.5233889103132832 	 acc:0.8036363636363636 | test: loss:0.565267396762671 	 acc:0.7377483443708609 	 lr:0.0001
epoch18: train: loss:0.503907610483406 	 acc:0.8218181818181818 | test: loss:0.5417280558718751 	 acc:0.7589403973509934 	 lr:0.0001
epoch19: train: loss:0.49793049782760873 	 acc:0.7550413223140496 | test: loss:0.530358070332483 	 acc:0.7112582781456953 	 lr:0.0001
epoch20: train: loss:0.4992192526592696 	 acc:0.7649586776859504 | test: loss:0.5313955272270354 	 acc:0.7152317880794702 	 lr:0.0001
epoch21: train: loss:0.49428043061051485 	 acc:0.8277685950413223 | test: loss:0.5426652424382847 	 acc:0.7576158940397351 	 lr:0.0001
epoch22: train: loss:0.48061783330499636 	 acc:0.8439669421487603 | test: loss:0.5374915570612775 	 acc:0.7629139072847683 	 lr:0.0001
epoch23: train: loss:0.45044495988483274 	 acc:0.8608264462809917 | test: loss:0.5133072928877066 	 acc:0.7735099337748345 	 lr:5e-05
epoch24: train: loss:0.4578459822441921 	 acc:0.8677685950413223 | test: loss:0.5436543838077823 	 acc:0.7509933774834437 	 lr:5e-05
epoch25: train: loss:0.4375031038158196 	 acc:0.8819834710743801 | test: loss:0.5216844962922154 	 acc:0.7748344370860927 	 lr:5e-05
epoch26: train: loss:0.4411378997909136 	 acc:0.8889256198347107 | test: loss:0.5226318188850453 	 acc:0.7589403973509934 	 lr:5e-05
epoch27: train: loss:0.43910356695001773 	 acc:0.8624793388429752 | test: loss:0.5061840099214718 	 acc:0.7827814569536424 	 lr:5e-05
epoch28: train: loss:0.42976201435751166 	 acc:0.8935537190082644 | test: loss:0.5161580151280031 	 acc:0.7748344370860927 	 lr:5e-05
epoch29: train: loss:0.42974624609159046 	 acc:0.8905785123966942 | test: loss:0.5308405126956914 	 acc:0.7655629139072848 	 lr:5e-05
epoch30: train: loss:0.42748158145541987 	 acc:0.8935537190082644 | test: loss:0.5240910139304913 	 acc:0.7721854304635761 	 lr:5e-05
epoch31: train: loss:0.4106065138805011 	 acc:0.9018181818181819 | test: loss:0.5188653373560369 	 acc:0.7695364238410596 	 lr:5e-05
epoch32: train: loss:0.4418527323746484 	 acc:0.8393388429752067 | test: loss:0.5152466845828176 	 acc:0.7562913907284768 	 lr:5e-05
epoch33: train: loss:0.42441008712634565 	 acc:0.8945454545454545 | test: loss:0.5259711519771854 	 acc:0.7615894039735099 	 lr:5e-05
epoch34: train: loss:0.41292003111405806 	 acc:0.8971900826446281 | test: loss:0.5045780654774596 	 acc:0.776158940397351 	 lr:2.5e-05
epoch35: train: loss:0.4016686829358093 	 acc:0.9137190082644628 | test: loss:0.5018649199151045 	 acc:0.7841059602649006 	 lr:2.5e-05
epoch36: train: loss:0.40481887470592154 	 acc:0.9130578512396694 | test: loss:0.5170553010031087 	 acc:0.7841059602649006 	 lr:2.5e-05
epoch37: train: loss:0.4042021550028777 	 acc:0.92 | test: loss:0.5121368251888957 	 acc:0.7774834437086092 	 lr:2.5e-05
epoch38: train: loss:0.39903493410299634 	 acc:0.9084297520661156 | test: loss:0.49833076560734124 	 acc:0.7774834437086092 	 lr:2.5e-05
epoch39: train: loss:0.4020144166631147 	 acc:0.9170247933884298 | test: loss:0.5382026444207754 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch40: train: loss:0.4022960062657506 	 acc:0.8995041322314049 | test: loss:0.4976193677510647 	 acc:0.7867549668874172 	 lr:2.5e-05
epoch41: train: loss:0.39332269821285215 	 acc:0.9285950413223141 | test: loss:0.5108256253185651 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch42: train: loss:0.39015736957226904 	 acc:0.9193388429752066 | test: loss:0.511158196420859 	 acc:0.7814569536423841 	 lr:2.5e-05
epoch43: train: loss:0.39220007021565084 	 acc:0.9262809917355372 | test: loss:0.5160492039674165 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch44: train: loss:0.3860417360904788 	 acc:0.9305785123966942 | test: loss:0.5108448199089 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch45: train: loss:0.3929624163710381 	 acc:0.9153719008264463 | test: loss:0.5102705004199451 	 acc:0.7721854304635761 	 lr:2.5e-05
epoch46: train: loss:0.38931719384902763 	 acc:0.9345454545454546 | test: loss:0.5185895221912308 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch47: train: loss:0.3850717514704082 	 acc:0.9371900826446281 | test: loss:0.5103003895835371 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch48: train: loss:0.39054718228411084 	 acc:0.932892561983471 | test: loss:0.5223890925085308 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch49: train: loss:0.3837150256988431 	 acc:0.9305785123966942 | test: loss:0.5044607007740349 	 acc:0.776158940397351 	 lr:1.25e-05
epoch50: train: loss:0.3827315961427925 	 acc:0.9322314049586777 | test: loss:0.502746646846367 	 acc:0.7814569536423841 	 lr:1.25e-05
epoch51: train: loss:0.38355082546383884 	 acc:0.9348760330578513 | test: loss:0.5216976461821045 	 acc:0.776158940397351 	 lr:1.25e-05
epoch52: train: loss:0.3841363188649012 	 acc:0.9309090909090909 | test: loss:0.50517899847978 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch53: train: loss:0.38989260173040974 	 acc:0.9223140495867769 | test: loss:0.5112059747146455 	 acc:0.7748344370860927 	 lr:6.25e-06
epoch54: train: loss:0.38034995752917833 	 acc:0.9434710743801653 | test: loss:0.51379143710168 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch55: train: loss:0.38315800753506746 	 acc:0.9305785123966942 | test: loss:0.5040696267260621 	 acc:0.7801324503311259 	 lr:6.25e-06
epoch56: train: loss:0.3811947417259216 	 acc:0.9368595041322314 | test: loss:0.5172103600786222 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch57: train: loss:0.37973344425524563 	 acc:0.9368595041322314 | test: loss:0.5102064840051512 	 acc:0.7774834437086092 	 lr:6.25e-06
epoch58: train: loss:0.386101102178747 	 acc:0.9252892561983471 | test: loss:0.5021542617816799 	 acc:0.7894039735099337 	 lr:6.25e-06
epoch59: train: loss:0.379749892930354 	 acc:0.936198347107438 | test: loss:0.5071549946109191 	 acc:0.776158940397351 	 lr:3.125e-06
epoch60: train: loss:0.37773134344865467 	 acc:0.9395041322314049 | test: loss:0.5078287265158647 	 acc:0.7748344370860927 	 lr:3.125e-06
epoch61: train: loss:0.3812129100295138 	 acc:0.9365289256198347 | test: loss:0.5069807846025126 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch62: train: loss:0.3823727638169754 	 acc:0.9322314049586777 | test: loss:0.5024816700954311 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch63: train: loss:0.3782564508422347 	 acc:0.9414876033057851 | test: loss:0.4995702456954299 	 acc:0.7920529801324503 	 lr:3.125e-06
epoch64: train: loss:0.37758534894501866 	 acc:0.9424793388429752 | test: loss:0.5078326467646669 	 acc:0.7801324503311259 	 lr:3.125e-06
epoch65: train: loss:0.38045565143104426 	 acc:0.935206611570248 | test: loss:0.5049132313159918 	 acc:0.7814569536423841 	 lr:1.5625e-06
epoch66: train: loss:0.3783240985574801 	 acc:0.931900826446281 | test: loss:0.5041125424650331 	 acc:0.7827814569536424 	 lr:1.5625e-06
epoch67: train: loss:0.3765880798800918 	 acc:0.9378512396694215 | test: loss:0.5049571624654807 	 acc:0.7814569536423841 	 lr:1.5625e-06
epoch68: train: loss:0.375076242705022 	 acc:0.9441322314049587 | test: loss:0.5060586339590565 	 acc:0.7827814569536424 	 lr:1.5625e-06
epoch69: train: loss:0.38348861330796863 	 acc:0.9338842975206612 | test: loss:0.506952415317889 	 acc:0.7814569536423841 	 lr:1.5625e-06
epoch70: train: loss:0.3784833149575005 	 acc:0.9385123966942148 | test: loss:0.5052487894399277 	 acc:0.785430463576159 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_15_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_15_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7209267987889691 	 acc:0.5256198347107438 | test: loss:0.7263428851468674 	 acc:0.528476821192053 	 lr:0.0001
epoch1: train: loss:0.6299079032574804 	 acc:0.617190082644628 | test: loss:0.6165278722908323 	 acc:0.6132450331125828 	 lr:0.0001
epoch2: train: loss:0.6127954258012378 	 acc:0.6112396694214876 | test: loss:0.6060072728340199 	 acc:0.5986754966887418 	 lr:0.0001
epoch3: train: loss:0.5907134097272699 	 acc:0.6690909090909091 | test: loss:0.5740514730775593 	 acc:0.7059602649006622 	 lr:0.0001
epoch4: train: loss:0.5762691525191315 	 acc:0.6730578512396694 | test: loss:0.5596123126168914 	 acc:0.7072847682119205 	 lr:0.0001
epoch5: train: loss:0.5745297738343231 	 acc:0.7388429752066116 | test: loss:0.5560317783166241 	 acc:0.7417218543046358 	 lr:0.0001
epoch6: train: loss:0.5872079899094321 	 acc:0.6340495867768595 | test: loss:0.5921431501969596 	 acc:0.6158940397350994 	 lr:0.0001
epoch7: train: loss:0.5607520182270649 	 acc:0.6998347107438017 | test: loss:0.5554363377836367 	 acc:0.7059602649006622 	 lr:0.0001
epoch8: train: loss:0.5506527864834494 	 acc:0.7652892561983471 | test: loss:0.5548374155499288 	 acc:0.743046357615894 	 lr:0.0001
epoch9: train: loss:0.5396583351024912 	 acc:0.7689256198347107 | test: loss:0.5482695214006285 	 acc:0.7536423841059603 	 lr:0.0001
epoch10: train: loss:0.5451901995446071 	 acc:0.7583471074380165 | test: loss:0.5422244423272594 	 acc:0.7456953642384105 	 lr:0.0001
epoch11: train: loss:0.5312646195317103 	 acc:0.7408264462809917 | test: loss:0.5456806162335225 	 acc:0.7112582781456953 	 lr:0.0001
epoch12: train: loss:0.5438106587898632 	 acc:0.7031404958677686 | test: loss:0.5550456812839635 	 acc:0.6927152317880795 	 lr:0.0001
epoch13: train: loss:0.5266496338134955 	 acc:0.7907438016528926 | test: loss:0.5418144895540957 	 acc:0.7562913907284768 	 lr:0.0001
epoch14: train: loss:0.5349013888343307 	 acc:0.7299173553719008 | test: loss:0.5503805792094856 	 acc:0.704635761589404 	 lr:0.0001
epoch15: train: loss:0.521571369663743 	 acc:0.7418181818181818 | test: loss:0.5348758339881897 	 acc:0.713907284768212 	 lr:0.0001
epoch16: train: loss:0.5169391613164224 	 acc:0.8026446280991736 | test: loss:0.5395749973935007 	 acc:0.7549668874172185 	 lr:0.0001
epoch17: train: loss:0.4976485575821774 	 acc:0.7900826446280992 | test: loss:0.5249652587814836 	 acc:0.743046357615894 	 lr:0.0001
epoch18: train: loss:0.497754972532761 	 acc:0.8224793388429752 | test: loss:0.5295978171935934 	 acc:0.7311258278145696 	 lr:0.0001
epoch19: train: loss:0.4985957570410957 	 acc:0.7811570247933884 | test: loss:0.5362643078462968 	 acc:0.7271523178807947 	 lr:0.0001
epoch20: train: loss:0.49519799308343365 	 acc:0.8277685950413223 | test: loss:0.547326864785706 	 acc:0.7377483443708609 	 lr:0.0001
epoch21: train: loss:0.4824831718748266 	 acc:0.8310743801652892 | test: loss:0.5348760658542052 	 acc:0.7483443708609272 	 lr:0.0001
epoch22: train: loss:0.4897625186817705 	 acc:0.8257851239669421 | test: loss:0.5238207402608253 	 acc:0.7509933774834437 	 lr:0.0001
epoch23: train: loss:0.5285256657718627 	 acc:0.7100826446280992 | test: loss:0.5654263333769034 	 acc:0.6609271523178808 	 lr:0.0001
epoch24: train: loss:0.47305276428372406 	 acc:0.8294214876033058 | test: loss:0.5203062684330719 	 acc:0.7483443708609272 	 lr:0.0001
epoch25: train: loss:0.47193253682664604 	 acc:0.8446280991735537 | test: loss:0.5252081301038628 	 acc:0.752317880794702 	 lr:0.0001
epoch26: train: loss:0.4686558595275091 	 acc:0.84 | test: loss:0.521647061654274 	 acc:0.7509933774834437 	 lr:0.0001
epoch27: train: loss:0.4635416436589454 	 acc:0.8456198347107438 | test: loss:0.5299611144507957 	 acc:0.7562913907284768 	 lr:0.0001
epoch28: train: loss:0.46219300056292006 	 acc:0.8393388429752067 | test: loss:0.5240333158448832 	 acc:0.7443708609271523 	 lr:0.0001
epoch29: train: loss:0.45460591159576225 	 acc:0.8571900826446281 | test: loss:0.5258657794914499 	 acc:0.7562913907284768 	 lr:0.0001
epoch30: train: loss:0.44647075390027574 	 acc:0.8704132231404959 | test: loss:0.5287452627491477 	 acc:0.7642384105960265 	 lr:0.0001
epoch31: train: loss:0.4430587158419869 	 acc:0.8548760330578512 | test: loss:0.518303033768736 	 acc:0.7629139072847683 	 lr:5e-05
epoch32: train: loss:0.4360083403469117 	 acc:0.8895867768595042 | test: loss:0.5472532967857967 	 acc:0.7470198675496689 	 lr:5e-05
epoch33: train: loss:0.4300312820938993 	 acc:0.8823140495867768 | test: loss:0.5076005588304128 	 acc:0.7827814569536424 	 lr:5e-05
epoch34: train: loss:0.42276390781087325 	 acc:0.8971900826446281 | test: loss:0.5219152088196862 	 acc:0.7748344370860927 	 lr:5e-05
epoch35: train: loss:0.41571784420446917 	 acc:0.8859504132231405 | test: loss:0.5128398774475451 	 acc:0.7629139072847683 	 lr:5e-05
epoch36: train: loss:0.443820421380445 	 acc:0.8826446280991735 | test: loss:0.5407340065532962 	 acc:0.7615894039735099 	 lr:5e-05
epoch37: train: loss:0.4321485781768137 	 acc:0.8905785123966942 | test: loss:0.5703115986672458 	 acc:0.7417218543046358 	 lr:5e-05
epoch38: train: loss:0.436599411284628 	 acc:0.8856198347107438 | test: loss:0.550575159322347 	 acc:0.7549668874172185 	 lr:5e-05
epoch39: train: loss:0.41092208112566925 	 acc:0.8995041322314049 | test: loss:0.5171620059487047 	 acc:0.7708609271523179 	 lr:5e-05
epoch40: train: loss:0.4104290196127143 	 acc:0.9137190082644628 | test: loss:0.5272662394094151 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch41: train: loss:0.4036199455123302 	 acc:0.9203305785123967 | test: loss:0.5229966432053522 	 acc:0.776158940397351 	 lr:2.5e-05
epoch42: train: loss:0.397638698205475 	 acc:0.9160330578512397 | test: loss:0.5048197016021274 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch43: train: loss:0.40212226924817424 	 acc:0.912396694214876 | test: loss:0.5111271896109676 	 acc:0.7721854304635761 	 lr:2.5e-05
epoch44: train: loss:0.40179341771385885 	 acc:0.9107438016528926 | test: loss:0.504041365992944 	 acc:0.7894039735099337 	 lr:2.5e-05
epoch45: train: loss:0.40299623785925304 	 acc:0.9133884297520661 | test: loss:0.5281531577078712 	 acc:0.7695364238410596 	 lr:2.5e-05
epoch46: train: loss:0.4027508899889702 	 acc:0.8955371900826447 | test: loss:0.5091919000575085 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch47: train: loss:0.3925318313236079 	 acc:0.9153719008264463 | test: loss:0.5151126106843253 	 acc:0.7774834437086092 	 lr:2.5e-05
epoch48: train: loss:0.38884806657625626 	 acc:0.931900826446281 | test: loss:0.5200980435933499 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch49: train: loss:0.39162486390634016 	 acc:0.9246280991735537 | test: loss:0.5225222027854414 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch50: train: loss:0.39504967733848195 	 acc:0.9170247933884298 | test: loss:0.5135571243747181 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch51: train: loss:0.3832103354970286 	 acc:0.9371900826446281 | test: loss:0.5129496735452816 	 acc:0.7841059602649006 	 lr:1.25e-05
epoch52: train: loss:0.38616560089686686 	 acc:0.9325619834710743 | test: loss:0.5145906614941477 	 acc:0.7748344370860927 	 lr:1.25e-05
epoch53: train: loss:0.3920420195248501 	 acc:0.9269421487603305 | test: loss:0.5188213693385093 	 acc:0.7801324503311259 	 lr:1.25e-05
epoch54: train: loss:0.3885890824538617 	 acc:0.9279338842975207 | test: loss:0.5130255441791964 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch55: train: loss:0.3877881436111513 	 acc:0.9332231404958677 | test: loss:0.5281108318575171 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch56: train: loss:0.3818259241068659 	 acc:0.9295867768595041 | test: loss:0.5119682459641766 	 acc:0.7814569536423841 	 lr:1.25e-05
epoch57: train: loss:0.3895553096069777 	 acc:0.923305785123967 | test: loss:0.5090146013837776 	 acc:0.785430463576159 	 lr:6.25e-06
epoch58: train: loss:0.3843014155439109 	 acc:0.9285950413223141 | test: loss:0.5123646499305371 	 acc:0.7814569536423841 	 lr:6.25e-06
epoch59: train: loss:0.3842640577761595 	 acc:0.932892561983471 | test: loss:0.5079294944441082 	 acc:0.7801324503311259 	 lr:6.25e-06
epoch60: train: loss:0.38355541428258594 	 acc:0.9338842975206612 | test: loss:0.5133065681583834 	 acc:0.7894039735099337 	 lr:6.25e-06
epoch61: train: loss:0.3798326560781022 	 acc:0.9385123966942148 | test: loss:0.51004136571821 	 acc:0.7827814569536424 	 lr:6.25e-06
epoch62: train: loss:0.3829541210497706 	 acc:0.9309090909090909 | test: loss:0.5064873867477013 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch63: train: loss:0.38136537971575396 	 acc:0.9388429752066115 | test: loss:0.5080312841775402 	 acc:0.7841059602649006 	 lr:3.125e-06
epoch64: train: loss:0.38256426868359905 	 acc:0.9315702479338843 | test: loss:0.5112269902071416 	 acc:0.7814569536423841 	 lr:3.125e-06
epoch65: train: loss:0.3844932501769263 	 acc:0.9312396694214876 | test: loss:0.5102484043070812 	 acc:0.7827814569536424 	 lr:3.125e-06
epoch66: train: loss:0.37876877040902446 	 acc:0.9391735537190082 | test: loss:0.5100122436782382 	 acc:0.7788079470198676 	 lr:3.125e-06
epoch67: train: loss:0.37615510226281224 	 acc:0.9371900826446281 | test: loss:0.5084238197629815 	 acc:0.7841059602649006 	 lr:3.125e-06
epoch68: train: loss:0.3770486071582668 	 acc:0.9398347107438016 | test: loss:0.5121952300040138 	 acc:0.7841059602649006 	 lr:3.125e-06
epoch69: train: loss:0.37978827285372524 	 acc:0.9335537190082644 | test: loss:0.5092224371354311 	 acc:0.785430463576159 	 lr:1.5625e-06
epoch70: train: loss:0.37683205662680064 	 acc:0.9391735537190082 | test: loss:0.5066451810448375 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch71: train: loss:0.37927569411017675 	 acc:0.9385123966942148 | test: loss:0.5070907224093052 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch72: train: loss:0.37720958452579406 	 acc:0.9395041322314049 | test: loss:0.5078582855644606 	 acc:0.7814569536423841 	 lr:1.5625e-06
epoch73: train: loss:0.38081232288652217 	 acc:0.9345454545454546 | test: loss:0.5062429722571216 	 acc:0.7801324503311259 	 lr:1.5625e-06
epoch74: train: loss:0.3810552450250988 	 acc:0.9332231404958677 | test: loss:0.508187208270395 	 acc:0.7801324503311259 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_16_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_16_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7003203325823318 	 acc:0.5715702479338843 | test: loss:0.7192784303860948 	 acc:0.5629139072847682 	 lr:0.0001
epoch1: train: loss:0.6235353584131919 	 acc:0.5692561983471074 | test: loss:0.6282282706128051 	 acc:0.5483443708609271 	 lr:0.0001
epoch2: train: loss:0.6026166226253037 	 acc:0.6449586776859504 | test: loss:0.5901386879927275 	 acc:0.6556291390728477 	 lr:0.0001
epoch3: train: loss:0.5862627666252703 	 acc:0.6614876033057852 | test: loss:0.577263544963685 	 acc:0.6675496688741722 	 lr:0.0001
epoch4: train: loss:0.5874097415238373 	 acc:0.6320661157024794 | test: loss:0.587108858531674 	 acc:0.6304635761589404 	 lr:0.0001
epoch5: train: loss:0.6045262908935547 	 acc:0.6998347107438017 | test: loss:0.5957401281161024 	 acc:0.7072847682119205 	 lr:0.0001
epoch6: train: loss:0.5567328762416998 	 acc:0.7117355371900826 | test: loss:0.5511824151538066 	 acc:0.704635761589404 	 lr:0.0001
epoch7: train: loss:0.5499788646067469 	 acc:0.6985123966942148 | test: loss:0.5538316803262723 	 acc:0.6993377483443709 	 lr:0.0001
epoch8: train: loss:0.5553517895099546 	 acc:0.7550413223140496 | test: loss:0.5707760502960508 	 acc:0.7298013245033113 	 lr:0.0001
epoch9: train: loss:0.535406660560734 	 acc:0.7428099173553719 | test: loss:0.5487990965906357 	 acc:0.7245033112582782 	 lr:0.0001
epoch10: train: loss:0.5314577142266202 	 acc:0.751404958677686 | test: loss:0.545306125302978 	 acc:0.7403973509933774 	 lr:0.0001
epoch11: train: loss:0.5404324136686719 	 acc:0.6965289256198347 | test: loss:0.569846525255418 	 acc:0.6490066225165563 	 lr:0.0001
epoch12: train: loss:0.5211434453380994 	 acc:0.7818181818181819 | test: loss:0.5529841716715832 	 acc:0.743046357615894 	 lr:0.0001
epoch13: train: loss:0.5102478161626611 	 acc:0.7884297520661157 | test: loss:0.5272983682866128 	 acc:0.7695364238410596 	 lr:0.0001
epoch14: train: loss:0.532971063625714 	 acc:0.7256198347107438 | test: loss:0.5522165306356569 	 acc:0.680794701986755 	 lr:0.0001
epoch15: train: loss:0.5021922880361888 	 acc:0.7765289256198347 | test: loss:0.5218311009817566 	 acc:0.7496688741721854 	 lr:0.0001
epoch16: train: loss:0.49810378790886933 	 acc:0.7980165289256198 | test: loss:0.5321088676421059 	 acc:0.7509933774834437 	 lr:0.0001
epoch17: train: loss:0.4987967684643328 	 acc:0.7861157024793388 | test: loss:0.5317049400695902 	 acc:0.7403973509933774 	 lr:0.0001
epoch18: train: loss:0.4856759991724629 	 acc:0.827107438016529 | test: loss:0.5287386429230898 	 acc:0.7443708609271523 	 lr:0.0001
epoch19: train: loss:0.4727713370224661 	 acc:0.8188429752066115 | test: loss:0.5216341681827773 	 acc:0.7576158940397351 	 lr:0.0001
epoch20: train: loss:0.47904687774082844 	 acc:0.8145454545454546 | test: loss:0.5271552681922913 	 acc:0.7443708609271523 	 lr:0.0001
epoch21: train: loss:0.4871181332079832 	 acc:0.823801652892562 | test: loss:0.5446190222999118 	 acc:0.743046357615894 	 lr:0.0001
epoch22: train: loss:0.48299696983384693 	 acc:0.8171900826446281 | test: loss:0.532638716855586 	 acc:0.7271523178807947 	 lr:0.0001
epoch23: train: loss:0.4929370455308394 	 acc:0.7775206611570248 | test: loss:0.5384328231116794 	 acc:0.713907284768212 	 lr:0.0001
epoch24: train: loss:0.49606909345004185 	 acc:0.7738842975206611 | test: loss:0.5437236563259402 	 acc:0.6940397350993377 	 lr:0.0001
epoch25: train: loss:0.46467637167489234 	 acc:0.8495867768595041 | test: loss:0.517811493763071 	 acc:0.7655629139072848 	 lr:0.0001
epoch26: train: loss:0.4671355308875565 	 acc:0.8211570247933885 | test: loss:0.522176189296293 	 acc:0.7390728476821192 	 lr:0.0001
epoch27: train: loss:0.45886251766819597 	 acc:0.8317355371900826 | test: loss:0.5252982051167268 	 acc:0.7337748344370861 	 lr:0.0001
epoch28: train: loss:0.45777469018274103 	 acc:0.8436363636363636 | test: loss:0.5214830807502696 	 acc:0.7496688741721854 	 lr:0.0001
epoch29: train: loss:0.4481923417611556 	 acc:0.8638016528925619 | test: loss:0.5332554904830377 	 acc:0.7417218543046358 	 lr:0.0001
epoch30: train: loss:0.4523216928923426 	 acc:0.8284297520661157 | test: loss:0.5189967463348085 	 acc:0.7562913907284768 	 lr:0.0001
epoch31: train: loss:0.43885291090681533 	 acc:0.8763636363636363 | test: loss:0.5289080823494109 	 acc:0.7549668874172185 	 lr:0.0001
epoch32: train: loss:0.4274427886639745 	 acc:0.8922314049586777 | test: loss:0.5488898433596883 	 acc:0.7483443708609272 	 lr:5e-05
epoch33: train: loss:0.42031619277867405 	 acc:0.900495867768595 | test: loss:0.5086410351936391 	 acc:0.7748344370860927 	 lr:5e-05
epoch34: train: loss:0.4206247205675141 	 acc:0.9031404958677686 | test: loss:0.5279307880938447 	 acc:0.7695364238410596 	 lr:5e-05
epoch35: train: loss:0.4167163593315881 	 acc:0.8790082644628099 | test: loss:0.51452768409489 	 acc:0.7682119205298014 	 lr:5e-05
epoch36: train: loss:0.43465682397204 	 acc:0.8938842975206611 | test: loss:0.5475712734342411 	 acc:0.7642384105960265 	 lr:5e-05
epoch37: train: loss:0.4019034414350494 	 acc:0.9196694214876033 | test: loss:0.5167092507248683 	 acc:0.7774834437086092 	 lr:5e-05
epoch38: train: loss:0.4136079776287079 	 acc:0.8958677685950414 | test: loss:0.5078780999246812 	 acc:0.7748344370860927 	 lr:5e-05
epoch39: train: loss:0.40202796818796266 	 acc:0.9077685950413223 | test: loss:0.5124399152023114 	 acc:0.776158940397351 	 lr:5e-05
epoch40: train: loss:0.4150153634173811 	 acc:0.9100826446280992 | test: loss:0.5272435341449763 	 acc:0.776158940397351 	 lr:5e-05
epoch41: train: loss:0.406185257474253 	 acc:0.9130578512396694 | test: loss:0.4972649059548283 	 acc:0.7986754966887417 	 lr:5e-05
epoch42: train: loss:0.40747962939837745 	 acc:0.8892561983471075 | test: loss:0.5050737065984713 	 acc:0.7721854304635761 	 lr:5e-05
epoch43: train: loss:0.40761030570534634 	 acc:0.8985123966942149 | test: loss:0.5255825994819995 	 acc:0.7589403973509934 	 lr:5e-05
epoch44: train: loss:0.4334499957147709 	 acc:0.8466115702479339 | test: loss:0.5439724944285209 	 acc:0.7072847682119205 	 lr:5e-05
epoch45: train: loss:0.41456588529358224 	 acc:0.8829752066115703 | test: loss:0.5203467076187892 	 acc:0.7562913907284768 	 lr:5e-05
epoch46: train: loss:0.4160617690835117 	 acc:0.9084297520661156 | test: loss:0.5169058222644376 	 acc:0.7867549668874172 	 lr:5e-05
epoch47: train: loss:0.40872018624928375 	 acc:0.8889256198347107 | test: loss:0.5165711125001212 	 acc:0.7536423841059603 	 lr:5e-05
epoch48: train: loss:0.40596753872130525 	 acc:0.9173553719008265 | test: loss:0.5513053425100466 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch49: train: loss:0.38708430392683046 	 acc:0.9246280991735537 | test: loss:0.5036860678369636 	 acc:0.7788079470198676 	 lr:2.5e-05
epoch50: train: loss:0.3787083076938125 	 acc:0.9385123966942148 | test: loss:0.5091127419313848 	 acc:0.7841059602649006 	 lr:2.5e-05
epoch51: train: loss:0.3812454970907574 	 acc:0.9355371900826446 | test: loss:0.5030881730925958 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch52: train: loss:0.3904242437240506 	 acc:0.9368595041322314 | test: loss:0.535096618829184 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch53: train: loss:0.38124652434971706 	 acc:0.9368595041322314 | test: loss:0.5100172563893905 	 acc:0.7867549668874172 	 lr:2.5e-05
epoch54: train: loss:0.3780659023887855 	 acc:0.9335537190082644 | test: loss:0.5084797486564181 	 acc:0.7827814569536424 	 lr:1.25e-05
epoch55: train: loss:0.3813499439156745 	 acc:0.9299173553719008 | test: loss:0.5067482506992012 	 acc:0.7933774834437086 	 lr:1.25e-05
epoch56: train: loss:0.37862263054887124 	 acc:0.9398347107438016 | test: loss:0.517128550690531 	 acc:0.7841059602649006 	 lr:1.25e-05
epoch57: train: loss:0.3791440099428508 	 acc:0.936198347107438 | test: loss:0.5022934374430321 	 acc:0.785430463576159 	 lr:1.25e-05
epoch58: train: loss:0.37919033523433465 	 acc:0.9355371900826446 | test: loss:0.505907447527576 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch59: train: loss:0.3773877052137674 	 acc:0.9385123966942148 | test: loss:0.5084839400866173 	 acc:0.7814569536423841 	 lr:1.25e-05
epoch60: train: loss:0.3727725503661416 	 acc:0.9441322314049587 | test: loss:0.5022244821321096 	 acc:0.7894039735099337 	 lr:6.25e-06
epoch61: train: loss:0.3766061557225945 	 acc:0.9381818181818182 | test: loss:0.5003263585614842 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch62: train: loss:0.37439048275474673 	 acc:0.9398347107438016 | test: loss:0.5025501747794499 	 acc:0.785430463576159 	 lr:6.25e-06
epoch63: train: loss:0.3743765089236015 	 acc:0.9441322314049587 | test: loss:0.5053860713314536 	 acc:0.7867549668874172 	 lr:6.25e-06
epoch64: train: loss:0.37731002941604486 	 acc:0.9428099173553719 | test: loss:0.5090725213486628 	 acc:0.7814569536423841 	 lr:6.25e-06
epoch65: train: loss:0.3784523229559591 	 acc:0.935206611570248 | test: loss:0.504261656312753 	 acc:0.7933774834437086 	 lr:6.25e-06
epoch66: train: loss:0.37248907575922563 	 acc:0.9451239669421487 | test: loss:0.5046834568314205 	 acc:0.7841059602649006 	 lr:3.125e-06
epoch67: train: loss:0.3695123812876457 	 acc:0.9461157024793388 | test: loss:0.5043503067351335 	 acc:0.7814569536423841 	 lr:3.125e-06
epoch68: train: loss:0.37350412225920304 	 acc:0.9434710743801653 | test: loss:0.5069018400268049 	 acc:0.7880794701986755 	 lr:3.125e-06
epoch69: train: loss:0.3784105568089761 	 acc:0.9322314049586777 | test: loss:0.5056429784029525 	 acc:0.785430463576159 	 lr:3.125e-06
epoch70: train: loss:0.36752353781510977 	 acc:0.9461157024793388 | test: loss:0.5040124784242238 	 acc:0.7880794701986755 	 lr:3.125e-06
epoch71: train: loss:0.37122373446945317 	 acc:0.948099173553719 | test: loss:0.5045163303811029 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch72: train: loss:0.3695059945071039 	 acc:0.947107438016529 | test: loss:0.5051021965134223 	 acc:0.7880794701986755 	 lr:1.5625e-06
epoch73: train: loss:0.3743156517241612 	 acc:0.9411570247933885 | test: loss:0.5036060655353874 	 acc:0.7841059602649006 	 lr:1.5625e-06
epoch74: train: loss:0.3706038954041221 	 acc:0.944793388429752 | test: loss:0.503879087176544 	 acc:0.7907284768211921 	 lr:1.5625e-06
epoch75: train: loss:0.3732824692450279 	 acc:0.9428099173553719 | test: loss:0.5051817138463456 	 acc:0.7894039735099337 	 lr:1.5625e-06
epoch76: train: loss:0.3684624048796567 	 acc:0.9461157024793388 | test: loss:0.5044264074192931 	 acc:0.7880794701986755 	 lr:1.5625e-06
epoch77: train: loss:0.3708396171439778 	 acc:0.9434710743801653 | test: loss:0.5032903871788884 	 acc:0.7907284768211921 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_17_1/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_17_1/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7062608125584184 	 acc:0.5715702479338843 | test: loss:0.7258783669661213 	 acc:0.5721854304635762 	 lr:0.0001
epoch1: train: loss:0.6534418990198245 	 acc:0.5447933884297521 | test: loss:0.6598117743106867 	 acc:0.5390728476821192 	 lr:0.0001
epoch2: train: loss:0.6114328390901739 	 acc:0.6591735537190082 | test: loss:0.6054605127959851 	 acc:0.6622516556291391 	 lr:0.0001
epoch3: train: loss:0.5943593416726294 	 acc:0.6390082644628099 | test: loss:0.5795216150631178 	 acc:0.6609271523178808 	 lr:0.0001
epoch4: train: loss:0.5750357653877952 	 acc:0.6776859504132231 | test: loss:0.5660365759142187 	 acc:0.7019867549668874 	 lr:0.0001
epoch5: train: loss:0.5771165869452737 	 acc:0.7133884297520661 | test: loss:0.5633944516150368 	 acc:0.7165562913907285 	 lr:0.0001
epoch6: train: loss:0.5675168665184462 	 acc:0.7219834710743802 | test: loss:0.5645810097258612 	 acc:0.7192052980132451 	 lr:0.0001
epoch7: train: loss:0.5683301144197953 	 acc:0.6680991735537191 | test: loss:0.5663504598156506 	 acc:0.6649006622516557 	 lr:0.0001
epoch8: train: loss:0.5406186468936195 	 acc:0.7573553719008265 | test: loss:0.5550095483167282 	 acc:0.7284768211920529 	 lr:0.0001
epoch9: train: loss:0.5455882748887558 	 acc:0.7461157024793389 | test: loss:0.5681755514334369 	 acc:0.7086092715231788 	 lr:0.0001
epoch10: train: loss:0.5376086963306774 	 acc:0.7570247933884298 | test: loss:0.5644765211256924 	 acc:0.7165562913907285 	 lr:0.0001
epoch11: train: loss:0.5294188781612176 	 acc:0.739504132231405 | test: loss:0.5636680419871349 	 acc:0.6794701986754967 	 lr:0.0001
epoch12: train: loss:0.5229187835346568 	 acc:0.7745454545454545 | test: loss:0.5608995654725081 	 acc:0.7165562913907285 	 lr:0.0001
epoch13: train: loss:0.5167615654842913 	 acc:0.7818181818181819 | test: loss:0.5494390030570377 	 acc:0.7377483443708609 	 lr:0.0001
epoch14: train: loss:0.5321208228355597 	 acc:0.7672727272727272 | test: loss:0.5582352631139439 	 acc:0.7284768211920529 	 lr:0.0001
epoch15: train: loss:0.5089804608565717 	 acc:0.7715702479338843 | test: loss:0.5401471920360793 	 acc:0.7403973509933774 	 lr:0.0001
epoch16: train: loss:0.49997793917813577 	 acc:0.7785123966942149 | test: loss:0.5365908678004284 	 acc:0.7390728476821192 	 lr:0.0001
epoch17: train: loss:0.5012410785738102 	 acc:0.8122314049586777 | test: loss:0.5543860309171361 	 acc:0.7509933774834437 	 lr:0.0001
epoch18: train: loss:0.5162052265080539 	 acc:0.8023140495867769 | test: loss:0.5557951255349923 	 acc:0.7390728476821192 	 lr:0.0001
epoch19: train: loss:0.4878626870616408 	 acc:0.8026446280991736 | test: loss:0.5385922939572113 	 acc:0.7403973509933774 	 lr:0.0001
epoch20: train: loss:0.498416798617229 	 acc:0.8142148760330579 | test: loss:0.5978004994771339 	 acc:0.695364238410596 	 lr:0.0001
epoch21: train: loss:0.48261666508745554 	 acc:0.804297520661157 | test: loss:0.5564201969974089 	 acc:0.7006622516556291 	 lr:0.0001
epoch22: train: loss:0.47754162539135325 	 acc:0.8294214876033058 | test: loss:0.545439174869992 	 acc:0.7271523178807947 	 lr:0.0001
epoch23: train: loss:0.46963874410006623 	 acc:0.8264462809917356 | test: loss:0.5362524416273003 	 acc:0.7483443708609272 	 lr:5e-05
epoch24: train: loss:0.4632316020894642 	 acc:0.8363636363636363 | test: loss:0.533356178912106 	 acc:0.752317880794702 	 lr:5e-05
epoch25: train: loss:0.45992397964493303 	 acc:0.8545454545454545 | test: loss:0.5422923922538757 	 acc:0.7390728476821192 	 lr:5e-05
epoch26: train: loss:0.46084040292038403 	 acc:0.859504132231405 | test: loss:0.5467401274782143 	 acc:0.7456953642384105 	 lr:5e-05
epoch27: train: loss:0.4756021730663363 	 acc:0.8423140495867769 | test: loss:0.5786567661146454 	 acc:0.7350993377483444 	 lr:5e-05
epoch28: train: loss:0.44871730535483556 	 acc:0.8591735537190083 | test: loss:0.5298716381685623 	 acc:0.7337748344370861 	 lr:5e-05
epoch29: train: loss:0.45009044486629074 	 acc:0.8690909090909091 | test: loss:0.5495929592492564 	 acc:0.7536423841059603 	 lr:5e-05
epoch30: train: loss:0.4532601191583744 	 acc:0.8634710743801652 | test: loss:0.565102343606633 	 acc:0.7390728476821192 	 lr:5e-05
epoch31: train: loss:0.43606935596663104 	 acc:0.871404958677686 | test: loss:0.5316736915253646 	 acc:0.7589403973509934 	 lr:5e-05
epoch32: train: loss:0.44367120133943794 	 acc:0.8770247933884298 | test: loss:0.5744501679938361 	 acc:0.7271523178807947 	 lr:5e-05
epoch33: train: loss:0.43851353241392405 	 acc:0.8747107438016529 | test: loss:0.5303763391955799 	 acc:0.7509933774834437 	 lr:5e-05
epoch34: train: loss:0.43033240420759217 	 acc:0.8727272727272727 | test: loss:0.5223296341517114 	 acc:0.7549668874172185 	 lr:5e-05
epoch35: train: loss:0.4287660424866952 	 acc:0.8727272727272727 | test: loss:0.5391939116629544 	 acc:0.7536423841059603 	 lr:5e-05
epoch36: train: loss:0.43017400609560247 	 acc:0.8796694214876033 | test: loss:0.5453279664184874 	 acc:0.7337748344370861 	 lr:5e-05
epoch37: train: loss:0.4229345368944909 	 acc:0.8809917355371901 | test: loss:0.5315526341760395 	 acc:0.766887417218543 	 lr:5e-05
epoch38: train: loss:0.41830622511461746 	 acc:0.8965289256198347 | test: loss:0.5363068154316075 	 acc:0.7549668874172185 	 lr:5e-05
epoch39: train: loss:0.43123989227389503 	 acc:0.8565289256198347 | test: loss:0.5371610020170149 	 acc:0.7377483443708609 	 lr:5e-05
epoch40: train: loss:0.425910528701199 	 acc:0.8942148760330578 | test: loss:0.5748047645518322 	 acc:0.7324503311258278 	 lr:5e-05
epoch41: train: loss:0.42080716867092227 	 acc:0.8909090909090909 | test: loss:0.5289739302452037 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch42: train: loss:0.4122754300822896 	 acc:0.8985123966942149 | test: loss:0.5407026387208345 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch43: train: loss:0.4134870183861945 	 acc:0.8945454545454545 | test: loss:0.5341052435881255 	 acc:0.752317880794702 	 lr:2.5e-05
epoch44: train: loss:0.41245097969189165 	 acc:0.9044628099173554 | test: loss:0.548121650645275 	 acc:0.743046357615894 	 lr:2.5e-05
epoch45: train: loss:0.4097274163439254 	 acc:0.9077685950413223 | test: loss:0.5432416322215503 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch46: train: loss:0.40695430528033866 	 acc:0.911404958677686 | test: loss:0.5514866955232937 	 acc:0.7364238410596027 	 lr:2.5e-05
epoch47: train: loss:0.40083494518414015 	 acc:0.9067768595041322 | test: loss:0.537501631904122 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch48: train: loss:0.4054497345814035 	 acc:0.9094214876033058 | test: loss:0.5333543916411747 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch49: train: loss:0.4009427345094602 	 acc:0.9150413223140496 | test: loss:0.5551925690758307 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch50: train: loss:0.39551673858618935 	 acc:0.9160330578512397 | test: loss:0.5255522485600402 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch51: train: loss:0.39409930605533694 	 acc:0.9203305785123967 | test: loss:0.5218093985753344 	 acc:0.7708609271523179 	 lr:1.25e-05
epoch52: train: loss:0.39494858823531914 	 acc:0.9229752066115703 | test: loss:0.5422384775237532 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch53: train: loss:0.39762910946341584 	 acc:0.9213223140495868 | test: loss:0.543168822108515 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch54: train: loss:0.39677873261703933 	 acc:0.9186776859504132 | test: loss:0.542952516536839 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch55: train: loss:0.3942496481513189 	 acc:0.9206611570247933 | test: loss:0.5430243866333109 	 acc:0.752317880794702 	 lr:1.25e-05
epoch56: train: loss:0.3936442081593285 	 acc:0.92 | test: loss:0.5404977731357347 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch57: train: loss:0.39451553832400926 	 acc:0.9183471074380165 | test: loss:0.5210756020040701 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch58: train: loss:0.39552378764822466 	 acc:0.9170247933884298 | test: loss:0.531487571798413 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch59: train: loss:0.39433701685637484 	 acc:0.9223140495867769 | test: loss:0.5334091315206313 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch60: train: loss:0.3948788755688786 	 acc:0.9173553719008265 | test: loss:0.5228139256009993 	 acc:0.7695364238410596 	 lr:1.25e-05
epoch61: train: loss:0.38901040053564656 	 acc:0.9229752066115703 | test: loss:0.5336987609105395 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch62: train: loss:0.39306381843306804 	 acc:0.923305785123967 | test: loss:0.5361134000961354 	 acc:0.743046357615894 	 lr:1.25e-05
epoch63: train: loss:0.39075222711917784 	 acc:0.9292561983471075 | test: loss:0.5380946348045046 	 acc:0.7602649006622516 	 lr:1.25e-05
epoch64: train: loss:0.3912940801766293 	 acc:0.9246280991735537 | test: loss:0.5403215833057631 	 acc:0.752317880794702 	 lr:6.25e-06
epoch65: train: loss:0.394692973854128 	 acc:0.9147107438016528 | test: loss:0.5231168852736618 	 acc:0.766887417218543 	 lr:6.25e-06
epoch66: train: loss:0.38663877719689993 	 acc:0.9266115702479338 | test: loss:0.5292130470275879 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch67: train: loss:0.39020935092090575 	 acc:0.9259504132231405 | test: loss:0.5366704558694599 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch68: train: loss:0.3919057050618258 	 acc:0.9219834710743802 | test: loss:0.5327855181220351 	 acc:0.752317880794702 	 lr:6.25e-06
epoch69: train: loss:0.3874857129636875 	 acc:0.924297520661157 | test: loss:0.5313889082693896 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch70: train: loss:0.38108796408353757 	 acc:0.9338842975206612 | test: loss:0.5276418778280548 	 acc:0.7589403973509934 	 lr:3.125e-06
epoch71: train: loss:0.38652988035816793 	 acc:0.9299173553719008 | test: loss:0.5284587830107733 	 acc:0.7576158940397351 	 lr:3.125e-06
epoch72: train: loss:0.3849871777305918 	 acc:0.9249586776859504 | test: loss:0.529766354102962 	 acc:0.7602649006622516 	 lr:3.125e-06
epoch73: train: loss:0.3898212505470623 	 acc:0.9219834710743802 | test: loss:0.5301507789567607 	 acc:0.7589403973509934 	 lr:3.125e-06
epoch74: train: loss:0.38436744328372735 	 acc:0.9302479338842975 | test: loss:0.5312149666003044 	 acc:0.7589403973509934 	 lr:3.125e-06
epoch75: train: loss:0.3894224591196076 	 acc:0.9266115702479338 | test: loss:0.5294865103746882 	 acc:0.7589403973509934 	 lr:3.125e-06
epoch76: train: loss:0.38265740290161004 	 acc:0.9332231404958677 | test: loss:0.5286706415233233 	 acc:0.7549668874172185 	 lr:1.5625e-06
epoch77: train: loss:0.3866891050535785 	 acc:0.9312396694214876 | test: loss:0.5304349774556444 	 acc:0.7562913907284768 	 lr:1.5625e-06
epoch78: train: loss:0.39060042399020234 	 acc:0.9223140495867769 | test: loss:0.527964815082929 	 acc:0.7576158940397351 	 lr:1.5625e-06
epoch79: train: loss:0.38239784464363225 	 acc:0.9388429752066115 | test: loss:0.5329063240266004 	 acc:0.752317880794702 	 lr:1.5625e-06
epoch80: train: loss:0.391129885202597 	 acc:0.9249586776859504 | test: loss:0.5368882061629895 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch81: train: loss:0.3818537649241361 	 acc:0.9342148760330579 | test: loss:0.5345832325764839 	 acc:0.752317880794702 	 lr:1.5625e-06
