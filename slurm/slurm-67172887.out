
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_-1_3/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_1_3/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_1_3/
pooling!! 256
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.9198216482217567 	 acc:0.43765625 | test: loss:1.9321444749089416 	 acc:0.4012461059190031 	 lr:0.0001
epoch1: train: loss:1.8832930839592177 	 acc:0.47875 | test: loss:1.8761945831441433 	 acc:0.4685358255451713 	 lr:0.0001
epoch2: train: loss:1.865860840606094 	 acc:0.4603125 | test: loss:1.8714045991036008 	 acc:0.4517133956386293 	 lr:0.0001
epoch3: train: loss:1.8273119173786958 	 acc:0.421875 | test: loss:1.8233220391555738 	 acc:0.4130841121495327 	 lr:0.0001
epoch4: train: loss:1.8178527546152299 	 acc:0.46453125 | test: loss:1.826868209734884 	 acc:0.4473520249221184 	 lr:0.0001
epoch5: train: loss:1.7885046000782165 	 acc:0.4953125 | test: loss:1.78637907155949 	 acc:0.48161993769470407 	 lr:0.0001
epoch6: train: loss:1.7851533935928048 	 acc:0.4934375 | test: loss:1.7713752603976527 	 acc:0.4635514018691589 	 lr:0.0001
epoch7: train: loss:1.763520311508953 	 acc:0.53203125 | test: loss:1.7614439169565836 	 acc:0.5077881619937694 	 lr:0.0001
epoch8: train: loss:1.7829952743256305 	 acc:0.47296875 | test: loss:1.8016748438743044 	 acc:0.42990654205607476 	 lr:0.0001
epoch9: train: loss:1.7501514586687645 	 acc:0.52359375 | test: loss:1.7644561903498996 	 acc:0.4884735202492212 	 lr:0.0001
epoch10: train: loss:1.7533467443914361 	 acc:0.55828125 | test: loss:1.7295595691954235 	 acc:0.5258566978193147 	 lr:0.0001
epoch11: train: loss:1.7394295781036544 	 acc:0.52 | test: loss:1.7625857317559073 	 acc:0.4897196261682243 	 lr:0.0001
epoch12: train: loss:1.7397663435090993 	 acc:0.52734375 | test: loss:1.7359927727054585 	 acc:0.4996884735202492 	 lr:0.0001
epoch13: train: loss:1.7275091476499989 	 acc:0.5828125 | test: loss:1.715259413332954 	 acc:0.5557632398753894 	 lr:0.0001
epoch14: train: loss:1.7358659087634478 	 acc:0.5059375 | test: loss:1.7375512953487884 	 acc:0.4853582554517134 	 lr:0.0001
epoch15: train: loss:1.7225483616658432 	 acc:0.57296875 | test: loss:1.7201114361159897 	 acc:0.5289719626168224 	 lr:0.0001
epoch16: train: loss:1.7124763911632148 	 acc:0.5553125 | test: loss:1.7323713503151297 	 acc:0.5227414330218069 	 lr:0.0001
epoch17: train: loss:1.6973135811941964 	 acc:0.5828125 | test: loss:1.706012915896478 	 acc:0.5557632398753894 	 lr:0.0001
epoch18: train: loss:1.7166316750829431 	 acc:0.56703125 | test: loss:1.6847106205711484 	 acc:0.535202492211838 	 lr:0.0001
epoch19: train: loss:1.7039470854259673 	 acc:0.564375 | test: loss:1.7181765147087358 	 acc:0.5295950155763239 	 lr:0.0001
epoch20: train: loss:1.689997147880989 	 acc:0.576875 | test: loss:1.6980268651451278 	 acc:0.5439252336448598 	 lr:0.0001
epoch21: train: loss:1.6884511931253354 	 acc:0.595625 | test: loss:1.662075053197201 	 acc:0.5588785046728972 	 lr:0.0001
epoch22: train: loss:1.682992615651377 	 acc:0.5746875 | test: loss:1.6868176760331863 	 acc:0.5433021806853583 	 lr:0.0001
epoch23: train: loss:1.696679342062933 	 acc:0.59734375 | test: loss:1.6683905224191065 	 acc:0.567601246105919 	 lr:0.0001
epoch24: train: loss:1.6999219907809755 	 acc:0.5128125 | test: loss:1.7210584909373725 	 acc:0.48785046728971965 	 lr:0.0001
epoch25: train: loss:1.6808133699296508 	 acc:0.639375 | test: loss:1.656376959750214 	 acc:0.577570093457944 	 lr:0.0001
epoch26: train: loss:1.7024522529739035 	 acc:0.55984375 | test: loss:1.692714365769027 	 acc:0.540809968847352 	 lr:0.0001
epoch27: train: loss:1.6843349293002292 	 acc:0.55140625 | test: loss:1.6873895232922562 	 acc:0.5127725856697819 	 lr:0.0001
epoch28: train: loss:1.6643048681754977 	 acc:0.59984375 | test: loss:1.670590055768735 	 acc:0.561993769470405 	 lr:0.0001
epoch29: train: loss:1.6669291964552535 	 acc:0.59046875 | test: loss:1.6813757799867528 	 acc:0.561993769470405 	 lr:0.0001
epoch30: train: loss:1.6962522246147114 	 acc:0.53890625 | test: loss:1.7134543655074645 	 acc:0.4928348909657321 	 lr:0.0001
epoch31: train: loss:1.6767111784959565 	 acc:0.63171875 | test: loss:1.644009169017043 	 acc:0.6006230529595016 	 lr:0.0001
epoch32: train: loss:1.6491176846434976 	 acc:0.63453125 | test: loss:1.650086636929497 	 acc:0.6037383177570094 	 lr:0.0001
epoch33: train: loss:1.6624916570992512 	 acc:0.6396875 | test: loss:1.6413177280039801 	 acc:0.5981308411214953 	 lr:0.0001
epoch34: train: loss:1.656635645941586 	 acc:0.6053125 | test: loss:1.6605278333770894 	 acc:0.5700934579439252 	 lr:0.0001
epoch35: train: loss:1.6466774576441745 	 acc:0.59 | test: loss:1.6819418773472865 	 acc:0.561993769470405 	 lr:0.0001
epoch36: train: loss:1.6539428687114255 	 acc:0.56140625 | test: loss:1.6703191793596261 	 acc:0.5302180685358255 	 lr:0.0001
epoch37: train: loss:1.642505649921021 	 acc:0.58546875 | test: loss:1.667557053328303 	 acc:0.5570093457943925 	 lr:0.0001
epoch38: train: loss:1.6334024969811183 	 acc:0.62296875 | test: loss:1.6429164115513597 	 acc:0.584423676012461 	 lr:0.0001
epoch39: train: loss:1.629533411244877 	 acc:0.619375 | test: loss:1.6419835320142941 	 acc:0.5813084112149532 	 lr:0.0001
epoch40: train: loss:1.63747168607213 	 acc:0.67109375 | test: loss:1.614762925953137 	 acc:0.6398753894080997 	 lr:5e-05
epoch41: train: loss:1.630013957086901 	 acc:0.65671875 | test: loss:1.6151010684877913 	 acc:0.621183800623053 	 lr:5e-05
epoch42: train: loss:1.6163405359582357 	 acc:0.6565625 | test: loss:1.6123564048719554 	 acc:0.6249221183800623 	 lr:5e-05
epoch43: train: loss:1.6260051194249792 	 acc:0.65 | test: loss:1.623210753384409 	 acc:0.6018691588785047 	 lr:5e-05
epoch44: train: loss:1.6104662595178632 	 acc:0.6625 | test: loss:1.6265921083937553 	 acc:0.616822429906542 	 lr:5e-05
epoch45: train: loss:1.6150067510314512 	 acc:0.65296875 | test: loss:1.6369392913449963 	 acc:0.6018691588785047 	 lr:5e-05
epoch46: train: loss:1.6137471207224885 	 acc:0.66953125 | test: loss:1.6160915443087664 	 acc:0.6205607476635514 	 lr:5e-05
epoch47: train: loss:1.6165289301875976 	 acc:0.6803125 | test: loss:1.6042260645334594 	 acc:0.6461059190031153 	 lr:5e-05
epoch48: train: loss:1.6176906898373462 	 acc:0.6675 | test: loss:1.6086361251516135 	 acc:0.6230529595015576 	 lr:5e-05
epoch49: train: loss:1.6137464822594958 	 acc:0.68578125 | test: loss:1.5975462205684816 	 acc:0.6292834890965732 	 lr:5e-05
epoch50: train: loss:1.612928175200344 	 acc:0.63171875 | test: loss:1.6410092116144959 	 acc:0.6006230529595016 	 lr:5e-05
epoch51: train: loss:1.6009841692624476 	 acc:0.66421875 | test: loss:1.6150027367927575 	 acc:0.621183800623053 	 lr:5e-05
epoch52: train: loss:1.6023918896331906 	 acc:0.67 | test: loss:1.6048500782232789 	 acc:0.6355140186915887 	 lr:5e-05
epoch53: train: loss:1.6007756420078918 	 acc:0.68015625 | test: loss:1.6064970629237523 	 acc:0.6423676012461059 	 lr:5e-05
epoch54: train: loss:1.6071747826748206 	 acc:0.6909375 | test: loss:1.5833671397509232 	 acc:0.6417445482866043 	 lr:5e-05
epoch55: train: loss:1.597689488192818 	 acc:0.66296875 | test: loss:1.6235031260136874 	 acc:0.6087227414330219 	 lr:5e-05
epoch56: train: loss:1.5981868460995439 	 acc:0.66234375 | test: loss:1.6116060285924751 	 acc:0.6292834890965732 	 lr:5e-05
epoch57: train: loss:1.6010013267642162 	 acc:0.693125 | test: loss:1.592151084644401 	 acc:0.6579439252336449 	 lr:5e-05
epoch58: train: loss:1.5948674245889443 	 acc:0.6696875 | test: loss:1.5930654998137572 	 acc:0.6242990654205608 	 lr:5e-05
epoch59: train: loss:1.5876250124833808 	 acc:0.64890625 | test: loss:1.625446101512493 	 acc:0.6118380062305296 	 lr:5e-05
epoch60: train: loss:1.6020319370624145 	 acc:0.70515625 | test: loss:1.593299910435424 	 acc:0.6623052959501557 	 lr:5e-05
epoch61: train: loss:1.5878406438298935 	 acc:0.67609375 | test: loss:1.6019079642875171 	 acc:0.6280373831775701 	 lr:2.5e-05
epoch62: train: loss:1.5790742848833308 	 acc:0.6765625 | test: loss:1.5964038567379628 	 acc:0.6355140186915887 	 lr:2.5e-05
epoch63: train: loss:1.578818132577698 	 acc:0.68 | test: loss:1.6077793945033232 	 acc:0.6292834890965732 	 lr:2.5e-05
epoch64: train: loss:1.5804097159964139 	 acc:0.6840625 | test: loss:1.6028719744578328 	 acc:0.6255451713395639 	 lr:2.5e-05
epoch65: train: loss:1.5819370981494865 	 acc:0.68734375 | test: loss:1.5906820050652524 	 acc:0.6517133956386293 	 lr:2.5e-05
epoch66: train: loss:1.577249600531811 	 acc:0.67890625 | test: loss:1.594292348938939 	 acc:0.6336448598130842 	 lr:2.5e-05
epoch67: train: loss:1.578050192625238 	 acc:0.679375 | test: loss:1.598635724177613 	 acc:0.6355140186915887 	 lr:1.25e-05
epoch68: train: loss:1.5780161039425376 	 acc:0.6921875 | test: loss:1.5983561680695721 	 acc:0.6348909657320873 	 lr:1.25e-05
epoch69: train: loss:1.5697519128160677 	 acc:0.6790625 | test: loss:1.5956827278820525 	 acc:0.6361370716510903 	 lr:1.25e-05
epoch70: train: loss:1.5730013106597018 	 acc:0.67515625 | test: loss:1.6061121471945743 	 acc:0.6230529595015576 	 lr:1.25e-05
epoch71: train: loss:1.5720255128858984 	 acc:0.68453125 | test: loss:1.592995523218054 	 acc:0.6348909657320873 	 lr:1.25e-05
epoch72: train: loss:1.5721751157237998 	 acc:0.685 | test: loss:1.6010720847923066 	 acc:0.6317757009345795 	 lr:1.25e-05
epoch73: train: loss:1.5672563587102362 	 acc:0.6871875 | test: loss:1.5961105932699187 	 acc:0.6404984423676012 	 lr:6.25e-06
epoch74: train: loss:1.5830337417209455 	 acc:0.69296875 | test: loss:1.593910640348155 	 acc:0.6417445482866043 	 lr:6.25e-06
epoch75: train: loss:1.575703262221897 	 acc:0.69546875 | test: loss:1.5897643021705365 	 acc:0.6479750778816199 	 lr:6.25e-06
epoch76: train: loss:1.5724043954079454 	 acc:0.68828125 | test: loss:1.5940640979838148 	 acc:0.636760124610592 	 lr:6.25e-06
epoch77: train: loss:1.571140541237467 	 acc:0.6915625 | test: loss:1.592057702251684 	 acc:0.643613707165109 	 lr:6.25e-06
epoch78: train: loss:1.5708760555603838 	 acc:0.68640625 | test: loss:1.5957928564689612 	 acc:0.6392523364485981 	 lr:6.25e-06
epoch79: train: loss:1.5728437222697416 	 acc:0.683125 | test: loss:1.5953863931964862 	 acc:0.6392523364485981 	 lr:3.125e-06
epoch80: train: loss:1.5662329335030312 	 acc:0.68328125 | test: loss:1.5972529201864083 	 acc:0.6398753894080997 	 lr:3.125e-06
epoch81: train: loss:1.5707324657097725 	 acc:0.68609375 | test: loss:1.5923180699719819 	 acc:0.6398753894080997 	 lr:3.125e-06
epoch82: train: loss:1.570913484262173 	 acc:0.69421875 | test: loss:1.5921812142909872 	 acc:0.6398753894080997 	 lr:3.125e-06
epoch83: train: loss:1.5706510569135441 	 acc:0.68484375 | test: loss:1.592282253485231 	 acc:0.6386292834890965 	 lr:3.125e-06
epoch84: train: loss:1.5798207100623292 	 acc:0.6903125 | test: loss:1.5909592831988943 	 acc:0.6429906542056075 	 lr:3.125e-06
epoch85: train: loss:1.5701598311103386 	 acc:0.68734375 | test: loss:1.596716776518064 	 acc:0.638006230529595 	 lr:1.5625e-06
epoch86: train: loss:1.5705968232940268 	 acc:0.69359375 | test: loss:1.5916830241123092 	 acc:0.638006230529595 	 lr:1.5625e-06
epoch87: train: loss:1.569209097773651 	 acc:0.6846875 | test: loss:1.5933566721800332 	 acc:0.636760124610592 	 lr:1.5625e-06
epoch88: train: loss:1.5742084464461052 	 acc:0.68828125 | test: loss:1.5950808669176428 	 acc:0.6417445482866043 	 lr:1.5625e-06
epoch89: train: loss:1.5731000818376146 	 acc:0.68234375 | test: loss:1.5939382181732082 	 acc:0.6342679127725857 	 lr:1.5625e-06
epoch90: train: loss:1.568620349130623 	 acc:0.6834375 | test: loss:1.5962436657456966 	 acc:0.6342679127725857 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_2_3/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_2_3/
pooling!! 512
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.8608729199447454 	 acc:0.4946875 | test: loss:1.83412104373409 	 acc:0.48722741433021804 	 lr:0.0001
epoch1: train: loss:1.7866075788225446 	 acc:0.47484375 | test: loss:1.7902198310953064 	 acc:0.4766355140186916 	 lr:0.0001
epoch2: train: loss:1.7417861640685988 	 acc:0.5346875 | test: loss:1.737792194892313 	 acc:0.5227414330218069 	 lr:0.0001
epoch3: train: loss:1.7320362957635027 	 acc:0.55140625 | test: loss:1.7145893073898981 	 acc:0.5320872274143302 	 lr:0.0001
epoch4: train: loss:1.710653493555145 	 acc:0.565625 | test: loss:1.7149459946564054 	 acc:0.52398753894081 	 lr:0.0001
epoch5: train: loss:1.6789440312858301 	 acc:0.58578125 | test: loss:1.6821779356567288 	 acc:0.5520249221183801 	 lr:0.0001
epoch6: train: loss:1.6509368300903224 	 acc:0.6003125 | test: loss:1.6627987450157 	 acc:0.5651090342679128 	 lr:0.0001
epoch7: train: loss:1.6526344626886038 	 acc:0.6378125 | test: loss:1.6412057792657633 	 acc:0.6193146417445483 	 lr:0.0001
epoch8: train: loss:1.637652361569788 	 acc:0.559375 | test: loss:1.7353886423081253 	 acc:0.5177570093457944 	 lr:0.0001
epoch9: train: loss:1.6180891312443586 	 acc:0.59734375 | test: loss:1.6621614685682493 	 acc:0.5582554517133956 	 lr:0.0001
epoch10: train: loss:1.6058795628186597 	 acc:0.63 | test: loss:1.6325716823060936 	 acc:0.6087227414330219 	 lr:0.0001
epoch11: train: loss:1.6245815661249452 	 acc:0.6103125 | test: loss:1.6332874610045245 	 acc:0.5694704049844237 	 lr:0.0001
epoch12: train: loss:1.5773657745909262 	 acc:0.62421875 | test: loss:1.6383980228150745 	 acc:0.5788161993769471 	 lr:0.0001
epoch13: train: loss:1.5796643890690563 	 acc:0.62203125 | test: loss:1.642213349476039 	 acc:0.5850467289719626 	 lr:0.0001
epoch14: train: loss:1.5928268587468286 	 acc:0.6278125 | test: loss:1.6260869211125597 	 acc:0.5757009345794393 	 lr:0.0001
epoch15: train: loss:1.5849494904302974 	 acc:0.68125 | test: loss:1.5974119003688063 	 acc:0.6274143302180686 	 lr:0.0001
epoch16: train: loss:1.570513820722641 	 acc:0.5921875 | test: loss:1.6547047616536743 	 acc:0.5563862928348909 	 lr:0.0001
epoch17: train: loss:1.5387132250080808 	 acc:0.66 | test: loss:1.597491209900639 	 acc:0.6124610591900311 	 lr:0.0001
epoch18: train: loss:1.55914672803916 	 acc:0.7140625 | test: loss:1.5590860064527328 	 acc:0.6722741433021807 	 lr:0.0001
epoch19: train: loss:1.5298764921928365 	 acc:0.64671875 | test: loss:1.6143865765812242 	 acc:0.6062305295950156 	 lr:0.0001
epoch20: train: loss:1.529555747920326 	 acc:0.70921875 | test: loss:1.5770254284421974 	 acc:0.6654205607476635 	 lr:0.0001
epoch21: train: loss:1.5153653933236972 	 acc:0.66671875 | test: loss:1.5719227902243071 	 acc:0.6255451713395639 	 lr:0.0001
epoch22: train: loss:1.5184826483864229 	 acc:0.72484375 | test: loss:1.5533342564217398 	 acc:0.6635514018691588 	 lr:0.0001
epoch23: train: loss:1.5591603215833272 	 acc:0.69765625 | test: loss:1.6037941408305896 	 acc:0.6361370716510903 	 lr:0.0001
epoch24: train: loss:1.5050463709954076 	 acc:0.71203125 | test: loss:1.5881145863518165 	 acc:0.6573208722741433 	 lr:0.0001
epoch25: train: loss:1.5042890276227678 	 acc:0.7034375 | test: loss:1.550805105078629 	 acc:0.6504672897196262 	 lr:0.0001
epoch26: train: loss:1.504017256797058 	 acc:0.71890625 | test: loss:1.5643623285947188 	 acc:0.6654205607476635 	 lr:0.0001
epoch27: train: loss:1.4898312427958504 	 acc:0.75609375 | test: loss:1.5466283936366856 	 acc:0.6816199376947041 	 lr:0.0001
epoch28: train: loss:1.4917638178638515 	 acc:0.7071875 | test: loss:1.5624952172935938 	 acc:0.6517133956386293 	 lr:0.0001
epoch29: train: loss:1.4608578493593263 	 acc:0.7228125 | test: loss:1.542146430357223 	 acc:0.6828660436137072 	 lr:0.0001
epoch30: train: loss:1.4766401018415178 	 acc:0.77890625 | test: loss:1.5023530576087976 	 acc:0.7264797507788162 	 lr:0.0001
epoch31: train: loss:1.4764343047309536 	 acc:0.73125 | test: loss:1.5141465737440876 	 acc:0.67601246105919 	 lr:0.0001
epoch32: train: loss:1.4984709335434354 	 acc:0.66875 | test: loss:1.5821920043582858 	 acc:0.6143302180685358 	 lr:0.0001
epoch33: train: loss:1.4566068174316025 	 acc:0.66125 | test: loss:1.591528818020568 	 acc:0.6230529595015576 	 lr:0.0001
epoch34: train: loss:1.4512953056198465 	 acc:0.73046875 | test: loss:1.5350724205421138 	 acc:0.6747663551401869 	 lr:0.0001
epoch35: train: loss:1.4398115147658384 	 acc:0.74640625 | test: loss:1.5279776778919303 	 acc:0.6884735202492211 	 lr:0.0001
epoch36: train: loss:1.443391393442623 	 acc:0.73015625 | test: loss:1.518786574227045 	 acc:0.6797507788161994 	 lr:0.0001
epoch37: train: loss:1.4156339980400139 	 acc:0.7909375 | test: loss:1.4906274179803247 	 acc:0.7165109034267912 	 lr:5e-05
epoch38: train: loss:1.4154040870398492 	 acc:0.8115625 | test: loss:1.473456901776085 	 acc:0.7439252336448599 	 lr:5e-05
epoch39: train: loss:1.4164247916323611 	 acc:0.7484375 | test: loss:1.5193515232418928 	 acc:0.685981308411215 	 lr:5e-05
epoch40: train: loss:1.412273764926693 	 acc:0.79203125 | test: loss:1.4926312804593476 	 acc:0.7208722741433021 	 lr:5e-05
epoch41: train: loss:1.396060059165508 	 acc:0.76234375 | test: loss:1.5105775573914668 	 acc:0.6940809968847352 	 lr:5e-05
epoch42: train: loss:1.4018339515942135 	 acc:0.7840625 | test: loss:1.4951316319522086 	 acc:0.7252336448598131 	 lr:5e-05
epoch43: train: loss:1.4084161596126987 	 acc:0.74328125 | test: loss:1.5057694941666266 	 acc:0.6934579439252336 	 lr:5e-05
epoch44: train: loss:1.4108660659968713 	 acc:0.8053125 | test: loss:1.4730276795562554 	 acc:0.7314641744548287 	 lr:5e-05
epoch45: train: loss:1.3883101475973971 	 acc:0.778125 | test: loss:1.486934360239736 	 acc:0.7227414330218068 	 lr:5e-05
epoch46: train: loss:1.3867327890388674 	 acc:0.76203125 | test: loss:1.4963018879340817 	 acc:0.6934579439252336 	 lr:5e-05
epoch47: train: loss:1.39742244151679 	 acc:0.755 | test: loss:1.4959106704527716 	 acc:0.6959501557632399 	 lr:5e-05
epoch48: train: loss:1.393045482293039 	 acc:0.78359375 | test: loss:1.4919365212924756 	 acc:0.7158878504672898 	 lr:5e-05
epoch49: train: loss:1.3831304521880794 	 acc:0.7821875 | test: loss:1.4792466934596267 	 acc:0.7252336448598131 	 lr:5e-05
epoch50: train: loss:1.38369652824789 	 acc:0.81953125 | test: loss:1.4642954501036172 	 acc:0.7582554517133956 	 lr:5e-05
epoch51: train: loss:1.3785924787915935 	 acc:0.7725 | test: loss:1.4861460945687932 	 acc:0.7096573208722742 	 lr:5e-05
epoch52: train: loss:1.3811573418670107 	 acc:0.78484375 | test: loss:1.4855912654199332 	 acc:0.7308411214953271 	 lr:5e-05
epoch53: train: loss:1.3810957230412337 	 acc:0.8028125 | test: loss:1.4806588385335382 	 acc:0.7314641744548287 	 lr:5e-05
epoch54: train: loss:1.4021171885482229 	 acc:0.7584375 | test: loss:1.496812441118781 	 acc:0.6934579439252336 	 lr:5e-05
epoch55: train: loss:1.3851529266571831 	 acc:0.78171875 | test: loss:1.482501369547621 	 acc:0.7177570093457943 	 lr:5e-05
epoch56: train: loss:1.3607391095366168 	 acc:0.81453125 | test: loss:1.4566950022617233 	 acc:0.746417445482866 	 lr:5e-05
epoch57: train: loss:1.3680847025029275 	 acc:0.766875 | test: loss:1.4898425140113474 	 acc:0.7090342679127726 	 lr:5e-05
epoch58: train: loss:1.3725594799747511 	 acc:0.8090625 | test: loss:1.45876822880124 	 acc:0.7408099688473521 	 lr:5e-05
epoch59: train: loss:1.392504382524334 	 acc:0.80421875 | test: loss:1.463095541460863 	 acc:0.7433021806853582 	 lr:5e-05
epoch60: train: loss:1.3707639790995805 	 acc:0.81703125 | test: loss:1.4584118157532355 	 acc:0.7489096573208722 	 lr:5e-05
epoch61: train: loss:1.3645798313161714 	 acc:0.765 | test: loss:1.4823351664706554 	 acc:0.7071651090342679 	 lr:5e-05
epoch62: train: loss:1.355072028631349 	 acc:0.79296875 | test: loss:1.4720990466180248 	 acc:0.7308411214953271 	 lr:5e-05
epoch63: train: loss:1.347093249521248 	 acc:0.816875 | test: loss:1.4550020948748723 	 acc:0.7538940809968847 	 lr:2.5e-05
epoch64: train: loss:1.366817222434408 	 acc:0.8153125 | test: loss:1.4570733976512684 	 acc:0.7501557632398754 	 lr:2.5e-05
epoch65: train: loss:1.3457063554321576 	 acc:0.808125 | test: loss:1.4644997223886744 	 acc:0.7414330218068536 	 lr:2.5e-05
epoch66: train: loss:1.3558174532340153 	 acc:0.81171875 | test: loss:1.4595779184240418 	 acc:0.7501557632398754 	 lr:2.5e-05
epoch67: train: loss:1.3486250913859716 	 acc:0.82453125 | test: loss:1.4571730785280745 	 acc:0.7526479750778816 	 lr:2.5e-05
epoch68: train: loss:1.3542127956178949 	 acc:0.82609375 | test: loss:1.4566076638913972 	 acc:0.7551401869158878 	 lr:2.5e-05
epoch69: train: loss:1.3472380591220543 	 acc:0.79609375 | test: loss:1.4647075838017687 	 acc:0.735202492211838 	 lr:2.5e-05
epoch70: train: loss:1.3456200247533054 	 acc:0.82828125 | test: loss:1.444574043832464 	 acc:0.7638629283489097 	 lr:1.25e-05
epoch71: train: loss:1.3394443580454722 	 acc:0.8125 | test: loss:1.4597031603721071 	 acc:0.7470404984423676 	 lr:1.25e-05
epoch72: train: loss:1.3299647739955358 	 acc:0.83203125 | test: loss:1.4476160473541306 	 acc:0.7607476635514019 	 lr:1.25e-05
epoch73: train: loss:1.331618557527231 	 acc:0.81625 | test: loss:1.4547366495073026 	 acc:0.7532710280373832 	 lr:1.25e-05
epoch74: train: loss:1.3353216214444281 	 acc:0.82875 | test: loss:1.45134126434445 	 acc:0.7601246105919003 	 lr:1.25e-05
epoch75: train: loss:1.3376413171129429 	 acc:0.821875 | test: loss:1.4488865927372394 	 acc:0.7626168224299066 	 lr:1.25e-05
epoch76: train: loss:1.3367056186268051 	 acc:0.816875 | test: loss:1.4578669504213184 	 acc:0.7476635514018691 	 lr:1.25e-05
epoch77: train: loss:1.3350919653529958 	 acc:0.8428125 | test: loss:1.4411141666667855 	 acc:0.7700934579439253 	 lr:6.25e-06
epoch78: train: loss:1.3269838498403652 	 acc:0.82703125 | test: loss:1.4462083749681989 	 acc:0.7582554517133956 	 lr:6.25e-06
epoch79: train: loss:1.3264687936441866 	 acc:0.83234375 | test: loss:1.4464963762930991 	 acc:0.7632398753894081 	 lr:6.25e-06
epoch80: train: loss:1.328547560061262 	 acc:0.8259375 | test: loss:1.4486036670542208 	 acc:0.7613707165109034 	 lr:6.25e-06
epoch81: train: loss:1.3253943986021663 	 acc:0.82921875 | test: loss:1.4445753617450083 	 acc:0.7557632398753894 	 lr:6.25e-06
epoch82: train: loss:1.3295601528385113 	 acc:0.82734375 | test: loss:1.448715793677951 	 acc:0.7588785046728972 	 lr:6.25e-06
epoch83: train: loss:1.3202270245756793 	 acc:0.8290625 | test: loss:1.4468495815342461 	 acc:0.7538940809968847 	 lr:6.25e-06
epoch84: train: loss:1.3297687086809622 	 acc:0.8296875 | test: loss:1.4465372246002481 	 acc:0.7657320872274144 	 lr:3.125e-06
epoch85: train: loss:1.329044732146669 	 acc:0.83875 | test: loss:1.4413203105005519 	 acc:0.7694704049844237 	 lr:3.125e-06
epoch86: train: loss:1.3342690032315756 	 acc:0.835625 | test: loss:1.441069407329381 	 acc:0.7657320872274144 	 lr:3.125e-06
epoch87: train: loss:1.3348188747194574 	 acc:0.825625 | test: loss:1.4460415056561384 	 acc:0.761993769470405 	 lr:3.125e-06
epoch88: train: loss:1.3237962113796595 	 acc:0.82859375 | test: loss:1.4483617790020145 	 acc:0.7538940809968847 	 lr:3.125e-06
epoch89: train: loss:1.3254031036162544 	 acc:0.8278125 | test: loss:1.4485335109389832 	 acc:0.756386292834891 	 lr:3.125e-06
epoch90: train: loss:1.3253325939550704 	 acc:0.820625 | test: loss:1.4506854049141904 	 acc:0.7545171339563863 	 lr:1.5625e-06
epoch91: train: loss:1.3303277473539044 	 acc:0.83046875 | test: loss:1.4439139919488972 	 acc:0.7632398753894081 	 lr:1.5625e-06
epoch92: train: loss:1.3282918866028737 	 acc:0.83203125 | test: loss:1.4455778354424926 	 acc:0.7576323987538941 	 lr:1.5625e-06
epoch93: train: loss:1.3248120441183646 	 acc:0.8321875 | test: loss:1.4460277513551563 	 acc:0.7570093457943925 	 lr:1.5625e-06
epoch94: train: loss:1.330374683913172 	 acc:0.829375 | test: loss:1.4430850148572358 	 acc:0.7663551401869159 	 lr:1.5625e-06
epoch95: train: loss:1.3206102834280165 	 acc:0.83734375 | test: loss:1.4423232065925717 	 acc:0.7651090342679128 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_3_3/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_3_3/
pooling!! 1024
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.7516972466616962 	 acc:0.51953125 | test: loss:1.7171611347673839 	 acc:0.518380062305296 	 lr:0.0001
epoch1: train: loss:1.6244029727789873 	 acc:0.57640625 | test: loss:1.6426451527069663 	 acc:0.5582554517133956 	 lr:0.0001
epoch2: train: loss:1.5665786921838407 	 acc:0.6125 | test: loss:1.6124812147699041 	 acc:0.5713395638629284 	 lr:0.0001
epoch3: train: loss:1.5646677319469349 	 acc:0.73984375 | test: loss:1.544541651734682 	 acc:0.7246105919003115 	 lr:0.0001
epoch4: train: loss:1.501598144340664 	 acc:0.64046875 | test: loss:1.612856023853813 	 acc:0.6080996884735203 	 lr:0.0001
epoch5: train: loss:1.5019453085929877 	 acc:0.68203125 | test: loss:1.5631347895411316 	 acc:0.6280373831775701 	 lr:0.0001
epoch6: train: loss:1.471165081563171 	 acc:0.77125 | test: loss:1.4878031436528 	 acc:0.7370716510903427 	 lr:0.0001
epoch7: train: loss:1.4650156942779993 	 acc:0.684375 | test: loss:1.5300472881935097 	 acc:0.659190031152648 	 lr:0.0001
epoch8: train: loss:1.416325105641802 	 acc:0.74328125 | test: loss:1.5012039040479332 	 acc:0.6890965732087228 	 lr:0.0001
epoch9: train: loss:1.4555753590258464 	 acc:0.70796875 | test: loss:1.5218099266569192 	 acc:0.6666666666666666 	 lr:0.0001
epoch10: train: loss:1.405502841065025 	 acc:0.7725 | test: loss:1.4919597067194192 	 acc:0.7208722741433021 	 lr:0.0001
epoch11: train: loss:1.4057613709306085 	 acc:0.74984375 | test: loss:1.4800087419997123 	 acc:0.702803738317757 	 lr:0.0001
epoch12: train: loss:1.4289510996429182 	 acc:0.71578125 | test: loss:1.4983843284232594 	 acc:0.7009345794392523 	 lr:0.0001
epoch13: train: loss:1.4141579740406665 	 acc:0.78296875 | test: loss:1.46525686812178 	 acc:0.7364485981308411 	 lr:0.0001
epoch14: train: loss:1.37738924287056 	 acc:0.78796875 | test: loss:1.4494437967876779 	 acc:0.7445482866043613 	 lr:0.0001
epoch15: train: loss:1.3823261031687586 	 acc:0.770625 | test: loss:1.4583737016838287 	 acc:0.7233644859813084 	 lr:0.0001
epoch16: train: loss:1.3645698589053963 	 acc:0.79578125 | test: loss:1.4405082781366843 	 acc:0.7370716510903427 	 lr:0.0001
epoch17: train: loss:1.3843549717970884 	 acc:0.74296875 | test: loss:1.4682348253570985 	 acc:0.7177570093457943 	 lr:0.0001
epoch18: train: loss:1.3671260431722958 	 acc:0.79796875 | test: loss:1.4195660161823498 	 acc:0.7607476635514019 	 lr:0.0001
epoch19: train: loss:1.3417621002078148 	 acc:0.83875 | test: loss:1.4113417942575948 	 acc:0.7881619937694704 	 lr:0.0001
epoch20: train: loss:1.3440493295566818 	 acc:0.81875 | test: loss:1.4118907891329946 	 acc:0.7825545171339564 	 lr:0.0001
epoch21: train: loss:1.3397880846033983 	 acc:0.7946875 | test: loss:1.4460992218920747 	 acc:0.7271028037383177 	 lr:0.0001
epoch22: train: loss:1.3179524429881135 	 acc:0.820625 | test: loss:1.4184359644060938 	 acc:0.75202492211838 	 lr:0.0001
epoch23: train: loss:1.3426624688201356 	 acc:0.84390625 | test: loss:1.395178568548874 	 acc:0.7856697819314642 	 lr:0.0001
epoch24: train: loss:1.3263544539750134 	 acc:0.72375 | test: loss:1.4862650736841456 	 acc:0.6884735202492211 	 lr:0.0001
epoch25: train: loss:1.3355498290825039 	 acc:0.7865625 | test: loss:1.446596970365055 	 acc:0.7190031152647975 	 lr:0.0001
epoch26: train: loss:1.3219014723909543 	 acc:0.76453125 | test: loss:1.471961301509465 	 acc:0.697196261682243 	 lr:0.0001
epoch27: train: loss:1.3016669959039264 	 acc:0.8634375 | test: loss:1.3803721699759224 	 acc:0.8124610591900312 	 lr:0.0001
epoch28: train: loss:1.3710659911537617 	 acc:0.82640625 | test: loss:1.4158486682677938 	 acc:0.767601246105919 	 lr:0.0001
epoch29: train: loss:1.3017205672372048 	 acc:0.8475 | test: loss:1.3929359781407864 	 acc:0.7781931464174455 	 lr:0.0001
epoch30: train: loss:1.297539282626793 	 acc:0.85875 | test: loss:1.3821934963683844 	 acc:0.7981308411214953 	 lr:0.0001
epoch31: train: loss:1.3060330679042558 	 acc:0.81390625 | test: loss:1.4313429311057118 	 acc:0.7501557632398754 	 lr:0.0001
epoch32: train: loss:1.31958115017107 	 acc:0.82578125 | test: loss:1.4166057281405011 	 acc:0.7644859813084112 	 lr:0.0001
epoch33: train: loss:1.2813578788048583 	 acc:0.870625 | test: loss:1.3629299356187243 	 acc:0.8205607476635514 	 lr:0.0001
epoch34: train: loss:1.2825594232754256 	 acc:0.8846875 | test: loss:1.3697663994221672 	 acc:0.8049844236760124 	 lr:0.0001
epoch35: train: loss:1.293743010259624 	 acc:0.8665625 | test: loss:1.3681367435930674 	 acc:0.805607476635514 	 lr:0.0001
epoch36: train: loss:1.3016287429178255 	 acc:0.86984375 | test: loss:1.3729756948732512 	 acc:0.805607476635514 	 lr:0.0001
epoch37: train: loss:1.2940447459641515 	 acc:0.89265625 | test: loss:1.3617525221031404 	 acc:0.822429906542056 	 lr:0.0001
epoch38: train: loss:1.3553697667951978 	 acc:0.86953125 | test: loss:1.3576561067705957 	 acc:0.8149532710280374 	 lr:0.0001
epoch39: train: loss:1.2889777750079283 	 acc:0.875625 | test: loss:1.3647183937447094 	 acc:0.8068535825545171 	 lr:0.0001
epoch40: train: loss:1.2907692402252269 	 acc:0.864375 | test: loss:1.3838411285127064 	 acc:0.7794392523364486 	 lr:0.0001
epoch41: train: loss:1.2927612233217762 	 acc:0.8953125 | test: loss:1.353732520768947 	 acc:0.8168224299065421 	 lr:0.0001
epoch42: train: loss:1.3042958839529664 	 acc:0.88375 | test: loss:1.3435012075388544 	 acc:0.8186915887850468 	 lr:0.0001
epoch43: train: loss:1.2758075055994158 	 acc:0.84640625 | test: loss:1.4001818694800974 	 acc:0.7744548286604361 	 lr:0.0001
epoch44: train: loss:1.272458745761368 	 acc:0.89734375 | test: loss:1.3477665290654262 	 acc:0.8236760124610591 	 lr:0.0001
epoch45: train: loss:1.2625550888740487 	 acc:0.89421875 | test: loss:1.3580477381792395 	 acc:0.8186915887850468 	 lr:0.0001
epoch46: train: loss:1.2891343818801535 	 acc:0.8603125 | test: loss:1.3866272437609617 	 acc:0.8006230529595015 	 lr:0.0001
epoch47: train: loss:1.3014492728764149 	 acc:0.8328125 | test: loss:1.4125571960972105 	 acc:0.7489096573208722 	 lr:0.0001
epoch48: train: loss:1.2595006652403213 	 acc:0.88 | test: loss:1.3638088487019049 	 acc:0.8143302180685358 	 lr:0.0001
epoch49: train: loss:1.2520947470504915 	 acc:0.921875 | test: loss:1.3256503489901343 	 acc:0.8411214953271028 	 lr:5e-05
epoch50: train: loss:1.2404365557418215 	 acc:0.9128125 | test: loss:1.3328656395647756 	 acc:0.8448598130841122 	 lr:5e-05
epoch51: train: loss:1.238985036333309 	 acc:0.92359375 | test: loss:1.3219978211453398 	 acc:0.8517133956386292 	 lr:5e-05
epoch52: train: loss:1.2327212950850166 	 acc:0.9296875 | test: loss:1.3185727869610178 	 acc:0.8554517133956386 	 lr:5e-05
epoch53: train: loss:1.2302372237837567 	 acc:0.9278125 | test: loss:1.3235543419638898 	 acc:0.8529595015576324 	 lr:5e-05
epoch54: train: loss:1.2332050380066537 	 acc:0.9334375 | test: loss:1.3178757561330112 	 acc:0.8523364485981308 	 lr:5e-05
epoch55: train: loss:1.2400341001178583 	 acc:0.90578125 | test: loss:1.3348973326965285 	 acc:0.8367601246105919 	 lr:5e-05
epoch56: train: loss:1.2339334409000537 	 acc:0.93890625 | test: loss:1.3155846361802004 	 acc:0.8573208722741433 	 lr:5e-05
epoch57: train: loss:1.2214626577289471 	 acc:0.9365625 | test: loss:1.313152634020535 	 acc:0.8554517133956386 	 lr:5e-05
epoch58: train: loss:1.2170780226553353 	 acc:0.93828125 | test: loss:1.3186244744749456 	 acc:0.8542056074766355 	 lr:5e-05
epoch59: train: loss:1.2291426838049193 	 acc:0.91125 | test: loss:1.3443590589030139 	 acc:0.8280373831775701 	 lr:5e-05
epoch60: train: loss:1.2189226518284055 	 acc:0.92703125 | test: loss:1.328492199446182 	 acc:0.8504672897196262 	 lr:5e-05
epoch61: train: loss:1.2297447210545656 	 acc:0.94859375 | test: loss:1.3146888299151744 	 acc:0.8579439252336448 	 lr:5e-05
epoch62: train: loss:1.2205376582625878 	 acc:0.94140625 | test: loss:1.3134510969447197 	 acc:0.854828660436137 	 lr:5e-05
epoch63: train: loss:1.216541663712584 	 acc:0.93609375 | test: loss:1.3233023640522705 	 acc:0.8492211838006231 	 lr:5e-05
epoch64: train: loss:1.2103423881679658 	 acc:0.9525 | test: loss:1.3083857563797188 	 acc:0.8660436137071651 	 lr:2.5e-05
epoch65: train: loss:1.2171466669563573 	 acc:0.93875 | test: loss:1.3154872238450332 	 acc:0.8598130841121495 	 lr:2.5e-05
epoch66: train: loss:1.2130472842833662 	 acc:0.9453125 | test: loss:1.3123827230893192 	 acc:0.8604361370716511 	 lr:2.5e-05
epoch67: train: loss:1.2144098071917997 	 acc:0.95109375 | test: loss:1.3090247963819177 	 acc:0.8604361370716511 	 lr:2.5e-05
epoch68: train: loss:1.2119824661862375 	 acc:0.94796875 | test: loss:1.3154803327310864 	 acc:0.854828660436137 	 lr:2.5e-05
epoch69: train: loss:1.206954815497536 	 acc:0.9465625 | test: loss:1.3117073134841206 	 acc:0.8610591900311526 	 lr:2.5e-05
epoch70: train: loss:1.2133992489197587 	 acc:0.95921875 | test: loss:1.3044666726269827 	 acc:0.8654205607476636 	 lr:2.5e-05
epoch71: train: loss:1.2125518870297862 	 acc:0.95203125 | test: loss:1.3095978622495943 	 acc:0.8616822429906542 	 lr:2.5e-05
epoch72: train: loss:1.2109179935261758 	 acc:0.9575 | test: loss:1.3036362664350467 	 acc:0.864797507788162 	 lr:2.5e-05
epoch73: train: loss:1.2123560361244268 	 acc:0.94703125 | test: loss:1.3114676451757319 	 acc:0.8616822429906542 	 lr:2.5e-05
epoch74: train: loss:1.2080476402026614 	 acc:0.9434375 | test: loss:1.3156269285909112 	 acc:0.8579439252336448 	 lr:2.5e-05
epoch75: train: loss:1.2146513344067136 	 acc:0.9525 | test: loss:1.312747771866225 	 acc:0.8685358255451714 	 lr:2.5e-05
epoch76: train: loss:1.2087604769871256 	 acc:0.9515625 | test: loss:1.314296610853011 	 acc:0.8554517133956386 	 lr:2.5e-05
epoch77: train: loss:1.2118330896691734 	 acc:0.93859375 | test: loss:1.324308714019918 	 acc:0.8492211838006231 	 lr:2.5e-05
epoch78: train: loss:1.2041092836140284 	 acc:0.949375 | test: loss:1.3115065487745767 	 acc:0.864797507788162 	 lr:2.5e-05
epoch79: train: loss:1.2032008937147798 	 acc:0.95578125 | test: loss:1.3079002625474305 	 acc:0.864797507788162 	 lr:1.25e-05
epoch80: train: loss:1.2058685255087882 	 acc:0.9475 | test: loss:1.315603126395157 	 acc:0.8598130841121495 	 lr:1.25e-05
epoch81: train: loss:1.2072700113360533 	 acc:0.95703125 | test: loss:1.3079741575264856 	 acc:0.8629283489096573 	 lr:1.25e-05
epoch82: train: loss:1.204954839701954 	 acc:0.9571875 | test: loss:1.3086700518925984 	 acc:0.8598130841121495 	 lr:1.25e-05
epoch83: train: loss:1.2057127095683304 	 acc:0.95765625 | test: loss:1.3074027214466226 	 acc:0.8672897196261682 	 lr:1.25e-05
epoch84: train: loss:1.205673815736912 	 acc:0.96140625 | test: loss:1.3005207199173925 	 acc:0.8735202492211838 	 lr:1.25e-05
epoch85: train: loss:1.208511462125994 	 acc:0.954375 | test: loss:1.3041000172356578 	 acc:0.8654205607476636 	 lr:1.25e-05
epoch86: train: loss:1.2054589824691404 	 acc:0.95453125 | test: loss:1.3048522459755063 	 acc:0.8679127725856698 	 lr:1.25e-05
epoch87: train: loss:1.2045861964110376 	 acc:0.965 | test: loss:1.2968571039746484 	 acc:0.8722741433021807 	 lr:1.25e-05
epoch88: train: loss:1.2064743030080565 	 acc:0.95234375 | test: loss:1.3021374385304911 	 acc:0.8697819314641745 	 lr:1.25e-05
epoch89: train: loss:1.204685946724361 	 acc:0.94296875 | test: loss:1.3179314247915679 	 acc:0.8535825545171339 	 lr:1.25e-05
epoch90: train: loss:1.1998259068652115 	 acc:0.95703125 | test: loss:1.3047350839662404 	 acc:0.864797507788162 	 lr:1.25e-05
epoch91: train: loss:1.203874171999262 	 acc:0.95796875 | test: loss:1.3066218541047283 	 acc:0.864797507788162 	 lr:1.25e-05
epoch92: train: loss:1.197438124359631 	 acc:0.9659375 | test: loss:1.2988295919798616 	 acc:0.874766355140187 	 lr:1.25e-05
epoch93: train: loss:1.2016981852975885 	 acc:0.95234375 | test: loss:1.306189891853808 	 acc:0.8654205607476636 	 lr:1.25e-05
epoch94: train: loss:1.2067495305867608 	 acc:0.95890625 | test: loss:1.3004533186879856 	 acc:0.8735202492211838 	 lr:6.25e-06
epoch95: train: loss:1.1971412986261039 	 acc:0.96140625 | test: loss:1.3027254874832535 	 acc:0.8685358255451714 	 lr:6.25e-06
epoch96: train: loss:1.2000377860504794 	 acc:0.9559375 | test: loss:1.30484914556842 	 acc:0.8728971962616823 	 lr:6.25e-06
epoch97: train: loss:1.2012593992234766 	 acc:0.9621875 | test: loss:1.3004678205537648 	 acc:0.8716510903426792 	 lr:6.25e-06
epoch98: train: loss:1.2008775935891454 	 acc:0.96203125 | test: loss:1.2994983887003961 	 acc:0.8722741433021807 	 lr:6.25e-06
epoch99: train: loss:1.202661275751976 	 acc:0.9609375 | test: loss:1.2996185846417865 	 acc:0.870404984423676 	 lr:6.25e-06
epoch100: train: loss:1.2010232489151846 	 acc:0.9628125 | test: loss:1.3011919405601478 	 acc:0.8710280373831776 	 lr:3.125e-06
epoch101: train: loss:1.1955246722111936 	 acc:0.96171875 | test: loss:1.3004522837582406 	 acc:0.8710280373831776 	 lr:3.125e-06
epoch102: train: loss:1.1987121605854496 	 acc:0.96515625 | test: loss:1.3005338886445186 	 acc:0.8722741433021807 	 lr:3.125e-06
epoch103: train: loss:1.205230951123085 	 acc:0.9621875 | test: loss:1.304090492376286 	 acc:0.8716510903426792 	 lr:3.125e-06
epoch104: train: loss:1.2008210491147664 	 acc:0.95859375 | test: loss:1.3039020417263947 	 acc:0.8716510903426792 	 lr:3.125e-06
epoch105: train: loss:1.196984589313921 	 acc:0.96296875 | test: loss:1.2995879745186303 	 acc:0.874766355140187 	 lr:3.125e-06
epoch106: train: loss:1.2007670727863058 	 acc:0.96375 | test: loss:1.2992400092870646 	 acc:0.8735202492211838 	 lr:1.5625e-06
epoch107: train: loss:1.2013354096721616 	 acc:0.96453125 | test: loss:1.2994020877968857 	 acc:0.8722741433021807 	 lr:1.5625e-06
epoch108: train: loss:1.208797179079912 	 acc:0.959375 | test: loss:1.3014250594878864 	 acc:0.870404984423676 	 lr:1.5625e-06
epoch109: train: loss:1.205523657817379 	 acc:0.96171875 | test: loss:1.2990244436858227 	 acc:0.874766355140187 	 lr:1.5625e-06
epoch110: train: loss:1.1993294447125353 	 acc:0.9615625 | test: loss:1.2988519149405935 	 acc:0.8716510903426792 	 lr:1.5625e-06
epoch111: train: loss:1.1972188289978838 	 acc:0.96234375 | test: loss:1.300659572224008 	 acc:0.8716510903426792 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_1_3/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_1_3/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.9215631298866243 	 acc:0.33140625 | test: loss:1.9123708199117786 	 acc:0.36386292834890965 	 lr:0.0001
epoch1: train: loss:1.8565814814094823 	 acc:0.365 | test: loss:1.8713805203125855 	 acc:0.3862928348909657 	 lr:0.0001
epoch2: train: loss:1.8082660755452284 	 acc:0.4128125 | test: loss:1.7856572349494864 	 acc:0.4442367601246106 	 lr:0.0001
epoch3: train: loss:1.7675383235029092 	 acc:0.48359375 | test: loss:1.725933006173725 	 acc:0.4716510903426791 	 lr:0.0001
epoch4: train: loss:1.7503453489209786 	 acc:0.4946875 | test: loss:1.7454807520655458 	 acc:0.4672897196261682 	 lr:0.0001
epoch5: train: loss:1.7509766029995182 	 acc:0.52359375 | test: loss:1.6736963372364222 	 acc:0.5414330218068536 	 lr:0.0001
epoch6: train: loss:1.7297207596337396 	 acc:0.46 | test: loss:1.7438750586405722 	 acc:0.4442367601246106 	 lr:0.0001
epoch7: train: loss:1.7092447559317976 	 acc:0.5596875 | test: loss:1.6524131260928334 	 acc:0.5557632398753894 	 lr:0.0001
epoch8: train: loss:1.7162231755014699 	 acc:0.4509375 | test: loss:1.7671152391166332 	 acc:0.4311526479750779 	 lr:0.0001
epoch9: train: loss:1.677647103302931 	 acc:0.51515625 | test: loss:1.6662016699990008 	 acc:0.508411214953271 	 lr:0.0001
epoch10: train: loss:1.6781643440907676 	 acc:0.49453125 | test: loss:1.69198255791471 	 acc:0.4928348909657321 	 lr:0.0001
epoch11: train: loss:1.6768023214705003 	 acc:0.58453125 | test: loss:1.6150517956861454 	 acc:0.5925233644859813 	 lr:0.0001
epoch12: train: loss:1.6463267651691185 	 acc:0.59421875 | test: loss:1.6022813356554026 	 acc:0.6031152647975078 	 lr:0.0001
epoch13: train: loss:1.6722163275570538 	 acc:0.6084375 | test: loss:1.605696424145565 	 acc:0.6130841121495327 	 lr:0.0001
epoch14: train: loss:1.6406188035737157 	 acc:0.6040625 | test: loss:1.5913519128460751 	 acc:0.6180685358255452 	 lr:0.0001
epoch15: train: loss:1.64495944902359 	 acc:0.52296875 | test: loss:1.6731172474745277 	 acc:0.5152647975077882 	 lr:0.0001
epoch16: train: loss:1.629583168476471 	 acc:0.5790625 | test: loss:1.6054160770226118 	 acc:0.6012461059190031 	 lr:0.0001
epoch17: train: loss:1.6335186710402334 	 acc:0.6346875 | test: loss:1.57625415770807 	 acc:0.6454828660436137 	 lr:0.0001
epoch18: train: loss:1.6106409005873097 	 acc:0.5771875 | test: loss:1.6073823184610527 	 acc:0.5688473520249221 	 lr:0.0001
epoch19: train: loss:1.626750227178474 	 acc:0.59359375 | test: loss:1.6059104705525336 	 acc:0.5838006230529595 	 lr:0.0001
epoch20: train: loss:1.60225189947505 	 acc:0.60453125 | test: loss:1.5941083345829141 	 acc:0.5894080996884735 	 lr:0.0001
epoch21: train: loss:1.6017360142299106 	 acc:0.6775 | test: loss:1.5518730707257709 	 acc:0.6529595015576324 	 lr:0.0001
epoch22: train: loss:1.6207307814062806 	 acc:0.706875 | test: loss:1.5190597130130756 	 acc:0.6978193146417445 	 lr:0.0001
epoch23: train: loss:1.5776872430901152 	 acc:0.6796875 | test: loss:1.5478628835945485 	 acc:0.664797507788162 	 lr:0.0001
epoch24: train: loss:1.5704455599758795 	 acc:0.66515625 | test: loss:1.5680600569626995 	 acc:0.6249221183800623 	 lr:0.0001
epoch25: train: loss:1.5814963871570977 	 acc:0.60328125 | test: loss:1.5820890062694608 	 acc:0.5900311526479751 	 lr:0.0001
epoch26: train: loss:1.5828750050505282 	 acc:0.61953125 | test: loss:1.5887702523733596 	 acc:0.5912772585669782 	 lr:0.0001
epoch27: train: loss:1.5786603154845762 	 acc:0.655625 | test: loss:1.573557318630991 	 acc:0.6348909657320873 	 lr:0.0001
epoch28: train: loss:1.54944857047183 	 acc:0.6109375 | test: loss:1.5924808478429682 	 acc:0.5800623052959502 	 lr:0.0001
epoch29: train: loss:1.5421476079745744 	 acc:0.69140625 | test: loss:1.534135589792721 	 acc:0.6517133956386293 	 lr:5e-05
epoch30: train: loss:1.5452682030563891 	 acc:0.72546875 | test: loss:1.5180047962895806 	 acc:0.6922118380062305 	 lr:5e-05
epoch31: train: loss:1.531557165022291 	 acc:0.66546875 | test: loss:1.563700587207283 	 acc:0.6186915887850467 	 lr:5e-05
epoch32: train: loss:1.5214895875262247 	 acc:0.69796875 | test: loss:1.5348514761880179 	 acc:0.6554517133956387 	 lr:5e-05
epoch33: train: loss:1.5187354797315635 	 acc:0.65734375 | test: loss:1.560864158360015 	 acc:0.6174454828660436 	 lr:5e-05
epoch34: train: loss:1.5226542702882575 	 acc:0.6690625 | test: loss:1.5660341149921357 	 acc:0.6199376947040498 	 lr:5e-05
epoch35: train: loss:1.5242916086332394 	 acc:0.66921875 | test: loss:1.5647448448377235 	 acc:0.6280373831775701 	 lr:5e-05
epoch36: train: loss:1.5188579645685438 	 acc:0.6975 | test: loss:1.5294199312216026 	 acc:0.6635514018691588 	 lr:5e-05
epoch37: train: loss:1.504391322258764 	 acc:0.6675 | test: loss:1.5629191273840788 	 acc:0.616822429906542 	 lr:2.5e-05
epoch38: train: loss:1.50210624891366 	 acc:0.711875 | test: loss:1.5290417191395507 	 acc:0.6679127725856698 	 lr:2.5e-05
epoch39: train: loss:1.5006173341559024 	 acc:0.706875 | test: loss:1.5291627804438273 	 acc:0.6635514018691588 	 lr:2.5e-05
epoch40: train: loss:1.4956165346477666 	 acc:0.72671875 | test: loss:1.5136795304646002 	 acc:0.6890965732087228 	 lr:2.5e-05
epoch41: train: loss:1.4901628862034055 	 acc:0.69796875 | test: loss:1.5285049021058366 	 acc:0.6485981308411215 	 lr:2.5e-05
epoch42: train: loss:1.4868852435193147 	 acc:0.69015625 | test: loss:1.5369644888464906 	 acc:0.6461059190031153 	 lr:2.5e-05
epoch43: train: loss:1.4902881202429743 	 acc:0.70328125 | test: loss:1.5273313528280763 	 acc:0.659190031152648 	 lr:2.5e-05
epoch44: train: loss:1.4868369013885332 	 acc:0.6928125 | test: loss:1.531447457524475 	 acc:0.6510903426791277 	 lr:2.5e-05
epoch45: train: loss:1.493096542656189 	 acc:0.71859375 | test: loss:1.5174799125885294 	 acc:0.6828660436137072 	 lr:2.5e-05
epoch46: train: loss:1.4799410198369498 	 acc:0.7075 | test: loss:1.5258283069200604 	 acc:0.6635514018691588 	 lr:2.5e-05
epoch47: train: loss:1.4764947228651322 	 acc:0.6959375 | test: loss:1.5279987015085428 	 acc:0.6573208722741433 	 lr:1.25e-05
epoch48: train: loss:1.4800308167980947 	 acc:0.70046875 | test: loss:1.5245811503995617 	 acc:0.6598130841121496 	 lr:1.25e-05
epoch49: train: loss:1.4729540810745085 	 acc:0.710625 | test: loss:1.5237697445343588 	 acc:0.6598130841121496 	 lr:1.25e-05
epoch50: train: loss:1.4655302525683365 	 acc:0.72390625 | test: loss:1.509775458020956 	 acc:0.6753894080996885 	 lr:1.25e-05
epoch51: train: loss:1.47496787435277 	 acc:0.71546875 | test: loss:1.5093436589493558 	 acc:0.6809968847352025 	 lr:1.25e-05
epoch52: train: loss:1.4678184920228337 	 acc:0.71234375 | test: loss:1.5147710270599413 	 acc:0.6722741433021807 	 lr:1.25e-05
epoch53: train: loss:1.4755611922943805 	 acc:0.71484375 | test: loss:1.5144898934527722 	 acc:0.6716510903426791 	 lr:1.25e-05
epoch54: train: loss:1.4627514448322234 	 acc:0.72796875 | test: loss:1.5095014430280786 	 acc:0.6809968847352025 	 lr:1.25e-05
epoch55: train: loss:1.4683361616290984 	 acc:0.72140625 | test: loss:1.510210449450484 	 acc:0.6791277258566978 	 lr:1.25e-05
epoch56: train: loss:1.4652087125994095 	 acc:0.72375 | test: loss:1.5034103631230529 	 acc:0.6866043613707166 	 lr:1.25e-05
epoch57: train: loss:1.4679431876570428 	 acc:0.7171875 | test: loss:1.5086215328204668 	 acc:0.6753894080996885 	 lr:1.25e-05
epoch58: train: loss:1.4571704325501011 	 acc:0.72140625 | test: loss:1.5085073915226066 	 acc:0.6728971962616822 	 lr:1.25e-05
epoch59: train: loss:1.460815012780695 	 acc:0.72875 | test: loss:1.5035738176274522 	 acc:0.6853582554517134 	 lr:1.25e-05
epoch60: train: loss:1.458934184185329 	 acc:0.71765625 | test: loss:1.5071949695872369 	 acc:0.6809968847352025 	 lr:1.25e-05
epoch61: train: loss:1.4629380713767322 	 acc:0.7340625 | test: loss:1.497549636861617 	 acc:0.6866043613707166 	 lr:1.25e-05
epoch62: train: loss:1.4560443053293934 	 acc:0.70890625 | test: loss:1.5159988805141034 	 acc:0.6623052959501557 	 lr:1.25e-05
epoch63: train: loss:1.4574837868517028 	 acc:0.74453125 | test: loss:1.4966731546823853 	 acc:0.6841121495327103 	 lr:1.25e-05
epoch64: train: loss:1.457522461509258 	 acc:0.7296875 | test: loss:1.5044107876834096 	 acc:0.6847352024922119 	 lr:1.25e-05
epoch65: train: loss:1.4572834023826295 	 acc:0.7209375 | test: loss:1.5173207633591888 	 acc:0.670404984423676 	 lr:1.25e-05
epoch66: train: loss:1.460188482851837 	 acc:0.7240625 | test: loss:1.5046340565072414 	 acc:0.6841121495327103 	 lr:1.25e-05
epoch67: train: loss:1.4541195871679231 	 acc:0.743125 | test: loss:1.4920779457716185 	 acc:0.7021806853582554 	 lr:1.25e-05
epoch68: train: loss:1.4549420799713968 	 acc:0.726875 | test: loss:1.508623945527359 	 acc:0.6866043613707166 	 lr:1.25e-05
epoch69: train: loss:1.4464272730616645 	 acc:0.72578125 | test: loss:1.5090942527646216 	 acc:0.6828660436137072 	 lr:1.25e-05
epoch70: train: loss:1.4472824751818953 	 acc:0.72421875 | test: loss:1.5038559762116905 	 acc:0.6878504672897197 	 lr:1.25e-05
epoch71: train: loss:1.4479285806719908 	 acc:0.73578125 | test: loss:1.4958085314878422 	 acc:0.6928348909657321 	 lr:1.25e-05
epoch72: train: loss:1.444312428720848 	 acc:0.734375 | test: loss:1.4932074254175585 	 acc:0.6953271028037383 	 lr:1.25e-05
epoch73: train: loss:1.4456906344721225 	 acc:0.71234375 | test: loss:1.5155803913639343 	 acc:0.6772585669781932 	 lr:1.25e-05
epoch74: train: loss:1.4428616549799351 	 acc:0.7190625 | test: loss:1.513349754862325 	 acc:0.6791277258566978 	 lr:6.25e-06
epoch75: train: loss:1.4426121565069099 	 acc:0.720625 | test: loss:1.504715372097455 	 acc:0.6841121495327103 	 lr:6.25e-06
epoch76: train: loss:1.4452375473034373 	 acc:0.7284375 | test: loss:1.4993250372253846 	 acc:0.6890965732087228 	 lr:6.25e-06
epoch77: train: loss:1.4449368955566024 	 acc:0.73703125 | test: loss:1.500157010889499 	 acc:0.6940809968847352 	 lr:6.25e-06
epoch78: train: loss:1.4421307363517577 	 acc:0.7425 | test: loss:1.4941770372361038 	 acc:0.6978193146417445 	 lr:6.25e-06
epoch79: train: loss:1.4378187121496269 	 acc:0.73296875 | test: loss:1.4987417529305194 	 acc:0.6872274143302181 	 lr:6.25e-06
epoch80: train: loss:1.4419618819487643 	 acc:0.7346875 | test: loss:1.4992366385979816 	 acc:0.6922118380062305 	 lr:3.125e-06
epoch81: train: loss:1.4455043724232777 	 acc:0.72625 | test: loss:1.5027727618024356 	 acc:0.6866043613707166 	 lr:3.125e-06
epoch82: train: loss:1.4464074687972652 	 acc:0.73546875 | test: loss:1.4965843914453858 	 acc:0.697196261682243 	 lr:3.125e-06
epoch83: train: loss:1.4315581227167802 	 acc:0.73546875 | test: loss:1.500304537101698 	 acc:0.6978193146417445 	 lr:3.125e-06
epoch84: train: loss:1.4398449792794936 	 acc:0.72765625 | test: loss:1.502672198702613 	 acc:0.6897196261682244 	 lr:3.125e-06
epoch85: train: loss:1.4460820480215653 	 acc:0.7278125 | test: loss:1.4987661381005497 	 acc:0.6922118380062305 	 lr:3.125e-06
epoch86: train: loss:1.4391096462038324 	 acc:0.7334375 | test: loss:1.4961686966946564 	 acc:0.6990654205607477 	 lr:1.5625e-06
epoch87: train: loss:1.449649216996609 	 acc:0.72984375 | test: loss:1.4987863060098572 	 acc:0.6978193146417445 	 lr:1.5625e-06
epoch88: train: loss:1.4386293338296192 	 acc:0.735 | test: loss:1.4971972580639372 	 acc:0.6959501557632399 	 lr:1.5625e-06
epoch89: train: loss:1.4362301995473201 	 acc:0.73484375 | test: loss:1.4984610753638723 	 acc:0.6953271028037383 	 lr:1.5625e-06
epoch90: train: loss:1.4327600030951162 	 acc:0.7334375 | test: loss:1.5032145850012235 	 acc:0.6915887850467289 	 lr:1.5625e-06
epoch91: train: loss:1.4378596166630073 	 acc:0.73796875 | test: loss:1.4942352817808728 	 acc:0.6996884735202492 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_2_3/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_2_3/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.877049566264454 	 acc:0.49671875 | test: loss:1.8637001492895442 	 acc:0.4604361370716511 	 lr:0.0001
epoch1: train: loss:1.8029823630792288 	 acc:0.39953125 | test: loss:1.7953875801644965 	 acc:0.3744548286604361 	 lr:0.0001
epoch2: train: loss:1.7474243378471714 	 acc:0.55203125 | test: loss:1.6879354280103405 	 acc:0.5320872274143302 	 lr:0.0001
epoch3: train: loss:1.7178824860262367 	 acc:0.47265625 | test: loss:1.7206957094394528 	 acc:0.4523364485981308 	 lr:0.0001
epoch4: train: loss:1.6460208437202686 	 acc:0.54234375 | test: loss:1.6746742705318416 	 acc:0.514018691588785 	 lr:0.0001
epoch5: train: loss:1.6291839337553669 	 acc:0.62296875 | test: loss:1.593232333325894 	 acc:0.5931464174454829 	 lr:0.0001
epoch6: train: loss:1.6361040721360265 	 acc:0.618125 | test: loss:1.5940212791211137 	 acc:0.6255451713395639 	 lr:0.0001
epoch7: train: loss:1.6088098935463018 	 acc:0.593125 | test: loss:1.6302893465553117 	 acc:0.5613707165109034 	 lr:0.0001
epoch8: train: loss:1.5928045840118195 	 acc:0.6203125 | test: loss:1.596018373706259 	 acc:0.5987538940809969 	 lr:0.0001
epoch9: train: loss:1.6037564946933243 	 acc:0.65359375 | test: loss:1.5444478901747232 	 acc:0.6361370716510903 	 lr:0.0001
epoch10: train: loss:1.564153600092701 	 acc:0.655625 | test: loss:1.5465652547521382 	 acc:0.6485981308411215 	 lr:0.0001
epoch11: train: loss:1.5403062521155042 	 acc:0.55796875 | test: loss:1.647757536748488 	 acc:0.5152647975077882 	 lr:0.0001
epoch12: train: loss:1.579677722939842 	 acc:0.716875 | test: loss:1.5242754376194558 	 acc:0.6947040498442367 	 lr:0.0001
epoch13: train: loss:1.5167916910616706 	 acc:0.66234375 | test: loss:1.5523275939103598 	 acc:0.621183800623053 	 lr:0.0001
epoch14: train: loss:1.5378414299970116 	 acc:0.66828125 | test: loss:1.5188675151064388 	 acc:0.6566978193146418 	 lr:0.0001
epoch15: train: loss:1.5309182875050316 	 acc:0.70359375 | test: loss:1.5480256197237152 	 acc:0.6616822429906543 	 lr:0.0001
epoch16: train: loss:1.5295868515503026 	 acc:0.6759375 | test: loss:1.5351113586039558 	 acc:0.6498442367601246 	 lr:0.0001
epoch17: train: loss:1.5307357156769918 	 acc:0.60484375 | test: loss:1.5823437184930962 	 acc:0.5757009345794393 	 lr:0.0001
epoch18: train: loss:1.5107292681536946 	 acc:0.74421875 | test: loss:1.5022938454262564 	 acc:0.7102803738317757 	 lr:0.0001
epoch19: train: loss:1.4720522425679097 	 acc:0.6971875 | test: loss:1.5131910789050045 	 acc:0.6685358255451713 	 lr:0.0001
epoch20: train: loss:1.4734335381941903 	 acc:0.6790625 | test: loss:1.5254283296727689 	 acc:0.6542056074766355 	 lr:0.0001
epoch21: train: loss:1.4601776813921008 	 acc:0.70765625 | test: loss:1.5016885931246748 	 acc:0.6822429906542056 	 lr:0.0001
epoch22: train: loss:1.4963423956008184 	 acc:0.705 | test: loss:1.51052537736863 	 acc:0.6685358255451713 	 lr:0.0001
epoch23: train: loss:1.500006317924839 	 acc:0.724375 | test: loss:1.5120284243164777 	 acc:0.6834890965732088 	 lr:0.0001
epoch24: train: loss:1.47498624445776 	 acc:0.691875 | test: loss:1.5224960294468008 	 acc:0.6548286604361371 	 lr:0.0001
epoch25: train: loss:1.4417978755763319 	 acc:0.735625 | test: loss:1.4923309835689462 	 acc:0.6928348909657321 	 lr:0.0001
epoch26: train: loss:1.4622599045621707 	 acc:0.6790625 | test: loss:1.5408267479447932 	 acc:0.6330218068535826 	 lr:0.0001
epoch27: train: loss:1.4298896926534446 	 acc:0.743125 | test: loss:1.4829498209314553 	 acc:0.7021806853582554 	 lr:0.0001
epoch28: train: loss:1.425794045465426 	 acc:0.729375 | test: loss:1.4583743727467142 	 acc:0.7146417445482866 	 lr:0.0001
epoch29: train: loss:1.446449981379751 	 acc:0.735625 | test: loss:1.4914432291672608 	 acc:0.7034267912772586 	 lr:0.0001
epoch30: train: loss:1.417487215493267 	 acc:0.80375 | test: loss:1.4134276364079887 	 acc:0.7838006230529595 	 lr:0.0001
epoch31: train: loss:1.422517589178986 	 acc:0.7421875 | test: loss:1.4742577568392887 	 acc:0.7059190031152648 	 lr:0.0001
epoch32: train: loss:1.4261729065465518 	 acc:0.77390625 | test: loss:1.4694509114803183 	 acc:0.7177570093457943 	 lr:0.0001
epoch33: train: loss:1.4402123647030214 	 acc:0.6834375 | test: loss:1.5350420278923533 	 acc:0.6299065420560748 	 lr:0.0001
epoch34: train: loss:1.423028838420454 	 acc:0.74078125 | test: loss:1.4682621193080676 	 acc:0.6928348909657321 	 lr:0.0001
epoch35: train: loss:1.4203758317171644 	 acc:0.75265625 | test: loss:1.4641576821187574 	 acc:0.7152647975077882 	 lr:0.0001
epoch36: train: loss:1.4047488853579662 	 acc:0.7515625 | test: loss:1.470347177573825 	 acc:0.7271028037383177 	 lr:0.0001
epoch37: train: loss:1.3910017574885039 	 acc:0.716875 | test: loss:1.4882312072771733 	 acc:0.685981308411215 	 lr:5e-05
epoch38: train: loss:1.3683838748261856 	 acc:0.77296875 | test: loss:1.4599753953957484 	 acc:0.712772585669782 	 lr:5e-05
epoch39: train: loss:1.3476265805294325 	 acc:0.80421875 | test: loss:1.4287532463251988 	 acc:0.7532710280373832 	 lr:5e-05
epoch40: train: loss:1.350918045460852 	 acc:0.79078125 | test: loss:1.4398961350182506 	 acc:0.735202492211838 	 lr:5e-05
epoch41: train: loss:1.370921672311823 	 acc:0.809375 | test: loss:1.4357097874549318 	 acc:0.7545171339563863 	 lr:5e-05
epoch42: train: loss:1.3655434720875415 	 acc:0.76125 | test: loss:1.4582632196283787 	 acc:0.7246105919003115 	 lr:5e-05
epoch43: train: loss:1.3314717253328394 	 acc:0.806875 | test: loss:1.433655640937829 	 acc:0.7420560747663552 	 lr:2.5e-05
epoch44: train: loss:1.340166295309908 	 acc:0.798125 | test: loss:1.4364281753141932 	 acc:0.7401869158878505 	 lr:2.5e-05
epoch45: train: loss:1.3372454486164984 	 acc:0.83234375 | test: loss:1.4133414202390058 	 acc:0.7713395638629283 	 lr:2.5e-05
epoch46: train: loss:1.3323969701786325 	 acc:0.8434375 | test: loss:1.4062174488822248 	 acc:0.7850467289719626 	 lr:2.5e-05
epoch47: train: loss:1.3311232055378184 	 acc:0.8078125 | test: loss:1.4370783319354428 	 acc:0.7370716510903427 	 lr:2.5e-05
epoch48: train: loss:1.3259551120493023 	 acc:0.839375 | test: loss:1.4098777017860769 	 acc:0.7757009345794392 	 lr:2.5e-05
epoch49: train: loss:1.3229350129484108 	 acc:0.8125 | test: loss:1.4193084217677607 	 acc:0.7588785046728972 	 lr:2.5e-05
epoch50: train: loss:1.3244178218082188 	 acc:0.79828125 | test: loss:1.435800010244423 	 acc:0.7314641744548287 	 lr:2.5e-05
epoch51: train: loss:1.3299147308849897 	 acc:0.7959375 | test: loss:1.4431298901358869 	 acc:0.7333333333333333 	 lr:2.5e-05
epoch52: train: loss:1.323643220876177 	 acc:0.81953125 | test: loss:1.4026990475312944 	 acc:0.7707165109034267 	 lr:2.5e-05
epoch53: train: loss:1.3226344040834188 	 acc:0.78515625 | test: loss:1.4459172778411817 	 acc:0.7271028037383177 	 lr:2.5e-05
epoch54: train: loss:1.3155450420394528 	 acc:0.81859375 | test: loss:1.41483335673252 	 acc:0.7644859813084112 	 lr:2.5e-05
epoch55: train: loss:1.3134654227501708 	 acc:0.82734375 | test: loss:1.4165325049670685 	 acc:0.7632398753894081 	 lr:2.5e-05
epoch56: train: loss:1.3283411078114327 	 acc:0.8378125 | test: loss:1.3981462652438155 	 acc:0.7894080996884735 	 lr:2.5e-05
epoch57: train: loss:1.321417426914093 	 acc:0.83828125 | test: loss:1.4017024340287918 	 acc:0.7894080996884735 	 lr:2.5e-05
epoch58: train: loss:1.3232511474228204 	 acc:0.83421875 | test: loss:1.4018824670916405 	 acc:0.7862928348909657 	 lr:2.5e-05
epoch59: train: loss:1.3135053385988424 	 acc:0.825625 | test: loss:1.411686185958601 	 acc:0.7744548286604361 	 lr:2.5e-05
epoch60: train: loss:1.3050529462113034 	 acc:0.8215625 | test: loss:1.4199877089800492 	 acc:0.7532710280373832 	 lr:2.5e-05
epoch61: train: loss:1.3114996047247023 	 acc:0.83671875 | test: loss:1.407201092711119 	 acc:0.7781931464174455 	 lr:2.5e-05
epoch62: train: loss:1.3144862011947454 	 acc:0.824375 | test: loss:1.4126513833940215 	 acc:0.7713395638629283 	 lr:2.5e-05
epoch63: train: loss:1.3065422087884526 	 acc:0.825625 | test: loss:1.4128447332857554 	 acc:0.7669781931464175 	 lr:1.25e-05
epoch64: train: loss:1.2939949458507147 	 acc:0.8284375 | test: loss:1.4092694145125393 	 acc:0.7713395638629283 	 lr:1.25e-05
epoch65: train: loss:1.3061726268616438 	 acc:0.83578125 | test: loss:1.4107935100329627 	 acc:0.7700934579439253 	 lr:1.25e-05
epoch66: train: loss:1.3015816943893015 	 acc:0.830625 | test: loss:1.404477699523403 	 acc:0.7794392523364486 	 lr:1.25e-05
epoch67: train: loss:1.3112247368770127 	 acc:0.83859375 | test: loss:1.4086391908728817 	 acc:0.7775700934579439 	 lr:1.25e-05
epoch68: train: loss:1.2934115097915446 	 acc:0.83890625 | test: loss:1.405981935816019 	 acc:0.7775700934579439 	 lr:1.25e-05
epoch69: train: loss:1.308290944631727 	 acc:0.83421875 | test: loss:1.4040075751479912 	 acc:0.7800623052959501 	 lr:6.25e-06
epoch70: train: loss:1.2948367902769138 	 acc:0.83765625 | test: loss:1.4036483839664875 	 acc:0.778816199376947 	 lr:6.25e-06
epoch71: train: loss:1.2958290975601947 	 acc:0.83859375 | test: loss:1.4076214704186745 	 acc:0.7707165109034267 	 lr:6.25e-06
epoch72: train: loss:1.2962799786963004 	 acc:0.8328125 | test: loss:1.403746341544891 	 acc:0.77196261682243 	 lr:6.25e-06
epoch73: train: loss:1.2928136511392467 	 acc:0.82484375 | test: loss:1.419120786271734 	 acc:0.7576323987538941 	 lr:6.25e-06
epoch74: train: loss:1.2954560564980666 	 acc:0.84390625 | test: loss:1.398947753787412 	 acc:0.7838006230529595 	 lr:6.25e-06
epoch75: train: loss:1.2920021098819587 	 acc:0.83828125 | test: loss:1.4028589355611356 	 acc:0.7800623052959501 	 lr:3.125e-06
epoch76: train: loss:1.2945668085769784 	 acc:0.84484375 | test: loss:1.40127866951476 	 acc:0.7813084112149533 	 lr:3.125e-06
epoch77: train: loss:1.2955536722485486 	 acc:0.840625 | test: loss:1.4019072315774603 	 acc:0.7850467289719626 	 lr:3.125e-06
epoch78: train: loss:1.2979959242237815 	 acc:0.84765625 | test: loss:1.3934087012044365 	 acc:0.7919003115264798 	 lr:3.125e-06
epoch79: train: loss:1.2976625036020748 	 acc:0.841875 | test: loss:1.4024581969341385 	 acc:0.7813084112149533 	 lr:3.125e-06
epoch80: train: loss:1.290874005331833 	 acc:0.84453125 | test: loss:1.3988880869755493 	 acc:0.788785046728972 	 lr:3.125e-06
epoch81: train: loss:1.2966706084609496 	 acc:0.83890625 | test: loss:1.4027692178328088 	 acc:0.7813084112149533 	 lr:3.125e-06
epoch82: train: loss:1.289196050734747 	 acc:0.85265625 | test: loss:1.3946216446588344 	 acc:0.794392523364486 	 lr:3.125e-06
epoch83: train: loss:1.2981775526512795 	 acc:0.83640625 | test: loss:1.4049808060268747 	 acc:0.7763239875389408 	 lr:3.125e-06
epoch84: train: loss:1.3011296887959847 	 acc:0.84703125 | test: loss:1.3943850546239693 	 acc:0.7875389408099689 	 lr:3.125e-06
epoch85: train: loss:1.296242866843683 	 acc:0.8471875 | test: loss:1.3982527791525345 	 acc:0.7881619937694704 	 lr:1.5625e-06
epoch86: train: loss:1.2990161880862425 	 acc:0.835625 | test: loss:1.4056847681509 	 acc:0.7707165109034267 	 lr:1.5625e-06
epoch87: train: loss:1.285369980251482 	 acc:0.84125 | test: loss:1.4047000798109537 	 acc:0.77196261682243 	 lr:1.5625e-06
epoch88: train: loss:1.2913635706547923 	 acc:0.8428125 | test: loss:1.401576004221432 	 acc:0.7813084112149533 	 lr:1.5625e-06
epoch89: train: loss:1.29440194456769 	 acc:0.8553125 | test: loss:1.3943022609128388 	 acc:0.7894080996884735 	 lr:1.5625e-06
epoch90: train: loss:1.2885639914304925 	 acc:0.84265625 | test: loss:1.4006434857288255 	 acc:0.7838006230529595 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_3_3/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_3_3/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.701897764540947 	 acc:0.60421875 | test: loss:1.618330221087019 	 acc:0.6124610591900311 	 lr:0.0001
epoch1: train: loss:1.5792598322347957 	 acc:0.57625 | test: loss:1.6214997200951027 	 acc:0.5713395638629284 	 lr:0.0001
epoch2: train: loss:1.516139279558359 	 acc:0.71171875 | test: loss:1.5130965549255087 	 acc:0.7052959501557632 	 lr:0.0001
epoch3: train: loss:1.4899568055962138 	 acc:0.7 | test: loss:1.4894097280650866 	 acc:0.685981308411215 	 lr:0.0001
epoch4: train: loss:1.4725833913667605 	 acc:0.76125 | test: loss:1.4664046762145568 	 acc:0.7451713395638629 	 lr:0.0001
epoch5: train: loss:1.4290612392738218 	 acc:0.67984375 | test: loss:1.5019027867421182 	 acc:0.6834890965732088 	 lr:0.0001
epoch6: train: loss:1.4673462652583127 	 acc:0.644375 | test: loss:1.570698326101927 	 acc:0.5956386292834891 	 lr:0.0001
epoch7: train: loss:1.3962795069960297 	 acc:0.71 | test: loss:1.49964663232227 	 acc:0.6697819314641744 	 lr:0.0001
epoch8: train: loss:1.371745189514875 	 acc:0.7859375 | test: loss:1.436786010035102 	 acc:0.756386292834891 	 lr:0.0001
epoch9: train: loss:1.3728183433657788 	 acc:0.78078125 | test: loss:1.4300962629347946 	 acc:0.7582554517133956 	 lr:0.0001
epoch10: train: loss:1.3726447671954283 	 acc:0.79609375 | test: loss:1.436552371340006 	 acc:0.7501557632398754 	 lr:0.0001
epoch11: train: loss:1.3497222924213872 	 acc:0.7428125 | test: loss:1.4660697924385189 	 acc:0.7065420560747664 	 lr:0.0001
epoch12: train: loss:1.3479649373277103 	 acc:0.83453125 | test: loss:1.3787395729082768 	 acc:0.8105919003115265 	 lr:0.0001
epoch13: train: loss:1.333873165705351 	 acc:0.8571875 | test: loss:1.3792661566600621 	 acc:0.7993769470404984 	 lr:0.0001
epoch14: train: loss:1.3092156582191343 	 acc:0.828125 | test: loss:1.38825296575778 	 acc:0.795638629283489 	 lr:0.0001
epoch15: train: loss:1.3531209333719079 	 acc:0.815 | test: loss:1.3891805218016247 	 acc:0.7881619937694704 	 lr:0.0001
epoch16: train: loss:1.310527039765381 	 acc:0.80703125 | test: loss:1.4131580532525558 	 acc:0.7601246105919003 	 lr:0.0001
epoch17: train: loss:1.314000411856277 	 acc:0.82140625 | test: loss:1.3980185836275048 	 acc:0.7750778816199377 	 lr:0.0001
epoch18: train: loss:1.3092597264595836 	 acc:0.87265625 | test: loss:1.353728787252836 	 acc:0.8218068535825546 	 lr:0.0001
epoch19: train: loss:1.2954480661813585 	 acc:0.8803125 | test: loss:1.3536772797783587 	 acc:0.8280373831775701 	 lr:0.0001
epoch20: train: loss:1.2791651538905457 	 acc:0.8609375 | test: loss:1.3660167922112059 	 acc:0.815576323987539 	 lr:0.0001
epoch21: train: loss:1.298726609384148 	 acc:0.88078125 | test: loss:1.3352364175416227 	 acc:0.8411214953271028 	 lr:0.0001
epoch22: train: loss:1.3129418473612229 	 acc:0.8659375 | test: loss:1.3500615102108393 	 acc:0.8149532710280374 	 lr:0.0001
epoch23: train: loss:1.2862611864433915 	 acc:0.8371875 | test: loss:1.4062748196711792 	 acc:0.7757009345794392 	 lr:0.0001
epoch24: train: loss:1.303971073052364 	 acc:0.89046875 | test: loss:1.3483425270359835 	 acc:0.8330218068535825 	 lr:0.0001
epoch25: train: loss:1.3157454765373426 	 acc:0.66921875 | test: loss:1.5444173359796638 	 acc:0.6087227414330219 	 lr:0.0001
epoch26: train: loss:1.2677369784043229 	 acc:0.871875 | test: loss:1.352589227774433 	 acc:0.8249221183800624 	 lr:0.0001
epoch27: train: loss:1.281513928213127 	 acc:0.8340625 | test: loss:1.3899408739303873 	 acc:0.7900311526479751 	 lr:0.0001
epoch28: train: loss:1.2469508913324552 	 acc:0.918125 | test: loss:1.3216446095166547 	 acc:0.8498442367601247 	 lr:5e-05
epoch29: train: loss:1.2304419393189525 	 acc:0.90453125 | test: loss:1.3494749243014326 	 acc:0.8218068535825546 	 lr:5e-05
epoch30: train: loss:1.2352728528775432 	 acc:0.91890625 | test: loss:1.325018054005513 	 acc:0.8510903426791278 	 lr:5e-05
epoch31: train: loss:1.2547731421870425 	 acc:0.91984375 | test: loss:1.32081798541583 	 acc:0.854828660436137 	 lr:5e-05
epoch32: train: loss:1.2463731752346494 	 acc:0.89859375 | test: loss:1.334507413742327 	 acc:0.8242990654205608 	 lr:5e-05
epoch33: train: loss:1.234930610507843 	 acc:0.91765625 | test: loss:1.3226776777398177 	 acc:0.8473520249221184 	 lr:5e-05
epoch34: train: loss:1.2325106856787604 	 acc:0.9146875 | test: loss:1.3338424593488747 	 acc:0.8367601246105919 	 lr:5e-05
epoch35: train: loss:1.2363554907626793 	 acc:0.8859375 | test: loss:1.3595759509137115 	 acc:0.8068535825545171 	 lr:5e-05
epoch36: train: loss:1.2261263416299217 	 acc:0.93953125 | test: loss:1.3044976865019753 	 acc:0.8629283489096573 	 lr:5e-05
epoch37: train: loss:1.2277937568229778 	 acc:0.89703125 | test: loss:1.3560417141869803 	 acc:0.815576323987539 	 lr:5e-05
epoch38: train: loss:1.231212378329918 	 acc:0.9346875 | test: loss:1.3106046566710665 	 acc:0.8554517133956386 	 lr:5e-05
epoch39: train: loss:1.223619319013466 	 acc:0.94875 | test: loss:1.2971434322844413 	 acc:0.8741433021806854 	 lr:5e-05
epoch40: train: loss:1.218317198325283 	 acc:0.93125 | test: loss:1.3233137334989982 	 acc:0.8485981308411215 	 lr:5e-05
epoch41: train: loss:1.2279411655399224 	 acc:0.91140625 | test: loss:1.3431026033894666 	 acc:0.8218068535825546 	 lr:5e-05
epoch42: train: loss:1.226796796859753 	 acc:0.925 | test: loss:1.307835735487418 	 acc:0.8616822429906542 	 lr:5e-05
epoch43: train: loss:1.2177904782678632 	 acc:0.9503125 | test: loss:1.2999727701472346 	 acc:0.870404984423676 	 lr:5e-05
epoch44: train: loss:1.2137474066014404 	 acc:0.93609375 | test: loss:1.318403943676815 	 acc:0.8529595015576324 	 lr:5e-05
epoch45: train: loss:1.2256443616284884 	 acc:0.94828125 | test: loss:1.3042197930107235 	 acc:0.8666666666666667 	 lr:5e-05
epoch46: train: loss:1.2130120425555597 	 acc:0.94578125 | test: loss:1.3064131408465613 	 acc:0.8666666666666667 	 lr:2.5e-05
epoch47: train: loss:1.2112289369152822 	 acc:0.95421875 | test: loss:1.2977011249815564 	 acc:0.8710280373831776 	 lr:2.5e-05
epoch48: train: loss:1.2042795936173523 	 acc:0.95359375 | test: loss:1.301035266798976 	 acc:0.8685358255451714 	 lr:2.5e-05
epoch49: train: loss:1.2097704920891577 	 acc:0.9575 | test: loss:1.2920256865730166 	 acc:0.8809968847352025 	 lr:2.5e-05
epoch50: train: loss:1.2033190474856374 	 acc:0.95671875 | test: loss:1.2996576477805402 	 acc:0.864797507788162 	 lr:2.5e-05
epoch51: train: loss:1.2040184408123842 	 acc:0.9575 | test: loss:1.2972036086138907 	 acc:0.8735202492211838 	 lr:2.5e-05
epoch52: train: loss:1.1995770301789068 	 acc:0.95828125 | test: loss:1.2999984950662773 	 acc:0.8741433021806854 	 lr:2.5e-05
epoch53: train: loss:1.2069575599354752 	 acc:0.9578125 | test: loss:1.3024557981907021 	 acc:0.859190031152648 	 lr:2.5e-05
epoch54: train: loss:1.2085907816235486 	 acc:0.95078125 | test: loss:1.3081782773276356 	 acc:0.8598130841121495 	 lr:2.5e-05
epoch55: train: loss:1.2104384959069758 	 acc:0.95859375 | test: loss:1.2956591646990672 	 acc:0.8728971962616823 	 lr:2.5e-05
epoch56: train: loss:1.2030201824822526 	 acc:0.95671875 | test: loss:1.3015632937630388 	 acc:0.8666666666666667 	 lr:1.25e-05
epoch57: train: loss:1.1987584944165934 	 acc:0.96515625 | test: loss:1.2951192447329607 	 acc:0.8735202492211838 	 lr:1.25e-05
epoch58: train: loss:1.2083115326809195 	 acc:0.95765625 | test: loss:1.2968269328833368 	 acc:0.8685358255451714 	 lr:1.25e-05
epoch59: train: loss:1.2030009952399248 	 acc:0.965625 | test: loss:1.2929495741645123 	 acc:0.8741433021806854 	 lr:1.25e-05
epoch60: train: loss:1.200014689413483 	 acc:0.96015625 | test: loss:1.2975249197624183 	 acc:0.8760124610591901 	 lr:1.25e-05
epoch61: train: loss:1.2010723938893564 	 acc:0.96296875 | test: loss:1.2932511845110362 	 acc:0.8735202492211838 	 lr:1.25e-05
epoch62: train: loss:1.1981722067148772 	 acc:0.96515625 | test: loss:1.2945636586608174 	 acc:0.8760124610591901 	 lr:6.25e-06
epoch63: train: loss:1.2035198274205943 	 acc:0.96390625 | test: loss:1.2925091152993318 	 acc:0.8766355140186916 	 lr:6.25e-06
epoch64: train: loss:1.1988722790041348 	 acc:0.9671875 | test: loss:1.295134062410515 	 acc:0.8753894080996885 	 lr:6.25e-06
epoch65: train: loss:1.195517132153835 	 acc:0.96859375 | test: loss:1.2914626390391792 	 acc:0.8834890965732087 	 lr:6.25e-06
epoch66: train: loss:1.1938268347329968 	 acc:0.963125 | test: loss:1.292877623819488 	 acc:0.8760124610591901 	 lr:6.25e-06
epoch67: train: loss:1.2016294147333626 	 acc:0.960625 | test: loss:1.2937672307558148 	 acc:0.8722741433021807 	 lr:6.25e-06
epoch68: train: loss:1.1929505490400567 	 acc:0.969375 | test: loss:1.2924510124687836 	 acc:0.8797507788161993 	 lr:6.25e-06
epoch69: train: loss:1.1992479191824015 	 acc:0.964375 | test: loss:1.2973144205188454 	 acc:0.8753894080996885 	 lr:6.25e-06
epoch70: train: loss:1.1980742907914959 	 acc:0.961875 | test: loss:1.2964642708918015 	 acc:0.8728971962616823 	 lr:6.25e-06
epoch71: train: loss:1.200666402895687 	 acc:0.965 | test: loss:1.2949839716759797 	 acc:0.8741433021806854 	 lr:6.25e-06
epoch72: train: loss:1.1987536225627866 	 acc:0.96734375 | test: loss:1.2952169788960728 	 acc:0.8735202492211838 	 lr:3.125e-06
epoch73: train: loss:1.194931909351215 	 acc:0.96703125 | test: loss:1.2945860612429563 	 acc:0.8735202492211838 	 lr:3.125e-06
epoch74: train: loss:1.197672959624744 	 acc:0.9646875 | test: loss:1.2947919440789386 	 acc:0.8735202492211838 	 lr:3.125e-06
epoch75: train: loss:1.203486810783964 	 acc:0.96609375 | test: loss:1.2913204438961183 	 acc:0.8766355140186916 	 lr:3.125e-06
epoch76: train: loss:1.19203306580781 	 acc:0.96953125 | test: loss:1.291299009843036 	 acc:0.8760124610591901 	 lr:3.125e-06
epoch77: train: loss:1.1961698748002658 	 acc:0.9646875 | test: loss:1.2934606318161865 	 acc:0.8778816199376948 	 lr:3.125e-06
epoch78: train: loss:1.1941238677287642 	 acc:0.9690625 | test: loss:1.2926309838844607 	 acc:0.8760124610591901 	 lr:3.125e-06
epoch79: train: loss:1.1972811529917422 	 acc:0.9615625 | test: loss:1.2945381261106592 	 acc:0.8760124610591901 	 lr:3.125e-06
epoch80: train: loss:1.2002761495383245 	 acc:0.96203125 | test: loss:1.2918867436524863 	 acc:0.8778816199376948 	 lr:3.125e-06
epoch81: train: loss:1.1977960424251988 	 acc:0.96640625 | test: loss:1.2908594598651304 	 acc:0.8791277258566979 	 lr:3.125e-06
epoch82: train: loss:1.1919188119023223 	 acc:0.96640625 | test: loss:1.292920264053939 	 acc:0.8760124610591901 	 lr:3.125e-06
epoch83: train: loss:1.1949792032591726 	 acc:0.96890625 | test: loss:1.2928980792422904 	 acc:0.8741433021806854 	 lr:3.125e-06
epoch84: train: loss:1.2012629655634026 	 acc:0.9678125 | test: loss:1.2907080661470645 	 acc:0.8785046728971962 	 lr:3.125e-06
epoch85: train: loss:1.1979797625336956 	 acc:0.9665625 | test: loss:1.2923921160980176 	 acc:0.8778816199376948 	 lr:3.125e-06
epoch86: train: loss:1.1958871166935012 	 acc:0.9628125 | test: loss:1.2904550078501953 	 acc:0.8791277258566979 	 lr:3.125e-06
epoch87: train: loss:1.1949015418595397 	 acc:0.9678125 | test: loss:1.2884698939843342 	 acc:0.8797507788161993 	 lr:3.125e-06
epoch88: train: loss:1.1952076586590066 	 acc:0.96890625 | test: loss:1.290282051585545 	 acc:0.8809968847352025 	 lr:3.125e-06
epoch89: train: loss:1.1936880905306217 	 acc:0.9665625 | test: loss:1.2894341731739936 	 acc:0.8778816199376948 	 lr:3.125e-06
epoch90: train: loss:1.1931738205015612 	 acc:0.96828125 | test: loss:1.2912230055651561 	 acc:0.8772585669781932 	 lr:3.125e-06
epoch91: train: loss:1.1929875798936378 	 acc:0.96734375 | test: loss:1.2903700514374492 	 acc:0.8785046728971962 	 lr:3.125e-06
epoch92: train: loss:1.1934266470820527 	 acc:0.9709375 | test: loss:1.2877095181622609 	 acc:0.8803738317757009 	 lr:3.125e-06
epoch93: train: loss:1.1953215766567258 	 acc:0.96875 | test: loss:1.289494906779019 	 acc:0.8803738317757009 	 lr:3.125e-06
epoch94: train: loss:1.1958626526021845 	 acc:0.96828125 | test: loss:1.2889822958414427 	 acc:0.8834890965732087 	 lr:3.125e-06
epoch95: train: loss:1.1986095443356326 	 acc:0.96296875 | test: loss:1.2899301635884792 	 acc:0.8772585669781932 	 lr:3.125e-06
epoch96: train: loss:1.1966155815273407 	 acc:0.96984375 | test: loss:1.2892430856591817 	 acc:0.8785046728971962 	 lr:3.125e-06
epoch97: train: loss:1.1948059129677742 	 acc:0.96578125 | test: loss:1.2930661874396778 	 acc:0.8760124610591901 	 lr:3.125e-06
epoch98: train: loss:1.1958408590967444 	 acc:0.97265625 | test: loss:1.2897377997543953 	 acc:0.881619937694704 	 lr:3.125e-06
epoch99: train: loss:1.1923998818557584 	 acc:0.96921875 | test: loss:1.2884981338108812 	 acc:0.881619937694704 	 lr:1.5625e-06
epoch100: train: loss:1.1914039296158396 	 acc:0.9690625 | test: loss:1.2886539065577902 	 acc:0.8828660436137071 	 lr:1.5625e-06
epoch101: train: loss:1.1980884013001012 	 acc:0.96984375 | test: loss:1.287408498365931 	 acc:0.8847352024922118 	 lr:1.5625e-06
epoch102: train: loss:1.1894890099554487 	 acc:0.96875 | test: loss:1.290026762923719 	 acc:0.881619937694704 	 lr:1.5625e-06
epoch103: train: loss:1.191643786821209 	 acc:0.96578125 | test: loss:1.2913426906520333 	 acc:0.8778816199376948 	 lr:1.5625e-06
epoch104: train: loss:1.1959524900628475 	 acc:0.97 | test: loss:1.2879973927762278 	 acc:0.881619937694704 	 lr:1.5625e-06
epoch105: train: loss:1.1912626339438184 	 acc:0.9709375 | test: loss:1.2874110765546283 	 acc:0.8822429906542056 	 lr:1.5625e-06
epoch106: train: loss:1.1980967799357192 	 acc:0.96890625 | test: loss:1.288007971059496 	 acc:0.8803738317757009 	 lr:1.5625e-06
epoch107: train: loss:1.1933704361331174 	 acc:0.96609375 | test: loss:1.2900116207443666 	 acc:0.8828660436137071 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_1_3/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_1_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_2_3/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_2_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_3_3/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_3_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_4_3/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_4_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_5_3/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_5_3/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'
