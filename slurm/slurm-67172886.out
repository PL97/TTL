
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_-1_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_1_2/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_1_2/
pooling!! 256
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.9230452121374293 	 acc:0.43046875 | test: loss:1.9331245036140037 	 acc:0.4255451713395639 	 lr:0.0001
epoch1: train: loss:1.8840555707706688 	 acc:0.484375 | test: loss:1.8666844985938147 	 acc:0.49781931464174456 	 lr:0.0001
epoch2: train: loss:1.853452410016741 	 acc:0.4934375 | test: loss:1.830540963719567 	 acc:0.49906542056074765 	 lr:0.0001
epoch3: train: loss:1.8336925045016406 	 acc:0.456875 | test: loss:1.8242603218815407 	 acc:0.4517133956386293 	 lr:0.0001
epoch4: train: loss:1.8082813367166155 	 acc:0.48796875 | test: loss:1.7585953199232107 	 acc:0.4996884735202492 	 lr:0.0001
epoch5: train: loss:1.8028960252534032 	 acc:0.46375 | test: loss:1.7799830790249358 	 acc:0.4654205607476635 	 lr:0.0001
epoch6: train: loss:1.785514129948374 	 acc:0.501875 | test: loss:1.7325539029647257 	 acc:0.5102803738317757 	 lr:0.0001
epoch7: train: loss:1.7710556828352177 	 acc:0.52109375 | test: loss:1.7315696224616695 	 acc:0.525233644859813 	 lr:0.0001
epoch8: train: loss:1.7722253732435598 	 acc:0.52765625 | test: loss:1.7307969938557468 	 acc:0.5227414330218069 	 lr:0.0001
epoch9: train: loss:1.7651064108164398 	 acc:0.52765625 | test: loss:1.7412275579488166 	 acc:0.5233644859813084 	 lr:0.0001
epoch10: train: loss:1.746880507934475 	 acc:0.5246875 | test: loss:1.6979778852789573 	 acc:0.5289719626168224 	 lr:0.0001
epoch11: train: loss:1.7419705321693868 	 acc:0.53234375 | test: loss:1.720136936654183 	 acc:0.5295950155763239 	 lr:0.0001
epoch12: train: loss:1.7355882798015467 	 acc:0.5446875 | test: loss:1.7206237153472188 	 acc:0.5401869158878505 	 lr:0.0001
epoch13: train: loss:1.7308202643770432 	 acc:0.56609375 | test: loss:1.6754196296971164 	 acc:0.5744548286604362 	 lr:0.0001
epoch14: train: loss:1.723735393014948 	 acc:0.54015625 | test: loss:1.7021693372280797 	 acc:0.5383177570093458 	 lr:0.0001
epoch15: train: loss:1.724833251571953 	 acc:0.52125 | test: loss:1.745768893025003 	 acc:0.5065420560747663 	 lr:0.0001
epoch16: train: loss:1.7148781412379244 	 acc:0.52875 | test: loss:1.672204432532052 	 acc:0.5264797507788161 	 lr:0.0001
epoch17: train: loss:1.71480510393034 	 acc:0.56703125 | test: loss:1.6716767835468518 	 acc:0.5563862928348909 	 lr:0.0001
epoch18: train: loss:1.6965195119055243 	 acc:0.5571875 | test: loss:1.6702150754096723 	 acc:0.5420560747663551 	 lr:0.0001
epoch19: train: loss:1.7029859365661288 	 acc:0.5640625 | test: loss:1.671312507884896 	 acc:0.5744548286604362 	 lr:0.0001
epoch20: train: loss:1.6943650185363912 	 acc:0.585625 | test: loss:1.6492900387891727 	 acc:0.5750778816199377 	 lr:0.0001
epoch21: train: loss:1.6989982182117853 	 acc:0.58578125 | test: loss:1.6566163246505357 	 acc:0.5707165109034268 	 lr:0.0001
epoch22: train: loss:1.6905444655168997 	 acc:0.59828125 | test: loss:1.6439968429993246 	 acc:0.5850467289719626 	 lr:0.0001
epoch23: train: loss:1.6905991374841431 	 acc:0.58953125 | test: loss:1.6664193243995262 	 acc:0.5738317757009346 	 lr:0.0001
epoch24: train: loss:1.6795083634840335 	 acc:0.62171875 | test: loss:1.6383189795544586 	 acc:0.6180685358255452 	 lr:0.0001
epoch25: train: loss:1.68171517746603 	 acc:0.555625 | test: loss:1.669202045265388 	 acc:0.5570093457943925 	 lr:0.0001
epoch26: train: loss:1.6756628794376036 	 acc:0.564375 | test: loss:1.678135455583115 	 acc:0.5439252336448598 	 lr:0.0001
epoch27: train: loss:1.6840258774768553 	 acc:0.6015625 | test: loss:1.6547757625579833 	 acc:0.5850467289719626 	 lr:0.0001
epoch28: train: loss:1.6646126716607814 	 acc:0.59859375 | test: loss:1.6618586622665976 	 acc:0.5925233644859813 	 lr:0.0001
epoch29: train: loss:1.6801389600409835 	 acc:0.59109375 | test: loss:1.6824584573228782 	 acc:0.567601246105919 	 lr:0.0001
epoch30: train: loss:1.6570935192003928 	 acc:0.6221875 | test: loss:1.622764469455707 	 acc:0.6186915887850467 	 lr:0.0001
epoch31: train: loss:1.6559965540151127 	 acc:0.59515625 | test: loss:1.6665243275069002 	 acc:0.577570093457944 	 lr:0.0001
epoch32: train: loss:1.6655940512211225 	 acc:0.63453125 | test: loss:1.627274967279761 	 acc:0.6105919003115264 	 lr:0.0001
epoch33: train: loss:1.657111548632965 	 acc:0.64984375 | test: loss:1.5974307517767696 	 acc:0.6548286604361371 	 lr:0.0001
epoch34: train: loss:1.6633562884602484 	 acc:0.55515625 | test: loss:1.6823482305461372 	 acc:0.540809968847352 	 lr:0.0001
epoch35: train: loss:1.6538027379961333 	 acc:0.60625 | test: loss:1.6180137599368705 	 acc:0.5875389408099688 	 lr:0.0001
epoch36: train: loss:1.6298774856221945 	 acc:0.62328125 | test: loss:1.6100387735901591 	 acc:0.616822429906542 	 lr:0.0001
epoch37: train: loss:1.641458592053785 	 acc:0.62765625 | test: loss:1.615405094066513 	 acc:0.6193146417445483 	 lr:0.0001
epoch38: train: loss:1.6392811354578332 	 acc:0.62953125 | test: loss:1.6212354062873626 	 acc:0.6137071651090342 	 lr:0.0001
epoch39: train: loss:1.6304915131115523 	 acc:0.6265625 | test: loss:1.5967450915095962 	 acc:0.6149532710280374 	 lr:0.0001
epoch40: train: loss:1.6339637321573417 	 acc:0.63640625 | test: loss:1.591526349979769 	 acc:0.6292834890965732 	 lr:0.0001
epoch41: train: loss:1.625115537978447 	 acc:0.67046875 | test: loss:1.5813466216916237 	 acc:0.6548286604361371 	 lr:0.0001
epoch42: train: loss:1.6616448502909105 	 acc:0.51421875 | test: loss:1.7256207144520364 	 acc:0.4965732087227414 	 lr:0.0001
epoch43: train: loss:1.620824790317318 	 acc:0.6346875 | test: loss:1.5955832323181296 	 acc:0.6137071651090342 	 lr:0.0001
epoch44: train: loss:1.619726383825655 	 acc:0.63109375 | test: loss:1.6181446693396642 	 acc:0.616822429906542 	 lr:0.0001
epoch45: train: loss:1.6158266923652786 	 acc:0.6340625 | test: loss:1.5993430039592993 	 acc:0.6130841121495327 	 lr:0.0001
epoch46: train: loss:1.6093589979256624 	 acc:0.66484375 | test: loss:1.592113328574231 	 acc:0.6616822429906543 	 lr:0.0001
epoch47: train: loss:1.61202416401371 	 acc:0.6484375 | test: loss:1.600261741560939 	 acc:0.643613707165109 	 lr:0.0001
epoch48: train: loss:1.5983351743937841 	 acc:0.6415625 | test: loss:1.592727533010679 	 acc:0.6280373831775701 	 lr:5e-05
epoch49: train: loss:1.6286945328128049 	 acc:0.64578125 | test: loss:1.605747896934224 	 acc:0.621183800623053 	 lr:5e-05
epoch50: train: loss:1.5945067096742962 	 acc:0.65125 | test: loss:1.5959150418314236 	 acc:0.636760124610592 	 lr:5e-05
epoch51: train: loss:1.586159644621969 	 acc:0.64328125 | test: loss:1.5961785788848022 	 acc:0.6242990654205608 	 lr:5e-05
epoch52: train: loss:1.6002533721328247 	 acc:0.65625 | test: loss:1.5925182990195967 	 acc:0.6348909657320873 	 lr:5e-05
epoch53: train: loss:1.594603360583315 	 acc:0.68671875 | test: loss:1.569124701089948 	 acc:0.6710280373831776 	 lr:5e-05
epoch54: train: loss:1.5819817881766565 	 acc:0.61625 | test: loss:1.6081201873464377 	 acc:0.5993769470404985 	 lr:5e-05
epoch55: train: loss:1.58761188825716 	 acc:0.6628125 | test: loss:1.5832955951631256 	 acc:0.638006230529595 	 lr:5e-05
epoch56: train: loss:1.5821471143569172 	 acc:0.66375 | test: loss:1.5811229701354126 	 acc:0.6504672897196262 	 lr:5e-05
epoch57: train: loss:1.5915268123010282 	 acc:0.6875 | test: loss:1.5660764922234127 	 acc:0.6735202492211838 	 lr:5e-05
epoch58: train: loss:1.5846009587236534 	 acc:0.66078125 | test: loss:1.5888444371683947 	 acc:0.6510903426791277 	 lr:5e-05
epoch59: train: loss:1.5793015110036714 	 acc:0.645625 | test: loss:1.598001407165765 	 acc:0.636760124610592 	 lr:5e-05
epoch60: train: loss:1.5915796403490314 	 acc:0.66921875 | test: loss:1.5981990550537346 	 acc:0.6629283489096574 	 lr:5e-05
epoch61: train: loss:1.5869025129903396 	 acc:0.69328125 | test: loss:1.5752410305623326 	 acc:0.6691588785046729 	 lr:5e-05
epoch62: train: loss:1.5745713602463591 	 acc:0.64984375 | test: loss:1.5835798895618998 	 acc:0.6330218068535826 	 lr:5e-05
epoch63: train: loss:1.5688064658576673 	 acc:0.675 | test: loss:1.5708694398588852 	 acc:0.664797507788162 	 lr:5e-05
epoch64: train: loss:1.5670395385837481 	 acc:0.68625 | test: loss:1.574732078495798 	 acc:0.659190031152648 	 lr:2.5e-05
epoch65: train: loss:1.564156304031122 	 acc:0.68234375 | test: loss:1.5623518840546178 	 acc:0.6697819314641744 	 lr:2.5e-05
epoch66: train: loss:1.560945931046759 	 acc:0.68625 | test: loss:1.5696728556326989 	 acc:0.6691588785046729 	 lr:2.5e-05
epoch67: train: loss:1.5731285434696098 	 acc:0.68140625 | test: loss:1.5563413740318512 	 acc:0.6735202492211838 	 lr:2.5e-05
epoch68: train: loss:1.563979099580406 	 acc:0.67703125 | test: loss:1.5723692173527037 	 acc:0.6560747663551402 	 lr:2.5e-05
epoch69: train: loss:1.5606125271013247 	 acc:0.681875 | test: loss:1.5645220790696663 	 acc:0.6691588785046729 	 lr:2.5e-05
epoch70: train: loss:1.5720489716362338 	 acc:0.67453125 | test: loss:1.5757568110558102 	 acc:0.6498442367601246 	 lr:2.5e-05
epoch71: train: loss:1.5623822941061671 	 acc:0.67515625 | test: loss:1.5700575665892842 	 acc:0.6566978193146418 	 lr:2.5e-05
epoch72: train: loss:1.560362680808908 	 acc:0.67765625 | test: loss:1.569104782517454 	 acc:0.6517133956386293 	 lr:2.5e-05
epoch73: train: loss:1.5550778581796447 	 acc:0.6859375 | test: loss:1.5591769228100405 	 acc:0.6623052959501557 	 lr:2.5e-05
epoch74: train: loss:1.5620944187661616 	 acc:0.6871875 | test: loss:1.5697843957912885 	 acc:0.6691588785046729 	 lr:1.25e-05
epoch75: train: loss:1.5562469720654335 	 acc:0.68828125 | test: loss:1.5659370815271159 	 acc:0.6672897196261682 	 lr:1.25e-05
epoch76: train: loss:1.558284269358198 	 acc:0.6846875 | test: loss:1.5665338584567157 	 acc:0.664797507788162 	 lr:1.25e-05
epoch77: train: loss:1.5539264378186597 	 acc:0.68328125 | test: loss:1.5657852919302253 	 acc:0.6616822429906543 	 lr:1.25e-05
epoch78: train: loss:1.5542121952720958 	 acc:0.68734375 | test: loss:1.5652606394432045 	 acc:0.6641744548286604 	 lr:1.25e-05
epoch79: train: loss:1.553777611618578 	 acc:0.6825 | test: loss:1.5646158001504584 	 acc:0.6710280373831776 	 lr:1.25e-05
epoch80: train: loss:1.5566676310316647 	 acc:0.68796875 | test: loss:1.5672371314693463 	 acc:0.6629283489096574 	 lr:6.25e-06
epoch81: train: loss:1.554028908746676 	 acc:0.6859375 | test: loss:1.5653744590616672 	 acc:0.6654205607476635 	 lr:6.25e-06
epoch82: train: loss:1.557206929762972 	 acc:0.69515625 | test: loss:1.5609102912409654 	 acc:0.670404984423676 	 lr:6.25e-06
epoch83: train: loss:1.5562702950232667 	 acc:0.68609375 | test: loss:1.5606279501662448 	 acc:0.6716510903426791 	 lr:6.25e-06
epoch84: train: loss:1.5549934333604727 	 acc:0.6890625 | test: loss:1.559005698682363 	 acc:0.6747663551401869 	 lr:6.25e-06
epoch85: train: loss:1.5590821765718006 	 acc:0.6828125 | test: loss:1.5674524528586604 	 acc:0.6654205607476635 	 lr:6.25e-06
epoch86: train: loss:1.5523758613532823 	 acc:0.6915625 | test: loss:1.5591266877926027 	 acc:0.6691588785046729 	 lr:3.125e-06
epoch87: train: loss:1.554082680195221 	 acc:0.67953125 | test: loss:1.5649111989873963 	 acc:0.6598130841121496 	 lr:3.125e-06
epoch88: train: loss:1.559224989784592 	 acc:0.68296875 | test: loss:1.5627698979273763 	 acc:0.664797507788162 	 lr:3.125e-06
epoch89: train: loss:1.5477930645194489 	 acc:0.69171875 | test: loss:1.559392826430894 	 acc:0.6691588785046729 	 lr:3.125e-06
epoch90: train: loss:1.5554439452362656 	 acc:0.684375 | test: loss:1.5631608034591438 	 acc:0.6641744548286604 	 lr:3.125e-06
epoch91: train: loss:1.546341166321325 	 acc:0.6853125 | test: loss:1.5608288888990693 	 acc:0.6654205607476635 	 lr:3.125e-06
epoch92: train: loss:1.5541043688783787 	 acc:0.6884375 | test: loss:1.5612973999011555 	 acc:0.6722741433021807 	 lr:1.5625e-06
epoch93: train: loss:1.55062776873765 	 acc:0.689375 | test: loss:1.5649147731121456 	 acc:0.6666666666666666 	 lr:1.5625e-06
epoch94: train: loss:1.5517926897321428 	 acc:0.68859375 | test: loss:1.5601920137524234 	 acc:0.6679127725856698 	 lr:1.5625e-06
epoch95: train: loss:1.5477448534165203 	 acc:0.6903125 | test: loss:1.5572294730635075 	 acc:0.67601246105919 	 lr:1.5625e-06
epoch96: train: loss:1.5486141231635135 	 acc:0.69140625 | test: loss:1.5621160242045038 	 acc:0.6697819314641744 	 lr:1.5625e-06
epoch97: train: loss:1.5525983204420986 	 acc:0.68765625 | test: loss:1.558613177549059 	 acc:0.6741433021806854 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_2_2/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_2_2/
pooling!! 512
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.8646798389205517 	 acc:0.495 | test: loss:1.8136999571434804 	 acc:0.4803738317757009 	 lr:0.0001
epoch1: train: loss:1.8042420577109557 	 acc:0.48078125 | test: loss:1.7786997549259032 	 acc:0.4866043613707165 	 lr:0.0001
epoch2: train: loss:1.7525970935449295 	 acc:0.47625 | test: loss:1.7646043914872167 	 acc:0.4691588785046729 	 lr:0.0001
epoch3: train: loss:1.7268316478863255 	 acc:0.528125 | test: loss:1.7051092551133342 	 acc:0.5090342679127726 	 lr:0.0001
epoch4: train: loss:1.713529492987961 	 acc:0.57078125 | test: loss:1.672560609241141 	 acc:0.5570093457943925 	 lr:0.0001
epoch5: train: loss:1.6933867312333808 	 acc:0.49140625 | test: loss:1.7422090609125631 	 acc:0.46105919003115264 	 lr:0.0001
epoch6: train: loss:1.6494042544696221 	 acc:0.549375 | test: loss:1.664591584695834 	 acc:0.5302180685358255 	 lr:0.0001
epoch7: train: loss:1.671072556840359 	 acc:0.53140625 | test: loss:1.6764809810483938 	 acc:0.5021806853582554 	 lr:0.0001
epoch8: train: loss:1.6411743688173912 	 acc:0.6490625 | test: loss:1.6203834441592018 	 acc:0.6299065420560748 	 lr:0.0001
epoch9: train: loss:1.6081573438681633 	 acc:0.6003125 | test: loss:1.6305605539280306 	 acc:0.5744548286604362 	 lr:0.0001
epoch10: train: loss:1.6077201015403921 	 acc:0.60421875 | test: loss:1.63335912235058 	 acc:0.584423676012461 	 lr:0.0001
epoch11: train: loss:1.6037465365764967 	 acc:0.624375 | test: loss:1.6354253052922425 	 acc:0.6037383177570094 	 lr:0.0001
epoch12: train: loss:1.5823390320443622 	 acc:0.62359375 | test: loss:1.612392661133288 	 acc:0.594392523364486 	 lr:0.0001
epoch13: train: loss:1.6166740256673557 	 acc:0.60328125 | test: loss:1.620421553697913 	 acc:0.5819314641744548 	 lr:0.0001
epoch14: train: loss:1.572974924032433 	 acc:0.62640625 | test: loss:1.5913696016478018 	 acc:0.5975077881619938 	 lr:0.0001
epoch15: train: loss:1.571151819162123 	 acc:0.6325 | test: loss:1.5848538522779756 	 acc:0.6118380062305296 	 lr:0.0001
epoch16: train: loss:1.5566024196603911 	 acc:0.65046875 | test: loss:1.5839830483974326 	 acc:0.6261682242990654 	 lr:0.0001
epoch17: train: loss:1.562428258676998 	 acc:0.703125 | test: loss:1.5520240253377184 	 acc:0.6965732087227414 	 lr:0.0001
epoch18: train: loss:1.5334489832810365 	 acc:0.68234375 | test: loss:1.5639464858907777 	 acc:0.6610591900311527 	 lr:0.0001
epoch19: train: loss:1.5418630750359825 	 acc:0.68171875 | test: loss:1.5420697406816335 	 acc:0.660436137071651 	 lr:0.0001
epoch20: train: loss:1.5068447425717213 	 acc:0.61265625 | test: loss:1.5860457689962655 	 acc:0.609968847352025 	 lr:0.0001
epoch21: train: loss:1.5052449043982667 	 acc:0.6628125 | test: loss:1.5598537852830976 	 acc:0.6473520249221184 	 lr:0.0001
epoch22: train: loss:1.512116562473318 	 acc:0.729375 | test: loss:1.5244455804705992 	 acc:0.7227414330218068 	 lr:0.0001
epoch23: train: loss:1.5312429721424303 	 acc:0.748125 | test: loss:1.520157083321212 	 acc:0.7246105919003115 	 lr:0.0001
epoch24: train: loss:1.530823735423985 	 acc:0.69125 | test: loss:1.5397699203818016 	 acc:0.6866043613707166 	 lr:0.0001
epoch25: train: loss:1.4960179611075026 	 acc:0.6821875 | test: loss:1.5332003428557208 	 acc:0.6598130841121496 	 lr:0.0001
epoch26: train: loss:1.5201629281323186 	 acc:0.60890625 | test: loss:1.5776859246310415 	 acc:0.6024922118380063 	 lr:0.0001
epoch27: train: loss:1.489634955925089 	 acc:0.7134375 | test: loss:1.5213638677032566 	 acc:0.7146417445482866 	 lr:0.0001
epoch28: train: loss:1.488536466052065 	 acc:0.75171875 | test: loss:1.4815063884325117 	 acc:0.75202492211838 	 lr:0.0001
epoch29: train: loss:1.4851709489427816 	 acc:0.68109375 | test: loss:1.5378440923779924 	 acc:0.6623052959501557 	 lr:0.0001
epoch30: train: loss:1.5347889693987546 	 acc:0.79375 | test: loss:1.4541731082019034 	 acc:0.7800623052959501 	 lr:0.0001
epoch31: train: loss:1.4597177760104105 	 acc:0.65359375 | test: loss:1.5516024596223208 	 acc:0.6492211838006231 	 lr:0.0001
epoch32: train: loss:1.4710568501742718 	 acc:0.79375 | test: loss:1.4771533492198243 	 acc:0.7532710280373832 	 lr:0.0001
epoch33: train: loss:1.4586528959728422 	 acc:0.76546875 | test: loss:1.484772953734591 	 acc:0.7476635514018691 	 lr:0.0001
epoch34: train: loss:1.437440680117462 	 acc:0.741875 | test: loss:1.475584955675951 	 acc:0.7227414330218068 	 lr:0.0001
epoch35: train: loss:1.4607428521685635 	 acc:0.7928125 | test: loss:1.451003178629177 	 acc:0.7744548286604361 	 lr:0.0001
epoch36: train: loss:1.4731872510947257 	 acc:0.7221875 | test: loss:1.4892048348518918 	 acc:0.6959501557632399 	 lr:0.0001
epoch37: train: loss:1.470978807602703 	 acc:0.69953125 | test: loss:1.5303243296911413 	 acc:0.6672897196261682 	 lr:0.0001
epoch38: train: loss:1.4644329347990157 	 acc:0.78234375 | test: loss:1.475431236151223 	 acc:0.7570093457943925 	 lr:0.0001
epoch39: train: loss:1.455987584562994 	 acc:0.69109375 | test: loss:1.501218877254617 	 acc:0.6697819314641744 	 lr:0.0001
epoch40: train: loss:1.4252305304045607 	 acc:0.77453125 | test: loss:1.4664187088190952 	 acc:0.7345794392523365 	 lr:0.0001
epoch41: train: loss:1.4328335621317878 	 acc:0.7840625 | test: loss:1.4697698540405322 	 acc:0.7632398753894081 	 lr:0.0001
epoch42: train: loss:1.3955963730346774 	 acc:0.7928125 | test: loss:1.4484281712974714 	 acc:0.7632398753894081 	 lr:5e-05
epoch43: train: loss:1.3977420303618693 	 acc:0.77859375 | test: loss:1.4556143686407452 	 acc:0.7526479750778816 	 lr:5e-05
epoch44: train: loss:1.4020832070701295 	 acc:0.7975 | test: loss:1.4551889129888231 	 acc:0.7651090342679128 	 lr:5e-05
epoch45: train: loss:1.3809823696544448 	 acc:0.8184375 | test: loss:1.4403256833738998 	 acc:0.7775700934579439 	 lr:5e-05
epoch46: train: loss:1.3742067407016918 	 acc:0.7734375 | test: loss:1.4595103850617215 	 acc:0.7439252336448599 	 lr:5e-05
epoch47: train: loss:1.3661935012662532 	 acc:0.78234375 | test: loss:1.447394856559896 	 acc:0.7495327102803738 	 lr:5e-05
epoch48: train: loss:1.3733967932195612 	 acc:0.80796875 | test: loss:1.43501163197455 	 acc:0.7813084112149533 	 lr:5e-05
epoch49: train: loss:1.363929008823368 	 acc:0.80140625 | test: loss:1.4362798560074186 	 acc:0.7825545171339564 	 lr:5e-05
epoch50: train: loss:1.3661308890111181 	 acc:0.74296875 | test: loss:1.4776561256509704 	 acc:0.7190031152647975 	 lr:5e-05
epoch51: train: loss:1.3719689588822208 	 acc:0.7790625 | test: loss:1.4557986899699749 	 acc:0.7426791277258566 	 lr:5e-05
epoch52: train: loss:1.3643735625053364 	 acc:0.79765625 | test: loss:1.432071595845564 	 acc:0.7725856697819314 	 lr:5e-05
epoch53: train: loss:1.373031030159086 	 acc:0.810625 | test: loss:1.4320952376101248 	 acc:0.7769470404984423 	 lr:5e-05
epoch54: train: loss:1.3832247565073672 	 acc:0.781875 | test: loss:1.4562873890095411 	 acc:0.7489096573208722 	 lr:5e-05
epoch55: train: loss:1.3675772649808193 	 acc:0.74046875 | test: loss:1.4745032822231636 	 acc:0.7190031152647975 	 lr:5e-05
epoch56: train: loss:1.371491228947874 	 acc:0.7984375 | test: loss:1.4375712447448683 	 acc:0.767601246105919 	 lr:5e-05
epoch57: train: loss:1.3543701624516673 	 acc:0.8203125 | test: loss:1.4342619616665944 	 acc:0.7856697819314642 	 lr:5e-05
epoch58: train: loss:1.384777760561512 	 acc:0.844375 | test: loss:1.4095951164994285 	 acc:0.8093457943925234 	 lr:5e-05
epoch59: train: loss:1.3625585623033152 	 acc:0.779375 | test: loss:1.4435111443944437 	 acc:0.7638629283489097 	 lr:5e-05
epoch60: train: loss:1.3618514645388125 	 acc:0.78578125 | test: loss:1.4505793307057793 	 acc:0.7538940809968847 	 lr:5e-05
epoch61: train: loss:1.3509364798886063 	 acc:0.82328125 | test: loss:1.4119985303403433 	 acc:0.7975077881619937 	 lr:5e-05
epoch62: train: loss:1.3572841718734754 	 acc:0.79953125 | test: loss:1.438142729548279 	 acc:0.7781931464174455 	 lr:5e-05
epoch63: train: loss:1.3479761890467956 	 acc:0.79375 | test: loss:1.4302493665077234 	 acc:0.7794392523364486 	 lr:5e-05
epoch64: train: loss:1.355622922136484 	 acc:0.834375 | test: loss:1.4153888051012224 	 acc:0.7993769470404984 	 lr:5e-05
epoch65: train: loss:1.3422204261082955 	 acc:0.7784375 | test: loss:1.4399248761922772 	 acc:0.7433021806853582 	 lr:2.5e-05
epoch66: train: loss:1.3311264931457663 	 acc:0.835 | test: loss:1.41346361065208 	 acc:0.794392523364486 	 lr:2.5e-05
epoch67: train: loss:1.324515582880501 	 acc:0.81625 | test: loss:1.4294385249740982 	 acc:0.7738317757009345 	 lr:2.5e-05
epoch68: train: loss:1.3394727124728605 	 acc:0.8196875 | test: loss:1.4297313386406112 	 acc:0.7707165109034267 	 lr:2.5e-05
epoch69: train: loss:1.3379618088590457 	 acc:0.834375 | test: loss:1.4206749668745238 	 acc:0.7931464174454829 	 lr:2.5e-05
epoch70: train: loss:1.3295832304169106 	 acc:0.81578125 | test: loss:1.4288362178475686 	 acc:0.778816199376947 	 lr:2.5e-05
epoch71: train: loss:1.3313281640701233 	 acc:0.821875 | test: loss:1.418450638883953 	 acc:0.7862928348909657 	 lr:1.25e-05
epoch72: train: loss:1.3291346386947454 	 acc:0.8325 | test: loss:1.4184106973101416 	 acc:0.7987538940809968 	 lr:1.25e-05
epoch73: train: loss:1.3210917631412837 	 acc:0.82078125 | test: loss:1.4224772737776379 	 acc:0.7838006230529595 	 lr:1.25e-05
epoch74: train: loss:1.3314968540927192 	 acc:0.839375 | test: loss:1.4157731576129284 	 acc:0.7962616822429907 	 lr:1.25e-05
epoch75: train: loss:1.323225084791697 	 acc:0.8365625 | test: loss:1.4156150659668112 	 acc:0.7975077881619937 	 lr:1.25e-05
epoch76: train: loss:1.3286173955245841 	 acc:0.8296875 | test: loss:1.4159054539285345 	 acc:0.7931464174454829 	 lr:1.25e-05
epoch77: train: loss:1.3168695478119206 	 acc:0.8396875 | test: loss:1.4116325661400768 	 acc:0.7975077881619937 	 lr:6.25e-06
epoch78: train: loss:1.3265537378101215 	 acc:0.835 | test: loss:1.4117947067427115 	 acc:0.8012461059190031 	 lr:6.25e-06
epoch79: train: loss:1.3155960857263307 	 acc:0.8353125 | test: loss:1.4117658155358097 	 acc:0.8024922118380062 	 lr:6.25e-06
epoch80: train: loss:1.3233942488968884 	 acc:0.82609375 | test: loss:1.4230806441321922 	 acc:0.788785046728972 	 lr:6.25e-06
epoch81: train: loss:1.3131503817627526 	 acc:0.835625 | test: loss:1.4151743134234183 	 acc:0.7931464174454829 	 lr:6.25e-06
epoch82: train: loss:1.3226499392221347 	 acc:0.84515625 | test: loss:1.4120881516614063 	 acc:0.8037383177570093 	 lr:6.25e-06
epoch83: train: loss:1.3165133545493632 	 acc:0.83703125 | test: loss:1.412396642649285 	 acc:0.8 	 lr:3.125e-06
epoch84: train: loss:1.31445017806447 	 acc:0.840625 | test: loss:1.4124682335095984 	 acc:0.7975077881619937 	 lr:3.125e-06
epoch85: train: loss:1.3202563867263735 	 acc:0.839375 | test: loss:1.4124514199491602 	 acc:0.7987538940809968 	 lr:3.125e-06
epoch86: train: loss:1.3226239432961004 	 acc:0.84 | test: loss:1.4117646466905827 	 acc:0.7993769470404984 	 lr:3.125e-06
epoch87: train: loss:1.3201410035990253 	 acc:0.835625 | test: loss:1.414355584394152 	 acc:0.7937694704049845 	 lr:3.125e-06
epoch88: train: loss:1.3180974217339665 	 acc:0.83234375 | test: loss:1.415397528324543 	 acc:0.7937694704049845 	 lr:3.125e-06
epoch89: train: loss:1.3151573735042814 	 acc:0.83375 | test: loss:1.413214096399111 	 acc:0.795638629283489 	 lr:1.5625e-06
epoch90: train: loss:1.3165177237326795 	 acc:0.8346875 | test: loss:1.4158726060130515 	 acc:0.7931464174454829 	 lr:1.5625e-06
epoch91: train: loss:1.3165792091482789 	 acc:0.83234375 | test: loss:1.4146501794411015 	 acc:0.788785046728972 	 lr:1.5625e-06
epoch92: train: loss:1.3205479141699905 	 acc:0.8334375 | test: loss:1.4141262869225855 	 acc:0.795638629283489 	 lr:1.5625e-06
epoch93: train: loss:1.3250809370959578 	 acc:0.84140625 | test: loss:1.4095018098658862 	 acc:0.8031152647975078 	 lr:1.5625e-06
epoch94: train: loss:1.318729766917173 	 acc:0.84140625 | test: loss:1.4132417847434309 	 acc:0.795638629283489 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/resnet50_imagenet_3_2/
Training on HAM, create new exp container at ../checkpoints/HAM/resnet50_imagenet_3_2/
pooling!! 1024
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.7410496636538837 	 acc:0.515625 | test: loss:1.7170315008668513 	 acc:0.5264797507788161 	 lr:0.0001
epoch1: train: loss:1.6562238277819452 	 acc:0.60453125 | test: loss:1.6666137326914945 	 acc:0.6105919003115264 	 lr:0.0001
epoch2: train: loss:1.6040779679571624 	 acc:0.58875 | test: loss:1.6300712822382324 	 acc:0.5669781931464174 	 lr:0.0001
epoch3: train: loss:1.55152754653347 	 acc:0.65078125 | test: loss:1.5899717268542708 	 acc:0.6311526479750779 	 lr:0.0001
epoch4: train: loss:1.4996174868152627 	 acc:0.6471875 | test: loss:1.5603399652558323 	 acc:0.6498442367601246 	 lr:0.0001
epoch5: train: loss:1.477934147211651 	 acc:0.72140625 | test: loss:1.4980237664463365 	 acc:0.7221183800623053 	 lr:0.0001
epoch6: train: loss:1.4580979668098348 	 acc:0.6759375 | test: loss:1.5046266688736056 	 acc:0.685981308411215 	 lr:0.0001
epoch7: train: loss:1.4948650089117999 	 acc:0.67125 | test: loss:1.5331338447202403 	 acc:0.67601246105919 	 lr:0.0001
epoch8: train: loss:1.4199386394182096 	 acc:0.75453125 | test: loss:1.463978687402244 	 acc:0.7595015576323988 	 lr:0.0001
epoch9: train: loss:1.4536255859565586 	 acc:0.66484375 | test: loss:1.504406220222188 	 acc:0.6585669781931465 	 lr:0.0001
epoch10: train: loss:1.450013438693068 	 acc:0.77484375 | test: loss:1.4698137817353103 	 acc:0.7707165109034267 	 lr:0.0001
epoch11: train: loss:1.3854563800177473 	 acc:0.76375 | test: loss:1.4445473434026368 	 acc:0.7420560747663552 	 lr:0.0001
epoch12: train: loss:1.4003339733210138 	 acc:0.75703125 | test: loss:1.4671726240918643 	 acc:0.7289719626168224 	 lr:0.0001
epoch13: train: loss:1.402049244650633 	 acc:0.78953125 | test: loss:1.420527123216528 	 acc:0.7875389408099689 	 lr:0.0001
epoch14: train: loss:1.3628885928771162 	 acc:0.75140625 | test: loss:1.4537098108422348 	 acc:0.7345794392523365 	 lr:0.0001
epoch15: train: loss:1.3761406236658982 	 acc:0.71703125 | test: loss:1.4774706689739525 	 acc:0.6922118380062305 	 lr:0.0001
epoch16: train: loss:1.3908924754944563 	 acc:0.78859375 | test: loss:1.4229224155996447 	 acc:0.7682242990654206 	 lr:0.0001
epoch17: train: loss:1.350705208581095 	 acc:0.78421875 | test: loss:1.4102013429748679 	 acc:0.7781931464174455 	 lr:0.0001
epoch18: train: loss:1.3285462474004075 	 acc:0.80421875 | test: loss:1.4322285345410262 	 acc:0.7607476635514019 	 lr:0.0001
epoch19: train: loss:1.3697795072819086 	 acc:0.76875 | test: loss:1.4378274348665991 	 acc:0.7401869158878505 	 lr:0.0001
epoch20: train: loss:1.3461777888825868 	 acc:0.775 | test: loss:1.4364972939743803 	 acc:0.7426791277258566 	 lr:0.0001
epoch21: train: loss:1.3188514703516845 	 acc:0.819375 | test: loss:1.386322893828989 	 acc:0.7912772585669782 	 lr:0.0001
epoch22: train: loss:1.353348876571953 	 acc:0.87796875 | test: loss:1.358085818379839 	 acc:0.8523364485981308 	 lr:0.0001
epoch23: train: loss:1.3491418771498098 	 acc:0.8503125 | test: loss:1.3867862280284133 	 acc:0.815576323987539 	 lr:0.0001
epoch24: train: loss:1.339227449772974 	 acc:0.79171875 | test: loss:1.4317988140189388 	 acc:0.7408099688473521 	 lr:0.0001
epoch25: train: loss:1.3353700207510002 	 acc:0.7525 | test: loss:1.4325461704040243 	 acc:0.746417445482866 	 lr:0.0001
epoch26: train: loss:1.3602003587399676 	 acc:0.8640625 | test: loss:1.351055836751825 	 acc:0.8330218068535825 	 lr:0.0001
epoch27: train: loss:1.3255247736982216 	 acc:0.821875 | test: loss:1.4099899745061761 	 acc:0.7682242990654206 	 lr:0.0001
epoch28: train: loss:1.3004333162568307 	 acc:0.8115625 | test: loss:1.3935180814095376 	 acc:0.778816199376947 	 lr:0.0001
epoch29: train: loss:1.3242316455230594 	 acc:0.8709375 | test: loss:1.3487832939142008 	 acc:0.8342679127725857 	 lr:0.0001
epoch30: train: loss:1.3074016776520418 	 acc:0.84546875 | test: loss:1.3854401907074116 	 acc:0.788785046728972 	 lr:0.0001
epoch31: train: loss:1.2902248933480922 	 acc:0.88625 | test: loss:1.34058708817788 	 acc:0.8436137071651091 	 lr:0.0001
epoch32: train: loss:1.3064524475621768 	 acc:0.856875 | test: loss:1.3702796628542036 	 acc:0.805607476635514 	 lr:0.0001
epoch33: train: loss:1.2792970989385124 	 acc:0.89703125 | test: loss:1.3391080063080119 	 acc:0.8392523364485981 	 lr:0.0001
epoch34: train: loss:1.2976150596076674 	 acc:0.8315625 | test: loss:1.379416590167726 	 acc:0.7844236760124611 	 lr:0.0001
epoch35: train: loss:1.294155838524895 	 acc:0.854375 | test: loss:1.3771525365912654 	 acc:0.8024922118380062 	 lr:0.0001
epoch36: train: loss:1.2904268215635062 	 acc:0.85328125 | test: loss:1.3900466848385298 	 acc:0.7962616822429907 	 lr:0.0001
epoch37: train: loss:1.2739735802107728 	 acc:0.8828125 | test: loss:1.3633712354107437 	 acc:0.8193146417445483 	 lr:0.0001
epoch38: train: loss:1.2902091533294606 	 acc:0.886875 | test: loss:1.3452109404442094 	 acc:0.8317757009345794 	 lr:0.0001
epoch39: train: loss:1.2686659158532458 	 acc:0.9040625 | test: loss:1.3285896170547817 	 acc:0.8485981308411215 	 lr:0.0001
epoch40: train: loss:1.2665125779115438 	 acc:0.91234375 | test: loss:1.3281640237736925 	 acc:0.8523364485981308 	 lr:0.0001
epoch41: train: loss:1.2734933988644126 	 acc:0.8546875 | test: loss:1.37035698407907 	 acc:0.8062305295950156 	 lr:0.0001
epoch42: train: loss:1.270062458356966 	 acc:0.81671875 | test: loss:1.4059547167329403 	 acc:0.7507788161993769 	 lr:0.0001
epoch43: train: loss:1.286596227045826 	 acc:0.9221875 | test: loss:1.3290997141989591 	 acc:0.8442367601246106 	 lr:0.0001
epoch44: train: loss:1.2711128300377208 	 acc:0.87453125 | test: loss:1.378405175922073 	 acc:0.8062305295950156 	 lr:0.0001
epoch45: train: loss:1.2844318851467968 	 acc:0.91015625 | test: loss:1.3322162551671917 	 acc:0.8429906542056075 	 lr:0.0001
epoch46: train: loss:1.2561955638828919 	 acc:0.91953125 | test: loss:1.3363019635744184 	 acc:0.838006230529595 	 lr:0.0001
epoch47: train: loss:1.2436341809072502 	 acc:0.9209375 | test: loss:1.3247465518404762 	 acc:0.8448598130841122 	 lr:5e-05
epoch48: train: loss:1.2497965232735961 	 acc:0.93953125 | test: loss:1.3066437153058632 	 acc:0.870404984423676 	 lr:5e-05
epoch49: train: loss:1.2405123541636174 	 acc:0.93515625 | test: loss:1.3056208338692923 	 acc:0.8666666666666667 	 lr:5e-05
epoch50: train: loss:1.2426404013473666 	 acc:0.9325 | test: loss:1.3084093767534535 	 acc:0.8641744548286604 	 lr:5e-05
epoch51: train: loss:1.2343623117391809 	 acc:0.933125 | test: loss:1.3126314426879646 	 acc:0.8629283489096573 	 lr:5e-05
epoch52: train: loss:1.2383035175880355 	 acc:0.945625 | test: loss:1.3035656262038282 	 acc:0.870404984423676 	 lr:5e-05
epoch53: train: loss:1.2230769327894773 	 acc:0.89875 | test: loss:1.340350060670918 	 acc:0.8386292834890966 	 lr:5e-05
epoch54: train: loss:1.2406393701819867 	 acc:0.93875 | test: loss:1.3014658932373901 	 acc:0.8697819314641745 	 lr:5e-05
epoch55: train: loss:1.2422611543296558 	 acc:0.90859375 | test: loss:1.3270881919474617 	 acc:0.8442367601246106 	 lr:5e-05
epoch56: train: loss:1.2274138975478448 	 acc:0.934375 | test: loss:1.309147534043618 	 acc:0.8697819314641745 	 lr:5e-05
epoch57: train: loss:1.22189241717515 	 acc:0.9484375 | test: loss:1.306015070130892 	 acc:0.8629283489096573 	 lr:5e-05
epoch58: train: loss:1.2226388876183159 	 acc:0.9328125 | test: loss:1.3029621261673925 	 acc:0.874766355140187 	 lr:5e-05
epoch59: train: loss:1.2291990376933304 	 acc:0.9396875 | test: loss:1.3018007958046744 	 acc:0.8716510903426792 	 lr:5e-05
epoch60: train: loss:1.2199978068319732 	 acc:0.89984375 | test: loss:1.3359411955622498 	 acc:0.8342679127725857 	 lr:5e-05
epoch61: train: loss:1.2171187246711248 	 acc:0.94390625 | test: loss:1.3080170743561979 	 acc:0.8641744548286604 	 lr:2.5e-05
epoch62: train: loss:1.217544291226032 	 acc:0.9521875 | test: loss:1.2920385843496827 	 acc:0.8753894080996885 	 lr:2.5e-05
epoch63: train: loss:1.2190278933999317 	 acc:0.93640625 | test: loss:1.307921623366644 	 acc:0.8635514018691589 	 lr:2.5e-05
epoch64: train: loss:1.2177618689317429 	 acc:0.95328125 | test: loss:1.2942282060967798 	 acc:0.874766355140187 	 lr:2.5e-05
epoch65: train: loss:1.2124248900700136 	 acc:0.93546875 | test: loss:1.3103620668809361 	 acc:0.8629283489096573 	 lr:2.5e-05
epoch66: train: loss:1.213434457295021 	 acc:0.95640625 | test: loss:1.2945835325205437 	 acc:0.8753894080996885 	 lr:2.5e-05
epoch67: train: loss:1.2153824745910788 	 acc:0.94828125 | test: loss:1.2947233276575154 	 acc:0.8741433021806854 	 lr:2.5e-05
epoch68: train: loss:1.2158389113081516 	 acc:0.95078125 | test: loss:1.3021533803405048 	 acc:0.8710280373831776 	 lr:2.5e-05
epoch69: train: loss:1.2054656148608265 	 acc:0.953125 | test: loss:1.3005296445709893 	 acc:0.8728971962616823 	 lr:1.25e-05
epoch70: train: loss:1.2082325681497303 	 acc:0.9446875 | test: loss:1.302484926107888 	 acc:0.8697819314641745 	 lr:1.25e-05
epoch71: train: loss:1.2056765411162544 	 acc:0.9578125 | test: loss:1.2974698505668996 	 acc:0.8760124610591901 	 lr:1.25e-05
epoch72: train: loss:1.2025951710089773 	 acc:0.95234375 | test: loss:1.300148482486095 	 acc:0.8722741433021807 	 lr:1.25e-05
epoch73: train: loss:1.2079876222990156 	 acc:0.9559375 | test: loss:1.296344921819146 	 acc:0.8772585669781932 	 lr:1.25e-05
epoch74: train: loss:1.2131819189758062 	 acc:0.95484375 | test: loss:1.2968499838748826 	 acc:0.8766355140186916 	 lr:1.25e-05
epoch75: train: loss:1.2101362360910362 	 acc:0.9503125 | test: loss:1.2975023629138032 	 acc:0.8778816199376948 	 lr:6.25e-06
epoch76: train: loss:1.2063584148278932 	 acc:0.95984375 | test: loss:1.2968366875455386 	 acc:0.8772585669781932 	 lr:6.25e-06
epoch77: train: loss:1.2098753858412923 	 acc:0.95359375 | test: loss:1.300366298729014 	 acc:0.8697819314641745 	 lr:6.25e-06
epoch78: train: loss:1.2056938796746925 	 acc:0.94984375 | test: loss:1.300049372254131 	 acc:0.8710280373831776 	 lr:6.25e-06
epoch79: train: loss:1.2049337680408678 	 acc:0.955 | test: loss:1.2983234900923162 	 acc:0.8722741433021807 	 lr:6.25e-06
epoch80: train: loss:1.206554877990675 	 acc:0.95359375 | test: loss:1.29711349040177 	 acc:0.874766355140187 	 lr:6.25e-06
epoch81: train: loss:1.2030429527407787 	 acc:0.95921875 | test: loss:1.2955299361843928 	 acc:0.8722741433021807 	 lr:3.125e-06
epoch82: train: loss:1.2024865989476605 	 acc:0.95296875 | test: loss:1.2969221492422705 	 acc:0.874766355140187 	 lr:3.125e-06
epoch83: train: loss:1.201825296608942 	 acc:0.96015625 | test: loss:1.2944511455167491 	 acc:0.874766355140187 	 lr:3.125e-06
epoch84: train: loss:1.203925358625616 	 acc:0.961875 | test: loss:1.293719921602267 	 acc:0.8772585669781932 	 lr:3.125e-06
epoch85: train: loss:1.2052133862438097 	 acc:0.96203125 | test: loss:1.2944982785673527 	 acc:0.8772585669781932 	 lr:3.125e-06
epoch86: train: loss:1.2078448067038996 	 acc:0.95921875 | test: loss:1.2967382667963379 	 acc:0.8728971962616823 	 lr:3.125e-06
epoch87: train: loss:1.2049871964346701 	 acc:0.96109375 | test: loss:1.2950484210457014 	 acc:0.8778816199376948 	 lr:1.5625e-06
epoch88: train: loss:1.2049682950712943 	 acc:0.9571875 | test: loss:1.295816901911085 	 acc:0.8766355140186916 	 lr:1.5625e-06
epoch89: train: loss:1.205291390698185 	 acc:0.96296875 | test: loss:1.2941130489575157 	 acc:0.8791277258566979 	 lr:1.5625e-06
epoch90: train: loss:1.204435981315714 	 acc:0.9553125 | test: loss:1.296694108036077 	 acc:0.8753894080996885 	 lr:1.5625e-06
epoch91: train: loss:1.2058053986715396 	 acc:0.96171875 | test: loss:1.2957167746493379 	 acc:0.8753894080996885 	 lr:1.5625e-06
epoch92: train: loss:1.1971865199116596 	 acc:0.96078125 | test: loss:1.295382807930682 	 acc:0.8778816199376948 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_1_2/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_1_2/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.929514374088999 	 acc:0.02546875 | test: loss:1.9384756819852786 	 acc:0.02367601246105919 	 lr:0.0001
epoch1: train: loss:1.8552489138505683 	 acc:0.50203125 | test: loss:1.812653550329238 	 acc:0.5109034267912772 	 lr:0.0001
epoch2: train: loss:1.7893569076740583 	 acc:0.46921875 | test: loss:1.7377413705130604 	 acc:0.4766355140186916 	 lr:0.0001
epoch3: train: loss:1.7705574458507147 	 acc:0.50265625 | test: loss:1.6890967332685476 	 acc:0.49034267912772583 	 lr:0.0001
epoch4: train: loss:1.7761453859327734 	 acc:0.49125 | test: loss:1.7335614581717138 	 acc:0.4691588785046729 	 lr:0.0001
epoch5: train: loss:1.7467159250394895 	 acc:0.54421875 | test: loss:1.6538480984459043 	 acc:0.5651090342679128 	 lr:0.0001
epoch6: train: loss:1.6952987754279798 	 acc:0.54171875 | test: loss:1.6516031186528666 	 acc:0.5302180685358255 	 lr:0.0001
epoch7: train: loss:1.7083911499690487 	 acc:0.501875 | test: loss:1.685979556146069 	 acc:0.4897196261682243 	 lr:0.0001
epoch8: train: loss:1.6901313013438597 	 acc:0.52953125 | test: loss:1.6671254456600295 	 acc:0.5221183800623053 	 lr:0.0001
epoch9: train: loss:1.6689000316563294 	 acc:0.52265625 | test: loss:1.6378683575217226 	 acc:0.5190031152647975 	 lr:0.0001
epoch10: train: loss:1.6744905875307987 	 acc:0.48328125 | test: loss:1.7147746613463881 	 acc:0.4660436137071651 	 lr:0.0001
epoch11: train: loss:1.6847736772571478 	 acc:0.4728125 | test: loss:1.7252684143102057 	 acc:0.4473520249221184 	 lr:0.0001
epoch12: train: loss:1.6632435425662324 	 acc:0.59609375 | test: loss:1.573829117965104 	 acc:0.6305295950155764 	 lr:0.0001
epoch13: train: loss:1.6484585931020077 	 acc:0.65453125 | test: loss:1.5663467960565631 	 acc:0.6691588785046729 	 lr:0.0001
epoch14: train: loss:1.6728733012119743 	 acc:0.595625 | test: loss:1.5867735784744548 	 acc:0.6411214953271028 	 lr:0.0001
epoch15: train: loss:1.6421716527767614 	 acc:0.61375 | test: loss:1.5832153561702027 	 acc:0.6037383177570094 	 lr:0.0001
epoch16: train: loss:1.6253673949528262 	 acc:0.60640625 | test: loss:1.5690063839760897 	 acc:0.6218068535825545 	 lr:0.0001
epoch17: train: loss:1.6404380685179425 	 acc:0.61375 | test: loss:1.5774822874603984 	 acc:0.6280373831775701 	 lr:0.0001
epoch18: train: loss:1.6335543344394943 	 acc:0.535 | test: loss:1.6311680593223215 	 acc:0.5376947040498442 	 lr:0.0001
epoch19: train: loss:1.63136956887167 	 acc:0.6490625 | test: loss:1.5457424391838621 	 acc:0.6485981308411215 	 lr:0.0001
epoch20: train: loss:1.6026317539855337 	 acc:0.56359375 | test: loss:1.599648691189252 	 acc:0.5663551401869159 	 lr:0.0001
epoch21: train: loss:1.5987815076722287 	 acc:0.6034375 | test: loss:1.5745090365038483 	 acc:0.6137071651090342 	 lr:0.0001
epoch22: train: loss:1.600377405480795 	 acc:0.61984375 | test: loss:1.5788012685805466 	 acc:0.621183800623053 	 lr:0.0001
epoch23: train: loss:1.5809272259869305 	 acc:0.65328125 | test: loss:1.535620874927794 	 acc:0.6672897196261682 	 lr:0.0001
epoch24: train: loss:1.587721680068672 	 acc:0.62734375 | test: loss:1.5469446374619862 	 acc:0.6473520249221184 	 lr:0.0001
epoch25: train: loss:1.5925903225019515 	 acc:0.68125 | test: loss:1.4967589727443327 	 acc:0.697196261682243 	 lr:0.0001
epoch26: train: loss:1.5833158804233143 	 acc:0.63828125 | test: loss:1.5421273777418048 	 acc:0.6467289719626168 	 lr:0.0001
epoch27: train: loss:1.5842628639810816 	 acc:0.61390625 | test: loss:1.5745982220611097 	 acc:0.6068535825545172 	 lr:0.0001
epoch28: train: loss:1.5655369445181377 	 acc:0.63203125 | test: loss:1.54887478752671 	 acc:0.6529595015576324 	 lr:0.0001
epoch29: train: loss:1.5671573921817061 	 acc:0.58890625 | test: loss:1.5734737514335417 	 acc:0.6024922118380063 	 lr:0.0001
epoch30: train: loss:1.5548321452204088 	 acc:0.7271875 | test: loss:1.4724274893044682 	 acc:0.7252336448598131 	 lr:0.0001
epoch31: train: loss:1.56090421416069 	 acc:0.61515625 | test: loss:1.5590659342079518 	 acc:0.6224299065420561 	 lr:0.0001
epoch32: train: loss:1.5560571555882856 	 acc:0.6546875 | test: loss:1.513033180890425 	 acc:0.6785046728971963 	 lr:0.0001
epoch33: train: loss:1.5564064925206442 	 acc:0.573125 | test: loss:1.588627903335191 	 acc:0.5813084112149532 	 lr:0.0001
epoch34: train: loss:1.5318316135808512 	 acc:0.5996875 | test: loss:1.5799078720752324 	 acc:0.5862928348909657 	 lr:0.0001
epoch35: train: loss:1.5519203489781543 	 acc:0.71890625 | test: loss:1.4889292543179522 	 acc:0.7090342679127726 	 lr:0.0001
epoch36: train: loss:1.525100448334431 	 acc:0.63796875 | test: loss:1.5315038182653742 	 acc:0.6292834890965732 	 lr:0.0001
epoch37: train: loss:1.516274216806023 	 acc:0.67 | test: loss:1.5146871931456332 	 acc:0.67601246105919 	 lr:5e-05
epoch38: train: loss:1.502922464291813 	 acc:0.725 | test: loss:1.481985670235298 	 acc:0.7071651090342679 	 lr:5e-05
epoch39: train: loss:1.504886128695843 	 acc:0.72515625 | test: loss:1.479367503198879 	 acc:0.7327102803738318 	 lr:5e-05
epoch40: train: loss:1.4854593984230156 	 acc:0.6884375 | test: loss:1.5027533933752422 	 acc:0.6828660436137072 	 lr:5e-05
epoch41: train: loss:1.4996369789951394 	 acc:0.6384375 | test: loss:1.5301354468425858 	 acc:0.6404984423676012 	 lr:5e-05
epoch42: train: loss:1.481931533039221 	 acc:0.66671875 | test: loss:1.5049879415012966 	 acc:0.6809968847352025 	 lr:5e-05
epoch43: train: loss:1.4876512728474458 	 acc:0.69921875 | test: loss:1.4880829717511328 	 acc:0.6990654205607477 	 lr:2.5e-05
epoch44: train: loss:1.4786899158677302 	 acc:0.7290625 | test: loss:1.4761994582470332 	 acc:0.7121495327102804 	 lr:2.5e-05
epoch45: train: loss:1.4733112177376073 	 acc:0.69546875 | test: loss:1.4902686791999318 	 acc:0.6978193146417445 	 lr:2.5e-05
epoch46: train: loss:1.467195199673107 	 acc:0.7009375 | test: loss:1.4955357750628224 	 acc:0.6884735202492211 	 lr:2.5e-05
epoch47: train: loss:1.4667176079135869 	 acc:0.7003125 | test: loss:1.4960197307610437 	 acc:0.6890965732087228 	 lr:2.5e-05
epoch48: train: loss:1.4684600544199173 	 acc:0.68625 | test: loss:1.5036597694563345 	 acc:0.6834890965732088 	 lr:2.5e-05
epoch49: train: loss:1.4587293924157458 	 acc:0.70703125 | test: loss:1.4803774132535465 	 acc:0.7015576323987539 	 lr:1.25e-05
epoch50: train: loss:1.4614027132008989 	 acc:0.70875 | test: loss:1.4811623359394965 	 acc:0.7090342679127726 	 lr:1.25e-05
epoch51: train: loss:1.4573132171749976 	 acc:0.698125 | test: loss:1.484505144383677 	 acc:0.7015576323987539 	 lr:1.25e-05
epoch52: train: loss:1.4619320657270016 	 acc:0.72046875 | test: loss:1.4755716999000479 	 acc:0.7109034267912773 	 lr:1.25e-05
epoch53: train: loss:1.4499467550451917 	 acc:0.715 | test: loss:1.4770668915127667 	 acc:0.7059190031152648 	 lr:1.25e-05
epoch54: train: loss:1.458864270105295 	 acc:0.72046875 | test: loss:1.476833808087857 	 acc:0.7121495327102804 	 lr:1.25e-05
epoch55: train: loss:1.4605000551746377 	 acc:0.715 | test: loss:1.4823091039033693 	 acc:0.702803738317757 	 lr:6.25e-06
epoch56: train: loss:1.45057337312006 	 acc:0.7096875 | test: loss:1.4842283687858937 	 acc:0.7003115264797508 	 lr:6.25e-06
epoch57: train: loss:1.4439308279664325 	 acc:0.71359375 | test: loss:1.4796595029741804 	 acc:0.7046728971962617 	 lr:6.25e-06
epoch58: train: loss:1.4392331387641186 	 acc:0.71984375 | test: loss:1.4784906098404407 	 acc:0.7034267912772586 	 lr:6.25e-06
epoch59: train: loss:1.4522539339802583 	 acc:0.72359375 | test: loss:1.4722606303907257 	 acc:0.7071651090342679 	 lr:6.25e-06
epoch60: train: loss:1.4491815380897493 	 acc:0.70671875 | test: loss:1.4828373908253845 	 acc:0.697196261682243 	 lr:6.25e-06
epoch61: train: loss:1.4514099645205163 	 acc:0.71890625 | test: loss:1.4746863091103384 	 acc:0.7046728971962617 	 lr:6.25e-06
epoch62: train: loss:1.4454210005915789 	 acc:0.7153125 | test: loss:1.4770568957581327 	 acc:0.702803738317757 	 lr:6.25e-06
epoch63: train: loss:1.4541006738929243 	 acc:0.71796875 | test: loss:1.473011937393949 	 acc:0.7015576323987539 	 lr:6.25e-06
epoch64: train: loss:1.4538284027790485 	 acc:0.7184375 | test: loss:1.4835726584229514 	 acc:0.6990654205607477 	 lr:6.25e-06
epoch65: train: loss:1.4449895330186378 	 acc:0.7215625 | test: loss:1.4802225862336678 	 acc:0.7040498442367601 	 lr:6.25e-06
epoch66: train: loss:1.441894948156805 	 acc:0.72875 | test: loss:1.4746884089764034 	 acc:0.7059190031152648 	 lr:3.125e-06
epoch67: train: loss:1.4491180824917056 	 acc:0.70796875 | test: loss:1.4776002496202416 	 acc:0.7015576323987539 	 lr:3.125e-06
epoch68: train: loss:1.4526547577118707 	 acc:0.719375 | test: loss:1.4790594093524778 	 acc:0.6996884735202492 	 lr:3.125e-06
epoch69: train: loss:1.443442460952747 	 acc:0.7265625 | test: loss:1.4742975795009055 	 acc:0.7059190031152648 	 lr:3.125e-06
epoch70: train: loss:1.4418544105214126 	 acc:0.726875 | test: loss:1.470246327777518 	 acc:0.708411214953271 	 lr:3.125e-06
epoch71: train: loss:1.444867295943416 	 acc:0.71203125 | test: loss:1.4767705072866422 	 acc:0.7046728971962617 	 lr:3.125e-06
epoch72: train: loss:1.4384688961794374 	 acc:0.7271875 | test: loss:1.469825139521067 	 acc:0.7077881619937695 	 lr:3.125e-06
epoch73: train: loss:1.4450773931498828 	 acc:0.71890625 | test: loss:1.4737044656016745 	 acc:0.7034267912772586 	 lr:3.125e-06
epoch74: train: loss:1.445862809835608 	 acc:0.72 | test: loss:1.4749704796948537 	 acc:0.7059190031152648 	 lr:3.125e-06
epoch75: train: loss:1.4439488764576014 	 acc:0.71515625 | test: loss:1.475914168877765 	 acc:0.702803738317757 	 lr:3.125e-06
epoch76: train: loss:1.4396680726193525 	 acc:0.723125 | test: loss:1.4761887965543992 	 acc:0.7046728971962617 	 lr:3.125e-06
epoch77: train: loss:1.4468575185020858 	 acc:0.71703125 | test: loss:1.480745251007912 	 acc:0.7009345794392523 	 lr:3.125e-06
epoch78: train: loss:1.4450973975295485 	 acc:0.72015625 | test: loss:1.4740537918988046 	 acc:0.7052959501557632 	 lr:3.125e-06
epoch79: train: loss:1.436890684748701 	 acc:0.72828125 | test: loss:1.476340526212413 	 acc:0.7065420560747664 	 lr:1.5625e-06
epoch80: train: loss:1.4366379201086492 	 acc:0.71859375 | test: loss:1.4774679002732132 	 acc:0.702803738317757 	 lr:1.5625e-06
epoch81: train: loss:1.4387882277334603 	 acc:0.7246875 | test: loss:1.4793320859332695 	 acc:0.7059190031152648 	 lr:1.5625e-06
epoch82: train: loss:1.4338972629186792 	 acc:0.72625 | test: loss:1.4760062164978074 	 acc:0.7059190031152648 	 lr:1.5625e-06
epoch83: train: loss:1.4355506414551924 	 acc:0.7284375 | test: loss:1.474342386447752 	 acc:0.7021806853582554 	 lr:1.5625e-06
epoch84: train: loss:1.4436243967001183 	 acc:0.720625 | test: loss:1.4783423507696372 	 acc:0.7040498442367601 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_2_2/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_2_2/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.8882922944568452 	 acc:0.5875 | test: loss:1.8016475830494059 	 acc:0.6018691588785047 	 lr:0.0001
epoch1: train: loss:1.7684246221061426 	 acc:0.5190625 | test: loss:1.7212902704874675 	 acc:0.5214953271028038 	 lr:0.0001
epoch2: train: loss:1.708615193303724 	 acc:0.64828125 | test: loss:1.5938239450395293 	 acc:0.643613707165109 	 lr:0.0001
epoch3: train: loss:1.697303866074478 	 acc:0.65734375 | test: loss:1.558700012566516 	 acc:0.659190031152648 	 lr:0.0001
epoch4: train: loss:1.661339391013033 	 acc:0.58703125 | test: loss:1.6325996166449097 	 acc:0.5657320872274143 	 lr:0.0001
epoch5: train: loss:1.6166018984077686 	 acc:0.6115625 | test: loss:1.584929532945342 	 acc:0.6180685358255452 	 lr:0.0001
epoch6: train: loss:1.658426739516247 	 acc:0.6265625 | test: loss:1.5910957031903608 	 acc:0.6074766355140186 	 lr:0.0001
epoch7: train: loss:1.5884799549302302 	 acc:0.5415625 | test: loss:1.6376789592136847 	 acc:0.5246105919003116 	 lr:0.0001
epoch8: train: loss:1.599549471447935 	 acc:0.6128125 | test: loss:1.6070855946555687 	 acc:0.5900311526479751 	 lr:0.0001
epoch9: train: loss:1.5715216655269626 	 acc:0.72375 | test: loss:1.4741297228685422 	 acc:0.7420560747663552 	 lr:0.0001
epoch10: train: loss:1.5548301869495877 	 acc:0.57953125 | test: loss:1.5927885191462865 	 acc:0.5819314641744548 	 lr:0.0001
epoch11: train: loss:1.5534931906492426 	 acc:0.67984375 | test: loss:1.5360373132325407 	 acc:0.6679127725856698 	 lr:0.0001
epoch12: train: loss:1.5364294500298838 	 acc:0.74640625 | test: loss:1.4623590130672277 	 acc:0.7451713395638629 	 lr:0.0001
epoch13: train: loss:1.5328773683165313 	 acc:0.64015625 | test: loss:1.5301444468839889 	 acc:0.6461059190031153 	 lr:0.0001
epoch14: train: loss:1.520117318527853 	 acc:0.69875 | test: loss:1.4849901574423008 	 acc:0.7146417445482866 	 lr:0.0001
epoch15: train: loss:1.5228094501480471 	 acc:0.7125 | test: loss:1.4820889640077253 	 acc:0.7009345794392523 	 lr:0.0001
epoch16: train: loss:1.4968392627486766 	 acc:0.73921875 | test: loss:1.4677925029647685 	 acc:0.735202492211838 	 lr:0.0001
epoch17: train: loss:1.534009008236363 	 acc:0.5828125 | test: loss:1.6086857564724124 	 acc:0.5607476635514018 	 lr:0.0001
epoch18: train: loss:1.474530636789648 	 acc:0.686875 | test: loss:1.4915200778628435 	 acc:0.6753894080996885 	 lr:0.0001
epoch19: train: loss:1.4489347411728204 	 acc:0.74734375 | test: loss:1.4550323959451599 	 acc:0.7320872274143302 	 lr:5e-05
epoch20: train: loss:1.4413227137879037 	 acc:0.7375 | test: loss:1.472227960806398 	 acc:0.719626168224299 	 lr:5e-05
epoch21: train: loss:1.468249070988699 	 acc:0.71515625 | test: loss:1.4591815904664844 	 acc:0.7246105919003115 	 lr:5e-05
epoch22: train: loss:1.4513742630040618 	 acc:0.7715625 | test: loss:1.440779925655353 	 acc:0.75202492211838 	 lr:5e-05
epoch23: train: loss:1.438322664330845 	 acc:0.72859375 | test: loss:1.46056318543039 	 acc:0.7233644859813084 	 lr:5e-05
epoch24: train: loss:1.4230255031660142 	 acc:0.76734375 | test: loss:1.4318570281114904 	 acc:0.761993769470405 	 lr:5e-05
epoch25: train: loss:1.418970422201283 	 acc:0.78375 | test: loss:1.425848954117558 	 acc:0.7688473520249222 	 lr:5e-05
epoch26: train: loss:1.425654486508038 	 acc:0.76640625 | test: loss:1.4185584176737942 	 acc:0.7769470404984423 	 lr:5e-05
epoch27: train: loss:1.4236079958246426 	 acc:0.70234375 | test: loss:1.4874276208729016 	 acc:0.6728971962616822 	 lr:5e-05
epoch28: train: loss:1.4403423705387637 	 acc:0.73921875 | test: loss:1.435295986980664 	 acc:0.7376947040498443 	 lr:5e-05
epoch29: train: loss:1.4123572987564648 	 acc:0.7603125 | test: loss:1.4263801388280042 	 acc:0.7644859813084112 	 lr:5e-05
epoch30: train: loss:1.424440048896736 	 acc:0.7721875 | test: loss:1.4220618196736987 	 acc:0.7738317757009345 	 lr:5e-05
epoch31: train: loss:1.399868128804096 	 acc:0.756875 | test: loss:1.4365802979543574 	 acc:0.7439252336448599 	 lr:5e-05
epoch32: train: loss:1.4071406225223824 	 acc:0.775625 | test: loss:1.4238910793144013 	 acc:0.7682242990654206 	 lr:5e-05
epoch33: train: loss:1.3874528013850262 	 acc:0.78171875 | test: loss:1.417455257730692 	 acc:0.7725856697819314 	 lr:2.5e-05
epoch34: train: loss:1.3801622043821051 	 acc:0.7721875 | test: loss:1.430131827707974 	 acc:0.7557632398753894 	 lr:2.5e-05
epoch35: train: loss:1.3786714385581333 	 acc:0.76578125 | test: loss:1.4344012003450008 	 acc:0.7445482866043613 	 lr:2.5e-05
epoch36: train: loss:1.3926801885505098 	 acc:0.76125 | test: loss:1.4325890741615652 	 acc:0.7514018691588785 	 lr:2.5e-05
epoch37: train: loss:1.3739096672064062 	 acc:0.73578125 | test: loss:1.4472366838811714 	 acc:0.7165109034267912 	 lr:2.5e-05
epoch38: train: loss:1.368313084050699 	 acc:0.763125 | test: loss:1.4279593581351164 	 acc:0.7451713395638629 	 lr:2.5e-05
epoch39: train: loss:1.3795302356061854 	 acc:0.76109375 | test: loss:1.421578164783965 	 acc:0.7532710280373832 	 lr:2.5e-05
epoch40: train: loss:1.3698028421513648 	 acc:0.7890625 | test: loss:1.4141010467879869 	 acc:0.7682242990654206 	 lr:1.25e-05
epoch41: train: loss:1.3603546785805767 	 acc:0.78234375 | test: loss:1.423982712487194 	 acc:0.7570093457943925 	 lr:1.25e-05
epoch42: train: loss:1.3686259309171607 	 acc:0.78953125 | test: loss:1.41281161798495 	 acc:0.7682242990654206 	 lr:1.25e-05
epoch43: train: loss:1.3529130350510465 	 acc:0.79546875 | test: loss:1.4083289877276555 	 acc:0.7738317757009345 	 lr:1.25e-05
epoch44: train: loss:1.3579734022034786 	 acc:0.7921875 | test: loss:1.4149007436271026 	 acc:0.7663551401869159 	 lr:1.25e-05
epoch45: train: loss:1.3692962884716835 	 acc:0.77640625 | test: loss:1.4159243820612304 	 acc:0.7601246105919003 	 lr:1.25e-05
epoch46: train: loss:1.357734064344872 	 acc:0.80171875 | test: loss:1.4072864386151513 	 acc:0.7763239875389408 	 lr:1.25e-05
epoch47: train: loss:1.3584871290625304 	 acc:0.78375 | test: loss:1.4144427450275123 	 acc:0.7694704049844237 	 lr:1.25e-05
epoch48: train: loss:1.3558188087767125 	 acc:0.80953125 | test: loss:1.398597269489015 	 acc:0.7781931464174455 	 lr:1.25e-05
epoch49: train: loss:1.350668511252958 	 acc:0.79640625 | test: loss:1.411995079064295 	 acc:0.7688473520249222 	 lr:1.25e-05
epoch50: train: loss:1.3589230992289654 	 acc:0.8034375 | test: loss:1.4046789682542795 	 acc:0.7806853582554517 	 lr:1.25e-05
epoch51: train: loss:1.3511118165968359 	 acc:0.8025 | test: loss:1.4084075176084525 	 acc:0.7750778816199377 	 lr:1.25e-05
epoch52: train: loss:1.3503651317444563 	 acc:0.79359375 | test: loss:1.4073000659081052 	 acc:0.7682242990654206 	 lr:1.25e-05
epoch53: train: loss:1.342409184833321 	 acc:0.80453125 | test: loss:1.4071274114917742 	 acc:0.7763239875389408 	 lr:1.25e-05
epoch54: train: loss:1.343243377232905 	 acc:0.8015625 | test: loss:1.4049693894906208 	 acc:0.7781931464174455 	 lr:1.25e-05
epoch55: train: loss:1.3437385052838053 	 acc:0.813125 | test: loss:1.3973842287360694 	 acc:0.7856697819314642 	 lr:6.25e-06
epoch56: train: loss:1.3442151703190561 	 acc:0.79953125 | test: loss:1.404112313543896 	 acc:0.7763239875389408 	 lr:6.25e-06
epoch57: train: loss:1.3405886933730973 	 acc:0.79953125 | test: loss:1.4076740616949919 	 acc:0.7700934579439253 	 lr:6.25e-06
epoch58: train: loss:1.341440762736479 	 acc:0.8028125 | test: loss:1.4048606796799419 	 acc:0.7838006230529595 	 lr:6.25e-06
epoch59: train: loss:1.3479767798632964 	 acc:0.80125 | test: loss:1.4047855051878457 	 acc:0.7825545171339564 	 lr:6.25e-06
epoch60: train: loss:1.338161316632666 	 acc:0.7975 | test: loss:1.4048260215658266 	 acc:0.7757009345794392 	 lr:6.25e-06
epoch61: train: loss:1.3456680667111875 	 acc:0.805 | test: loss:1.4036472981592576 	 acc:0.7800623052959501 	 lr:6.25e-06
epoch62: train: loss:1.3371760872357716 	 acc:0.79671875 | test: loss:1.4082077481664972 	 acc:0.7744548286604361 	 lr:3.125e-06
epoch63: train: loss:1.3477923450574198 	 acc:0.8028125 | test: loss:1.4054933683894506 	 acc:0.7800623052959501 	 lr:3.125e-06
epoch64: train: loss:1.3404347332635027 	 acc:0.79671875 | test: loss:1.4118742258006538 	 acc:0.767601246105919 	 lr:3.125e-06
epoch65: train: loss:1.3312736636302511 	 acc:0.8090625 | test: loss:1.4031507523260385 	 acc:0.7794392523364486 	 lr:3.125e-06
epoch66: train: loss:1.3426833070134112 	 acc:0.7959375 | test: loss:1.4065697542974882 	 acc:0.7744548286604361 	 lr:3.125e-06
epoch67: train: loss:1.3399983866897809 	 acc:0.7940625 | test: loss:1.407731525623167 	 acc:0.77196261682243 	 lr:3.125e-06
epoch68: train: loss:1.3365402281237801 	 acc:0.806875 | test: loss:1.4080131546359196 	 acc:0.7725856697819314 	 lr:1.5625e-06
epoch69: train: loss:1.3410617015400872 	 acc:0.795 | test: loss:1.4085695957469049 	 acc:0.7713395638629283 	 lr:1.5625e-06
epoch70: train: loss:1.3306821133734936 	 acc:0.8065625 | test: loss:1.4059989672212214 	 acc:0.7757009345794392 	 lr:1.5625e-06
epoch71: train: loss:1.337492371796631 	 acc:0.80640625 | test: loss:1.4054234661417215 	 acc:0.7757009345794392 	 lr:1.5625e-06
epoch72: train: loss:1.3396604587099312 	 acc:0.80625 | test: loss:1.4042493000208776 	 acc:0.7800623052959501 	 lr:1.5625e-06
epoch73: train: loss:1.3339161190178876 	 acc:0.79953125 | test: loss:1.4084634113905956 	 acc:0.77196261682243 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/slim_resnet50_imagenet_3_2/
Training on HAM, create new exp container at ../checkpoints/HAM/slim_resnet50_imagenet_3_2/
training with  slim_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:1.7139088591219018 	 acc:0.5871875 | test: loss:1.6395352766892621 	 acc:0.6124610591900311 	 lr:0.0001
epoch1: train: loss:1.5744240563516967 	 acc:0.66984375 | test: loss:1.5328498185980728 	 acc:0.685981308411215 	 lr:0.0001
epoch2: train: loss:1.5199059038958822 	 acc:0.63609375 | test: loss:1.5604627211145894 	 acc:0.643613707165109 	 lr:0.0001
epoch3: train: loss:1.4676787305678547 	 acc:0.7078125 | test: loss:1.498217448638607 	 acc:0.7065420560747664 	 lr:0.0001
epoch4: train: loss:1.5043118979389272 	 acc:0.7384375 | test: loss:1.463702687073348 	 acc:0.7358255451713396 	 lr:0.0001
epoch5: train: loss:1.4753951514167398 	 acc:0.7575 | test: loss:1.4234480975201569 	 acc:0.7744548286604361 	 lr:0.0001
epoch6: train: loss:1.4066652129721957 	 acc:0.7803125 | test: loss:1.4102219919549341 	 acc:0.7682242990654206 	 lr:0.0001
epoch7: train: loss:1.4011966988223312 	 acc:0.7940625 | test: loss:1.4235602142654846 	 acc:0.7763239875389408 	 lr:0.0001
epoch8: train: loss:1.3809024498110911 	 acc:0.73484375 | test: loss:1.4575099624205972 	 acc:0.7090342679127726 	 lr:0.0001
epoch9: train: loss:1.3906033946982033 	 acc:0.768125 | test: loss:1.4313302302286262 	 acc:0.7489096573208722 	 lr:0.0001
epoch10: train: loss:1.3515905185196198 	 acc:0.72421875 | test: loss:1.44824621944784 	 acc:0.7227414330218068 	 lr:0.0001
epoch11: train: loss:1.3420868039038105 	 acc:0.849375 | test: loss:1.3498978946810571 	 acc:0.8230529595015577 	 lr:0.0001
epoch12: train: loss:1.3473754620756795 	 acc:0.84234375 | test: loss:1.3783846494193388 	 acc:0.815576323987539 	 lr:0.0001
epoch13: train: loss:1.3236599852943867 	 acc:0.790625 | test: loss:1.4262723002478341 	 acc:0.7439252336448599 	 lr:0.0001
epoch14: train: loss:1.3318942282183108 	 acc:0.856875 | test: loss:1.3316039353516242 	 acc:0.8342679127725857 	 lr:0.0001
epoch15: train: loss:1.3430171278656506 	 acc:0.853125 | test: loss:1.3694276750273422 	 acc:0.8105919003115265 	 lr:0.0001
epoch16: train: loss:1.3157199546939038 	 acc:0.8875 | test: loss:1.328488193791232 	 acc:0.8423676012461059 	 lr:0.0001
epoch17: train: loss:1.3000949308706578 	 acc:0.80109375 | test: loss:1.393114567694263 	 acc:0.7738317757009345 	 lr:0.0001
epoch18: train: loss:1.2998249301121256 	 acc:0.89546875 | test: loss:1.3309186222397278 	 acc:0.8492211838006231 	 lr:0.0001
epoch19: train: loss:1.287794557760508 	 acc:0.87171875 | test: loss:1.3530285963759616 	 acc:0.8137071651090343 	 lr:0.0001
epoch20: train: loss:1.2932568349845701 	 acc:0.8828125 | test: loss:1.3276796321631221 	 acc:0.8454828660436137 	 lr:0.0001
epoch21: train: loss:1.2870458050503757 	 acc:0.87734375 | test: loss:1.3397002870048689 	 acc:0.8286604361370716 	 lr:0.0001
epoch22: train: loss:1.3035749258239413 	 acc:0.90140625 | test: loss:1.3328675104450214 	 acc:0.8473520249221184 	 lr:0.0001
epoch23: train: loss:1.274890100983881 	 acc:0.8765625 | test: loss:1.3449529934523632 	 acc:0.8274143302180685 	 lr:0.0001
epoch24: train: loss:1.2582580780815464 	 acc:0.881875 | test: loss:1.3439701986461414 	 acc:0.8330218068535825 	 lr:0.0001
epoch25: train: loss:1.2619843196347762 	 acc:0.87578125 | test: loss:1.3397491057713826 	 acc:0.8311526479750779 	 lr:0.0001
epoch26: train: loss:1.2691143717829088 	 acc:0.88765625 | test: loss:1.3249177234566472 	 acc:0.8498442367601247 	 lr:0.0001
epoch27: train: loss:1.2620155638219044 	 acc:0.8790625 | test: loss:1.3391059567252424 	 acc:0.832398753894081 	 lr:0.0001
epoch28: train: loss:1.291152689663532 	 acc:0.90265625 | test: loss:1.3353134014896144 	 acc:0.8386292834890966 	 lr:0.0001
epoch29: train: loss:1.265259782491858 	 acc:0.90484375 | test: loss:1.3354056036732278 	 acc:0.8330218068535825 	 lr:0.0001
epoch30: train: loss:1.2931549262851594 	 acc:0.91 | test: loss:1.3410634434483133 	 acc:0.8473520249221184 	 lr:0.0001
epoch31: train: loss:1.2739000426150224 	 acc:0.8803125 | test: loss:1.3327400829933143 	 acc:0.8274143302180685 	 lr:0.0001
epoch32: train: loss:1.2896702071821942 	 acc:0.91125 | test: loss:1.310565302899322 	 acc:0.8467289719626169 	 lr:0.0001
epoch33: train: loss:1.2589535667782739 	 acc:0.9034375 | test: loss:1.327099536141131 	 acc:0.8473520249221184 	 lr:0.0001
epoch34: train: loss:1.2820615031400944 	 acc:0.8396875 | test: loss:1.3729752496766896 	 acc:0.7912772585669782 	 lr:0.0001
epoch35: train: loss:1.2582871900881574 	 acc:0.90171875 | test: loss:1.3283006520286156 	 acc:0.8392523364485981 	 lr:0.0001
epoch36: train: loss:1.2944999057552389 	 acc:0.81546875 | test: loss:1.4015708607676616 	 acc:0.7738317757009345 	 lr:0.0001
epoch37: train: loss:1.2444714107706991 	 acc:0.90265625 | test: loss:1.3248144371115902 	 acc:0.8398753894080997 	 lr:0.0001
epoch38: train: loss:1.2603131260749048 	 acc:0.880625 | test: loss:1.3496546693308702 	 acc:0.8168224299065421 	 lr:0.0001
epoch39: train: loss:1.226336169633709 	 acc:0.94109375 | test: loss:1.3000390734627982 	 acc:0.8716510903426792 	 lr:5e-05
epoch40: train: loss:1.235223614992712 	 acc:0.9275 | test: loss:1.301396496496468 	 acc:0.8672897196261682 	 lr:5e-05
epoch41: train: loss:1.228212907777737 	 acc:0.92390625 | test: loss:1.303497611696475 	 acc:0.8585669781931464 	 lr:5e-05
epoch42: train: loss:1.2203540548879965 	 acc:0.93453125 | test: loss:1.2947911392491183 	 acc:0.8697819314641745 	 lr:5e-05
epoch43: train: loss:1.2164670302475178 	 acc:0.94765625 | test: loss:1.2895417116884131 	 acc:0.870404984423676 	 lr:5e-05
epoch44: train: loss:1.215668217750567 	 acc:0.93765625 | test: loss:1.3014350571736368 	 acc:0.8616822429906542 	 lr:5e-05
epoch45: train: loss:1.2272291268341993 	 acc:0.949375 | test: loss:1.28500343460903 	 acc:0.8778816199376948 	 lr:5e-05
epoch46: train: loss:1.2138324984715005 	 acc:0.9340625 | test: loss:1.3033784483080713 	 acc:0.8616822429906542 	 lr:5e-05
epoch47: train: loss:1.216196395790642 	 acc:0.9459375 | test: loss:1.2991095702596172 	 acc:0.8679127725856698 	 lr:5e-05
epoch48: train: loss:1.2138361172225678 	 acc:0.9240625 | test: loss:1.3020641554924557 	 acc:0.8585669781931464 	 lr:5e-05
epoch49: train: loss:1.215175650698612 	 acc:0.9425 | test: loss:1.2953023764946008 	 acc:0.8697819314641745 	 lr:5e-05
epoch50: train: loss:1.2106502107118462 	 acc:0.939375 | test: loss:1.2927839664655312 	 acc:0.8672897196261682 	 lr:5e-05
epoch51: train: loss:1.229446675570843 	 acc:0.93296875 | test: loss:1.3052765761580423 	 acc:0.8560747663551402 	 lr:5e-05
epoch52: train: loss:1.2102220450408006 	 acc:0.95703125 | test: loss:1.286075771857645 	 acc:0.8735202492211838 	 lr:2.5e-05
epoch53: train: loss:1.2034607481528408 	 acc:0.96203125 | test: loss:1.2860683454531376 	 acc:0.8741433021806854 	 lr:2.5e-05
epoch54: train: loss:1.20331414942626 	 acc:0.9546875 | test: loss:1.2852400493770373 	 acc:0.8728971962616823 	 lr:2.5e-05
epoch55: train: loss:1.205676877024023 	 acc:0.9575 | test: loss:1.2841929810069432 	 acc:0.874766355140187 	 lr:2.5e-05
epoch56: train: loss:1.206861411696947 	 acc:0.95765625 | test: loss:1.2786530232503779 	 acc:0.8809968847352025 	 lr:2.5e-05
epoch57: train: loss:1.2041294023452747 	 acc:0.9571875 | test: loss:1.283429221794984 	 acc:0.8760124610591901 	 lr:2.5e-05
epoch58: train: loss:1.2056260072468408 	 acc:0.96203125 | test: loss:1.282223977329575 	 acc:0.8803738317757009 	 lr:2.5e-05
epoch59: train: loss:1.2015298192711383 	 acc:0.9584375 | test: loss:1.290330463406453 	 acc:0.8716510903426792 	 lr:2.5e-05
epoch60: train: loss:1.2014177332810365 	 acc:0.9478125 | test: loss:1.2920898987868121 	 acc:0.8691588785046729 	 lr:2.5e-05
epoch61: train: loss:1.2040275317630573 	 acc:0.9571875 | test: loss:1.2820290842531625 	 acc:0.8760124610591901 	 lr:2.5e-05
epoch62: train: loss:1.2012681614133551 	 acc:0.95984375 | test: loss:1.2840751483061603 	 acc:0.8766355140186916 	 lr:2.5e-05
epoch63: train: loss:1.202763987294777 	 acc:0.96484375 | test: loss:1.2826526784451209 	 acc:0.8822429906542056 	 lr:1.25e-05
epoch64: train: loss:1.1951649840784482 	 acc:0.9621875 | test: loss:1.2834839252668007 	 acc:0.8828660436137071 	 lr:1.25e-05
epoch65: train: loss:1.1974344079332553 	 acc:0.965625 | test: loss:1.27883189229579 	 acc:0.8872274143302181 	 lr:1.25e-05
epoch66: train: loss:1.1978862658224472 	 acc:0.9665625 | test: loss:1.277469121555673 	 acc:0.8847352024922118 	 lr:1.25e-05
epoch67: train: loss:1.2017147138656628 	 acc:0.96171875 | test: loss:1.2806013254361732 	 acc:0.8822429906542056 	 lr:1.25e-05
epoch68: train: loss:1.199032571332516 	 acc:0.9634375 | test: loss:1.2813806408290922 	 acc:0.8847352024922118 	 lr:1.25e-05
epoch69: train: loss:1.1958465004413972 	 acc:0.9659375 | test: loss:1.2798138588759758 	 acc:0.8828660436137071 	 lr:1.25e-05
epoch70: train: loss:1.1988403225019515 	 acc:0.96859375 | test: loss:1.2813081402644932 	 acc:0.8841121495327103 	 lr:1.25e-05
epoch71: train: loss:1.1963164663798729 	 acc:0.96578125 | test: loss:1.2795040812447807 	 acc:0.8866043613707165 	 lr:1.25e-05
epoch72: train: loss:1.1999335903194526 	 acc:0.96640625 | test: loss:1.2767691719197782 	 acc:0.8866043613707165 	 lr:1.25e-05
epoch73: train: loss:1.1964562659520455 	 acc:0.9671875 | test: loss:1.27873378787828 	 acc:0.8866043613707165 	 lr:1.25e-05
epoch74: train: loss:1.1924144926525297 	 acc:0.9696875 | test: loss:1.2807843243964365 	 acc:0.881619937694704 	 lr:1.25e-05
epoch75: train: loss:1.2027792842010332 	 acc:0.9696875 | test: loss:1.2763615890455395 	 acc:0.8847352024922118 	 lr:1.25e-05
epoch76: train: loss:1.194782327954235 	 acc:0.96640625 | test: loss:1.278251916820015 	 acc:0.8778816199376948 	 lr:1.25e-05
epoch77: train: loss:1.1966871227350764 	 acc:0.9653125 | test: loss:1.2781512695680897 	 acc:0.8841121495327103 	 lr:1.25e-05
epoch78: train: loss:1.1921969054175205 	 acc:0.97375 | test: loss:1.2770415237759505 	 acc:0.8866043613707165 	 lr:1.25e-05
epoch79: train: loss:1.1949148233191842 	 acc:0.96578125 | test: loss:1.2814105004907768 	 acc:0.8791277258566979 	 lr:1.25e-05
epoch80: train: loss:1.19383401982399 	 acc:0.9696875 | test: loss:1.2776691349867348 	 acc:0.8853582554517134 	 lr:1.25e-05
epoch81: train: loss:1.199380316872042 	 acc:0.9678125 | test: loss:1.276572627516179 	 acc:0.8884735202492212 	 lr:1.25e-05
epoch82: train: loss:1.1913899024885954 	 acc:0.9675 | test: loss:1.2776149624233306 	 acc:0.8872274143302181 	 lr:6.25e-06
epoch83: train: loss:1.1937219624217834 	 acc:0.96921875 | test: loss:1.2761255716609063 	 acc:0.8884735202492212 	 lr:6.25e-06
epoch84: train: loss:1.1957633049017187 	 acc:0.97125 | test: loss:1.2752165665879056 	 acc:0.8909657320872274 	 lr:6.25e-06
epoch85: train: loss:1.1951719214076832 	 acc:0.9703125 | test: loss:1.2771644494243872 	 acc:0.8872274143302181 	 lr:6.25e-06
epoch86: train: loss:1.1912595845683305 	 acc:0.97140625 | test: loss:1.2776218281356717 	 acc:0.8878504672897196 	 lr:6.25e-06
epoch87: train: loss:1.1897546558246122 	 acc:0.9709375 | test: loss:1.2774140064590074 	 acc:0.8866043613707165 	 lr:6.25e-06
epoch88: train: loss:1.189507408648334 	 acc:0.97296875 | test: loss:1.2764395462761045 	 acc:0.8897196261682243 	 lr:6.25e-06
epoch89: train: loss:1.1903896144179047 	 acc:0.9734375 | test: loss:1.2766602082415905 	 acc:0.8872274143302181 	 lr:6.25e-06
epoch90: train: loss:1.1955095349206857 	 acc:0.9715625 | test: loss:1.2746703987923738 	 acc:0.8909657320872274 	 lr:6.25e-06
epoch91: train: loss:1.1924303779929621 	 acc:0.97328125 | test: loss:1.2780828760420422 	 acc:0.8940809968847352 	 lr:6.25e-06
epoch92: train: loss:1.188821206252897 	 acc:0.97453125 | test: loss:1.2770583936358537 	 acc:0.8928348909657321 	 lr:6.25e-06
epoch93: train: loss:1.1914153957441391 	 acc:0.97453125 | test: loss:1.2755984049348446 	 acc:0.8940809968847352 	 lr:6.25e-06
epoch94: train: loss:1.1963685011137843 	 acc:0.9728125 | test: loss:1.2765356525082454 	 acc:0.8872274143302181 	 lr:6.25e-06
epoch95: train: loss:1.1959837842787922 	 acc:0.97046875 | test: loss:1.2733867821292342 	 acc:0.8959501557632399 	 lr:6.25e-06
epoch96: train: loss:1.1949354089860522 	 acc:0.9725 | test: loss:1.2756828488590561 	 acc:0.8897196261682243 	 lr:6.25e-06
epoch97: train: loss:1.1891277018419752 	 acc:0.96859375 | test: loss:1.2760370851677154 	 acc:0.8934579439252337 	 lr:6.25e-06
epoch98: train: loss:1.1945573650422643 	 acc:0.97265625 | test: loss:1.2766448159084143 	 acc:0.8947040498442368 	 lr:6.25e-06
epoch99: train: loss:1.1916920384236558 	 acc:0.971875 | test: loss:1.2768190760478795 	 acc:0.8922118380062305 	 lr:6.25e-06
epoch100: train: loss:1.187634837190031 	 acc:0.97453125 | test: loss:1.2731993482120312 	 acc:0.8909657320872274 	 lr:6.25e-06
epoch101: train: loss:1.1932432843222458 	 acc:0.97171875 | test: loss:1.2725607341695055 	 acc:0.8959501557632399 	 lr:6.25e-06
epoch102: train: loss:1.1899598287661313 	 acc:0.97453125 | test: loss:1.273903206278602 	 acc:0.8928348909657321 	 lr:6.25e-06
epoch103: train: loss:1.1890822494709334 	 acc:0.9740625 | test: loss:1.2752318282736426 	 acc:0.8890965732087227 	 lr:6.25e-06
epoch104: train: loss:1.1925156938015344 	 acc:0.97515625 | test: loss:1.2747133281743415 	 acc:0.8922118380062305 	 lr:6.25e-06
epoch105: train: loss:1.190218756479923 	 acc:0.97484375 | test: loss:1.2735879362557907 	 acc:0.8897196261682243 	 lr:6.25e-06
epoch106: train: loss:1.195507943527853 	 acc:0.9715625 | test: loss:1.2757380277568307 	 acc:0.8922118380062305 	 lr:6.25e-06
epoch107: train: loss:1.1909635378549472 	 acc:0.975 | test: loss:1.2748930596116919 	 acc:0.891588785046729 	 lr:6.25e-06
epoch108: train: loss:1.1930133232932645 	 acc:0.9703125 | test: loss:1.2751511799583555 	 acc:0.8903426791277259 	 lr:3.125e-06
epoch109: train: loss:1.1943619096027884 	 acc:0.975625 | test: loss:1.2731406680520079 	 acc:0.8922118380062305 	 lr:3.125e-06
epoch110: train: loss:1.1898132610097703 	 acc:0.97140625 | test: loss:1.275933231223038 	 acc:0.8884735202492212 	 lr:3.125e-06
epoch111: train: loss:1.1858080087463712 	 acc:0.97640625 | test: loss:1.274512064419803 	 acc:0.8928348909657321 	 lr:3.125e-06
epoch112: train: loss:1.188470149282177 	 acc:0.97453125 | test: loss:1.2760572440156313 	 acc:0.8884735202492212 	 lr:3.125e-06
epoch113: train: loss:1.1903336321721312 	 acc:0.975 | test: loss:1.2743353011823517 	 acc:0.8922118380062305 	 lr:3.125e-06
epoch114: train: loss:1.1885880838791716 	 acc:0.97625 | test: loss:1.2737833326107988 	 acc:0.8909657320872274 	 lr:1.5625e-06
epoch115: train: loss:1.1887300061025627 	 acc:0.9725 | test: loss:1.2758805912974467 	 acc:0.8922118380062305 	 lr:1.5625e-06
epoch116: train: loss:1.1919718781827857 	 acc:0.97359375 | test: loss:1.2751446576133323 	 acc:0.8897196261682243 	 lr:1.5625e-06
epoch117: train: loss:1.1900266791767296 	 acc:0.9721875 | test: loss:1.275152071094216 	 acc:0.8909657320872274 	 lr:1.5625e-06
epoch118: train: loss:1.187904492511496 	 acc:0.97625 | test: loss:1.2738858843889562 	 acc:0.8928348909657321 	 lr:1.5625e-06
epoch119: train: loss:1.1857516643872585 	 acc:0.97671875 | test: loss:1.2733168380654118 	 acc:0.8959501557632399 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_1_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_1_2/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_2_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_2_2/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_3_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_3_2/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_4_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_4_2/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
weights for positive classes: tensor([0.0065, 0.0393, 0.0398, 0.0853, 0.1346, 0.3110, 0.3834],
       device='cuda:0')
../checkpoints/HAM/freeze_resnet50_imagenet_5_2/
Training on HAM, create new exp container at ../checkpoints/HAM/freeze_resnet50_imagenet_5_2/
Traceback (most recent call last):
  File "main.py", line 412, in <module>
    model = freeze_resnet50(finetune_from=args.finetune_from, classes=args.classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 283, in freeze_resnet50
    net = resnet50(pretrained="imagenet", trunc=-1, classes=classes)
  File "/panfs/roc/groups/9/jusun/peng0347/TL/src/utils/models.py", line 183, in resnet50
    if args.pooling:
AttributeError: 'NoneType' object has no attribute 'pooling'
