
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/resnet50_imagenet_-1_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/resnet50_imagenet_1_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/resnet50_imagenet_2_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/resnet50_imagenet_3_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/slim_resnet50_imagenet_1_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/slim_resnet50_imagenet_2_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/slim_resnet50_imagenet_3_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/freeze_resnet50_imagenet_1_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/freeze_resnet50_imagenet_2_2/
working folder already exists

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/freeze_resnet50_imagenet_3_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy/BIMCV/freeze_resnet50_imagenet_3_2/
0.conv1.weight
0.conv1.weight freeze
0.bn1.weight
0.bn1.weight freeze
0.bn1.bias
0.bn1.bias freeze
0.layer1.0.conv1.weight
0.layer1.0.conv1.weight freeze
0.layer1.0.bn1.weight
0.layer1.0.bn1.weight freeze
0.layer1.0.bn1.bias
0.layer1.0.bn1.bias freeze
0.layer1.0.conv2.weight
0.layer1.0.conv2.weight freeze
0.layer1.0.bn2.weight
0.layer1.0.bn2.weight freeze
0.layer1.0.bn2.bias
0.layer1.0.bn2.bias freeze
0.layer1.0.conv3.weight
0.layer1.0.conv3.weight freeze
0.layer1.0.bn3.weight
0.layer1.0.bn3.weight freeze
0.layer1.0.bn3.bias
0.layer1.0.bn3.bias freeze
0.layer1.0.downsample.0.weight
0.layer1.0.downsample.0.weight freeze
0.layer1.0.downsample.1.weight
0.layer1.0.downsample.1.weight freeze
0.layer1.0.downsample.1.bias
0.layer1.0.downsample.1.bias freeze
0.layer1.1.conv1.weight
0.layer1.1.conv1.weight freeze
0.layer1.1.bn1.weight
0.layer1.1.bn1.weight freeze
0.layer1.1.bn1.bias
0.layer1.1.bn1.bias freeze
0.layer1.1.conv2.weight
0.layer1.1.conv2.weight freeze
0.layer1.1.bn2.weight
0.layer1.1.bn2.weight freeze
0.layer1.1.bn2.bias
0.layer1.1.bn2.bias freeze
0.layer1.1.conv3.weight
0.layer1.1.conv3.weight freeze
0.layer1.1.bn3.weight
0.layer1.1.bn3.weight freeze
0.layer1.1.bn3.bias
0.layer1.1.bn3.bias freeze
0.layer1.2.conv1.weight
0.layer1.2.conv1.weight freeze
0.layer1.2.bn1.weight
0.layer1.2.bn1.weight freeze
0.layer1.2.bn1.bias
0.layer1.2.bn1.bias freeze
0.layer1.2.conv2.weight
0.layer1.2.conv2.weight freeze
0.layer1.2.bn2.weight
0.layer1.2.bn2.weight freeze
0.layer1.2.bn2.bias
0.layer1.2.bn2.bias freeze
0.layer1.2.conv3.weight
0.layer1.2.conv3.weight freeze
0.layer1.2.bn3.weight
0.layer1.2.bn3.weight freeze
0.layer1.2.bn3.bias
0.layer1.2.bn3.bias freeze
0.layer2.0.conv1.weight
0.layer2.0.conv1.weight freeze
0.layer2.0.bn1.weight
0.layer2.0.bn1.weight freeze
0.layer2.0.bn1.bias
0.layer2.0.bn1.bias freeze
0.layer2.0.conv2.weight
0.layer2.0.conv2.weight freeze
0.layer2.0.bn2.weight
0.layer2.0.bn2.weight freeze
0.layer2.0.bn2.bias
0.layer2.0.bn2.bias freeze
0.layer2.0.conv3.weight
0.layer2.0.conv3.weight freeze
0.layer2.0.bn3.weight
0.layer2.0.bn3.weight freeze
0.layer2.0.bn3.bias
0.layer2.0.bn3.bias freeze
0.layer2.0.downsample.0.weight
0.layer2.0.downsample.0.weight freeze
0.layer2.0.downsample.1.weight
0.layer2.0.downsample.1.weight freeze
0.layer2.0.downsample.1.bias
0.layer2.0.downsample.1.bias freeze
0.layer2.1.conv1.weight
0.layer2.1.conv1.weight freeze
0.layer2.1.bn1.weight
0.layer2.1.bn1.weight freeze
0.layer2.1.bn1.bias
0.layer2.1.bn1.bias freeze
0.layer2.1.conv2.weight
0.layer2.1.conv2.weight freeze
0.layer2.1.bn2.weight
0.layer2.1.bn2.weight freeze
0.layer2.1.bn2.bias
0.layer2.1.bn2.bias freeze
0.layer2.1.conv3.weight
0.layer2.1.conv3.weight freeze
0.layer2.1.bn3.weight
0.layer2.1.bn3.weight freeze
0.layer2.1.bn3.bias
0.layer2.1.bn3.bias freeze
0.layer2.2.conv1.weight
0.layer2.2.conv1.weight freeze
0.layer2.2.bn1.weight
0.layer2.2.bn1.weight freeze
0.layer2.2.bn1.bias
0.layer2.2.bn1.bias freeze
0.layer2.2.conv2.weight
0.layer2.2.conv2.weight freeze
0.layer2.2.bn2.weight
0.layer2.2.bn2.weight freeze
0.layer2.2.bn2.bias
0.layer2.2.bn2.bias freeze
0.layer2.2.conv3.weight
0.layer2.2.conv3.weight freeze
0.layer2.2.bn3.weight
0.layer2.2.bn3.weight freeze
0.layer2.2.bn3.bias
0.layer2.2.bn3.bias freeze
0.layer2.3.conv1.weight
0.layer2.3.conv1.weight freeze
0.layer2.3.bn1.weight
0.layer2.3.bn1.weight freeze
0.layer2.3.bn1.bias
0.layer2.3.bn1.bias freeze
0.layer2.3.conv2.weight
0.layer2.3.conv2.weight freeze
0.layer2.3.bn2.weight
0.layer2.3.bn2.weight freeze
0.layer2.3.bn2.bias
0.layer2.3.bn2.bias freeze
0.layer2.3.conv3.weight
0.layer2.3.conv3.weight freeze
0.layer2.3.bn3.weight
0.layer2.3.bn3.weight freeze
0.layer2.3.bn3.bias
0.layer2.3.bn3.bias freeze
0.layer3.0.conv1.weight
0.layer3.0.bn1.weight
0.layer3.0.bn1.bias
0.layer3.0.conv2.weight
0.layer3.0.bn2.weight
0.layer3.0.bn2.bias
0.layer3.0.conv3.weight
0.layer3.0.bn3.weight
0.layer3.0.bn3.bias
0.layer3.0.downsample.0.weight
0.layer3.0.downsample.1.weight
0.layer3.0.downsample.1.bias
0.layer3.1.conv1.weight
0.layer3.1.bn1.weight
0.layer3.1.bn1.bias
0.layer3.1.conv2.weight
0.layer3.1.bn2.weight
0.layer3.1.bn2.bias
0.layer3.1.conv3.weight
0.layer3.1.bn3.weight
0.layer3.1.bn3.bias
0.layer3.2.conv1.weight
0.layer3.2.bn1.weight
0.layer3.2.bn1.bias
0.layer3.2.conv2.weight
0.layer3.2.bn2.weight
0.layer3.2.bn2.bias
0.layer3.2.conv3.weight
0.layer3.2.bn3.weight
0.layer3.2.bn3.bias
0.layer3.3.conv1.weight
0.layer3.3.bn1.weight
0.layer3.3.bn1.bias
0.layer3.3.conv2.weight
0.layer3.3.bn2.weight
0.layer3.3.bn2.bias
0.layer3.3.conv3.weight
0.layer3.3.bn3.weight
0.layer3.3.bn3.bias
0.layer3.4.conv1.weight
0.layer3.4.bn1.weight
0.layer3.4.bn1.bias
0.layer3.4.conv2.weight
0.layer3.4.bn2.weight
0.layer3.4.bn2.bias
0.layer3.4.conv3.weight
0.layer3.4.bn3.weight
0.layer3.4.bn3.bias
0.layer3.5.conv1.weight
0.layer3.5.bn1.weight
0.layer3.5.bn1.bias
0.layer3.5.conv2.weight
0.layer3.5.bn2.weight
0.layer3.5.bn2.bias
0.layer3.5.conv3.weight
0.layer3.5.bn3.weight
0.layer3.5.bn3.bias
0.layer4.0.conv1.weight
0.layer4.0.bn1.weight
0.layer4.0.bn1.bias
0.layer4.0.conv2.weight
0.layer4.0.bn2.weight
0.layer4.0.bn2.bias
0.layer4.0.conv3.weight
0.layer4.0.bn3.weight
0.layer4.0.bn3.bias
0.layer4.0.downsample.0.weight
0.layer4.0.downsample.1.weight
0.layer4.0.downsample.1.bias
0.layer4.1.conv1.weight
0.layer4.1.bn1.weight
0.layer4.1.bn1.bias
0.layer4.1.conv2.weight
0.layer4.1.bn2.weight
0.layer4.1.bn2.bias
0.layer4.1.conv3.weight
0.layer4.1.bn3.weight
0.layer4.1.bn3.bias
0.layer4.2.conv1.weight
0.layer4.2.bn1.weight
0.layer4.2.bn1.bias
0.layer4.2.conv2.weight
0.layer4.2.bn2.weight
0.layer4.2.bn2.bias
0.layer4.2.conv3.weight
0.layer4.2.bn3.weight
0.layer4.2.bn3.bias
1.1.weight
1.1.bias
training with  freeze_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6309933798175212 	 acc:0.552396694214876 | test: loss:0.6353123150124456 	 acc:0.552317880794702 	 lr:0.0001
epoch1: train: loss:0.6150495044259 	 acc:0.5765289256198347 | test: loss:0.6259728290387336 	 acc:0.5748344370860927 	 lr:0.0001
epoch2: train: loss:0.5966184877167063 	 acc:0.6072727272727273 | test: loss:0.6017726287147067 	 acc:0.6066225165562914 	 lr:0.0001
epoch3: train: loss:0.5606902435397314 	 acc:0.6872727272727273 | test: loss:0.5818290505977656 	 acc:0.6649006622516557 	 lr:0.0001
epoch4: train: loss:0.5426633094361991 	 acc:0.7738842975206611 | test: loss:0.5663512543337235 	 acc:0.7099337748344371 	 lr:0.0001
epoch5: train: loss:0.522659387647613 	 acc:0.7738842975206611 | test: loss:0.5629130811091291 	 acc:0.7218543046357616 	 lr:0.0001
epoch6: train: loss:0.5240829726487152 	 acc:0.7986776859504132 | test: loss:0.6066229831303982 	 acc:0.6900662251655629 	 lr:0.0001
epoch7: train: loss:0.49831562986058636 	 acc:0.807603305785124 | test: loss:0.5690825161554955 	 acc:0.7231788079470198 	 lr:0.0001
epoch8: train: loss:0.4921366240268896 	 acc:0.8310743801652892 | test: loss:0.5595246445264248 	 acc:0.7284768211920529 	 lr:0.0001
epoch9: train: loss:0.496537384405609 	 acc:0.8208264462809918 | test: loss:0.5773868906576902 	 acc:0.7112582781456953 	 lr:0.0001
epoch10: train: loss:0.4757064033145747 	 acc:0.8112396694214876 | test: loss:0.5460098685807739 	 acc:0.7205298013245033 	 lr:0.0001
epoch11: train: loss:0.4787764390638052 	 acc:0.8370247933884297 | test: loss:0.557071470424829 	 acc:0.743046357615894 	 lr:0.0001
epoch12: train: loss:0.4716225657876858 	 acc:0.8168595041322314 | test: loss:0.543081058255884 	 acc:0.7165562913907285 	 lr:0.0001
epoch13: train: loss:0.4659506217014691 	 acc:0.8528925619834711 | test: loss:0.5478749559415097 	 acc:0.7284768211920529 	 lr:0.0001
epoch14: train: loss:0.4574788288735161 	 acc:0.8581818181818182 | test: loss:0.5828289969077963 	 acc:0.7245033112582782 	 lr:0.0001
epoch15: train: loss:0.4560271638385521 	 acc:0.8585123966942149 | test: loss:0.587556623307285 	 acc:0.6966887417218544 	 lr:0.0001
epoch16: train: loss:0.4424662150233245 	 acc:0.8809917355371901 | test: loss:0.5530960579581609 	 acc:0.7483443708609272 	 lr:0.0001
epoch17: train: loss:0.44740521198461863 	 acc:0.8677685950413223 | test: loss:0.5709470624165819 	 acc:0.7245033112582782 	 lr:0.0001
epoch18: train: loss:0.47541092835182003 	 acc:0.7831404958677686 | test: loss:0.5548201363607748 	 acc:0.6993377483443709 	 lr:0.0001
epoch19: train: loss:0.4296368566623404 	 acc:0.8889256198347107 | test: loss:0.5426438401076967 	 acc:0.7390728476821192 	 lr:5e-05
epoch20: train: loss:0.42065745610836125 	 acc:0.8925619834710744 | test: loss:0.5382240558302166 	 acc:0.7337748344370861 	 lr:5e-05
epoch21: train: loss:0.41511736113177844 	 acc:0.8955371900826447 | test: loss:0.5349912981323849 	 acc:0.7417218543046358 	 lr:5e-05
epoch22: train: loss:0.4112173967223522 	 acc:0.9084297520661156 | test: loss:0.5415008191241334 	 acc:0.7536423841059603 	 lr:5e-05
epoch23: train: loss:0.4149281777429187 	 acc:0.9077685950413223 | test: loss:0.5696845300939699 	 acc:0.7364238410596027 	 lr:5e-05
epoch24: train: loss:0.3951961556939054 	 acc:0.9246280991735537 | test: loss:0.547341930234669 	 acc:0.7470198675496689 	 lr:5e-05
epoch25: train: loss:0.4063759696483612 	 acc:0.9137190082644628 | test: loss:0.5571341195643343 	 acc:0.7443708609271523 	 lr:5e-05
epoch26: train: loss:0.4081239073335632 	 acc:0.9008264462809917 | test: loss:0.5397826458444659 	 acc:0.7629139072847683 	 lr:5e-05
epoch27: train: loss:0.403673853135306 	 acc:0.911404958677686 | test: loss:0.5474593843845342 	 acc:0.7417218543046358 	 lr:5e-05
epoch28: train: loss:0.3923659777050176 	 acc:0.9279338842975207 | test: loss:0.546689562371235 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch29: train: loss:0.39426530060689313 	 acc:0.9193388429752066 | test: loss:0.5310681628075656 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch30: train: loss:0.39076193537593873 	 acc:0.923305785123967 | test: loss:0.5322047296738782 	 acc:0.7562913907284768 	 lr:2.5e-05
epoch31: train: loss:0.3880298843167045 	 acc:0.927603305785124 | test: loss:0.5416273664954482 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch32: train: loss:0.3858720278444369 	 acc:0.9305785123966942 | test: loss:0.5408366716460676 	 acc:0.7562913907284768 	 lr:2.5e-05
epoch33: train: loss:0.38801610377209245 	 acc:0.9355371900826446 | test: loss:0.5551815445849437 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch34: train: loss:0.38255126384664173 	 acc:0.9325619834710743 | test: loss:0.5357670742944376 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch35: train: loss:0.3798295951283668 	 acc:0.9371900826446281 | test: loss:0.5464762111373295 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch36: train: loss:0.3855026185611063 	 acc:0.9322314049586777 | test: loss:0.5453980649543914 	 acc:0.7417218543046358 	 lr:1.25e-05
epoch37: train: loss:0.38435999247653424 	 acc:0.936198347107438 | test: loss:0.5482335665368087 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch38: train: loss:0.3824339029808675 	 acc:0.9315702479338843 | test: loss:0.5326213867459076 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch39: train: loss:0.37624357836305605 	 acc:0.9408264462809918 | test: loss:0.5346038946252785 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch40: train: loss:0.3775808071301988 	 acc:0.9404958677685951 | test: loss:0.5358421145685461 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch41: train: loss:0.37462485813897506 	 acc:0.9441322314049587 | test: loss:0.5445774023896022 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch42: train: loss:0.3747515258119126 	 acc:0.9414876033057851 | test: loss:0.5378517082195409 	 acc:0.752317880794702 	 lr:6.25e-06
epoch43: train: loss:0.37559763370466626 	 acc:0.9464462809917356 | test: loss:0.5353800043365023 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch44: train: loss:0.3801011284225243 	 acc:0.9368595041322314 | test: loss:0.5413326140271117 	 acc:0.7496688741721854 	 lr:6.25e-06
epoch45: train: loss:0.3788038245212933 	 acc:0.9414876033057851 | test: loss:0.5415280122630644 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch46: train: loss:0.37989848815705163 	 acc:0.9355371900826446 | test: loss:0.5427277005271406 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch47: train: loss:0.37711129417104167 	 acc:0.9434710743801653 | test: loss:0.5382912758170374 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch48: train: loss:0.3751574314231715 	 acc:0.9421487603305785 | test: loss:0.5374477399895523 	 acc:0.7549668874172185 	 lr:3.125e-06
epoch49: train: loss:0.3797971361727754 	 acc:0.9355371900826446 | test: loss:0.5341628737007545 	 acc:0.7629139072847683 	 lr:3.125e-06
epoch50: train: loss:0.3712443206231456 	 acc:0.9451239669421487 | test: loss:0.5333759108916024 	 acc:0.7615894039735099 	 lr:3.125e-06
epoch51: train: loss:0.374498533335599 	 acc:0.9418181818181818 | test: loss:0.5366706469201095 	 acc:0.7562913907284768 	 lr:3.125e-06
epoch52: train: loss:0.37572363809120557 	 acc:0.9408264462809918 | test: loss:0.5379210537632569 	 acc:0.7589403973509934 	 lr:3.125e-06
epoch53: train: loss:0.37809160571453 	 acc:0.9348760330578513 | test: loss:0.5370922890719988 	 acc:0.7602649006622516 	 lr:3.125e-06
epoch54: train: loss:0.3771953283853767 	 acc:0.9414876033057851 | test: loss:0.53840770871434 	 acc:0.7536423841059603 	 lr:1.5625e-06
epoch55: train: loss:0.37339587926864626 	 acc:0.9451239669421487 | test: loss:0.538369623872618 	 acc:0.7536423841059603 	 lr:1.5625e-06
epoch56: train: loss:0.3760097336769104 	 acc:0.9408264462809918 | test: loss:0.5403455593728071 	 acc:0.752317880794702 	 lr:1.5625e-06
epoch57: train: loss:0.37410059607718604 	 acc:0.9391735537190082 | test: loss:0.5357143327889853 	 acc:0.7509933774834437 	 lr:1.5625e-06
epoch58: train: loss:0.37520716936135096 	 acc:0.9365289256198347 | test: loss:0.5328195652424894 	 acc:0.7576158940397351 	 lr:1.5625e-06
epoch59: train: loss:0.3782271795627499 	 acc:0.9375206611570248 | test: loss:0.5333127377838488 	 acc:0.7562913907284768 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/freeze_resnet50_imagenet_4_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy/BIMCV/freeze_resnet50_imagenet_4_2/
0.conv1.weight
0.conv1.weight freeze
0.bn1.weight
0.bn1.weight freeze
0.bn1.bias
0.bn1.bias freeze
0.layer1.0.conv1.weight
0.layer1.0.conv1.weight freeze
0.layer1.0.bn1.weight
0.layer1.0.bn1.weight freeze
0.layer1.0.bn1.bias
0.layer1.0.bn1.bias freeze
0.layer1.0.conv2.weight
0.layer1.0.conv2.weight freeze
0.layer1.0.bn2.weight
0.layer1.0.bn2.weight freeze
0.layer1.0.bn2.bias
0.layer1.0.bn2.bias freeze
0.layer1.0.conv3.weight
0.layer1.0.conv3.weight freeze
0.layer1.0.bn3.weight
0.layer1.0.bn3.weight freeze
0.layer1.0.bn3.bias
0.layer1.0.bn3.bias freeze
0.layer1.0.downsample.0.weight
0.layer1.0.downsample.0.weight freeze
0.layer1.0.downsample.1.weight
0.layer1.0.downsample.1.weight freeze
0.layer1.0.downsample.1.bias
0.layer1.0.downsample.1.bias freeze
0.layer1.1.conv1.weight
0.layer1.1.conv1.weight freeze
0.layer1.1.bn1.weight
0.layer1.1.bn1.weight freeze
0.layer1.1.bn1.bias
0.layer1.1.bn1.bias freeze
0.layer1.1.conv2.weight
0.layer1.1.conv2.weight freeze
0.layer1.1.bn2.weight
0.layer1.1.bn2.weight freeze
0.layer1.1.bn2.bias
0.layer1.1.bn2.bias freeze
0.layer1.1.conv3.weight
0.layer1.1.conv3.weight freeze
0.layer1.1.bn3.weight
0.layer1.1.bn3.weight freeze
0.layer1.1.bn3.bias
0.layer1.1.bn3.bias freeze
0.layer1.2.conv1.weight
0.layer1.2.conv1.weight freeze
0.layer1.2.bn1.weight
0.layer1.2.bn1.weight freeze
0.layer1.2.bn1.bias
0.layer1.2.bn1.bias freeze
0.layer1.2.conv2.weight
0.layer1.2.conv2.weight freeze
0.layer1.2.bn2.weight
0.layer1.2.bn2.weight freeze
0.layer1.2.bn2.bias
0.layer1.2.bn2.bias freeze
0.layer1.2.conv3.weight
0.layer1.2.conv3.weight freeze
0.layer1.2.bn3.weight
0.layer1.2.bn3.weight freeze
0.layer1.2.bn3.bias
0.layer1.2.bn3.bias freeze
0.layer2.0.conv1.weight
0.layer2.0.conv1.weight freeze
0.layer2.0.bn1.weight
0.layer2.0.bn1.weight freeze
0.layer2.0.bn1.bias
0.layer2.0.bn1.bias freeze
0.layer2.0.conv2.weight
0.layer2.0.conv2.weight freeze
0.layer2.0.bn2.weight
0.layer2.0.bn2.weight freeze
0.layer2.0.bn2.bias
0.layer2.0.bn2.bias freeze
0.layer2.0.conv3.weight
0.layer2.0.conv3.weight freeze
0.layer2.0.bn3.weight
0.layer2.0.bn3.weight freeze
0.layer2.0.bn3.bias
0.layer2.0.bn3.bias freeze
0.layer2.0.downsample.0.weight
0.layer2.0.downsample.0.weight freeze
0.layer2.0.downsample.1.weight
0.layer2.0.downsample.1.weight freeze
0.layer2.0.downsample.1.bias
0.layer2.0.downsample.1.bias freeze
0.layer2.1.conv1.weight
0.layer2.1.conv1.weight freeze
0.layer2.1.bn1.weight
0.layer2.1.bn1.weight freeze
0.layer2.1.bn1.bias
0.layer2.1.bn1.bias freeze
0.layer2.1.conv2.weight
0.layer2.1.conv2.weight freeze
0.layer2.1.bn2.weight
0.layer2.1.bn2.weight freeze
0.layer2.1.bn2.bias
0.layer2.1.bn2.bias freeze
0.layer2.1.conv3.weight
0.layer2.1.conv3.weight freeze
0.layer2.1.bn3.weight
0.layer2.1.bn3.weight freeze
0.layer2.1.bn3.bias
0.layer2.1.bn3.bias freeze
0.layer2.2.conv1.weight
0.layer2.2.conv1.weight freeze
0.layer2.2.bn1.weight
0.layer2.2.bn1.weight freeze
0.layer2.2.bn1.bias
0.layer2.2.bn1.bias freeze
0.layer2.2.conv2.weight
0.layer2.2.conv2.weight freeze
0.layer2.2.bn2.weight
0.layer2.2.bn2.weight freeze
0.layer2.2.bn2.bias
0.layer2.2.bn2.bias freeze
0.layer2.2.conv3.weight
0.layer2.2.conv3.weight freeze
0.layer2.2.bn3.weight
0.layer2.2.bn3.weight freeze
0.layer2.2.bn3.bias
0.layer2.2.bn3.bias freeze
0.layer2.3.conv1.weight
0.layer2.3.conv1.weight freeze
0.layer2.3.bn1.weight
0.layer2.3.bn1.weight freeze
0.layer2.3.bn1.bias
0.layer2.3.bn1.bias freeze
0.layer2.3.conv2.weight
0.layer2.3.conv2.weight freeze
0.layer2.3.bn2.weight
0.layer2.3.bn2.weight freeze
0.layer2.3.bn2.bias
0.layer2.3.bn2.bias freeze
0.layer2.3.conv3.weight
0.layer2.3.conv3.weight freeze
0.layer2.3.bn3.weight
0.layer2.3.bn3.weight freeze
0.layer2.3.bn3.bias
0.layer2.3.bn3.bias freeze
0.layer3.0.conv1.weight
0.layer3.0.conv1.weight freeze
0.layer3.0.bn1.weight
0.layer3.0.bn1.weight freeze
0.layer3.0.bn1.bias
0.layer3.0.bn1.bias freeze
0.layer3.0.conv2.weight
0.layer3.0.conv2.weight freeze
0.layer3.0.bn2.weight
0.layer3.0.bn2.weight freeze
0.layer3.0.bn2.bias
0.layer3.0.bn2.bias freeze
0.layer3.0.conv3.weight
0.layer3.0.conv3.weight freeze
0.layer3.0.bn3.weight
0.layer3.0.bn3.weight freeze
0.layer3.0.bn3.bias
0.layer3.0.bn3.bias freeze
0.layer3.0.downsample.0.weight
0.layer3.0.downsample.0.weight freeze
0.layer3.0.downsample.1.weight
0.layer3.0.downsample.1.weight freeze
0.layer3.0.downsample.1.bias
0.layer3.0.downsample.1.bias freeze
0.layer3.1.conv1.weight
0.layer3.1.conv1.weight freeze
0.layer3.1.bn1.weight
0.layer3.1.bn1.weight freeze
0.layer3.1.bn1.bias
0.layer3.1.bn1.bias freeze
0.layer3.1.conv2.weight
0.layer3.1.conv2.weight freeze
0.layer3.1.bn2.weight
0.layer3.1.bn2.weight freeze
0.layer3.1.bn2.bias
0.layer3.1.bn2.bias freeze
0.layer3.1.conv3.weight
0.layer3.1.conv3.weight freeze
0.layer3.1.bn3.weight
0.layer3.1.bn3.weight freeze
0.layer3.1.bn3.bias
0.layer3.1.bn3.bias freeze
0.layer3.2.conv1.weight
0.layer3.2.conv1.weight freeze
0.layer3.2.bn1.weight
0.layer3.2.bn1.weight freeze
0.layer3.2.bn1.bias
0.layer3.2.bn1.bias freeze
0.layer3.2.conv2.weight
0.layer3.2.conv2.weight freeze
0.layer3.2.bn2.weight
0.layer3.2.bn2.weight freeze
0.layer3.2.bn2.bias
0.layer3.2.bn2.bias freeze
0.layer3.2.conv3.weight
0.layer3.2.conv3.weight freeze
0.layer3.2.bn3.weight
0.layer3.2.bn3.weight freeze
0.layer3.2.bn3.bias
0.layer3.2.bn3.bias freeze
0.layer3.3.conv1.weight
0.layer3.3.conv1.weight freeze
0.layer3.3.bn1.weight
0.layer3.3.bn1.weight freeze
0.layer3.3.bn1.bias
0.layer3.3.bn1.bias freeze
0.layer3.3.conv2.weight
0.layer3.3.conv2.weight freeze
0.layer3.3.bn2.weight
0.layer3.3.bn2.weight freeze
0.layer3.3.bn2.bias
0.layer3.3.bn2.bias freeze
0.layer3.3.conv3.weight
0.layer3.3.conv3.weight freeze
0.layer3.3.bn3.weight
0.layer3.3.bn3.weight freeze
0.layer3.3.bn3.bias
0.layer3.3.bn3.bias freeze
0.layer3.4.conv1.weight
0.layer3.4.conv1.weight freeze
0.layer3.4.bn1.weight
0.layer3.4.bn1.weight freeze
0.layer3.4.bn1.bias
0.layer3.4.bn1.bias freeze
0.layer3.4.conv2.weight
0.layer3.4.conv2.weight freeze
0.layer3.4.bn2.weight
0.layer3.4.bn2.weight freeze
0.layer3.4.bn2.bias
0.layer3.4.bn2.bias freeze
0.layer3.4.conv3.weight
0.layer3.4.conv3.weight freeze
0.layer3.4.bn3.weight
0.layer3.4.bn3.weight freeze
0.layer3.4.bn3.bias
0.layer3.4.bn3.bias freeze
0.layer3.5.conv1.weight
0.layer3.5.conv1.weight freeze
0.layer3.5.bn1.weight
0.layer3.5.bn1.weight freeze
0.layer3.5.bn1.bias
0.layer3.5.bn1.bias freeze
0.layer3.5.conv2.weight
0.layer3.5.conv2.weight freeze
0.layer3.5.bn2.weight
0.layer3.5.bn2.weight freeze
0.layer3.5.bn2.bias
0.layer3.5.bn2.bias freeze
0.layer3.5.conv3.weight
0.layer3.5.conv3.weight freeze
0.layer3.5.bn3.weight
0.layer3.5.bn3.weight freeze
0.layer3.5.bn3.bias
0.layer3.5.bn3.bias freeze
0.layer4.0.conv1.weight
0.layer4.0.bn1.weight
0.layer4.0.bn1.bias
0.layer4.0.conv2.weight
0.layer4.0.bn2.weight
0.layer4.0.bn2.bias
0.layer4.0.conv3.weight
0.layer4.0.bn3.weight
0.layer4.0.bn3.bias
0.layer4.0.downsample.0.weight
0.layer4.0.downsample.1.weight
0.layer4.0.downsample.1.bias
0.layer4.1.conv1.weight
0.layer4.1.bn1.weight
0.layer4.1.bn1.bias
0.layer4.1.conv2.weight
0.layer4.1.bn2.weight
0.layer4.1.bn2.bias
0.layer4.1.conv3.weight
0.layer4.1.bn3.weight
0.layer4.1.bn3.bias
0.layer4.2.conv1.weight
0.layer4.2.bn1.weight
0.layer4.2.bn1.bias
0.layer4.2.conv2.weight
0.layer4.2.bn2.weight
0.layer4.2.bn2.bias
0.layer4.2.conv3.weight
0.layer4.2.bn3.weight
0.layer4.2.bn3.bias
1.1.weight
1.1.bias
training with  freeze_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6358848723892339 	 acc:0.5408264462809917 | test: loss:0.6358031664463069 	 acc:0.5377483443708609 	 lr:0.0001
epoch1: train: loss:0.6278634448287901 	 acc:0.5414876033057852 | test: loss:0.633586650336815 	 acc:0.543046357615894 	 lr:0.0001
epoch2: train: loss:0.6004898977673744 	 acc:0.616198347107438 | test: loss:0.6039831108604835 	 acc:0.6052980132450331 	 lr:0.0001
epoch3: train: loss:0.5762744560911636 	 acc:0.6624793388429752 | test: loss:0.5965353228398507 	 acc:0.6450331125827815 	 lr:0.0001
epoch4: train: loss:0.5494708361113367 	 acc:0.7196694214876033 | test: loss:0.576132611486296 	 acc:0.6781456953642384 	 lr:0.0001
epoch5: train: loss:0.541685057454858 	 acc:0.7315702479338843 | test: loss:0.5730134025314786 	 acc:0.6966887417218544 	 lr:0.0001
epoch6: train: loss:0.5390163004890947 	 acc:0.7811570247933884 | test: loss:0.5943837093201694 	 acc:0.7019867549668874 	 lr:0.0001
epoch7: train: loss:0.5424586534697162 	 acc:0.7649586776859504 | test: loss:0.6099097034789079 	 acc:0.6913907284768211 	 lr:0.0001
epoch8: train: loss:0.542535022309989 	 acc:0.775206611570248 | test: loss:0.5964573252280027 	 acc:0.713907284768212 	 lr:0.0001
epoch9: train: loss:0.5386161124016627 	 acc:0.7824793388429752 | test: loss:0.604262128807851 	 acc:0.695364238410596 	 lr:0.0001
epoch10: train: loss:0.5017808120703894 	 acc:0.7914049586776859 | test: loss:0.5551903474409848 	 acc:0.7059602649006622 	 lr:0.0001
epoch11: train: loss:0.5082871469387338 	 acc:0.7847933884297521 | test: loss:0.5636234885019972 	 acc:0.7006622516556291 	 lr:0.0001
epoch12: train: loss:0.4977935437032999 	 acc:0.7808264462809917 | test: loss:0.5548351039949632 	 acc:0.6940397350993377 	 lr:0.0001
epoch13: train: loss:0.4917586090840584 	 acc:0.7996694214876033 | test: loss:0.5578044942672679 	 acc:0.6900662251655629 	 lr:0.0001
epoch14: train: loss:0.4812700698493926 	 acc:0.8241322314049587 | test: loss:0.5662619110764258 	 acc:0.6993377483443709 	 lr:0.0001
epoch15: train: loss:0.49175293999269976 	 acc:0.8231404958677686 | test: loss:0.5998880732138425 	 acc:0.7006622516556291 	 lr:0.0001
epoch16: train: loss:0.4745956110363164 	 acc:0.8234710743801653 | test: loss:0.5674493558359462 	 acc:0.7033112582781457 	 lr:0.0001
epoch17: train: loss:0.4690741956233978 	 acc:0.844297520661157 | test: loss:0.572522783989938 	 acc:0.704635761589404 	 lr:0.0001
epoch18: train: loss:0.5036356543706468 	 acc:0.7474380165289256 | test: loss:0.5714043737247291 	 acc:0.680794701986755 	 lr:0.0001
epoch19: train: loss:0.48674051590202266 	 acc:0.8419834710743802 | test: loss:0.5833577995268714 	 acc:0.7125827814569536 	 lr:5e-05
epoch20: train: loss:0.46009844890310747 	 acc:0.8449586776859505 | test: loss:0.557212852484343 	 acc:0.7231788079470198 	 lr:5e-05
epoch21: train: loss:0.4544790918866465 	 acc:0.8588429752066116 | test: loss:0.5637900930366768 	 acc:0.713907284768212 	 lr:5e-05
epoch22: train: loss:0.45108211272019 	 acc:0.871404958677686 | test: loss:0.5730673632084928 	 acc:0.7231788079470198 	 lr:5e-05
epoch23: train: loss:0.45142841824815294 	 acc:0.8730578512396694 | test: loss:0.5710305618924021 	 acc:0.7271523178807947 	 lr:5e-05
epoch24: train: loss:0.44066231951240664 	 acc:0.8743801652892562 | test: loss:0.5580922890972617 	 acc:0.7337748344370861 	 lr:5e-05
epoch25: train: loss:0.43890714980353995 	 acc:0.8776859504132232 | test: loss:0.5543403351543755 	 acc:0.7218543046357616 	 lr:2.5e-05
epoch26: train: loss:0.4403894779209263 	 acc:0.872396694214876 | test: loss:0.5559544566451319 	 acc:0.7324503311258278 	 lr:2.5e-05
epoch27: train: loss:0.44961081436842926 	 acc:0.8757024793388429 | test: loss:0.578160935048236 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch28: train: loss:0.4409513123271879 	 acc:0.8859504132231405 | test: loss:0.5761659796664257 	 acc:0.7284768211920529 	 lr:2.5e-05
epoch29: train: loss:0.43060432873481563 	 acc:0.8869421487603306 | test: loss:0.5486698606156355 	 acc:0.713907284768212 	 lr:2.5e-05
epoch30: train: loss:0.4308586054991099 	 acc:0.8846280991735537 | test: loss:0.5560165888426319 	 acc:0.7112582781456953 	 lr:2.5e-05
epoch31: train: loss:0.4302882881598039 	 acc:0.8737190082644628 | test: loss:0.5572177162233567 	 acc:0.7099337748344371 	 lr:2.5e-05
epoch32: train: loss:0.4269247622233777 	 acc:0.8872727272727273 | test: loss:0.5590346292944144 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch33: train: loss:0.4212649224710859 	 acc:0.8998347107438016 | test: loss:0.5625437732563903 	 acc:0.7298013245033113 	 lr:2.5e-05
epoch34: train: loss:0.4233695565274924 	 acc:0.9011570247933884 | test: loss:0.5651123605027104 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch35: train: loss:0.42625226297654395 	 acc:0.8915702479338843 | test: loss:0.5671575892050534 	 acc:0.7218543046357616 	 lr:2.5e-05
epoch36: train: loss:0.4304876803563646 	 acc:0.8879338842975206 | test: loss:0.5627249881921225 	 acc:0.7205298013245033 	 lr:1.25e-05
epoch37: train: loss:0.4261293937352078 	 acc:0.8912396694214876 | test: loss:0.5559305817875642 	 acc:0.7218543046357616 	 lr:1.25e-05
epoch38: train: loss:0.42594387378574405 	 acc:0.8733884297520661 | test: loss:0.5475335094312959 	 acc:0.7218543046357616 	 lr:1.25e-05
epoch39: train: loss:0.4181103306565403 	 acc:0.8932231404958678 | test: loss:0.5521405588712124 	 acc:0.7271523178807947 	 lr:1.25e-05
epoch40: train: loss:0.41773747778135883 	 acc:0.8971900826446281 | test: loss:0.5504925252586012 	 acc:0.7324503311258278 	 lr:1.25e-05
epoch41: train: loss:0.41561951483576753 	 acc:0.9087603305785124 | test: loss:0.5543436415148097 	 acc:0.7324503311258278 	 lr:1.25e-05
epoch42: train: loss:0.4181810029774658 	 acc:0.900495867768595 | test: loss:0.5551285521084109 	 acc:0.7192052980132451 	 lr:1.25e-05
epoch43: train: loss:0.4177635543011437 	 acc:0.895206611570248 | test: loss:0.554529413008532 	 acc:0.7205298013245033 	 lr:1.25e-05
epoch44: train: loss:0.424513683575244 	 acc:0.8998347107438016 | test: loss:0.5618859689756734 	 acc:0.7231788079470198 	 lr:1.25e-05
epoch45: train: loss:0.41868071299939114 	 acc:0.8945454545454545 | test: loss:0.5576581776536853 	 acc:0.7271523178807947 	 lr:6.25e-06
epoch46: train: loss:0.4179006542844221 	 acc:0.895206611570248 | test: loss:0.5560739641947462 	 acc:0.7258278145695364 	 lr:6.25e-06
epoch47: train: loss:0.4190507604366492 	 acc:0.9001652892561983 | test: loss:0.5577508797708726 	 acc:0.7258278145695364 	 lr:6.25e-06
epoch48: train: loss:0.41696140216401784 	 acc:0.9024793388429752 | test: loss:0.5601474016707465 	 acc:0.7165562913907285 	 lr:6.25e-06
epoch49: train: loss:0.42000889775181605 	 acc:0.8895867768595042 | test: loss:0.5609279248888129 	 acc:0.7231788079470198 	 lr:6.25e-06
epoch50: train: loss:0.41126396536827087 	 acc:0.9047933884297521 | test: loss:0.5569719657203219 	 acc:0.7192052980132451 	 lr:6.25e-06
epoch51: train: loss:0.4097210736018567 	 acc:0.908099173553719 | test: loss:0.5586474777846936 	 acc:0.7205298013245033 	 lr:3.125e-06
epoch52: train: loss:0.41406167126884147 	 acc:0.9054545454545454 | test: loss:0.5594454219009702 	 acc:0.7192052980132451 	 lr:3.125e-06
epoch53: train: loss:0.41760269221195506 	 acc:0.8985123966942149 | test: loss:0.5616710300477136 	 acc:0.7192052980132451 	 lr:3.125e-06
epoch54: train: loss:0.4167480232991463 	 acc:0.9031404958677686 | test: loss:0.5631794997398427 	 acc:0.7165562913907285 	 lr:3.125e-06
epoch55: train: loss:0.4101967070122396 	 acc:0.9064462809917355 | test: loss:0.5615577745911301 	 acc:0.7205298013245033 	 lr:3.125e-06
epoch56: train: loss:0.41768530602297504 	 acc:0.8985123966942149 | test: loss:0.560406693164876 	 acc:0.7192052980132451 	 lr:3.125e-06
epoch57: train: loss:0.409882606159557 	 acc:0.9054545454545454 | test: loss:0.5585201889473871 	 acc:0.7205298013245033 	 lr:1.5625e-06
epoch58: train: loss:0.41167604554783216 	 acc:0.9041322314049587 | test: loss:0.5587648481722699 	 acc:0.7178807947019867 	 lr:1.5625e-06
epoch59: train: loss:0.41522979336336624 	 acc:0.8961983471074381 | test: loss:0.5591578914629702 	 acc:0.7245033112582782 	 lr:1.5625e-06
epoch60: train: loss:0.4209405689003054 	 acc:0.8932231404958678 | test: loss:0.5590448589514423 	 acc:0.7245033112582782 	 lr:1.5625e-06
epoch61: train: loss:0.4164373162659732 	 acc:0.9011570247933884 | test: loss:0.5594252891887892 	 acc:0.7218543046357616 	 lr:1.5625e-06
epoch62: train: loss:0.41539022979657514 	 acc:0.8961983471074381 | test: loss:0.556495850844099 	 acc:0.7165562913907285 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy/BIMCV/freeze_resnet50_imagenet_5_2/
Training on BIMCV, create new exp container at ../checkpoints_noisy/BIMCV/freeze_resnet50_imagenet_5_2/
0.conv1.weight
0.conv1.weight freeze
0.bn1.weight
0.bn1.weight freeze
0.bn1.bias
0.bn1.bias freeze
0.layer1.0.conv1.weight
0.layer1.0.conv1.weight freeze
0.layer1.0.bn1.weight
0.layer1.0.bn1.weight freeze
0.layer1.0.bn1.bias
0.layer1.0.bn1.bias freeze
0.layer1.0.conv2.weight
0.layer1.0.conv2.weight freeze
0.layer1.0.bn2.weight
0.layer1.0.bn2.weight freeze
0.layer1.0.bn2.bias
0.layer1.0.bn2.bias freeze
0.layer1.0.conv3.weight
0.layer1.0.conv3.weight freeze
0.layer1.0.bn3.weight
0.layer1.0.bn3.weight freeze
0.layer1.0.bn3.bias
0.layer1.0.bn3.bias freeze
0.layer1.0.downsample.0.weight
0.layer1.0.downsample.0.weight freeze
0.layer1.0.downsample.1.weight
0.layer1.0.downsample.1.weight freeze
0.layer1.0.downsample.1.bias
0.layer1.0.downsample.1.bias freeze
0.layer1.1.conv1.weight
0.layer1.1.conv1.weight freeze
0.layer1.1.bn1.weight
0.layer1.1.bn1.weight freeze
0.layer1.1.bn1.bias
0.layer1.1.bn1.bias freeze
0.layer1.1.conv2.weight
0.layer1.1.conv2.weight freeze
0.layer1.1.bn2.weight
0.layer1.1.bn2.weight freeze
0.layer1.1.bn2.bias
0.layer1.1.bn2.bias freeze
0.layer1.1.conv3.weight
0.layer1.1.conv3.weight freeze
0.layer1.1.bn3.weight
0.layer1.1.bn3.weight freeze
0.layer1.1.bn3.bias
0.layer1.1.bn3.bias freeze
0.layer1.2.conv1.weight
0.layer1.2.conv1.weight freeze
0.layer1.2.bn1.weight
0.layer1.2.bn1.weight freeze
0.layer1.2.bn1.bias
0.layer1.2.bn1.bias freeze
0.layer1.2.conv2.weight
0.layer1.2.conv2.weight freeze
0.layer1.2.bn2.weight
0.layer1.2.bn2.weight freeze
0.layer1.2.bn2.bias
0.layer1.2.bn2.bias freeze
0.layer1.2.conv3.weight
0.layer1.2.conv3.weight freeze
0.layer1.2.bn3.weight
0.layer1.2.bn3.weight freeze
0.layer1.2.bn3.bias
0.layer1.2.bn3.bias freeze
0.layer2.0.conv1.weight
0.layer2.0.conv1.weight freeze
0.layer2.0.bn1.weight
0.layer2.0.bn1.weight freeze
0.layer2.0.bn1.bias
0.layer2.0.bn1.bias freeze
0.layer2.0.conv2.weight
0.layer2.0.conv2.weight freeze
0.layer2.0.bn2.weight
0.layer2.0.bn2.weight freeze
0.layer2.0.bn2.bias
0.layer2.0.bn2.bias freeze
0.layer2.0.conv3.weight
0.layer2.0.conv3.weight freeze
0.layer2.0.bn3.weight
0.layer2.0.bn3.weight freeze
0.layer2.0.bn3.bias
0.layer2.0.bn3.bias freeze
0.layer2.0.downsample.0.weight
0.layer2.0.downsample.0.weight freeze
0.layer2.0.downsample.1.weight
0.layer2.0.downsample.1.weight freeze
0.layer2.0.downsample.1.bias
0.layer2.0.downsample.1.bias freeze
0.layer2.1.conv1.weight
0.layer2.1.conv1.weight freeze
0.layer2.1.bn1.weight
0.layer2.1.bn1.weight freeze
0.layer2.1.bn1.bias
0.layer2.1.bn1.bias freeze
0.layer2.1.conv2.weight
0.layer2.1.conv2.weight freeze
0.layer2.1.bn2.weight
0.layer2.1.bn2.weight freeze
0.layer2.1.bn2.bias
0.layer2.1.bn2.bias freeze
0.layer2.1.conv3.weight
0.layer2.1.conv3.weight freeze
0.layer2.1.bn3.weight
0.layer2.1.bn3.weight freeze
0.layer2.1.bn3.bias
0.layer2.1.bn3.bias freeze
0.layer2.2.conv1.weight
0.layer2.2.conv1.weight freeze
0.layer2.2.bn1.weight
0.layer2.2.bn1.weight freeze
0.layer2.2.bn1.bias
0.layer2.2.bn1.bias freeze
0.layer2.2.conv2.weight
0.layer2.2.conv2.weight freeze
0.layer2.2.bn2.weight
0.layer2.2.bn2.weight freeze
0.layer2.2.bn2.bias
0.layer2.2.bn2.bias freeze
0.layer2.2.conv3.weight
0.layer2.2.conv3.weight freeze
0.layer2.2.bn3.weight
0.layer2.2.bn3.weight freeze
0.layer2.2.bn3.bias
0.layer2.2.bn3.bias freeze
0.layer2.3.conv1.weight
0.layer2.3.conv1.weight freeze
0.layer2.3.bn1.weight
0.layer2.3.bn1.weight freeze
0.layer2.3.bn1.bias
0.layer2.3.bn1.bias freeze
0.layer2.3.conv2.weight
0.layer2.3.conv2.weight freeze
0.layer2.3.bn2.weight
0.layer2.3.bn2.weight freeze
0.layer2.3.bn2.bias
0.layer2.3.bn2.bias freeze
0.layer2.3.conv3.weight
0.layer2.3.conv3.weight freeze
0.layer2.3.bn3.weight
0.layer2.3.bn3.weight freeze
0.layer2.3.bn3.bias
0.layer2.3.bn3.bias freeze
0.layer3.0.conv1.weight
0.layer3.0.conv1.weight freeze
0.layer3.0.bn1.weight
0.layer3.0.bn1.weight freeze
0.layer3.0.bn1.bias
0.layer3.0.bn1.bias freeze
0.layer3.0.conv2.weight
0.layer3.0.conv2.weight freeze
0.layer3.0.bn2.weight
0.layer3.0.bn2.weight freeze
0.layer3.0.bn2.bias
0.layer3.0.bn2.bias freeze
0.layer3.0.conv3.weight
0.layer3.0.conv3.weight freeze
0.layer3.0.bn3.weight
0.layer3.0.bn3.weight freeze
0.layer3.0.bn3.bias
0.layer3.0.bn3.bias freeze
0.layer3.0.downsample.0.weight
0.layer3.0.downsample.0.weight freeze
0.layer3.0.downsample.1.weight
0.layer3.0.downsample.1.weight freeze
0.layer3.0.downsample.1.bias
0.layer3.0.downsample.1.bias freeze
0.layer3.1.conv1.weight
0.layer3.1.conv1.weight freeze
0.layer3.1.bn1.weight
0.layer3.1.bn1.weight freeze
0.layer3.1.bn1.bias
0.layer3.1.bn1.bias freeze
0.layer3.1.conv2.weight
0.layer3.1.conv2.weight freeze
0.layer3.1.bn2.weight
0.layer3.1.bn2.weight freeze
0.layer3.1.bn2.bias
0.layer3.1.bn2.bias freeze
0.layer3.1.conv3.weight
0.layer3.1.conv3.weight freeze
0.layer3.1.bn3.weight
0.layer3.1.bn3.weight freeze
0.layer3.1.bn3.bias
0.layer3.1.bn3.bias freeze
0.layer3.2.conv1.weight
0.layer3.2.conv1.weight freeze
0.layer3.2.bn1.weight
0.layer3.2.bn1.weight freeze
0.layer3.2.bn1.bias
0.layer3.2.bn1.bias freeze
0.layer3.2.conv2.weight
0.layer3.2.conv2.weight freeze
0.layer3.2.bn2.weight
0.layer3.2.bn2.weight freeze
0.layer3.2.bn2.bias
0.layer3.2.bn2.bias freeze
0.layer3.2.conv3.weight
0.layer3.2.conv3.weight freeze
0.layer3.2.bn3.weight
0.layer3.2.bn3.weight freeze
0.layer3.2.bn3.bias
0.layer3.2.bn3.bias freeze
0.layer3.3.conv1.weight
0.layer3.3.conv1.weight freeze
0.layer3.3.bn1.weight
0.layer3.3.bn1.weight freeze
0.layer3.3.bn1.bias
0.layer3.3.bn1.bias freeze
0.layer3.3.conv2.weight
0.layer3.3.conv2.weight freeze
0.layer3.3.bn2.weight
0.layer3.3.bn2.weight freeze
0.layer3.3.bn2.bias
0.layer3.3.bn2.bias freeze
0.layer3.3.conv3.weight
0.layer3.3.conv3.weight freeze
0.layer3.3.bn3.weight
0.layer3.3.bn3.weight freeze
0.layer3.3.bn3.bias
0.layer3.3.bn3.bias freeze
0.layer3.4.conv1.weight
0.layer3.4.conv1.weight freeze
0.layer3.4.bn1.weight
0.layer3.4.bn1.weight freeze
0.layer3.4.bn1.bias
0.layer3.4.bn1.bias freeze
0.layer3.4.conv2.weight
0.layer3.4.conv2.weight freeze
0.layer3.4.bn2.weight
0.layer3.4.bn2.weight freeze
0.layer3.4.bn2.bias
0.layer3.4.bn2.bias freeze
0.layer3.4.conv3.weight
0.layer3.4.conv3.weight freeze
0.layer3.4.bn3.weight
0.layer3.4.bn3.weight freeze
0.layer3.4.bn3.bias
0.layer3.4.bn3.bias freeze
0.layer3.5.conv1.weight
0.layer3.5.conv1.weight freeze
0.layer3.5.bn1.weight
0.layer3.5.bn1.weight freeze
0.layer3.5.bn1.bias
0.layer3.5.bn1.bias freeze
0.layer3.5.conv2.weight
0.layer3.5.conv2.weight freeze
0.layer3.5.bn2.weight
0.layer3.5.bn2.weight freeze
0.layer3.5.bn2.bias
0.layer3.5.bn2.bias freeze
0.layer3.5.conv3.weight
0.layer3.5.conv3.weight freeze
0.layer3.5.bn3.weight
0.layer3.5.bn3.weight freeze
0.layer3.5.bn3.bias
0.layer3.5.bn3.bias freeze
0.layer4.0.conv1.weight
0.layer4.0.conv1.weight freeze
0.layer4.0.bn1.weight
0.layer4.0.bn1.weight freeze
0.layer4.0.bn1.bias
0.layer4.0.bn1.bias freeze
0.layer4.0.conv2.weight
0.layer4.0.conv2.weight freeze
0.layer4.0.bn2.weight
0.layer4.0.bn2.weight freeze
0.layer4.0.bn2.bias
0.layer4.0.bn2.bias freeze
0.layer4.0.conv3.weight
0.layer4.0.conv3.weight freeze
0.layer4.0.bn3.weight
0.layer4.0.bn3.weight freeze
0.layer4.0.bn3.bias
0.layer4.0.bn3.bias freeze
0.layer4.0.downsample.0.weight
0.layer4.0.downsample.0.weight freeze
0.layer4.0.downsample.1.weight
0.layer4.0.downsample.1.weight freeze
0.layer4.0.downsample.1.bias
0.layer4.0.downsample.1.bias freeze
0.layer4.1.conv1.weight
0.layer4.1.conv1.weight freeze
0.layer4.1.bn1.weight
0.layer4.1.bn1.weight freeze
0.layer4.1.bn1.bias
0.layer4.1.bn1.bias freeze
0.layer4.1.conv2.weight
0.layer4.1.conv2.weight freeze
0.layer4.1.bn2.weight
0.layer4.1.bn2.weight freeze
0.layer4.1.bn2.bias
0.layer4.1.bn2.bias freeze
0.layer4.1.conv3.weight
0.layer4.1.conv3.weight freeze
0.layer4.1.bn3.weight
0.layer4.1.bn3.weight freeze
0.layer4.1.bn3.bias
0.layer4.1.bn3.bias freeze
0.layer4.2.conv1.weight
0.layer4.2.conv1.weight freeze
0.layer4.2.bn1.weight
0.layer4.2.bn1.weight freeze
0.layer4.2.bn1.bias
0.layer4.2.bn1.bias freeze
0.layer4.2.conv2.weight
0.layer4.2.conv2.weight freeze
0.layer4.2.bn2.weight
0.layer4.2.bn2.weight freeze
0.layer4.2.bn2.bias
0.layer4.2.bn2.bias freeze
0.layer4.2.conv3.weight
0.layer4.2.conv3.weight freeze
0.layer4.2.bn3.weight
0.layer4.2.bn3.weight freeze
0.layer4.2.bn3.bias
0.layer4.2.bn3.bias freeze
1.1.weight
1.1.bias
training with  freeze_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6725058715402588 	 acc:0.5219834710743801 | test: loss:0.6727363700108813 	 acc:0.5152317880794702 	 lr:0.0001
epoch1: train: loss:0.667593490978903 	 acc:0.5209917355371901 | test: loss:0.6708024977058764 	 acc:0.5152317880794702 	 lr:0.0001
epoch2: train: loss:0.6654537015907035 	 acc:0.524297520661157 | test: loss:0.669517184882764 	 acc:0.5165562913907285 	 lr:0.0001
epoch3: train: loss:0.6668676407672157 	 acc:0.5269421487603306 | test: loss:0.6735242951784702 	 acc:0.5112582781456954 	 lr:0.0001
epoch4: train: loss:0.6604055070088914 	 acc:0.5365289256198347 | test: loss:0.6666050552532373 	 acc:0.5311258278145695 	 lr:0.0001
epoch5: train: loss:0.666775823388218 	 acc:0.5490909090909091 | test: loss:0.6694573044776917 	 acc:0.5377483443708609 	 lr:0.0001
epoch6: train: loss:0.6609449729643577 	 acc:0.5461157024793388 | test: loss:0.6638283866920218 	 acc:0.5443708609271524 	 lr:0.0001
epoch7: train: loss:0.6630300537810838 	 acc:0.5530578512396694 | test: loss:0.6667649694625905 	 acc:0.552317880794702 	 lr:0.0001
epoch8: train: loss:0.6569208785719123 	 acc:0.5547107438016529 | test: loss:0.6614146204973688 	 acc:0.5470198675496689 	 lr:0.0001
epoch9: train: loss:0.6586409230665727 	 acc:0.5765289256198347 | test: loss:0.6639964633430077 	 acc:0.5589403973509933 	 lr:0.0001
epoch10: train: loss:0.6588782048816523 	 acc:0.5679338842975207 | test: loss:0.6590861690754922 	 acc:0.5562913907284768 	 lr:0.0001
epoch11: train: loss:0.6548163496364247 	 acc:0.5563636363636364 | test: loss:0.6558234422412139 	 acc:0.5576158940397351 	 lr:0.0001
epoch12: train: loss:0.6510803782841391 	 acc:0.5775206611570248 | test: loss:0.654077853588079 	 acc:0.5642384105960265 	 lr:0.0001
epoch13: train: loss:0.6532927649868421 	 acc:0.5742148760330579 | test: loss:0.6529382874634092 	 acc:0.5682119205298013 	 lr:0.0001
epoch14: train: loss:0.6478664990101964 	 acc:0.5824793388429752 | test: loss:0.6509695320729388 	 acc:0.5708609271523178 	 lr:0.0001
epoch15: train: loss:0.6481296443151049 	 acc:0.5927272727272728 | test: loss:0.6491765958583907 	 acc:0.5788079470198676 	 lr:0.0001
epoch16: train: loss:0.6491066055258443 	 acc:0.5907438016528925 | test: loss:0.650323495959604 	 acc:0.5894039735099338 	 lr:0.0001
epoch17: train: loss:0.6424847197729694 	 acc:0.5960330578512397 | test: loss:0.6457422190944091 	 acc:0.5655629139072847 	 lr:0.0001
epoch18: train: loss:0.6514258297612845 	 acc:0.5864462809917356 | test: loss:0.6510288153263117 	 acc:0.6052980132450331 	 lr:0.0001
epoch19: train: loss:0.6390733741531688 	 acc:0.6036363636363636 | test: loss:0.6448349308493911 	 acc:0.5827814569536424 	 lr:0.0001
epoch20: train: loss:0.6448016900464523 	 acc:0.6099173553719008 | test: loss:0.6479035506974783 	 acc:0.6105960264900663 	 lr:0.0001
epoch21: train: loss:0.6436495702325805 	 acc:0.611900826446281 | test: loss:0.6460025353147494 	 acc:0.609271523178808 	 lr:0.0001
epoch22: train: loss:0.6432356185164333 	 acc:0.6115702479338843 | test: loss:0.6446823644322275 	 acc:0.614569536423841 	 lr:0.0001
epoch23: train: loss:0.6391269266900937 	 acc:0.6125619834710744 | test: loss:0.6431808194577299 	 acc:0.6105960264900663 	 lr:0.0001
epoch24: train: loss:0.6412360748377713 	 acc:0.6079338842975207 | test: loss:0.6414929557320298 	 acc:0.6079470198675496 	 lr:0.0001
epoch25: train: loss:0.6424349817953819 	 acc:0.6105785123966943 | test: loss:0.6428459619844197 	 acc:0.6158940397350994 	 lr:0.0001
epoch26: train: loss:0.6394062960837498 	 acc:0.628099173553719 | test: loss:0.6413970544638223 	 acc:0.6172185430463576 	 lr:0.0001
epoch27: train: loss:0.6387905540939205 	 acc:0.6241322314049587 | test: loss:0.6420894675696922 	 acc:0.6198675496688741 	 lr:0.0001
epoch28: train: loss:0.639085877000793 	 acc:0.5980165289256199 | test: loss:0.63697164934992 	 acc:0.5920529801324503 	 lr:0.0001
epoch29: train: loss:0.6359963078538249 	 acc:0.6214876033057851 | test: loss:0.6394681661334258 	 acc:0.623841059602649 	 lr:0.0001
epoch30: train: loss:0.6332954652644386 	 acc:0.6284297520661157 | test: loss:0.6381734154082292 	 acc:0.6185430463576159 	 lr:0.0001
epoch31: train: loss:0.6375305839412468 	 acc:0.6122314049586777 | test: loss:0.6366033025135268 	 acc:0.609271523178808 	 lr:0.0001
epoch32: train: loss:0.6367968292078696 	 acc:0.6148760330578512 | test: loss:0.6354706022123627 	 acc:0.609271523178808 	 lr:0.0001
epoch33: train: loss:0.6357666326751393 	 acc:0.6257851239669422 | test: loss:0.6365856953014601 	 acc:0.6264900662251656 	 lr:0.0001
epoch34: train: loss:0.6302546591798136 	 acc:0.6257851239669422 | test: loss:0.6336360462453982 	 acc:0.6052980132450331 	 lr:0.0001
epoch35: train: loss:0.6345321064744114 	 acc:0.6247933884297521 | test: loss:0.6357723528186217 	 acc:0.6304635761589404 	 lr:0.0001
epoch36: train: loss:0.6363301439324687 	 acc:0.6297520661157024 | test: loss:0.6356225262414541 	 acc:0.6384105960264901 	 lr:0.0001
epoch37: train: loss:0.633140537679688 	 acc:0.6152066115702479 | test: loss:0.6325390680736264 	 acc:0.6119205298013245 	 lr:0.0001
epoch38: train: loss:0.6341734423125086 	 acc:0.6307438016528926 | test: loss:0.6341843721882396 	 acc:0.6344370860927152 	 lr:0.0001
epoch39: train: loss:0.6330550813477887 	 acc:0.6300826446280992 | test: loss:0.6332929157263396 	 acc:0.6304635761589404 	 lr:0.0001
epoch40: train: loss:0.6332015745501873 	 acc:0.6261157024793389 | test: loss:0.6323880868242276 	 acc:0.6251655629139072 	 lr:0.0001
epoch41: train: loss:0.6373831846694316 	 acc:0.6271074380165289 | test: loss:0.6350767599825827 	 acc:0.6370860927152318 	 lr:0.0001
epoch42: train: loss:0.6287909726071949 	 acc:0.6142148760330578 | test: loss:0.630329550812576 	 acc:0.6052980132450331 	 lr:0.0001
epoch43: train: loss:0.6343983060663396 	 acc:0.6390082644628099 | test: loss:0.6375522776944748 	 acc:0.6423841059602649 	 lr:0.0001
epoch44: train: loss:0.6314481042239292 	 acc:0.6330578512396694 | test: loss:0.6330145616405057 	 acc:0.6384105960264901 	 lr:0.0001
epoch45: train: loss:0.6304218700700555 	 acc:0.6287603305785124 | test: loss:0.6317177041476926 	 acc:0.6264900662251656 	 lr:0.0001
epoch46: train: loss:0.6311745723416983 	 acc:0.6409917355371901 | test: loss:0.6369556341739679 	 acc:0.6490066225165563 	 lr:0.0001
epoch47: train: loss:0.6292847589421864 	 acc:0.6310743801652893 | test: loss:0.6318163218087708 	 acc:0.633112582781457 	 lr:0.0001
epoch48: train: loss:0.628723251051154 	 acc:0.6284297520661157 | test: loss:0.6310297572060136 	 acc:0.6304635761589404 	 lr:0.0001
epoch49: train: loss:0.632948615846555 	 acc:0.6294214876033057 | test: loss:0.6326219820028899 	 acc:0.6410596026490066 	 lr:5e-05
epoch50: train: loss:0.6300732680785754 	 acc:0.6380165289256199 | test: loss:0.632121330933855 	 acc:0.6397350993377483 	 lr:5e-05
epoch51: train: loss:0.631161467634942 	 acc:0.6327272727272727 | test: loss:0.6317629037313903 	 acc:0.6357615894039735 	 lr:5e-05
epoch52: train: loss:0.6316327067052038 	 acc:0.6310743801652893 | test: loss:0.632348573760481 	 acc:0.6357615894039735 	 lr:5e-05
epoch53: train: loss:0.6306210261337029 	 acc:0.6290909090909091 | test: loss:0.6316317857495997 	 acc:0.633112582781457 	 lr:5e-05
epoch54: train: loss:0.631620287028226 	 acc:0.6244628099173554 | test: loss:0.6322270987839099 	 acc:0.633112582781457 	 lr:5e-05
epoch55: train: loss:0.6306302279283192 	 acc:0.6419834710743801 | test: loss:0.6330371911162572 	 acc:0.6423841059602649 	 lr:2.5e-05
epoch56: train: loss:0.6296819907180534 	 acc:0.6290909090909091 | test: loss:0.6319699775304226 	 acc:0.6423841059602649 	 lr:2.5e-05
epoch57: train: loss:0.631483300756817 	 acc:0.6350413223140496 | test: loss:0.6320736428759746 	 acc:0.6423841059602649 	 lr:2.5e-05
epoch58: train: loss:0.6290160008698455 	 acc:0.6320661157024794 | test: loss:0.6311865183691315 	 acc:0.6370860927152318 	 lr:2.5e-05
epoch59: train: loss:0.6309774106396131 	 acc:0.6247933884297521 | test: loss:0.6311282364737909 	 acc:0.6384105960264901 	 lr:2.5e-05
epoch60: train: loss:0.6318129755839829 	 acc:0.6343801652892562 | test: loss:0.6322730795437137 	 acc:0.6384105960264901 	 lr:2.5e-05
epoch61: train: loss:0.6265664985357238 	 acc:0.6429752066115703 | test: loss:0.631861465419365 	 acc:0.6397350993377483 	 lr:1.25e-05
epoch62: train: loss:0.6295003078200601 	 acc:0.6290909090909091 | test: loss:0.6307322569240797 	 acc:0.6397350993377483 	 lr:1.25e-05
epoch63: train: loss:0.6300060237340691 	 acc:0.6390082644628099 | test: loss:0.631822670216592 	 acc:0.6397350993377483 	 lr:1.25e-05
epoch64: train: loss:0.6299381460237109 	 acc:0.6416528925619834 | test: loss:0.6315651880984274 	 acc:0.6437086092715232 	 lr:1.25e-05
epoch65: train: loss:0.6306004490734132 	 acc:0.6376859504132232 | test: loss:0.6315082293472543 	 acc:0.6450331125827815 	 lr:1.25e-05
epoch66: train: loss:0.6285499091187785 	 acc:0.6376859504132232 | test: loss:0.6312203504391853 	 acc:0.6357615894039735 	 lr:1.25e-05
epoch67: train: loss:0.628427554615273 	 acc:0.628099173553719 | test: loss:0.6313346880950675 	 acc:0.6357615894039735 	 lr:6.25e-06
epoch68: train: loss:0.6291115815186303 	 acc:0.6380165289256199 | test: loss:0.6315525403875388 	 acc:0.6437086092715232 	 lr:6.25e-06
epoch69: train: loss:0.6261170664109474 	 acc:0.6423140495867768 | test: loss:0.6312648881350132 	 acc:0.6410596026490066 	 lr:6.25e-06
epoch70: train: loss:0.6286034783055959 	 acc:0.6330578512396694 | test: loss:0.6316495619862285 	 acc:0.6437086092715232 	 lr:6.25e-06
epoch71: train: loss:0.6267345850723834 	 acc:0.6406611570247934 | test: loss:0.6308656978291391 	 acc:0.6463576158940397 	 lr:6.25e-06
epoch72: train: loss:0.6253944991048702 	 acc:0.6548760330578512 | test: loss:0.6311072194813103 	 acc:0.6384105960264901 	 lr:6.25e-06
epoch73: train: loss:0.6248622860002123 	 acc:0.6575206611570248 | test: loss:0.6310864553546274 	 acc:0.6410596026490066 	 lr:3.125e-06
epoch74: train: loss:0.6275407443558875 	 acc:0.6489256198347108 | test: loss:0.6318439084962504 	 acc:0.6410596026490066 	 lr:3.125e-06
epoch75: train: loss:0.6277554780589647 	 acc:0.6423140495867768 | test: loss:0.6308454789073262 	 acc:0.6423841059602649 	 lr:3.125e-06
epoch76: train: loss:0.630568252673819 	 acc:0.632396694214876 | test: loss:0.6319605070234134 	 acc:0.6450331125827815 	 lr:3.125e-06
epoch77: train: loss:0.6271745224235472 	 acc:0.6284297520661157 | test: loss:0.6310749335004794 	 acc:0.6384105960264901 	 lr:3.125e-06
epoch78: train: loss:0.6331977730940196 	 acc:0.6300826446280992 | test: loss:0.6306200428514291 	 acc:0.6397350993377483 	 lr:3.125e-06
epoch79: train: loss:0.6307320080315771 	 acc:0.6386776859504132 | test: loss:0.6320303549829698 	 acc:0.6450331125827815 	 lr:1.5625e-06
epoch80: train: loss:0.6325722788779203 	 acc:0.6284297520661157 | test: loss:0.6318300613504372 	 acc:0.6423841059602649 	 lr:1.5625e-06
epoch81: train: loss:0.6294783756555604 	 acc:0.6363636363636364 | test: loss:0.6315324032543511 	 acc:0.6384105960264901 	 lr:1.5625e-06
epoch82: train: loss:0.6286529491755588 	 acc:0.6320661157024794 | test: loss:0.631162981402795 	 acc:0.6410596026490066 	 lr:1.5625e-06
epoch83: train: loss:0.6235898864367777 	 acc:0.6459504132231405 | test: loss:0.6309120029803144 	 acc:0.6397350993377483 	 lr:1.5625e-06
epoch84: train: loss:0.6256711485760271 	 acc:0.644297520661157 | test: loss:0.630945197556982 	 acc:0.6384105960264901 	 lr:1.5625e-06
