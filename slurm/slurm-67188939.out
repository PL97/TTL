
Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/resnet50_imagenet_-1_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/resnet50_imagenet_-1_3/
training with  resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6727165812500252 	 acc:0.6191735537190083 | test: loss:0.723990348790655 	 acc:0.5602649006622517 	 lr:0.0001
epoch1: train: loss:0.6384608376715795 	 acc:0.5431404958677686 | test: loss:0.6560607848577942 	 acc:0.5364238410596026 	 lr:0.0001
epoch2: train: loss:0.5871937010899063 	 acc:0.71900826446281 | test: loss:0.5816655609781379 	 acc:0.7059602649006622 	 lr:0.0001
epoch3: train: loss:0.5878561758798015 	 acc:0.6211570247933884 | test: loss:0.5973399579130261 	 acc:0.6079470198675496 	 lr:0.0001
epoch4: train: loss:0.5571401586217328 	 acc:0.6819834710743802 | test: loss:0.5608054785538983 	 acc:0.6913907284768211 	 lr:0.0001
epoch5: train: loss:0.5465912957033835 	 acc:0.7768595041322314 | test: loss:0.5777526426788987 	 acc:0.7245033112582782 	 lr:0.0001
epoch6: train: loss:0.5338894539431107 	 acc:0.7170247933884297 | test: loss:0.5611972665944637 	 acc:0.6847682119205298 	 lr:0.0001
epoch7: train: loss:0.5079322729425982 	 acc:0.7983471074380165 | test: loss:0.5335542720674679 	 acc:0.7496688741721854 	 lr:0.0001
epoch8: train: loss:0.4991901332287749 	 acc:0.8019834710743802 | test: loss:0.5399278739430257 	 acc:0.7284768211920529 	 lr:0.0001
epoch9: train: loss:0.4972767876199454 	 acc:0.8343801652892562 | test: loss:0.5603177163774604 	 acc:0.7390728476821192 	 lr:0.0001
epoch10: train: loss:0.49047850441341556 	 acc:0.7814876033057852 | test: loss:0.546699746476104 	 acc:0.7258278145695364 	 lr:0.0001
epoch11: train: loss:0.49949540862367175 	 acc:0.8185123966942148 | test: loss:0.577359944068833 	 acc:0.7112582781456953 	 lr:0.0001
epoch12: train: loss:0.46890652040804714 	 acc:0.8327272727272728 | test: loss:0.5358109497866094 	 acc:0.7350993377483444 	 lr:0.0001
epoch13: train: loss:0.4959894905208556 	 acc:0.7609917355371901 | test: loss:0.559545861648408 	 acc:0.6874172185430464 	 lr:0.0001
epoch14: train: loss:0.47255327947868786 	 acc:0.848595041322314 | test: loss:0.5617891416644418 	 acc:0.7496688741721854 	 lr:5e-05
epoch15: train: loss:0.45166206872167663 	 acc:0.8677685950413223 | test: loss:0.559218172284941 	 acc:0.743046357615894 	 lr:5e-05
epoch16: train: loss:0.4625958916668064 	 acc:0.8608264462809917 | test: loss:0.5902133599811832 	 acc:0.7152317880794702 	 lr:5e-05
epoch17: train: loss:0.44136193811400865 	 acc:0.8869421487603306 | test: loss:0.5586178916179582 	 acc:0.7403973509933774 	 lr:5e-05
epoch18: train: loss:0.43433290792890816 	 acc:0.8780165289256199 | test: loss:0.531377642123115 	 acc:0.7483443708609272 	 lr:5e-05
epoch19: train: loss:0.42334743681032794 	 acc:0.88 | test: loss:0.5332754412234224 	 acc:0.7536423841059603 	 lr:5e-05
epoch20: train: loss:0.42219768606926783 	 acc:0.8968595041322314 | test: loss:0.5467069930588173 	 acc:0.7284768211920529 	 lr:5e-05
epoch21: train: loss:0.4166498651287772 	 acc:0.8981818181818182 | test: loss:0.5504499283847429 	 acc:0.7324503311258278 	 lr:5e-05
epoch22: train: loss:0.4104034228856899 	 acc:0.9014876033057851 | test: loss:0.5395212134778105 	 acc:0.7536423841059603 	 lr:5e-05
epoch23: train: loss:0.41551314680044316 	 acc:0.8869421487603306 | test: loss:0.5260009188525724 	 acc:0.7509933774834437 	 lr:5e-05
epoch24: train: loss:0.41507423678705513 	 acc:0.8975206611570248 | test: loss:0.5346317772833716 	 acc:0.743046357615894 	 lr:5e-05
epoch25: train: loss:0.41470521767277363 	 acc:0.9031404958677686 | test: loss:0.5525482343522129 	 acc:0.743046357615894 	 lr:5e-05
epoch26: train: loss:0.4063492900182393 	 acc:0.9143801652892563 | test: loss:0.5425105302539093 	 acc:0.7576158940397351 	 lr:5e-05
epoch27: train: loss:0.4233043791735468 	 acc:0.9018181818181819 | test: loss:0.5934486420738776 	 acc:0.7231788079470198 	 lr:5e-05
epoch28: train: loss:0.40963290937675917 	 acc:0.9130578512396694 | test: loss:0.5618228073941162 	 acc:0.7443708609271523 	 lr:5e-05
epoch29: train: loss:0.39867569482030946 	 acc:0.9223140495867769 | test: loss:0.5579190221053875 	 acc:0.7284768211920529 	 lr:5e-05
epoch30: train: loss:0.3993884446995317 	 acc:0.9067768595041322 | test: loss:0.5323114226195986 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch31: train: loss:0.40551607112253996 	 acc:0.9147107438016528 | test: loss:0.5776431281835038 	 acc:0.7245033112582782 	 lr:2.5e-05
epoch32: train: loss:0.3875001415142343 	 acc:0.9371900826446281 | test: loss:0.533290973483332 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch33: train: loss:0.3846081027511723 	 acc:0.9236363636363636 | test: loss:0.5327532212465804 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch34: train: loss:0.38118927539872727 	 acc:0.9302479338842975 | test: loss:0.5241097850515353 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch35: train: loss:0.37966594954167515 	 acc:0.9322314049586777 | test: loss:0.531218510984585 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch36: train: loss:0.38172230173733607 	 acc:0.9345454545454546 | test: loss:0.534503483377545 	 acc:0.7615894039735099 	 lr:2.5e-05
epoch37: train: loss:0.37477689188373975 	 acc:0.9434710743801653 | test: loss:0.5284678347063381 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch38: train: loss:0.38253488193858753 	 acc:0.9295867768595041 | test: loss:0.5412047659324495 	 acc:0.7337748344370861 	 lr:2.5e-05
epoch39: train: loss:0.38013489910393705 	 acc:0.9332231404958677 | test: loss:0.5271922840977347 	 acc:0.7602649006622516 	 lr:2.5e-05
epoch40: train: loss:0.37414528370888767 	 acc:0.9371900826446281 | test: loss:0.53393270211504 	 acc:0.7576158940397351 	 lr:2.5e-05
epoch41: train: loss:0.37775347435769957 	 acc:0.9378512396694215 | test: loss:0.5258037764110313 	 acc:0.7602649006622516 	 lr:1.25e-05
epoch42: train: loss:0.3761550140183819 	 acc:0.9411570247933885 | test: loss:0.5429116603554479 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch43: train: loss:0.3739594163874949 	 acc:0.9467768595041323 | test: loss:0.5289236173724496 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch44: train: loss:0.3706586975302578 	 acc:0.9477685950413223 | test: loss:0.5258359112486934 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch45: train: loss:0.373385896318215 	 acc:0.9388429752066115 | test: loss:0.5276467294882465 	 acc:0.766887417218543 	 lr:1.25e-05
epoch46: train: loss:0.37525028922341086 	 acc:0.9365289256198347 | test: loss:0.5283507563420479 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch47: train: loss:0.37165147401084586 	 acc:0.9418181818181818 | test: loss:0.5262570728529368 	 acc:0.7602649006622516 	 lr:6.25e-06
epoch48: train: loss:0.3689632048784209 	 acc:0.9477685950413223 | test: loss:0.5256454218302341 	 acc:0.7642384105960265 	 lr:6.25e-06
epoch49: train: loss:0.37115947922399223 	 acc:0.9477685950413223 | test: loss:0.5334759510905538 	 acc:0.7602649006622516 	 lr:6.25e-06
epoch50: train: loss:0.36708515265756403 	 acc:0.9457851239669421 | test: loss:0.5266329535585366 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch51: train: loss:0.36625415365558023 	 acc:0.9517355371900826 | test: loss:0.5243760812756242 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch52: train: loss:0.3696862238005173 	 acc:0.9441322314049587 | test: loss:0.5261115903885949 	 acc:0.7629139072847683 	 lr:6.25e-06
epoch53: train: loss:0.3727456753411569 	 acc:0.9451239669421487 | test: loss:0.5303663511939396 	 acc:0.7629139072847683 	 lr:3.125e-06
epoch54: train: loss:0.367685445734292 	 acc:0.9510743801652892 | test: loss:0.5296315756065166 	 acc:0.7629139072847683 	 lr:3.125e-06
epoch55: train: loss:0.36699751804682834 	 acc:0.9474380165289257 | test: loss:0.5265323412339419 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch56: train: loss:0.3680192923151757 	 acc:0.9507438016528925 | test: loss:0.5268805219637637 	 acc:0.7615894039735099 	 lr:3.125e-06
epoch57: train: loss:0.36631235380803256 	 acc:0.9497520661157025 | test: loss:0.5247521859920578 	 acc:0.7615894039735099 	 lr:3.125e-06
epoch58: train: loss:0.3700298284400593 	 acc:0.9464462809917356 | test: loss:0.5290926331715868 	 acc:0.7682119205298014 	 lr:3.125e-06
epoch59: train: loss:0.3759665044083083 	 acc:0.9398347107438016 | test: loss:0.5313992470305487 	 acc:0.7642384105960265 	 lr:1.5625e-06
epoch60: train: loss:0.36756866239319164 	 acc:0.9484297520661157 | test: loss:0.5313332755834062 	 acc:0.766887417218543 	 lr:1.5625e-06
epoch61: train: loss:0.36573085107094 	 acc:0.9500826446280992 | test: loss:0.5277734104765962 	 acc:0.7708609271523179 	 lr:1.5625e-06
epoch62: train: loss:0.3666995270685716 	 acc:0.9494214876033058 | test: loss:0.5268127093251967 	 acc:0.7655629139072848 	 lr:1.5625e-06
epoch63: train: loss:0.36083133350719104 	 acc:0.9547107438016529 | test: loss:0.5256097274110807 	 acc:0.7682119205298014 	 lr:1.5625e-06
epoch64: train: loss:0.36428404567655454 	 acc:0.9517355371900826 | test: loss:0.5279110186147374 	 acc:0.7682119205298014 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_1_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_1_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.8464536780956363 	 acc:0.48 | test: loss:0.8538677933200306 	 acc:0.4794701986754967 	 lr:0.0001
epoch1: train: loss:0.6890382253828128 	 acc:0.5213223140495867 | test: loss:0.6883313382698211 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.750893480068396 	 acc:0.5074380165289256 | test: loss:0.7638961643572675 	 acc:0.49801324503311256 	 lr:0.0001
epoch3: train: loss:0.6612062884165236 	 acc:0.5213223140495867 | test: loss:0.6551846662104525 	 acc:0.5218543046357615 	 lr:0.0001
epoch4: train: loss:0.6658995223833509 	 acc:0.5213223140495867 | test: loss:0.6638204868266124 	 acc:0.5231788079470199 	 lr:0.0001
epoch5: train: loss:0.6572656411178841 	 acc:0.5398347107438016 | test: loss:0.6558291196823121 	 acc:0.5496688741721855 	 lr:0.0001
epoch6: train: loss:0.6640448370058675 	 acc:0.5596694214876033 | test: loss:0.6623983056339997 	 acc:0.5655629139072847 	 lr:0.0001
epoch7: train: loss:0.6611876211481647 	 acc:0.5514049586776859 | test: loss:0.6583496979530284 	 acc:0.5801324503311258 	 lr:0.0001
epoch8: train: loss:0.6558879998695751 	 acc:0.5226446280991736 | test: loss:0.6508230682240417 	 acc:0.528476821192053 	 lr:0.0001
epoch9: train: loss:0.6706055307191265 	 acc:0.5682644628099174 | test: loss:0.6669591645531306 	 acc:0.5854304635761589 	 lr:0.0001
epoch10: train: loss:0.6549544009492417 	 acc:0.5395041322314049 | test: loss:0.6518599703611917 	 acc:0.5390728476821192 	 lr:0.0001
epoch11: train: loss:0.6531137027819295 	 acc:0.524297520661157 | test: loss:0.6499055286906413 	 acc:0.5403973509933775 	 lr:0.0001
epoch12: train: loss:0.6612622293558987 	 acc:0.5629752066115703 | test: loss:0.6572440857129381 	 acc:0.5774834437086093 	 lr:0.0001
epoch13: train: loss:0.6607062289340437 	 acc:0.5679338842975207 | test: loss:0.6595059146154795 	 acc:0.5814569536423841 	 lr:0.0001
epoch14: train: loss:0.6823067633179594 	 acc:0.5748760330578513 | test: loss:0.6785437377872846 	 acc:0.5920529801324503 	 lr:0.0001
epoch15: train: loss:0.6619940999913807 	 acc:0.5785123966942148 | test: loss:0.6608592187332002 	 acc:0.5774834437086093 	 lr:0.0001
epoch16: train: loss:0.6689602847532793 	 acc:0.5804958677685951 | test: loss:0.6612897587138296 	 acc:0.5960264900662252 	 lr:0.0001
epoch17: train: loss:0.6529642860357426 	 acc:0.5223140495867769 | test: loss:0.6484592054853376 	 acc:0.5258278145695364 	 lr:0.0001
epoch18: train: loss:0.6530390844266277 	 acc:0.5239669421487604 | test: loss:0.646553363863206 	 acc:0.5403973509933775 	 lr:0.0001
epoch19: train: loss:0.6552177294029677 	 acc:0.56 | test: loss:0.6484264268780386 	 acc:0.5827814569536424 	 lr:0.0001
epoch20: train: loss:0.6813696516840911 	 acc:0.5709090909090909 | test: loss:0.6830532677126246 	 acc:0.590728476821192 	 lr:0.0001
epoch21: train: loss:0.6554079222679138 	 acc:0.5738842975206612 | test: loss:0.6490613019229561 	 acc:0.5854304635761589 	 lr:0.0001
epoch22: train: loss:0.6516184688402602 	 acc:0.5573553719008264 | test: loss:0.6457120346707224 	 acc:0.5589403973509933 	 lr:0.0001
epoch23: train: loss:0.6520122998411005 	 acc:0.5603305785123966 | test: loss:0.6470129613844764 	 acc:0.5774834437086093 	 lr:0.0001
epoch24: train: loss:0.6478280647333003 	 acc:0.5259504132231405 | test: loss:0.6439016695054162 	 acc:0.5337748344370861 	 lr:0.0001
epoch25: train: loss:0.7349410705329958 	 acc:0.5173553719008265 | test: loss:0.7378773894531048 	 acc:0.5178807947019868 	 lr:0.0001
epoch26: train: loss:0.6504485576803034 	 acc:0.5322314049586777 | test: loss:0.6417627336963123 	 acc:0.5417218543046357 	 lr:0.0001
epoch27: train: loss:0.656383783502027 	 acc:0.5213223140495867 | test: loss:0.651688550165947 	 acc:0.5218543046357615 	 lr:0.0001
epoch28: train: loss:0.6514830365653865 	 acc:0.5431404958677686 | test: loss:0.642506687451672 	 acc:0.552317880794702 	 lr:0.0001
epoch29: train: loss:0.6694993830515333 	 acc:0.5900826446280992 | test: loss:0.6598305380897017 	 acc:0.6172185430463576 	 lr:0.0001
epoch30: train: loss:0.6769268285735579 	 acc:0.5765289256198347 | test: loss:0.6766458414248283 	 acc:0.5841059602649007 	 lr:0.0001
epoch31: train: loss:0.6497752830016712 	 acc:0.5388429752066116 | test: loss:0.6405809694568053 	 acc:0.5536423841059602 	 lr:0.0001
epoch32: train: loss:0.6758747298658386 	 acc:0.5781818181818181 | test: loss:0.6750805505064149 	 acc:0.5774834437086093 	 lr:0.0001
epoch33: train: loss:0.6774846388288766 	 acc:0.5871074380165289 | test: loss:0.6729241521942695 	 acc:0.5947019867549669 	 lr:0.0001
epoch34: train: loss:0.6456826746759335 	 acc:0.5299173553719009 | test: loss:0.6402368470533004 	 acc:0.5496688741721855 	 lr:0.0001
epoch35: train: loss:0.6516641881643248 	 acc:0.5256198347107438 | test: loss:0.6425984897360897 	 acc:0.5377483443708609 	 lr:0.0001
epoch36: train: loss:0.6990162722926494 	 acc:0.5547107438016529 | test: loss:0.6998796090384982 	 acc:0.5668874172185431 	 lr:0.0001
epoch37: train: loss:0.6492928779814854 	 acc:0.535206611570248 | test: loss:0.6419261396325977 	 acc:0.5470198675496689 	 lr:0.0001
epoch38: train: loss:0.6477155554393106 	 acc:0.5252892561983471 | test: loss:0.6431594978105154 	 acc:0.5298013245033113 	 lr:0.0001
epoch39: train: loss:0.6553684413137515 	 acc:0.6049586776859505 | test: loss:0.6526010645935867 	 acc:0.609271523178808 	 lr:0.0001
epoch40: train: loss:0.6461582309155425 	 acc:0.5451239669421487 | test: loss:0.6387640867801692 	 acc:0.5602649006622517 	 lr:0.0001
epoch41: train: loss:0.6479483014098869 	 acc:0.5226446280991736 | test: loss:0.6406097397898997 	 acc:0.5311258278145695 	 lr:0.0001
epoch42: train: loss:0.6625484938463889 	 acc:0.6072727272727273 | test: loss:0.6504123320642686 	 acc:0.6052980132450331 	 lr:0.0001
epoch43: train: loss:0.6690646018469629 	 acc:0.6033057851239669 | test: loss:0.6648299800639121 	 acc:0.6013245033112583 	 lr:0.0001
epoch44: train: loss:0.6496781304059935 	 acc:0.5213223140495867 | test: loss:0.641810306097498 	 acc:0.528476821192053 	 lr:0.0001
epoch45: train: loss:0.6817894170106935 	 acc:0.5814876033057851 | test: loss:0.667451138449031 	 acc:0.6158940397350994 	 lr:0.0001
epoch46: train: loss:0.6901819422619402 	 acc:0.5672727272727273 | test: loss:0.6772880594461959 	 acc:0.5947019867549669 	 lr:0.0001
epoch47: train: loss:0.6796105713686668 	 acc:0.5897520661157025 | test: loss:0.6664455596184888 	 acc:0.6039735099337749 	 lr:5e-05
epoch48: train: loss:0.6800883946536986 	 acc:0.5808264462809918 | test: loss:0.6697419545508378 	 acc:0.6 	 lr:5e-05
epoch49: train: loss:0.6510462938852547 	 acc:0.6003305785123967 | test: loss:0.6411906493420633 	 acc:0.6 	 lr:5e-05
epoch50: train: loss:0.651818528195058 	 acc:0.6158677685950413 | test: loss:0.6481148606104566 	 acc:0.6079470198675496 	 lr:5e-05
epoch51: train: loss:0.657260195026713 	 acc:0.6069421487603306 | test: loss:0.6515104310402018 	 acc:0.6158940397350994 	 lr:5e-05
epoch52: train: loss:0.6498535906381844 	 acc:0.5884297520661157 | test: loss:0.6382584354735368 	 acc:0.6 	 lr:5e-05
epoch53: train: loss:0.6605442257755059 	 acc:0.6221487603305785 | test: loss:0.6518140312851659 	 acc:0.6304635761589404 	 lr:5e-05
epoch54: train: loss:0.6527599650769195 	 acc:0.6185123966942149 | test: loss:0.6469292624896725 	 acc:0.6225165562913907 	 lr:5e-05
epoch55: train: loss:0.679821005261634 	 acc:0.5973553719008264 | test: loss:0.6740225191937377 	 acc:0.6079470198675496 	 lr:5e-05
epoch56: train: loss:0.6473207374840728 	 acc:0.6115702479338843 | test: loss:0.6445122283815549 	 acc:0.6079470198675496 	 lr:5e-05
epoch57: train: loss:0.6554487983451401 	 acc:0.6135537190082645 | test: loss:0.6479219266910426 	 acc:0.6344370860927152 	 lr:5e-05
epoch58: train: loss:0.6529250396381725 	 acc:0.5953719008264463 | test: loss:0.6399373752391891 	 acc:0.6119205298013245 	 lr:5e-05
epoch59: train: loss:0.6501108730331925 	 acc:0.5937190082644628 | test: loss:0.6387824862998053 	 acc:0.6026490066225165 	 lr:2.5e-05
epoch60: train: loss:0.6434046242453835 	 acc:0.5570247933884298 | test: loss:0.6349633788430927 	 acc:0.5735099337748344 	 lr:2.5e-05
epoch61: train: loss:0.6519471465260529 	 acc:0.6142148760330578 | test: loss:0.6445457176656912 	 acc:0.623841059602649 	 lr:2.5e-05
epoch62: train: loss:0.6516262686942235 	 acc:0.6039669421487603 | test: loss:0.6424383424765227 	 acc:0.6198675496688741 	 lr:2.5e-05
epoch63: train: loss:0.644157054601622 	 acc:0.5821487603305785 | test: loss:0.6364509247786162 	 acc:0.5841059602649007 	 lr:2.5e-05
epoch64: train: loss:0.6455553881787072 	 acc:0.6188429752066116 | test: loss:0.6422152244492082 	 acc:0.6132450331125828 	 lr:2.5e-05
epoch65: train: loss:0.6486848791374648 	 acc:0.6046280991735538 | test: loss:0.6432531408916246 	 acc:0.6158940397350994 	 lr:2.5e-05
epoch66: train: loss:0.6438724608854814 	 acc:0.6009917355371901 | test: loss:0.6379020039608937 	 acc:0.5880794701986755 	 lr:2.5e-05
epoch67: train: loss:0.6466069792321891 	 acc:0.5933884297520661 | test: loss:0.6376627825743315 	 acc:0.5973509933774834 	 lr:1.25e-05
epoch68: train: loss:0.646455982105791 	 acc:0.6003305785123967 | test: loss:0.6380926292463643 	 acc:0.5986754966887418 	 lr:1.25e-05
epoch69: train: loss:0.6448067405401182 	 acc:0.5884297520661157 | test: loss:0.6380134864358713 	 acc:0.6052980132450331 	 lr:1.25e-05
epoch70: train: loss:0.645568526973409 	 acc:0.6092561983471074 | test: loss:0.6389962373979834 	 acc:0.5960264900662252 	 lr:1.25e-05
epoch71: train: loss:0.6503568252059054 	 acc:0.6089256198347107 | test: loss:0.6414362827673653 	 acc:0.6119205298013245 	 lr:1.25e-05
epoch72: train: loss:0.6482787608509222 	 acc:0.6029752066115702 | test: loss:0.641712130931829 	 acc:0.6119205298013245 	 lr:1.25e-05
epoch73: train: loss:0.640065036627872 	 acc:0.5791735537190082 | test: loss:0.6365502203537139 	 acc:0.5814569536423841 	 lr:6.25e-06
epoch74: train: loss:0.6451835531833743 	 acc:0.5666115702479338 | test: loss:0.6358592668116487 	 acc:0.5761589403973509 	 lr:6.25e-06
epoch75: train: loss:0.6448266790129922 	 acc:0.5960330578512397 | test: loss:0.6388600561003022 	 acc:0.6013245033112583 	 lr:6.25e-06
epoch76: train: loss:0.6543586807408609 	 acc:0.6264462809917355 | test: loss:0.646045236871732 	 acc:0.6198675496688741 	 lr:6.25e-06
epoch77: train: loss:0.6444568367240843 	 acc:0.5861157024793389 | test: loss:0.6365170359611512 	 acc:0.5841059602649007 	 lr:6.25e-06
epoch78: train: loss:0.6405497162795264 	 acc:0.5861157024793389 | test: loss:0.635946532906286 	 acc:0.5854304635761589 	 lr:6.25e-06
epoch79: train: loss:0.6479753700957811 	 acc:0.5993388429752066 | test: loss:0.6402910825432531 	 acc:0.6132450331125828 	 lr:3.125e-06
epoch80: train: loss:0.644421135157593 	 acc:0.5900826446280992 | test: loss:0.637148545672562 	 acc:0.6026490066225165 	 lr:3.125e-06
epoch81: train: loss:0.6445768413661925 	 acc:0.6046280991735538 | test: loss:0.6384376068778386 	 acc:0.6039735099337749 	 lr:3.125e-06
epoch82: train: loss:0.6451770466812387 	 acc:0.5983471074380166 | test: loss:0.6374836747220021 	 acc:0.5973509933774834 	 lr:3.125e-06
epoch83: train: loss:0.6458329625169108 	 acc:0.5960330578512397 | test: loss:0.6373688159399474 	 acc:0.5933774834437087 	 lr:3.125e-06
epoch84: train: loss:0.6448241191462052 	 acc:0.5884297520661157 | test: loss:0.6369243772614082 	 acc:0.590728476821192 	 lr:3.125e-06
epoch85: train: loss:0.6457167780300802 	 acc:0.596694214876033 | test: loss:0.63719362170491 	 acc:0.590728476821192 	 lr:1.5625e-06
epoch86: train: loss:0.6489125295907012 	 acc:0.5841322314049586 | test: loss:0.6381028915872637 	 acc:0.6026490066225165 	 lr:1.5625e-06
epoch87: train: loss:0.6452430830120055 	 acc:0.5890909090909091 | test: loss:0.6368851431158205 	 acc:0.590728476821192 	 lr:1.5625e-06
epoch88: train: loss:0.6457493436632078 	 acc:0.5937190082644628 | test: loss:0.6379036931802106 	 acc:0.6026490066225165 	 lr:1.5625e-06
epoch89: train: loss:0.645585914840383 	 acc:0.5960330578512397 | test: loss:0.6371500218940886 	 acc:0.5973509933774834 	 lr:1.5625e-06
epoch90: train: loss:0.6454962974343418 	 acc:0.5923966942148761 | test: loss:0.6377224661656563 	 acc:0.6026490066225165 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_2_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_2_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.9116285214739398 	 acc:0.4790082644628099 | test: loss:0.914034706314668 	 acc:0.4781456953642384 	 lr:0.0001
epoch1: train: loss:0.6479518836982979 	 acc:0.5570247933884298 | test: loss:0.6472989739961182 	 acc:0.5761589403973509 	 lr:0.0001
epoch2: train: loss:0.6885577874341287 	 acc:0.5616528925619835 | test: loss:0.6702144193333506 	 acc:0.6 	 lr:0.0001
epoch3: train: loss:0.650816836830013 	 acc:0.536198347107438 | test: loss:0.6480466216605231 	 acc:0.5350993377483444 	 lr:0.0001
epoch4: train: loss:0.6614400465429322 	 acc:0.6056198347107438 | test: loss:0.6545614476235497 	 acc:0.614569536423841 	 lr:0.0001
epoch5: train: loss:0.6502541942241763 	 acc:0.6033057851239669 | test: loss:0.6467987773434216 	 acc:0.5880794701986755 	 lr:0.0001
epoch6: train: loss:0.6706874796772792 	 acc:0.6056198347107438 | test: loss:0.6661737121493612 	 acc:0.6344370860927152 	 lr:0.0001
epoch7: train: loss:0.6656694308194248 	 acc:0.6188429752066116 | test: loss:0.6657482324057068 	 acc:0.6079470198675496 	 lr:0.0001
epoch8: train: loss:0.6388316878011404 	 acc:0.5586776859504132 | test: loss:0.6343545758961052 	 acc:0.5589403973509933 	 lr:0.0001
epoch9: train: loss:0.6557217314026572 	 acc:0.6294214876033057 | test: loss:0.6419901065479051 	 acc:0.6225165562913907 	 lr:0.0001
epoch10: train: loss:0.6341899483460041 	 acc:0.5874380165289256 | test: loss:0.6315862322485211 	 acc:0.5960264900662252 	 lr:0.0001
epoch11: train: loss:0.6358619333692819 	 acc:0.5395041322314049 | test: loss:0.6301720879725273 	 acc:0.5509933774834437 	 lr:0.0001
epoch12: train: loss:0.651038384398153 	 acc:0.5223140495867769 | test: loss:0.6414863722213846 	 acc:0.5258278145695364 	 lr:0.0001
epoch13: train: loss:0.6312897909377232 	 acc:0.5768595041322314 | test: loss:0.6248939665737531 	 acc:0.5748344370860927 	 lr:0.0001
epoch14: train: loss:0.629597677849541 	 acc:0.59900826446281 | test: loss:0.6223353073296958 	 acc:0.5854304635761589 	 lr:0.0001
epoch15: train: loss:0.6355680267869933 	 acc:0.6168595041322315 | test: loss:0.623850987210179 	 acc:0.6423841059602649 	 lr:0.0001
epoch16: train: loss:0.6603305508873679 	 acc:0.6300826446280992 | test: loss:0.6617585223242147 	 acc:0.6291390728476821 	 lr:0.0001
epoch17: train: loss:0.6381906260340667 	 acc:0.6373553719008265 | test: loss:0.622632855374292 	 acc:0.6569536423841059 	 lr:0.0001
epoch18: train: loss:0.6254494953352557 	 acc:0.6066115702479339 | test: loss:0.6200507667680449 	 acc:0.6158940397350994 	 lr:0.0001
epoch19: train: loss:0.6249232280549925 	 acc:0.604297520661157 | test: loss:0.6192715452206845 	 acc:0.6079470198675496 	 lr:0.0001
epoch20: train: loss:0.6294550341811062 	 acc:0.5748760330578513 | test: loss:0.6206718066670247 	 acc:0.5788079470198676 	 lr:0.0001
epoch21: train: loss:0.6365042326667092 	 acc:0.6300826446280992 | test: loss:0.6230972622404035 	 acc:0.6622516556291391 	 lr:0.0001
epoch22: train: loss:0.631606499951733 	 acc:0.631404958677686 | test: loss:0.6228145675153922 	 acc:0.6357615894039735 	 lr:0.0001
epoch23: train: loss:0.6360756907581298 	 acc:0.5444628099173554 | test: loss:0.6398192135703484 	 acc:0.5417218543046357 	 lr:0.0001
epoch24: train: loss:0.6708063707469909 	 acc:0.5950413223140496 | test: loss:0.6574071207583345 	 acc:0.6490066225165563 	 lr:0.0001
epoch25: train: loss:0.6948415782431926 	 acc:0.5781818181818181 | test: loss:0.6825271971967836 	 acc:0.5986754966887418 	 lr:0.0001
epoch26: train: loss:0.6477782142063803 	 acc:0.6456198347107438 | test: loss:0.6350136519267859 	 acc:0.6821192052980133 	 lr:5e-05
epoch27: train: loss:0.6285044129032734 	 acc:0.6634710743801653 | test: loss:0.6209130651114003 	 acc:0.6609271523178808 	 lr:5e-05
epoch28: train: loss:0.6175765920276485 	 acc:0.6079338842975207 | test: loss:0.6140721152160341 	 acc:0.6026490066225165 	 lr:5e-05
epoch29: train: loss:0.615583026172701 	 acc:0.6317355371900827 | test: loss:0.6126272533903059 	 acc:0.6291390728476821 	 lr:5e-05
epoch30: train: loss:0.633126620044393 	 acc:0.6628099173553719 | test: loss:0.6253282724626806 	 acc:0.6874172185430464 	 lr:5e-05
epoch31: train: loss:0.6240276168397636 	 acc:0.6598347107438016 | test: loss:0.6224858575309349 	 acc:0.6450331125827815 	 lr:5e-05
epoch32: train: loss:0.6183502659324772 	 acc:0.6492561983471075 | test: loss:0.6130055385709599 	 acc:0.6476821192052981 	 lr:5e-05
epoch33: train: loss:0.6150346045257632 	 acc:0.6165289256198347 | test: loss:0.6121980459484833 	 acc:0.609271523178808 	 lr:5e-05
epoch34: train: loss:0.6227571758356961 	 acc:0.6502479338842975 | test: loss:0.6107797156106557 	 acc:0.6556291390728477 	 lr:5e-05
epoch35: train: loss:0.6259024836209195 	 acc:0.6674380165289256 | test: loss:0.6268323232796018 	 acc:0.6543046357615894 	 lr:5e-05
epoch36: train: loss:0.6324654870584977 	 acc:0.6528925619834711 | test: loss:0.6199368915021025 	 acc:0.671523178807947 	 lr:5e-05
epoch37: train: loss:0.6551040600745146 	 acc:0.6363636363636364 | test: loss:0.6480065404184606 	 acc:0.6754966887417219 	 lr:5e-05
epoch38: train: loss:0.6303628983379396 	 acc:0.6674380165289256 | test: loss:0.6223280074580616 	 acc:0.6622516556291391 	 lr:5e-05
epoch39: train: loss:0.6140648862153045 	 acc:0.6743801652892562 | test: loss:0.6117474223604266 	 acc:0.6503311258278146 	 lr:5e-05
epoch40: train: loss:0.6114414733500521 	 acc:0.6370247933884298 | test: loss:0.6096304565076007 	 acc:0.6304635761589404 	 lr:5e-05
epoch41: train: loss:0.6174254735245193 	 acc:0.6740495867768596 | test: loss:0.6100249738093243 	 acc:0.6609271523178808 	 lr:5e-05
epoch42: train: loss:0.6180832289270133 	 acc:0.6614876033057852 | test: loss:0.6128481727562203 	 acc:0.6728476821192053 	 lr:5e-05
epoch43: train: loss:0.6177294343562165 	 acc:0.6376859504132232 | test: loss:0.6116995660674494 	 acc:0.6476821192052981 	 lr:5e-05
epoch44: train: loss:0.6221071349293732 	 acc:0.68 | test: loss:0.6111113106178132 	 acc:0.6675496688741722 	 lr:5e-05
epoch45: train: loss:0.6297553666564059 	 acc:0.6664462809917355 | test: loss:0.6245118170384539 	 acc:0.6847682119205298 	 lr:5e-05
epoch46: train: loss:0.6290269315144247 	 acc:0.5490909090909091 | test: loss:0.6255382093372724 	 acc:0.5576158940397351 	 lr:5e-05
epoch47: train: loss:0.6256132879927139 	 acc:0.6730578512396694 | test: loss:0.6216780855166202 	 acc:0.6754966887417219 	 lr:2.5e-05
epoch48: train: loss:0.6118481429155208 	 acc:0.643305785123967 | test: loss:0.6093826502364202 	 acc:0.6423841059602649 	 lr:2.5e-05
epoch49: train: loss:0.6092139813328578 	 acc:0.6390082644628099 | test: loss:0.6074703551286104 	 acc:0.6397350993377483 	 lr:2.5e-05
epoch50: train: loss:0.6129437264332102 	 acc:0.6611570247933884 | test: loss:0.6082258686324619 	 acc:0.6556291390728477 	 lr:2.5e-05
epoch51: train: loss:0.6280771707897344 	 acc:0.6849586776859504 | test: loss:0.6235689520046411 	 acc:0.6874172185430464 	 lr:2.5e-05
epoch52: train: loss:0.6140113664067481 	 acc:0.6773553719008264 | test: loss:0.6102310211453217 	 acc:0.6688741721854304 	 lr:2.5e-05
epoch53: train: loss:0.6076112827781803 	 acc:0.6601652892561983 | test: loss:0.6078184512277313 	 acc:0.6543046357615894 	 lr:2.5e-05
epoch54: train: loss:0.6165306576618478 	 acc:0.6776859504132231 | test: loss:0.6121195827888337 	 acc:0.6768211920529801 	 lr:2.5e-05
epoch55: train: loss:0.6083126690368021 	 acc:0.6317355371900827 | test: loss:0.6073129082357647 	 acc:0.6198675496688741 	 lr:2.5e-05
epoch56: train: loss:0.6113439641511145 	 acc:0.6231404958677685 | test: loss:0.6071081599652373 	 acc:0.6225165562913907 	 lr:2.5e-05
epoch57: train: loss:0.6170171468514056 	 acc:0.6763636363636364 | test: loss:0.6105805450717345 	 acc:0.6635761589403973 	 lr:2.5e-05
epoch58: train: loss:0.6093296322743754 	 acc:0.6512396694214876 | test: loss:0.6058643645008668 	 acc:0.6596026490066225 	 lr:2.5e-05
epoch59: train: loss:0.6179985678688553 	 acc:0.6720661157024793 | test: loss:0.6164895857406768 	 acc:0.6834437086092715 	 lr:2.5e-05
epoch60: train: loss:0.6150685964931141 	 acc:0.6697520661157025 | test: loss:0.6102955409233144 	 acc:0.6754966887417219 	 lr:2.5e-05
epoch61: train: loss:0.6142922196506468 	 acc:0.6965289256198347 | test: loss:0.6104996017272899 	 acc:0.6781456953642384 	 lr:2.5e-05
epoch62: train: loss:0.6229052190347152 	 acc:0.6849586776859504 | test: loss:0.6234370558467133 	 acc:0.6874172185430464 	 lr:2.5e-05
epoch63: train: loss:0.6234578310359608 	 acc:0.6717355371900826 | test: loss:0.6157145309921922 	 acc:0.6834437086092715 	 lr:2.5e-05
epoch64: train: loss:0.6128502309420877 	 acc:0.5933884297520661 | test: loss:0.6125682335815682 	 acc:0.5841059602649007 	 lr:2.5e-05
epoch65: train: loss:0.6108197330837407 	 acc:0.6763636363636364 | test: loss:0.606971081518969 	 acc:0.6582781456953642 	 lr:1.25e-05
epoch66: train: loss:0.6212646772841777 	 acc:0.6816528925619835 | test: loss:0.6148151526387954 	 acc:0.6781456953642384 	 lr:1.25e-05
epoch67: train: loss:0.6131407329070666 	 acc:0.6677685950413224 | test: loss:0.6099686444200427 	 acc:0.6662251655629139 	 lr:1.25e-05
epoch68: train: loss:0.6091291366135778 	 acc:0.6790082644628099 | test: loss:0.6070130238469863 	 acc:0.6635761589403973 	 lr:1.25e-05
epoch69: train: loss:0.6059121704889723 	 acc:0.6720661157024793 | test: loss:0.606176972941847 	 acc:0.671523178807947 	 lr:1.25e-05
epoch70: train: loss:0.6111099642367402 	 acc:0.6783471074380165 | test: loss:0.6077654781720496 	 acc:0.6754966887417219 	 lr:1.25e-05
epoch71: train: loss:0.6083656155373439 	 acc:0.6714049586776859 | test: loss:0.6059711199722543 	 acc:0.6701986754966888 	 lr:6.25e-06
epoch72: train: loss:0.6137202007120306 	 acc:0.6823140495867769 | test: loss:0.610948114363563 	 acc:0.6821192052980133 	 lr:6.25e-06
epoch73: train: loss:0.6072696753966906 	 acc:0.64 | test: loss:0.6055548799748453 	 acc:0.633112582781457 	 lr:6.25e-06
epoch74: train: loss:0.6087624570357898 	 acc:0.6697520661157025 | test: loss:0.6056816939486573 	 acc:0.6596026490066225 	 lr:6.25e-06
epoch75: train: loss:0.6079202394643106 	 acc:0.6697520661157025 | test: loss:0.6051504847229712 	 acc:0.6609271523178808 	 lr:6.25e-06
epoch76: train: loss:0.6058058591125425 	 acc:0.6528925619834711 | test: loss:0.6047054958659293 	 acc:0.6450331125827815 	 lr:6.25e-06
epoch77: train: loss:0.6070990707263474 	 acc:0.6776859504132231 | test: loss:0.6055954976587106 	 acc:0.6582781456953642 	 lr:6.25e-06
epoch78: train: loss:0.6097513675689697 	 acc:0.692892561983471 | test: loss:0.60941489666503 	 acc:0.671523178807947 	 lr:6.25e-06
epoch79: train: loss:0.6081065906374907 	 acc:0.6819834710743802 | test: loss:0.6062765531982017 	 acc:0.6675496688741722 	 lr:6.25e-06
epoch80: train: loss:0.6102752690472879 	 acc:0.6803305785123966 | test: loss:0.6072106893488903 	 acc:0.6741721854304635 	 lr:6.25e-06
epoch81: train: loss:0.6121355115086579 	 acc:0.6816528925619835 | test: loss:0.6083710174686862 	 acc:0.6741721854304635 	 lr:6.25e-06
epoch82: train: loss:0.6130734831242521 	 acc:0.6826446280991736 | test: loss:0.6099028934706126 	 acc:0.6794701986754967 	 lr:6.25e-06
epoch83: train: loss:0.6112343850608699 	 acc:0.6680991735537191 | test: loss:0.606327023885108 	 acc:0.671523178807947 	 lr:3.125e-06
epoch84: train: loss:0.6063552136263571 	 acc:0.6776859504132231 | test: loss:0.6066807242418757 	 acc:0.671523178807947 	 lr:3.125e-06
epoch85: train: loss:0.6113860397693539 	 acc:0.6892561983471074 | test: loss:0.6093548568668744 	 acc:0.671523178807947 	 lr:3.125e-06
epoch86: train: loss:0.6064574102330799 	 acc:0.6578512396694215 | test: loss:0.6048202704909621 	 acc:0.6503311258278146 	 lr:3.125e-06
epoch87: train: loss:0.608607959097082 	 acc:0.6651239669421488 | test: loss:0.6051387308449145 	 acc:0.6622516556291391 	 lr:3.125e-06
epoch88: train: loss:0.6059243886135827 	 acc:0.6826446280991736 | test: loss:0.6057504657877991 	 acc:0.6701986754966888 	 lr:3.125e-06
epoch89: train: loss:0.6058022445686593 	 acc:0.684297520661157 | test: loss:0.6054814736574691 	 acc:0.6649006622516557 	 lr:1.5625e-06
epoch90: train: loss:0.607319189457854 	 acc:0.6717355371900826 | test: loss:0.6056761420325728 	 acc:0.6675496688741722 	 lr:1.5625e-06
epoch91: train: loss:0.6075423777004904 	 acc:0.6806611570247934 | test: loss:0.6057685723367906 	 acc:0.6662251655629139 	 lr:1.5625e-06
epoch92: train: loss:0.607353691443924 	 acc:0.672396694214876 | test: loss:0.6054111206768364 	 acc:0.6675496688741722 	 lr:1.5625e-06
epoch93: train: loss:0.6102315031398426 	 acc:0.6826446280991736 | test: loss:0.6065321200730784 	 acc:0.671523178807947 	 lr:1.5625e-06
epoch94: train: loss:0.6104837981531442 	 acc:0.6707438016528926 | test: loss:0.6055591752987034 	 acc:0.6662251655629139 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_3_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_3_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6937905887926905 	 acc:0.5213223140495867 | test: loss:0.6929932756929208 	 acc:0.5218543046357615 	 lr:0.0001
epoch1: train: loss:0.6672971703395371 	 acc:0.5216528925619834 | test: loss:0.6706280584366906 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6797383575597086 	 acc:0.5748760330578513 | test: loss:0.6520813128016643 	 acc:0.6079470198675496 	 lr:0.0001
epoch3: train: loss:0.6536173541092676 	 acc:0.5219834710743801 | test: loss:0.656049052611092 	 acc:0.5218543046357615 	 lr:0.0001
epoch4: train: loss:0.6403210332176902 	 acc:0.5593388429752066 | test: loss:0.637386219075184 	 acc:0.552317880794702 	 lr:0.0001
epoch5: train: loss:0.673365008476352 	 acc:0.6125619834710744 | test: loss:0.6634870168389074 	 acc:0.6344370860927152 	 lr:0.0001
epoch6: train: loss:0.6523170553357148 	 acc:0.6290909090909091 | test: loss:0.6523737627149417 	 acc:0.6410596026490066 	 lr:0.0001
epoch7: train: loss:0.6467000156197666 	 acc:0.64 | test: loss:0.6452709282470854 	 acc:0.6304635761589404 	 lr:0.0001
epoch8: train: loss:0.6327368934095399 	 acc:0.5682644628099174 | test: loss:0.6296826495240067 	 acc:0.5562913907284768 	 lr:0.0001
epoch9: train: loss:0.6473572963328401 	 acc:0.6446280991735537 | test: loss:0.6391161625748438 	 acc:0.6225165562913907 	 lr:0.0001
epoch10: train: loss:0.6291627784602898 	 acc:0.6102479338842975 | test: loss:0.6261315627603341 	 acc:0.6264900662251656 	 lr:0.0001
epoch11: train: loss:0.6318977012910133 	 acc:0.5497520661157025 | test: loss:0.6273040487276798 	 acc:0.5562913907284768 	 lr:0.0001
epoch12: train: loss:0.6399418996385307 	 acc:0.5342148760330578 | test: loss:0.6347678887923032 	 acc:0.5417218543046357 	 lr:0.0001
epoch13: train: loss:0.6299446188910933 	 acc:0.6360330578512396 | test: loss:0.6244829592325829 	 acc:0.633112582781457 	 lr:0.0001
epoch14: train: loss:0.6379110929591597 	 acc:0.6482644628099173 | test: loss:0.6253355445451294 	 acc:0.680794701986755 	 lr:0.0001
epoch15: train: loss:0.6628313554614044 	 acc:0.623801652892562 | test: loss:0.659817137939251 	 acc:0.6264900662251656 	 lr:0.0001
epoch16: train: loss:0.6375203016769787 	 acc:0.660495867768595 | test: loss:0.6400656471189284 	 acc:0.6516556291390728 	 lr:0.0001
epoch17: train: loss:0.6280075027134793 	 acc:0.6611570247933884 | test: loss:0.6207723594659211 	 acc:0.6649006622516557 	 lr:0.0001
epoch18: train: loss:0.6255163472743074 	 acc:0.6304132231404959 | test: loss:0.6178605405700128 	 acc:0.6529801324503312 	 lr:0.0001
epoch19: train: loss:0.6173306748492658 	 acc:0.6112396694214876 | test: loss:0.6152801636828492 	 acc:0.6198675496688741 	 lr:0.0001
epoch20: train: loss:0.621566055648583 	 acc:0.5765289256198347 | test: loss:0.6188548127547006 	 acc:0.5708609271523178 	 lr:0.0001
epoch21: train: loss:0.6146087343239587 	 acc:0.6416528925619834 | test: loss:0.613179759316097 	 acc:0.633112582781457 	 lr:0.0001
epoch22: train: loss:0.640993264884003 	 acc:0.6509090909090909 | test: loss:0.6323211158348235 	 acc:0.6556291390728477 	 lr:0.0001
epoch23: train: loss:0.6221991411516489 	 acc:0.5887603305785124 | test: loss:0.6242092548616675 	 acc:0.5721854304635762 	 lr:0.0001
epoch24: train: loss:0.7272712788897112 	 acc:0.5461157024793388 | test: loss:0.7277751870502699 	 acc:0.5443708609271524 	 lr:0.0001
epoch25: train: loss:0.6572985970875448 	 acc:0.6254545454545455 | test: loss:0.6556230720305285 	 acc:0.6490066225165563 	 lr:0.0001
epoch26: train: loss:0.6152111199079466 	 acc:0.6165289256198347 | test: loss:0.6133353860962469 	 acc:0.6039735099337749 	 lr:0.0001
epoch27: train: loss:0.6138037760198609 	 acc:0.6274380165289256 | test: loss:0.6105271931515625 	 acc:0.6198675496688741 	 lr:0.0001
epoch28: train: loss:0.6277902460295307 	 acc:0.6710743801652893 | test: loss:0.6270802092078506 	 acc:0.6675496688741722 	 lr:0.0001
epoch29: train: loss:0.6810175736876559 	 acc:0.6082644628099173 | test: loss:0.6709313467638383 	 acc:0.6079470198675496 	 lr:0.0001
epoch30: train: loss:0.6464150271730975 	 acc:0.651900826446281 | test: loss:0.6416601788918703 	 acc:0.6622516556291391 	 lr:0.0001
epoch31: train: loss:0.631801763191696 	 acc:0.6558677685950414 | test: loss:0.6276197213210807 	 acc:0.6649006622516557 	 lr:0.0001
epoch32: train: loss:0.6137531156973405 	 acc:0.5844628099173553 | test: loss:0.6135203524141122 	 acc:0.5841059602649007 	 lr:0.0001
epoch33: train: loss:0.6140915342598907 	 acc:0.5874380165289256 | test: loss:0.6117373167284277 	 acc:0.5827814569536424 	 lr:0.0001
epoch34: train: loss:0.6068019951473583 	 acc:0.6644628099173554 | test: loss:0.6005112194067596 	 acc:0.6529801324503312 	 lr:5e-05
epoch35: train: loss:0.6133222885368285 	 acc:0.6856198347107438 | test: loss:0.6138154116687395 	 acc:0.6900662251655629 	 lr:5e-05
epoch36: train: loss:0.6238534993376613 	 acc:0.6720661157024793 | test: loss:0.6191460303912889 	 acc:0.6794701986754967 	 lr:5e-05
epoch37: train: loss:0.6369816028185127 	 acc:0.6595041322314049 | test: loss:0.6302870443325169 	 acc:0.6754966887417219 	 lr:5e-05
epoch38: train: loss:0.6256361800579985 	 acc:0.6790082644628099 | test: loss:0.6229140833513627 	 acc:0.6728476821192053 	 lr:5e-05
epoch39: train: loss:0.6011338305276287 	 acc:0.692892561983471 | test: loss:0.6017438866444771 	 acc:0.6635761589403973 	 lr:5e-05
epoch40: train: loss:0.6017889675621159 	 acc:0.6476033057851239 | test: loss:0.5977992793582133 	 acc:0.6556291390728477 	 lr:5e-05
epoch41: train: loss:0.6042317643047365 	 acc:0.6522314049586777 | test: loss:0.5983667469182551 	 acc:0.6635761589403973 	 lr:5e-05
epoch42: train: loss:0.5977412708731722 	 acc:0.6667768595041322 | test: loss:0.5971363062100694 	 acc:0.6582781456953642 	 lr:5e-05
epoch43: train: loss:0.6022315961073252 	 acc:0.6578512396694215 | test: loss:0.5999439399763449 	 acc:0.6490066225165563 	 lr:5e-05
epoch44: train: loss:0.6037692240644092 	 acc:0.6981818181818182 | test: loss:0.5982547909218744 	 acc:0.6940397350993377 	 lr:5e-05
epoch45: train: loss:0.6205734439526708 	 acc:0.6859504132231405 | test: loss:0.6206986132047034 	 acc:0.6794701986754967 	 lr:5e-05
epoch46: train: loss:0.6117713446853574 	 acc:0.5844628099173553 | test: loss:0.6119436697454642 	 acc:0.5933774834437087 	 lr:5e-05
epoch47: train: loss:0.6153175458632225 	 acc:0.6948760330578513 | test: loss:0.6138370194971956 	 acc:0.686092715231788 	 lr:5e-05
epoch48: train: loss:0.5969984997993658 	 acc:0.672396694214876 | test: loss:0.5973777711786181 	 acc:0.6556291390728477 	 lr:5e-05
epoch49: train: loss:0.5965914240159279 	 acc:0.6621487603305786 | test: loss:0.5972534855469963 	 acc:0.6556291390728477 	 lr:2.5e-05
epoch50: train: loss:0.5980081404142144 	 acc:0.6809917355371901 | test: loss:0.5964838364266402 	 acc:0.6569536423841059 	 lr:2.5e-05
epoch51: train: loss:0.6114693643435959 	 acc:0.7064462809917356 | test: loss:0.6090488948569392 	 acc:0.6966887417218544 	 lr:2.5e-05
epoch52: train: loss:0.6017054893359665 	 acc:0.7041322314049587 | test: loss:0.5990618461015209 	 acc:0.6927152317880795 	 lr:2.5e-05
epoch53: train: loss:0.5961164720393409 	 acc:0.6819834710743802 | test: loss:0.5952056194772783 	 acc:0.6688741721854304 	 lr:2.5e-05
epoch54: train: loss:0.6106815771820131 	 acc:0.6925619834710743 | test: loss:0.6087168695121411 	 acc:0.6980132450331126 	 lr:2.5e-05
epoch55: train: loss:0.5979936171760244 	 acc:0.6360330578512396 | test: loss:0.5977533956237187 	 acc:0.6278145695364239 	 lr:2.5e-05
epoch56: train: loss:0.5950827800340889 	 acc:0.6690909090909091 | test: loss:0.593600418393975 	 acc:0.6490066225165563 	 lr:2.5e-05
epoch57: train: loss:0.6004232571932895 	 acc:0.6925619834710743 | test: loss:0.5946183615172935 	 acc:0.6794701986754967 	 lr:2.5e-05
epoch58: train: loss:0.5968547684299059 	 acc:0.6608264462809917 | test: loss:0.5939441419595125 	 acc:0.6728476821192053 	 lr:2.5e-05
epoch59: train: loss:0.605956340407537 	 acc:0.6889256198347108 | test: loss:0.605429848772011 	 acc:0.6980132450331126 	 lr:2.5e-05
epoch60: train: loss:0.6049810389644843 	 acc:0.6948760330578513 | test: loss:0.6014149303467858 	 acc:0.6993377483443709 	 lr:2.5e-05
epoch61: train: loss:0.5977148712173966 	 acc:0.7041322314049587 | test: loss:0.5974636312352111 	 acc:0.6940397350993377 	 lr:2.5e-05
epoch62: train: loss:0.6092207969909857 	 acc:0.7004958677685951 | test: loss:0.609846796026293 	 acc:0.695364238410596 	 lr:2.5e-05
epoch63: train: loss:0.5986657966464018 	 acc:0.6948760330578513 | test: loss:0.5957826454118388 	 acc:0.695364238410596 	 lr:1.25e-05
epoch64: train: loss:0.5943328516148338 	 acc:0.6733884297520661 | test: loss:0.5930381431484854 	 acc:0.6635761589403973 	 lr:1.25e-05
epoch65: train: loss:0.5958864921380665 	 acc:0.6948760330578513 | test: loss:0.5939492266699178 	 acc:0.6794701986754967 	 lr:1.25e-05
epoch66: train: loss:0.6010480642712805 	 acc:0.692892561983471 | test: loss:0.596842736282096 	 acc:0.6980132450331126 	 lr:1.25e-05
epoch67: train: loss:0.5976755308119719 	 acc:0.6885950413223141 | test: loss:0.5966622474177784 	 acc:0.6927152317880795 	 lr:1.25e-05
epoch68: train: loss:0.5965260367354086 	 acc:0.7041322314049587 | test: loss:0.5972583942065965 	 acc:0.6993377483443709 	 lr:1.25e-05
epoch69: train: loss:0.5938050146339353 	 acc:0.699504132231405 | test: loss:0.5954601684942941 	 acc:0.6980132450331126 	 lr:1.25e-05
epoch70: train: loss:0.597066855430603 	 acc:0.7074380165289256 | test: loss:0.5950696175461573 	 acc:0.6927152317880795 	 lr:1.25e-05
epoch71: train: loss:0.5953069562360275 	 acc:0.6869421487603306 | test: loss:0.5934266009867586 	 acc:0.6821192052980133 	 lr:6.25e-06
epoch72: train: loss:0.5990175218621562 	 acc:0.7044628099173553 | test: loss:0.5980122876483084 	 acc:0.7112582781456953 	 lr:6.25e-06
epoch73: train: loss:0.5916522162807875 	 acc:0.6717355371900826 | test: loss:0.5933516422644356 	 acc:0.6503311258278146 	 lr:6.25e-06
epoch74: train: loss:0.5946767406424215 	 acc:0.6905785123966942 | test: loss:0.5935671915281687 	 acc:0.6821192052980133 	 lr:6.25e-06
epoch75: train: loss:0.595640168938755 	 acc:0.6988429752066115 | test: loss:0.5937175140475596 	 acc:0.6874172185430464 	 lr:6.25e-06
epoch76: train: loss:0.593485970063643 	 acc:0.6909090909090909 | test: loss:0.5927455986572417 	 acc:0.6635761589403973 	 lr:6.25e-06
epoch77: train: loss:0.5949352309526491 	 acc:0.7034710743801653 | test: loss:0.593747758944303 	 acc:0.6794701986754967 	 lr:6.25e-06
epoch78: train: loss:0.597492933903844 	 acc:0.7140495867768595 | test: loss:0.5973548162062436 	 acc:0.695364238410596 	 lr:6.25e-06
epoch79: train: loss:0.5934516624182709 	 acc:0.696198347107438 | test: loss:0.5938842778963759 	 acc:0.6754966887417219 	 lr:6.25e-06
epoch80: train: loss:0.5937524881047651 	 acc:0.7044628099173553 | test: loss:0.5940922367651731 	 acc:0.6821192052980133 	 lr:6.25e-06
epoch81: train: loss:0.5947136999555855 	 acc:0.7018181818181818 | test: loss:0.594022011046378 	 acc:0.6794701986754967 	 lr:6.25e-06
epoch82: train: loss:0.5986572327101526 	 acc:0.7064462809917356 | test: loss:0.5967477780304208 	 acc:0.7006622516556291 	 lr:6.25e-06
epoch83: train: loss:0.5963800069123261 	 acc:0.6932231404958678 | test: loss:0.5941048030821693 	 acc:0.6834437086092715 	 lr:3.125e-06
epoch84: train: loss:0.5913442338990771 	 acc:0.6971900826446281 | test: loss:0.5937269588179935 	 acc:0.6834437086092715 	 lr:3.125e-06
epoch85: train: loss:0.5972300031165446 	 acc:0.716694214876033 | test: loss:0.5968665180616821 	 acc:0.7006622516556291 	 lr:3.125e-06
epoch86: train: loss:0.5931579086406171 	 acc:0.6803305785123966 | test: loss:0.5928695076348766 	 acc:0.671523178807947 	 lr:3.125e-06
epoch87: train: loss:0.5966138405839274 	 acc:0.6902479338842975 | test: loss:0.5935176815418218 	 acc:0.680794701986755 	 lr:3.125e-06
epoch88: train: loss:0.5906029688622341 	 acc:0.7061157024793389 | test: loss:0.5935880192857704 	 acc:0.680794701986755 	 lr:3.125e-06
epoch89: train: loss:0.5908295153586333 	 acc:0.7031404958677686 | test: loss:0.5933057588457272 	 acc:0.6768211920529801 	 lr:1.5625e-06
epoch90: train: loss:0.5932583518855827 	 acc:0.6985123966942148 | test: loss:0.593633180027766 	 acc:0.6781456953642384 	 lr:1.5625e-06
epoch91: train: loss:0.591739854832326 	 acc:0.6991735537190082 | test: loss:0.593467056435465 	 acc:0.6781456953642384 	 lr:1.5625e-06
epoch92: train: loss:0.5932589537447149 	 acc:0.6938842975206612 | test: loss:0.5934360176522211 	 acc:0.6781456953642384 	 lr:1.5625e-06
epoch93: train: loss:0.5950351696763156 	 acc:0.7054545454545454 | test: loss:0.594494280278288 	 acc:0.6927152317880795 	 lr:1.5625e-06
epoch94: train: loss:0.5956865781397859 	 acc:0.6922314049586776 | test: loss:0.5931281013204562 	 acc:0.6754966887417219 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_4_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_4_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6893652604828197 	 acc:0.5213223140495867 | test: loss:0.6883621678447092 	 acc:0.5218543046357615 	 lr:0.0001
epoch1: train: loss:0.6859116281556689 	 acc:0.5213223140495867 | test: loss:0.6873976749300167 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6504162884152626 	 acc:0.5256198347107438 | test: loss:0.6525660896932842 	 acc:0.5218543046357615 	 lr:0.0001
epoch3: train: loss:0.6561014551958761 	 acc:0.6155371900826446 | test: loss:0.6516732199302572 	 acc:0.5933774834437087 	 lr:0.0001
epoch4: train: loss:0.6662627570491192 	 acc:0.6211570247933884 | test: loss:0.6605197143081009 	 acc:0.6291390728476821 	 lr:0.0001
epoch5: train: loss:0.6391726146847748 	 acc:0.5735537190082645 | test: loss:0.6371083279319157 	 acc:0.5602649006622517 	 lr:0.0001
epoch6: train: loss:0.6390661430161847 	 acc:0.620495867768595 | test: loss:0.6388724210246509 	 acc:0.6119205298013245 	 lr:0.0001
epoch7: train: loss:0.637714276589638 	 acc:0.6304132231404959 | test: loss:0.6373377855250377 	 acc:0.6185430463576159 	 lr:0.0001
epoch8: train: loss:0.6302839387152805 	 acc:0.5735537190082645 | test: loss:0.6281148683156399 	 acc:0.5629139072847682 	 lr:0.0001
epoch9: train: loss:0.6390842884434156 	 acc:0.6380165289256199 | test: loss:0.6326435450686524 	 acc:0.6476821192052981 	 lr:0.0001
epoch10: train: loss:0.631822676087214 	 acc:0.555702479338843 | test: loss:0.6286288171414508 	 acc:0.5629139072847682 	 lr:0.0001
epoch11: train: loss:0.6264829490992648 	 acc:0.576198347107438 | test: loss:0.6245775207778476 	 acc:0.5774834437086093 	 lr:0.0001
epoch12: train: loss:0.6311892531923026 	 acc:0.5510743801652892 | test: loss:0.6267207178059003 	 acc:0.5562913907284768 	 lr:0.0001
epoch13: train: loss:0.6342690070207454 	 acc:0.6459504132231405 | test: loss:0.6296021464644679 	 acc:0.6596026490066225 	 lr:0.0001
epoch14: train: loss:0.6207320516759699 	 acc:0.5914049586776859 | test: loss:0.6196034656455185 	 acc:0.5761589403973509 	 lr:0.0001
epoch15: train: loss:0.7114720522273671 	 acc:0.5609917355371901 | test: loss:0.7060798702650513 	 acc:0.5562913907284768 	 lr:0.0001
epoch16: train: loss:0.6307001333985447 	 acc:0.6565289256198347 | test: loss:0.6282126101436994 	 acc:0.6741721854304635 	 lr:0.0001
epoch17: train: loss:0.6204472717962974 	 acc:0.6674380165289256 | test: loss:0.6186167383825543 	 acc:0.6781456953642384 	 lr:0.0001
epoch18: train: loss:0.6261032218184353 	 acc:0.6588429752066116 | test: loss:0.6255242435347955 	 acc:0.6900662251655629 	 lr:0.0001
epoch19: train: loss:0.6116883939947964 	 acc:0.6482644628099173 | test: loss:0.6115933252486172 	 acc:0.6569536423841059 	 lr:0.0001
epoch20: train: loss:0.6247991075003443 	 acc:0.5510743801652892 | test: loss:0.6239730523911533 	 acc:0.552317880794702 	 lr:0.0001
epoch21: train: loss:0.6088296585831761 	 acc:0.6287603305785124 | test: loss:0.6085281625488737 	 acc:0.6132450331125828 	 lr:0.0001
epoch22: train: loss:0.627388434252463 	 acc:0.6611570247933884 | test: loss:0.6182472740577546 	 acc:0.6635761589403973 	 lr:0.0001
epoch23: train: loss:0.6204776172598532 	 acc:0.6687603305785124 | test: loss:0.622685579432557 	 acc:0.6834437086092715 	 lr:0.0001
epoch24: train: loss:0.7168428374124952 	 acc:0.5590082644628099 | test: loss:0.7197813519578896 	 acc:0.5642384105960265 	 lr:0.0001
epoch25: train: loss:0.6096907952403234 	 acc:0.6651239669421488 | test: loss:0.6079162122398023 	 acc:0.6503311258278146 	 lr:0.0001
epoch26: train: loss:0.6129905793686543 	 acc:0.595702479338843 | test: loss:0.6164903007595744 	 acc:0.5708609271523178 	 lr:0.0001
epoch27: train: loss:0.6086694278795857 	 acc:0.6366942148760331 | test: loss:0.6037998208147011 	 acc:0.633112582781457 	 lr:0.0001
epoch28: train: loss:0.6496829151319078 	 acc:0.6528925619834711 | test: loss:0.6476465398902135 	 acc:0.6543046357615894 	 lr:0.0001
epoch29: train: loss:0.6565287932088553 	 acc:0.64 | test: loss:0.647381259274009 	 acc:0.6503311258278146 	 lr:0.0001
epoch30: train: loss:0.6457830704736316 	 acc:0.6509090909090909 | test: loss:0.6508961563868239 	 acc:0.6503311258278146 	 lr:0.0001
epoch31: train: loss:0.6117197553973552 	 acc:0.675702479338843 | test: loss:0.6109641771442843 	 acc:0.6662251655629139 	 lr:0.0001
epoch32: train: loss:0.5991953078380301 	 acc:0.6396694214876033 | test: loss:0.605996522682392 	 acc:0.6172185430463576 	 lr:0.0001
epoch33: train: loss:0.5997565424146731 	 acc:0.628099173553719 | test: loss:0.6059103025505874 	 acc:0.5986754966887418 	 lr:0.0001
epoch34: train: loss:0.6167767627574195 	 acc:0.6770247933884298 | test: loss:0.6035930225391262 	 acc:0.7165562913907285 	 lr:5e-05
epoch35: train: loss:0.597019673989824 	 acc:0.6998347107438017 | test: loss:0.6047779505615992 	 acc:0.6768211920529801 	 lr:5e-05
epoch36: train: loss:0.624960843038953 	 acc:0.6704132231404959 | test: loss:0.615033095640852 	 acc:0.695364238410596 	 lr:5e-05
epoch37: train: loss:0.6364969377084212 	 acc:0.663801652892562 | test: loss:0.6406958298967375 	 acc:0.6622516556291391 	 lr:5e-05
epoch38: train: loss:0.6121371593160078 	 acc:0.6912396694214876 | test: loss:0.6130810572611575 	 acc:0.6821192052980133 	 lr:5e-05
epoch39: train: loss:0.589978478605097 	 acc:0.6938842975206612 | test: loss:0.5954661426954712 	 acc:0.6794701986754967 	 lr:5e-05
epoch40: train: loss:0.5915327887692727 	 acc:0.6869421487603306 | test: loss:0.5933491392640878 	 acc:0.680794701986755 	 lr:5e-05
epoch41: train: loss:0.5985765725916082 	 acc:0.691900826446281 | test: loss:0.5957239489681674 	 acc:0.695364238410596 	 lr:5e-05
epoch42: train: loss:0.593259307944085 	 acc:0.6694214876033058 | test: loss:0.5963234668535902 	 acc:0.6450331125827815 	 lr:5e-05
epoch43: train: loss:0.5944544139184242 	 acc:0.6671074380165289 | test: loss:0.594009712042398 	 acc:0.6529801324503312 	 lr:5e-05
epoch44: train: loss:0.6017623954173947 	 acc:0.707107438016529 | test: loss:0.5978573390190175 	 acc:0.6980132450331126 	 lr:5e-05
epoch45: train: loss:0.6004622746696157 	 acc:0.6971900826446281 | test: loss:0.6031744574868916 	 acc:0.6887417218543046 	 lr:5e-05
epoch46: train: loss:0.6065389503329254 	 acc:0.5907438016528925 | test: loss:0.6087783423480608 	 acc:0.5960264900662252 	 lr:5e-05
epoch47: train: loss:0.611658504285103 	 acc:0.6948760330578513 | test: loss:0.6101869863390134 	 acc:0.695364238410596 	 lr:2.5e-05
epoch48: train: loss:0.5910313233856327 	 acc:0.7054545454545454 | test: loss:0.5952945024761933 	 acc:0.6887417218543046 	 lr:2.5e-05
epoch49: train: loss:0.5878327804163468 	 acc:0.6710743801652893 | test: loss:0.5922230515258992 	 acc:0.6503311258278146 	 lr:2.5e-05
epoch50: train: loss:0.5915083316140923 	 acc:0.6971900826446281 | test: loss:0.5931214758891933 	 acc:0.680794701986755 	 lr:2.5e-05
epoch51: train: loss:0.6013424288142811 	 acc:0.7173553719008264 | test: loss:0.6041836624903395 	 acc:0.7112582781456953 	 lr:2.5e-05
epoch52: train: loss:0.5944695947781082 	 acc:0.7203305785123967 | test: loss:0.5932879843459224 	 acc:0.6980132450331126 	 lr:2.5e-05
epoch53: train: loss:0.5875020970982954 	 acc:0.6823140495867769 | test: loss:0.5898307489243564 	 acc:0.6596026490066225 	 lr:2.5e-05
epoch54: train: loss:0.5982755514215832 	 acc:0.7107438016528925 | test: loss:0.6008308268540742 	 acc:0.7152317880794702 	 lr:2.5e-05
epoch55: train: loss:0.5905275406916279 	 acc:0.6525619834710744 | test: loss:0.592088749156093 	 acc:0.6370860927152318 	 lr:2.5e-05
epoch56: train: loss:0.5890363548806876 	 acc:0.6684297520661157 | test: loss:0.5911785450992205 	 acc:0.6397350993377483 	 lr:2.5e-05
epoch57: train: loss:0.5921217991891972 	 acc:0.7133884297520661 | test: loss:0.5905214987843241 	 acc:0.6900662251655629 	 lr:2.5e-05
epoch58: train: loss:0.5899107783688001 	 acc:0.6700826446280992 | test: loss:0.5906599224008472 	 acc:0.6728476821192053 	 lr:2.5e-05
epoch59: train: loss:0.593153367810998 	 acc:0.6958677685950413 | test: loss:0.5969809001644716 	 acc:0.6927152317880795 	 lr:2.5e-05
epoch60: train: loss:0.5884698464456669 	 acc:0.6667768595041322 | test: loss:0.5913785326559812 	 acc:0.6450331125827815 	 lr:1.25e-05
epoch61: train: loss:0.5866350343404723 	 acc:0.7107438016528925 | test: loss:0.590133663913272 	 acc:0.6887417218543046 	 lr:1.25e-05
epoch62: train: loss:0.5984608716215969 	 acc:0.7223140495867768 | test: loss:0.6033908937151069 	 acc:0.7178807947019867 	 lr:1.25e-05
epoch63: train: loss:0.5910296847800578 	 acc:0.7064462809917356 | test: loss:0.5905247392243896 	 acc:0.6980132450331126 	 lr:1.25e-05
epoch64: train: loss:0.5853430499124133 	 acc:0.6899173553719008 | test: loss:0.5885286795382468 	 acc:0.6688741721854304 	 lr:1.25e-05
epoch65: train: loss:0.5869216458856567 	 acc:0.708099173553719 | test: loss:0.5891040179113679 	 acc:0.6794701986754967 	 lr:1.25e-05
epoch66: train: loss:0.5935695411548142 	 acc:0.699504132231405 | test: loss:0.5939378612878307 	 acc:0.6900662251655629 	 lr:1.25e-05
epoch67: train: loss:0.5878644379899521 	 acc:0.6955371900826446 | test: loss:0.5909774020018167 	 acc:0.6834437086092715 	 lr:1.25e-05
epoch68: train: loss:0.5902946245571798 	 acc:0.7209917355371901 | test: loss:0.5932116687692554 	 acc:0.7086092715231788 	 lr:1.25e-05
epoch69: train: loss:0.5835326941742385 	 acc:0.7137190082644628 | test: loss:0.590123162916954 	 acc:0.6927152317880795 	 lr:1.25e-05
epoch70: train: loss:0.590958250534436 	 acc:0.7077685950413223 | test: loss:0.5903223276138305 	 acc:0.7019867549668874 	 lr:1.25e-05
epoch71: train: loss:0.5876347380630241 	 acc:0.7034710743801653 | test: loss:0.5881174438836558 	 acc:0.6887417218543046 	 lr:6.25e-06
epoch72: train: loss:0.5908365538691687 	 acc:0.7130578512396695 | test: loss:0.5924498947250922 	 acc:0.7086092715231788 	 lr:6.25e-06
epoch73: train: loss:0.5836518869715288 	 acc:0.6836363636363636 | test: loss:0.5881695010804182 	 acc:0.6675496688741722 	 lr:6.25e-06
epoch74: train: loss:0.5860665965868422 	 acc:0.707107438016529 | test: loss:0.5884307974221691 	 acc:0.6847682119205298 	 lr:6.25e-06
epoch75: train: loss:0.5865184291729257 	 acc:0.7094214876033058 | test: loss:0.5881308648759955 	 acc:0.6900662251655629 	 lr:6.25e-06
epoch76: train: loss:0.5854967454051183 	 acc:0.6958677685950413 | test: loss:0.5874773457350321 	 acc:0.6675496688741722 	 lr:6.25e-06
epoch77: train: loss:0.5847074889151518 	 acc:0.7077685950413223 | test: loss:0.5876697077656424 	 acc:0.6794701986754967 	 lr:6.25e-06
epoch78: train: loss:0.5873514986038209 	 acc:0.7269421487603306 | test: loss:0.5914898620535995 	 acc:0.6980132450331126 	 lr:6.25e-06
epoch79: train: loss:0.5876263986934315 	 acc:0.7090909090909091 | test: loss:0.5890061687160012 	 acc:0.6887417218543046 	 lr:6.25e-06
epoch80: train: loss:0.5829851551686437 	 acc:0.7266115702479339 | test: loss:0.5897032106949004 	 acc:0.6887417218543046 	 lr:6.25e-06
epoch81: train: loss:0.587419904834968 	 acc:0.7193388429752066 | test: loss:0.5894332640218419 	 acc:0.6847682119205298 	 lr:6.25e-06
epoch82: train: loss:0.5910197546068302 	 acc:0.7130578512396695 | test: loss:0.5929378395838454 	 acc:0.7072847682119205 	 lr:6.25e-06
epoch83: train: loss:0.5896063284440474 	 acc:0.7051239669421487 | test: loss:0.5897712747782271 	 acc:0.6900662251655629 	 lr:3.125e-06
epoch84: train: loss:0.5823231739643191 	 acc:0.7130578512396695 | test: loss:0.5889971623357558 	 acc:0.6821192052980133 	 lr:3.125e-06
epoch85: train: loss:0.5890263004933507 	 acc:0.7127272727272728 | test: loss:0.592405755472499 	 acc:0.704635761589404 	 lr:3.125e-06
epoch86: train: loss:0.5850713393510866 	 acc:0.6988429752066115 | test: loss:0.5880103636261643 	 acc:0.6794701986754967 	 lr:3.125e-06
epoch87: train: loss:0.5875717377465619 	 acc:0.6975206611570248 | test: loss:0.5882629543740229 	 acc:0.6834437086092715 	 lr:3.125e-06
epoch88: train: loss:0.5821327485525903 	 acc:0.7196694214876033 | test: loss:0.5888882326763987 	 acc:0.6887417218543046 	 lr:3.125e-06
epoch89: train: loss:0.5809050909940862 	 acc:0.7226446280991735 | test: loss:0.5883853264202346 	 acc:0.6821192052980133 	 lr:1.5625e-06
epoch90: train: loss:0.5859619277173822 	 acc:0.7160330578512397 | test: loss:0.5887523770332337 	 acc:0.6821192052980133 	 lr:1.5625e-06
epoch91: train: loss:0.5848587939758931 	 acc:0.7094214876033058 | test: loss:0.5885460933312675 	 acc:0.6821192052980133 	 lr:1.5625e-06
epoch92: train: loss:0.5844034874537759 	 acc:0.708099173553719 | test: loss:0.5884265661239624 	 acc:0.6834437086092715 	 lr:1.5625e-06
epoch93: train: loss:0.587512503951049 	 acc:0.7180165289256198 | test: loss:0.5897155613299238 	 acc:0.6913907284768211 	 lr:1.5625e-06
epoch94: train: loss:0.5882259946421158 	 acc:0.7054545454545454 | test: loss:0.5882928663531676 	 acc:0.6834437086092715 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_5_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_5_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7986712300284835 	 acc:0.4849586776859504 | test: loss:0.8026277787638026 	 acc:0.48344370860927155 	 lr:0.0001
epoch1: train: loss:0.6703784847259522 	 acc:0.5213223140495867 | test: loss:0.67054602870878 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6759443790853517 	 acc:0.5947107438016529 | test: loss:0.658618516558843 	 acc:0.6304635761589404 	 lr:0.0001
epoch3: train: loss:0.6418885646969819 	 acc:0.5537190082644629 | test: loss:0.6351375258521529 	 acc:0.5271523178807948 	 lr:0.0001
epoch4: train: loss:0.6373035489232087 	 acc:0.5527272727272727 | test: loss:0.6300048367866617 	 acc:0.5470198675496689 	 lr:0.0001
epoch5: train: loss:0.6357797397857855 	 acc:0.5917355371900826 | test: loss:0.6246196334725184 	 acc:0.5973509933774834 	 lr:0.0001
epoch6: train: loss:0.6361942367711343 	 acc:0.5348760330578513 | test: loss:0.6282886835912995 	 acc:0.5311258278145695 	 lr:0.0001
epoch7: train: loss:0.6293583386200519 	 acc:0.6095867768595041 | test: loss:0.6191991003143866 	 acc:0.6158940397350994 	 lr:0.0001
epoch8: train: loss:0.628945663921104 	 acc:0.6674380165289256 | test: loss:0.6282308645595778 	 acc:0.6821192052980133 	 lr:0.0001
epoch9: train: loss:0.6272572495129483 	 acc:0.6538842975206611 | test: loss:0.6180946168520592 	 acc:0.6980132450331126 	 lr:0.0001
epoch10: train: loss:0.6230625265491896 	 acc:0.6740495867768596 | test: loss:0.618925023157865 	 acc:0.695364238410596 	 lr:0.0001
epoch11: train: loss:0.6333739499218207 	 acc:0.6664462809917355 | test: loss:0.6252232720520323 	 acc:0.695364238410596 	 lr:0.0001
epoch12: train: loss:0.6665266756774966 	 acc:0.6267768595041322 | test: loss:0.6541567998216642 	 acc:0.6225165562913907 	 lr:0.0001
epoch13: train: loss:0.614667750311292 	 acc:0.68 | test: loss:0.608345699626089 	 acc:0.6966887417218544 	 lr:0.0001
epoch14: train: loss:0.6065047249518151 	 acc:0.6330578512396694 | test: loss:0.5972897633811496 	 acc:0.6582781456953642 	 lr:0.0001
epoch15: train: loss:0.6007133070496489 	 acc:0.6254545454545455 | test: loss:0.602146229207121 	 acc:0.6172185430463576 	 lr:0.0001
epoch16: train: loss:0.6183450289206072 	 acc:0.6826446280991736 | test: loss:0.6131770041604705 	 acc:0.7059602649006622 	 lr:0.0001
epoch17: train: loss:0.6038184057385468 	 acc:0.6892561983471074 | test: loss:0.5950332867388693 	 acc:0.6980132450331126 	 lr:0.0001
epoch18: train: loss:0.598068273855635 	 acc:0.6284297520661157 | test: loss:0.5971800468615349 	 acc:0.6158940397350994 	 lr:0.0001
epoch19: train: loss:0.6115608498872804 	 acc:0.5778512396694215 | test: loss:0.604140915539091 	 acc:0.5894039735099338 	 lr:0.0001
epoch20: train: loss:0.5910381695455756 	 acc:0.7137190082644628 | test: loss:0.5926580988018718 	 acc:0.7152317880794702 	 lr:0.0001
epoch21: train: loss:0.6075180016273309 	 acc:0.6968595041322314 | test: loss:0.6047517051759934 	 acc:0.6993377483443709 	 lr:0.0001
epoch22: train: loss:0.6416866853611528 	 acc:0.6575206611570248 | test: loss:0.6437790149095043 	 acc:0.6476821192052981 	 lr:0.0001
epoch23: train: loss:0.5920545296826638 	 acc:0.651900826446281 | test: loss:0.5868630103717577 	 acc:0.6423841059602649 	 lr:0.0001
epoch24: train: loss:0.5868014045589226 	 acc:0.6885950413223141 | test: loss:0.5843978221842785 	 acc:0.6940397350993377 	 lr:0.0001
epoch25: train: loss:0.6629721061454331 	 acc:0.6320661157024794 | test: loss:0.677154556252309 	 acc:0.609271523178808 	 lr:0.0001
epoch26: train: loss:0.6248073359757416 	 acc:0.6889256198347108 | test: loss:0.6045174336591304 	 acc:0.713907284768212 	 lr:0.0001
epoch27: train: loss:0.592499119998995 	 acc:0.6704132231404959 | test: loss:0.5842173084517978 	 acc:0.686092715231788 	 lr:0.0001
epoch28: train: loss:0.5794212593125903 	 acc:0.7021487603305785 | test: loss:0.5785652059592948 	 acc:0.7019867549668874 	 lr:0.0001
epoch29: train: loss:0.6006180593001941 	 acc:0.6988429752066115 | test: loss:0.5989528992318159 	 acc:0.704635761589404 	 lr:0.0001
epoch30: train: loss:0.5935295087640936 	 acc:0.699504132231405 | test: loss:0.5833729890008635 	 acc:0.695364238410596 	 lr:0.0001
epoch31: train: loss:0.5930407952080088 	 acc:0.622809917355372 | test: loss:0.5865185837082515 	 acc:0.6437086092715232 	 lr:0.0001
epoch32: train: loss:0.5905439506286432 	 acc:0.7173553719008264 | test: loss:0.5784203967511259 	 acc:0.7324503311258278 	 lr:0.0001
epoch33: train: loss:0.5922244577565469 	 acc:0.7077685950413223 | test: loss:0.5825803909080708 	 acc:0.7311258278145696 	 lr:0.0001
epoch34: train: loss:0.5810902806944098 	 acc:0.7137190082644628 | test: loss:0.5785555669014028 	 acc:0.7099337748344371 	 lr:0.0001
epoch35: train: loss:0.5851966838009102 	 acc:0.6680991735537191 | test: loss:0.5854295350068452 	 acc:0.6834437086092715 	 lr:0.0001
epoch36: train: loss:0.6157333292251777 	 acc:0.5725619834710743 | test: loss:0.6229853277174843 	 acc:0.5536423841059602 	 lr:0.0001
epoch37: train: loss:0.5812482598399328 	 acc:0.6932231404958678 | test: loss:0.5809990909715362 	 acc:0.6940397350993377 	 lr:0.0001
epoch38: train: loss:0.5787110601574922 	 acc:0.6872727272727273 | test: loss:0.5746535369102529 	 acc:0.6900662251655629 	 lr:0.0001
epoch39: train: loss:0.573018870787187 	 acc:0.7137190082644628 | test: loss:0.5702216356795355 	 acc:0.7059602649006622 	 lr:0.0001
epoch40: train: loss:0.5715459321747142 	 acc:0.7087603305785124 | test: loss:0.5720232667512452 	 acc:0.7019867549668874 	 lr:0.0001
epoch41: train: loss:0.6093513141584791 	 acc:0.5692561983471074 | test: loss:0.6048447412370846 	 acc:0.5867549668874172 	 lr:0.0001
epoch42: train: loss:0.5969845001756652 	 acc:0.6069421487603306 | test: loss:0.610145770871876 	 acc:0.5761589403973509 	 lr:0.0001
epoch43: train: loss:0.5979716246581275 	 acc:0.7120661157024794 | test: loss:0.5995449711155417 	 acc:0.7059602649006622 	 lr:0.0001
epoch44: train: loss:0.5676066877428165 	 acc:0.6998347107438017 | test: loss:0.5681546017034165 	 acc:0.686092715231788 	 lr:0.0001
epoch45: train: loss:0.5633025042872783 	 acc:0.7388429752066116 | test: loss:0.5653909699016849 	 acc:0.7298013245033113 	 lr:0.0001
epoch46: train: loss:0.5673289660579902 	 acc:0.7233057851239669 | test: loss:0.5681664927116293 	 acc:0.7019867549668874 	 lr:0.0001
epoch47: train: loss:0.6432285352974884 	 acc:0.6499173553719009 | test: loss:0.6591247665961057 	 acc:0.6450331125827815 	 lr:0.0001
epoch48: train: loss:0.5810612320308843 	 acc:0.7302479338842975 | test: loss:0.5656137567482247 	 acc:0.7324503311258278 	 lr:0.0001
epoch49: train: loss:0.5728534246279189 	 acc:0.6700826446280992 | test: loss:0.5690216922602117 	 acc:0.6662251655629139 	 lr:0.0001
epoch50: train: loss:0.570707411450788 	 acc:0.6651239669421488 | test: loss:0.5783959880569913 	 acc:0.6516556291390728 	 lr:0.0001
epoch51: train: loss:0.6045198009427913 	 acc:0.704793388429752 | test: loss:0.6086320014978877 	 acc:0.6913907284768211 	 lr:0.0001
epoch52: train: loss:0.5573440138367582 	 acc:0.7209917355371901 | test: loss:0.5563999029184808 	 acc:0.713907284768212 	 lr:5e-05
epoch53: train: loss:0.5660516375549569 	 acc:0.7431404958677686 | test: loss:0.5790306031309216 	 acc:0.7218543046357616 	 lr:5e-05
epoch54: train: loss:0.557551523713041 	 acc:0.7216528925619835 | test: loss:0.557513302130415 	 acc:0.7231788079470198 	 lr:5e-05
epoch55: train: loss:0.5650269474076831 	 acc:0.7517355371900827 | test: loss:0.559111420760881 	 acc:0.7483443708609272 	 lr:5e-05
epoch56: train: loss:0.5668880236050314 	 acc:0.672396694214876 | test: loss:0.5648016626471716 	 acc:0.6887417218543046 	 lr:5e-05
epoch57: train: loss:0.561666262484779 	 acc:0.7593388429752066 | test: loss:0.5606236432561811 	 acc:0.7483443708609272 	 lr:5e-05
epoch58: train: loss:0.555072334344722 	 acc:0.7609917355371901 | test: loss:0.5552197934775952 	 acc:0.7456953642384105 	 lr:5e-05
epoch59: train: loss:0.5517639723494033 	 acc:0.7282644628099173 | test: loss:0.5543456987829398 	 acc:0.7284768211920529 	 lr:5e-05
epoch60: train: loss:0.5547842039746687 	 acc:0.7051239669421487 | test: loss:0.5601989037943202 	 acc:0.6927152317880795 	 lr:5e-05
epoch61: train: loss:0.5676403261019178 	 acc:0.7461157024793389 | test: loss:0.566228367713903 	 acc:0.7337748344370861 	 lr:5e-05
epoch62: train: loss:0.5615477552689797 	 acc:0.7557024793388429 | test: loss:0.5513720209235388 	 acc:0.7496688741721854 	 lr:5e-05
epoch63: train: loss:0.5607389930260083 	 acc:0.7467768595041322 | test: loss:0.5539375681750822 	 acc:0.7403973509933774 	 lr:5e-05
epoch64: train: loss:0.5603911194919554 	 acc:0.751404958677686 | test: loss:0.5556195008044211 	 acc:0.7456953642384105 	 lr:5e-05
epoch65: train: loss:0.5621001134234026 	 acc:0.6899173553719008 | test: loss:0.5589347575673994 	 acc:0.7033112582781457 	 lr:5e-05
epoch66: train: loss:0.5526153022395678 	 acc:0.730909090909091 | test: loss:0.5573838702100792 	 acc:0.7258278145695364 	 lr:5e-05
epoch67: train: loss:0.5541793937131393 	 acc:0.7398347107438017 | test: loss:0.5531617368293914 	 acc:0.7298013245033113 	 lr:5e-05
epoch68: train: loss:0.5682733598425369 	 acc:0.7431404958677686 | test: loss:0.5727127318350684 	 acc:0.7390728476821192 	 lr:5e-05
epoch69: train: loss:0.5569592515102103 	 acc:0.743801652892562 | test: loss:0.5498973732752516 	 acc:0.743046357615894 	 lr:2.5e-05
epoch70: train: loss:0.5567355703322355 	 acc:0.7540495867768595 | test: loss:0.5608583754261598 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch71: train: loss:0.5500468508074107 	 acc:0.7444628099173554 | test: loss:0.5497133727105248 	 acc:0.7337748344370861 	 lr:2.5e-05
epoch72: train: loss:0.5503060753483417 	 acc:0.7322314049586777 | test: loss:0.5476305083723257 	 acc:0.7324503311258278 	 lr:2.5e-05
epoch73: train: loss:0.5579724447392235 	 acc:0.7609917355371901 | test: loss:0.5603102980860022 	 acc:0.7496688741721854 	 lr:2.5e-05
epoch74: train: loss:0.5509288420953041 	 acc:0.7629752066115703 | test: loss:0.5578766394924644 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch75: train: loss:0.5488279257924104 	 acc:0.7560330578512396 | test: loss:0.5512890333371446 	 acc:0.7417218543046358 	 lr:2.5e-05
epoch76: train: loss:0.549717746037097 	 acc:0.7563636363636363 | test: loss:0.5512657832625686 	 acc:0.7417218543046358 	 lr:2.5e-05
epoch77: train: loss:0.547753565587288 	 acc:0.7428099173553719 | test: loss:0.5502244001192762 	 acc:0.7099337748344371 	 lr:2.5e-05
epoch78: train: loss:0.5528425838927592 	 acc:0.7646280991735537 | test: loss:0.5507160233346042 	 acc:0.7562913907284768 	 lr:2.5e-05
epoch79: train: loss:0.5508178806304932 	 acc:0.7550413223140496 | test: loss:0.5507930810088354 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch80: train: loss:0.5475601801202317 	 acc:0.7530578512396694 | test: loss:0.5479488337276787 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch81: train: loss:0.5472463259815185 	 acc:0.7646280991735537 | test: loss:0.5479321379535246 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch82: train: loss:0.5463769853804722 	 acc:0.748099173553719 | test: loss:0.5480489014789758 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch83: train: loss:0.556581646450295 	 acc:0.7553719008264462 | test: loss:0.5570497543606537 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch84: train: loss:0.5456872726274916 	 acc:0.7662809917355372 | test: loss:0.5481127780004842 	 acc:0.7483443708609272 	 lr:1.25e-05
epoch85: train: loss:0.543543112809993 	 acc:0.768595041322314 | test: loss:0.547092002669707 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch86: train: loss:0.5416266217310567 	 acc:0.7748760330578512 | test: loss:0.5480225678311278 	 acc:0.752317880794702 	 lr:6.25e-06
epoch87: train: loss:0.5445253695929346 	 acc:0.7537190082644628 | test: loss:0.5461967982993221 	 acc:0.7456953642384105 	 lr:6.25e-06
epoch88: train: loss:0.5417206599889708 	 acc:0.7669421487603306 | test: loss:0.546458719345118 	 acc:0.743046357615894 	 lr:6.25e-06
epoch89: train: loss:0.546791012425068 	 acc:0.7583471074380165 | test: loss:0.5468017009709845 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch90: train: loss:0.5469467173726106 	 acc:0.7676033057851239 | test: loss:0.5469566582843958 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch91: train: loss:0.5433313117736627 	 acc:0.7821487603305786 | test: loss:0.5473082543998364 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch92: train: loss:0.5544515853282834 	 acc:0.7646280991735537 | test: loss:0.5564014456130022 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch93: train: loss:0.5437998329903468 	 acc:0.7702479338842976 | test: loss:0.5490080578437704 	 acc:0.7536423841059603 	 lr:6.25e-06
epoch94: train: loss:0.5475965788147666 	 acc:0.7583471074380165 | test: loss:0.5484380279945222 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch95: train: loss:0.5486465813305752 	 acc:0.7550413223140496 | test: loss:0.5478750664666788 	 acc:0.752317880794702 	 lr:3.125e-06
epoch96: train: loss:0.5445917893441256 	 acc:0.7619834710743801 | test: loss:0.5471413746574857 	 acc:0.7496688741721854 	 lr:3.125e-06
epoch97: train: loss:0.5458114547374819 	 acc:0.7629752066115703 | test: loss:0.545993241095385 	 acc:0.7509933774834437 	 lr:3.125e-06
epoch98: train: loss:0.5450210140756339 	 acc:0.7626446280991735 | test: loss:0.5468199309923791 	 acc:0.752317880794702 	 lr:3.125e-06
epoch99: train: loss:0.5457841232788464 	 acc:0.7709090909090909 | test: loss:0.5467871750427398 	 acc:0.752317880794702 	 lr:3.125e-06
epoch100: train: loss:0.545538870618363 	 acc:0.7702479338842976 | test: loss:0.5469925702012928 	 acc:0.7509933774834437 	 lr:3.125e-06
epoch101: train: loss:0.5441950325729433 	 acc:0.7692561983471075 | test: loss:0.5474999941737446 	 acc:0.7496688741721854 	 lr:3.125e-06
epoch102: train: loss:0.5437219607534487 	 acc:0.7689256198347107 | test: loss:0.547669773070228 	 acc:0.752317880794702 	 lr:3.125e-06
epoch103: train: loss:0.5432924950418393 	 acc:0.76 | test: loss:0.546579815615092 	 acc:0.7470198675496689 	 lr:3.125e-06
epoch104: train: loss:0.5475509015193656 	 acc:0.7656198347107438 | test: loss:0.5476952514901067 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch105: train: loss:0.5419589351819566 	 acc:0.7672727272727272 | test: loss:0.5466053180347216 	 acc:0.7443708609271523 	 lr:1.5625e-06
epoch106: train: loss:0.5437405418364469 	 acc:0.7586776859504132 | test: loss:0.5478687836634403 	 acc:0.7549668874172185 	 lr:1.5625e-06
epoch107: train: loss:0.5506988259189385 	 acc:0.7596694214876033 | test: loss:0.5484945465397361 	 acc:0.7615894039735099 	 lr:1.5625e-06
epoch108: train: loss:0.5447067508618694 	 acc:0.7682644628099173 | test: loss:0.5473671821568975 	 acc:0.752317880794702 	 lr:1.5625e-06
epoch109: train: loss:0.5416836032394535 	 acc:0.771900826446281 | test: loss:0.5480859244896087 	 acc:0.7549668874172185 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_6_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_6_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.840193965356212 	 acc:0.48198347107438017 | test: loss:0.8459590600026364 	 acc:0.4781456953642384 	 lr:0.0001
epoch1: train: loss:0.6614192794374198 	 acc:0.5213223140495867 | test: loss:0.6601441104680497 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6460590210236794 	 acc:0.5312396694214876 | test: loss:0.6413372839523467 	 acc:0.528476821192053 	 lr:0.0001
epoch3: train: loss:0.6456614176892052 	 acc:0.5917355371900826 | test: loss:0.6377801037782075 	 acc:0.5854304635761589 	 lr:0.0001
epoch4: train: loss:0.6433369562448549 	 acc:0.5884297520661157 | test: loss:0.6349227940009919 	 acc:0.6132450331125828 	 lr:0.0001
epoch5: train: loss:0.6358163828889201 	 acc:0.5841322314049586 | test: loss:0.6254931242260712 	 acc:0.590728476821192 	 lr:0.0001
epoch6: train: loss:0.636200466254526 	 acc:0.5338842975206611 | test: loss:0.6271062655164706 	 acc:0.5390728476821192 	 lr:0.0001
epoch7: train: loss:0.6259402762759816 	 acc:0.5930578512396695 | test: loss:0.6176255444027731 	 acc:0.5894039735099338 	 lr:0.0001
epoch8: train: loss:0.6342694465384996 	 acc:0.6634710743801653 | test: loss:0.6330726557220054 	 acc:0.6741721854304635 	 lr:0.0001
epoch9: train: loss:0.6389689676623699 	 acc:0.6591735537190082 | test: loss:0.6281096745011033 	 acc:0.6940397350993377 	 lr:0.0001
epoch10: train: loss:0.6246108011765914 	 acc:0.6790082644628099 | test: loss:0.619701553496304 	 acc:0.6927152317880795 	 lr:0.0001
epoch11: train: loss:0.6287988726166654 	 acc:0.6743801652892562 | test: loss:0.6225903087893858 	 acc:0.695364238410596 	 lr:0.0001
epoch12: train: loss:0.6684634894575955 	 acc:0.6221487603305785 | test: loss:0.66117072144881 	 acc:0.6172185430463576 	 lr:0.0001
epoch13: train: loss:0.6263739664101403 	 acc:0.6776859504132231 | test: loss:0.616582953772008 	 acc:0.6781456953642384 	 lr:0.0001
epoch14: train: loss:0.6052509424115016 	 acc:0.6664462809917355 | test: loss:0.5947914650108641 	 acc:0.695364238410596 	 lr:0.0001
epoch15: train: loss:0.6044087398150736 	 acc:0.6066115702479339 | test: loss:0.6098687037726901 	 acc:0.5867549668874172 	 lr:0.0001
epoch16: train: loss:0.6132011011612317 	 acc:0.6806611570247934 | test: loss:0.6019438408857939 	 acc:0.7112582781456953 	 lr:0.0001
epoch17: train: loss:0.5978326869010925 	 acc:0.6899173553719008 | test: loss:0.590374339258434 	 acc:0.6874172185430464 	 lr:0.0001
epoch18: train: loss:0.6013512742814939 	 acc:0.6135537190082645 | test: loss:0.5980745170290107 	 acc:0.6132450331125828 	 lr:0.0001
epoch19: train: loss:0.6021102551192291 	 acc:0.6128925619834711 | test: loss:0.5922504626362529 	 acc:0.614569536423841 	 lr:0.0001
epoch20: train: loss:0.5843800919509131 	 acc:0.6882644628099174 | test: loss:0.5839780090660449 	 acc:0.6768211920529801 	 lr:0.0001
epoch21: train: loss:0.6097790672562339 	 acc:0.6945454545454546 | test: loss:0.6048550544985083 	 acc:0.7125827814569536 	 lr:0.0001
epoch22: train: loss:0.639018713068371 	 acc:0.6657851239669421 | test: loss:0.64387676826376 	 acc:0.6503311258278146 	 lr:0.0001
epoch23: train: loss:0.585152643810619 	 acc:0.6796694214876033 | test: loss:0.5793041818189305 	 acc:0.6741721854304635 	 lr:0.0001
epoch24: train: loss:0.5823654706813087 	 acc:0.7130578512396695 | test: loss:0.5796431951964928 	 acc:0.7099337748344371 	 lr:0.0001
epoch25: train: loss:0.673265864001818 	 acc:0.6198347107438017 | test: loss:0.6754378327470741 	 acc:0.6066225165562914 	 lr:0.0001
epoch26: train: loss:0.638954033260503 	 acc:0.6704132231404959 | test: loss:0.6244519229756286 	 acc:0.671523178807947 	 lr:0.0001
epoch27: train: loss:0.5929578113358868 	 acc:0.6823140495867769 | test: loss:0.5773498646470885 	 acc:0.6913907284768211 	 lr:0.0001
epoch28: train: loss:0.5756111360581453 	 acc:0.7140495867768595 | test: loss:0.5701830248169552 	 acc:0.7178807947019867 	 lr:0.0001
epoch29: train: loss:0.5830610882349251 	 acc:0.7097520661157025 | test: loss:0.5721204389799509 	 acc:0.7178807947019867 	 lr:0.0001
epoch30: train: loss:0.6172152320018485 	 acc:0.6872727272727273 | test: loss:0.580576503434718 	 acc:0.7258278145695364 	 lr:0.0001
epoch31: train: loss:0.5765023335149465 	 acc:0.6585123966942149 | test: loss:0.5741106923842272 	 acc:0.6635761589403973 	 lr:0.0001
epoch32: train: loss:0.5875575402945526 	 acc:0.725289256198347 | test: loss:0.5743748007231201 	 acc:0.7298013245033113 	 lr:0.0001
epoch33: train: loss:0.5807456187768416 	 acc:0.7325619834710744 | test: loss:0.5792748992016773 	 acc:0.7337748344370861 	 lr:0.0001
epoch34: train: loss:0.5764436464861404 	 acc:0.7262809917355372 | test: loss:0.5740264809684248 	 acc:0.7311258278145696 	 lr:0.0001
epoch35: train: loss:0.5722824722282157 	 acc:0.724297520661157 | test: loss:0.5642482501781539 	 acc:0.7205298013245033 	 lr:5e-05
epoch36: train: loss:0.5947640999486624 	 acc:0.7249586776859505 | test: loss:0.5810219832603505 	 acc:0.7218543046357616 	 lr:5e-05
epoch37: train: loss:0.5673558843431394 	 acc:0.7368595041322314 | test: loss:0.5659459999065526 	 acc:0.743046357615894 	 lr:5e-05
epoch38: train: loss:0.5657730471595259 	 acc:0.6912396694214876 | test: loss:0.5665981072463737 	 acc:0.6794701986754967 	 lr:5e-05
epoch39: train: loss:0.5605691820136772 	 acc:0.7209917355371901 | test: loss:0.5596378567202991 	 acc:0.7218543046357616 	 lr:5e-05
epoch40: train: loss:0.5801924158916001 	 acc:0.7378512396694215 | test: loss:0.5779329854921 	 acc:0.7311258278145696 	 lr:5e-05
epoch41: train: loss:0.5663790692967817 	 acc:0.7051239669421487 | test: loss:0.5620801036721034 	 acc:0.704635761589404 	 lr:5e-05
epoch42: train: loss:0.5590737228354147 	 acc:0.727603305785124 | test: loss:0.5560404378057315 	 acc:0.7245033112582782 	 lr:5e-05
epoch43: train: loss:0.5622936455474412 	 acc:0.7332231404958678 | test: loss:0.5547714141030975 	 acc:0.7311258278145696 	 lr:5e-05
epoch44: train: loss:0.5574294690849367 	 acc:0.7312396694214875 | test: loss:0.5569384630942187 	 acc:0.7165562913907285 	 lr:5e-05
epoch45: train: loss:0.6032941253126161 	 acc:0.7160330578512397 | test: loss:0.5975020590207435 	 acc:0.6993377483443709 	 lr:5e-05
epoch46: train: loss:0.5646872906645467 	 acc:0.6859504132231405 | test: loss:0.5635127874399652 	 acc:0.6940397350993377 	 lr:5e-05
epoch47: train: loss:0.5843174600010076 	 acc:0.7279338842975207 | test: loss:0.5900969794254429 	 acc:0.7298013245033113 	 lr:5e-05
epoch48: train: loss:0.5630282080469052 	 acc:0.7193388429752066 | test: loss:0.5610121371730274 	 acc:0.7072847682119205 	 lr:5e-05
epoch49: train: loss:0.5609939005946325 	 acc:0.7540495867768595 | test: loss:0.5559252270010133 	 acc:0.7470198675496689 	 lr:5e-05
epoch50: train: loss:0.5504601332569911 	 acc:0.7596694214876033 | test: loss:0.5540273638750544 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch51: train: loss:0.5474971826017395 	 acc:0.7428099173553719 | test: loss:0.5527946696376169 	 acc:0.7324503311258278 	 lr:2.5e-05
epoch52: train: loss:0.5534795180825163 	 acc:0.7408264462809917 | test: loss:0.5523995694735192 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch53: train: loss:0.5596670464641792 	 acc:0.7570247933884298 | test: loss:0.5604538670438804 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch54: train: loss:0.5523572571218507 	 acc:0.7471074380165289 | test: loss:0.5510021549186959 	 acc:0.7337748344370861 	 lr:2.5e-05
epoch55: train: loss:0.5475895047975966 	 acc:0.7464462809917355 | test: loss:0.5500407519719459 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch56: train: loss:0.5518905595511444 	 acc:0.7256198347107438 | test: loss:0.552014663124716 	 acc:0.7271523178807947 	 lr:2.5e-05
epoch57: train: loss:0.5534760223735462 	 acc:0.7576859504132232 | test: loss:0.5517917116746208 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch58: train: loss:0.5496144179273242 	 acc:0.7239669421487603 | test: loss:0.5515118111837779 	 acc:0.7258278145695364 	 lr:2.5e-05
epoch59: train: loss:0.5557329611147731 	 acc:0.7619834710743801 | test: loss:0.5609172524995362 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch60: train: loss:0.5567046723089928 	 acc:0.6869421487603306 | test: loss:0.5604548324812327 	 acc:0.6847682119205298 	 lr:2.5e-05
epoch61: train: loss:0.5545856843309954 	 acc:0.7596694214876033 | test: loss:0.5552629676086224 	 acc:0.7576158940397351 	 lr:2.5e-05
epoch62: train: loss:0.5504996307822299 	 acc:0.7573553719008265 | test: loss:0.5473602969914872 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch63: train: loss:0.553688492242955 	 acc:0.7563636363636363 | test: loss:0.5482241447398205 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch64: train: loss:0.5453904445309284 	 acc:0.7408264462809917 | test: loss:0.5468646973174139 	 acc:0.7377483443708609 	 lr:1.25e-05
epoch65: train: loss:0.5520136125816787 	 acc:0.7064462809917356 | test: loss:0.5560576427851291 	 acc:0.695364238410596 	 lr:1.25e-05
epoch66: train: loss:0.5460778876572601 	 acc:0.7580165289256199 | test: loss:0.5458608372321981 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch67: train: loss:0.5515479404670148 	 acc:0.7613223140495867 | test: loss:0.5486193579553769 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch68: train: loss:0.5569917482975101 	 acc:0.7553719008264462 | test: loss:0.5577113677334312 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch69: train: loss:0.5557472813622025 	 acc:0.7447933884297521 | test: loss:0.5495918196558163 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch70: train: loss:0.5479707673561475 	 acc:0.7626446280991735 | test: loss:0.548558094485706 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch71: train: loss:0.5472099481929432 	 acc:0.7537190082644628 | test: loss:0.5460825876684379 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch72: train: loss:0.5495148797468705 	 acc:0.7421487603305785 | test: loss:0.5451573061627268 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch73: train: loss:0.5521387447995588 	 acc:0.763305785123967 | test: loss:0.5527887736724701 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch74: train: loss:0.5491916544772377 	 acc:0.7596694214876033 | test: loss:0.5552034288052692 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch75: train: loss:0.5451540576327931 	 acc:0.7623140495867768 | test: loss:0.5475549394721227 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch76: train: loss:0.5532304187254472 	 acc:0.7573553719008265 | test: loss:0.5533506305012482 	 acc:0.752317880794702 	 lr:1.25e-05
epoch77: train: loss:0.5480788251388171 	 acc:0.763305785123967 | test: loss:0.5448495838815802 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch78: train: loss:0.5594131177516023 	 acc:0.7639669421487604 | test: loss:0.5539245913360292 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch79: train: loss:0.5506749369093209 	 acc:0.7623140495867768 | test: loss:0.548316285152309 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch80: train: loss:0.5459319073700708 	 acc:0.7497520661157024 | test: loss:0.5434629670831541 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch81: train: loss:0.5461339965339534 	 acc:0.7732231404958678 | test: loss:0.5445423201219926 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch82: train: loss:0.5459823854698622 	 acc:0.7510743801652893 | test: loss:0.5436729660886802 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch83: train: loss:0.549732524875767 	 acc:0.751404958677686 | test: loss:0.5457306028991346 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch84: train: loss:0.5441893529300847 	 acc:0.7557024793388429 | test: loss:0.5434061243834085 	 acc:0.7417218543046358 	 lr:1.25e-05
epoch85: train: loss:0.5477487616105513 	 acc:0.7709090909090909 | test: loss:0.5465359010443782 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch86: train: loss:0.5441443497484381 	 acc:0.7748760330578512 | test: loss:0.5477239352188363 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch87: train: loss:0.5438397818557487 	 acc:0.7414876033057851 | test: loss:0.5439616857774999 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch88: train: loss:0.5386178857038829 	 acc:0.7603305785123967 | test: loss:0.5428732533328581 	 acc:0.743046357615894 	 lr:1.25e-05
epoch89: train: loss:0.5428241150438293 	 acc:0.7656198347107438 | test: loss:0.5418182070681591 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch90: train: loss:0.5438891917614898 	 acc:0.752396694214876 | test: loss:0.5419706009081657 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch91: train: loss:0.5400486747095408 	 acc:0.768595041322314 | test: loss:0.5424158734201595 	 acc:0.752317880794702 	 lr:1.25e-05
epoch92: train: loss:0.5535818741144227 	 acc:0.764297520661157 | test: loss:0.5543720872986395 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch93: train: loss:0.544175910811779 	 acc:0.7682644628099173 | test: loss:0.54455276096104 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch94: train: loss:0.5465029620533147 	 acc:0.7639669421487604 | test: loss:0.5473967908233996 	 acc:0.7602649006622516 	 lr:1.25e-05
epoch95: train: loss:0.548405000197986 	 acc:0.7606611570247934 | test: loss:0.5445196678306883 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch96: train: loss:0.542869803747855 	 acc:0.7662809917355372 | test: loss:0.5427160105168425 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch97: train: loss:0.5420035516132008 	 acc:0.7623140495867768 | test: loss:0.5413801790073218 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch98: train: loss:0.5513326938290242 	 acc:0.764297520661157 | test: loss:0.548681412706312 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch99: train: loss:0.5401260339327095 	 acc:0.7679338842975206 | test: loss:0.5411732460489336 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch100: train: loss:0.5420913650575748 	 acc:0.7728925619834711 | test: loss:0.543561342694112 	 acc:0.7589403973509934 	 lr:6.25e-06
epoch101: train: loss:0.5414115219273843 	 acc:0.7616528925619834 | test: loss:0.5428057593225644 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch102: train: loss:0.5416586843797984 	 acc:0.7755371900826447 | test: loss:0.5435166021056522 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch103: train: loss:0.5471240552397799 	 acc:0.7692561983471075 | test: loss:0.5465824226669918 	 acc:0.7642384105960265 	 lr:6.25e-06
epoch104: train: loss:0.5442543280814305 	 acc:0.7467768595041322 | test: loss:0.5419856133050477 	 acc:0.7470198675496689 	 lr:6.25e-06
epoch105: train: loss:0.5435535823806258 	 acc:0.7761983471074381 | test: loss:0.5466245761770286 	 acc:0.766887417218543 	 lr:6.25e-06
epoch106: train: loss:0.5393712263461972 	 acc:0.7662809917355372 | test: loss:0.5430774002675189 	 acc:0.7682119205298014 	 lr:3.125e-06
epoch107: train: loss:0.547887348931683 	 acc:0.7672727272727272 | test: loss:0.5444028330954495 	 acc:0.7642384105960265 	 lr:3.125e-06
epoch108: train: loss:0.5405739895174326 	 acc:0.7702479338842976 | test: loss:0.5425954828988637 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch109: train: loss:0.5386629887848846 	 acc:0.7771900826446281 | test: loss:0.5441851304066891 	 acc:0.7655629139072848 	 lr:3.125e-06
epoch110: train: loss:0.5390679051265244 	 acc:0.7702479338842976 | test: loss:0.5424369612276949 	 acc:0.7536423841059603 	 lr:3.125e-06
epoch111: train: loss:0.5421618751651984 	 acc:0.7623140495867768 | test: loss:0.543733255673718 	 acc:0.7576158940397351 	 lr:3.125e-06
epoch112: train: loss:0.537825468492902 	 acc:0.7712396694214876 | test: loss:0.5425309541209644 	 acc:0.7562913907284768 	 lr:1.5625e-06
epoch113: train: loss:0.5397133774402713 	 acc:0.7748760330578512 | test: loss:0.5418645705608343 	 acc:0.7576158940397351 	 lr:1.5625e-06
epoch114: train: loss:0.5431114240914336 	 acc:0.7586776859504132 | test: loss:0.5422877596703586 	 acc:0.7642384105960265 	 lr:1.5625e-06
epoch115: train: loss:0.541165665062991 	 acc:0.7679338842975206 | test: loss:0.5426050013264283 	 acc:0.7655629139072848 	 lr:1.5625e-06
epoch116: train: loss:0.5366691627384217 	 acc:0.7725619834710744 | test: loss:0.5422202808967489 	 acc:0.7629139072847683 	 lr:1.5625e-06
epoch117: train: loss:0.5383804979994278 	 acc:0.7758677685950414 | test: loss:0.5429546690934541 	 acc:0.7642384105960265 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_7_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_7_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.802833567493218 	 acc:0.4859504132231405 | test: loss:0.8086658925410138 	 acc:0.4847682119205298 	 lr:0.0001
epoch1: train: loss:0.6708264863195498 	 acc:0.5213223140495867 | test: loss:0.6694366717970135 	 acc:0.5218543046357615 	 lr:0.0001
epoch2: train: loss:0.6543809542380089 	 acc:0.6052892561983471 | test: loss:0.6435557499626614 	 acc:0.6066225165562914 	 lr:0.0001
epoch3: train: loss:0.6413192753949442 	 acc:0.5639669421487603 | test: loss:0.6346819943939613 	 acc:0.5496688741721855 	 lr:0.0001
epoch4: train: loss:0.6387012058644256 	 acc:0.5831404958677686 | test: loss:0.6308591081606632 	 acc:0.5801324503311258 	 lr:0.0001
epoch5: train: loss:0.6388262203114092 	 acc:0.620495867768595 | test: loss:0.627789886345137 	 acc:0.6476821192052981 	 lr:0.0001
epoch6: train: loss:0.6304234636519566 	 acc:0.5685950413223141 | test: loss:0.6194712884378749 	 acc:0.5708609271523178 	 lr:0.0001
epoch7: train: loss:0.6242018787131822 	 acc:0.6046280991735538 | test: loss:0.614252878735397 	 acc:0.623841059602649 	 lr:0.0001
epoch8: train: loss:0.6221607819076412 	 acc:0.6740495867768596 | test: loss:0.622840079013875 	 acc:0.6622516556291391 	 lr:0.0001
epoch9: train: loss:0.6477908883607092 	 acc:0.6505785123966942 | test: loss:0.6424105472912062 	 acc:0.6569536423841059 	 lr:0.0001
epoch10: train: loss:0.6302016306908662 	 acc:0.6862809917355371 | test: loss:0.6270370054718675 	 acc:0.6821192052980133 	 lr:0.0001
epoch11: train: loss:0.6158954051900501 	 acc:0.6849586776859504 | test: loss:0.6064002243888299 	 acc:0.6980132450331126 	 lr:0.0001
epoch12: train: loss:0.650845384105178 	 acc:0.6476033057851239 | test: loss:0.6426747649710699 	 acc:0.6609271523178808 	 lr:0.0001
epoch13: train: loss:0.6373755855796751 	 acc:0.6548760330578512 | test: loss:0.6281768501989099 	 acc:0.6649006622516557 	 lr:0.0001
epoch14: train: loss:0.620378069247096 	 acc:0.6846280991735537 | test: loss:0.6202309492408045 	 acc:0.6874172185430464 	 lr:0.0001
epoch15: train: loss:0.5914199145372249 	 acc:0.6568595041322314 | test: loss:0.5956636937248786 	 acc:0.614569536423841 	 lr:0.0001
epoch16: train: loss:0.6117512885204032 	 acc:0.6935537190082645 | test: loss:0.6064909357108818 	 acc:0.6913907284768211 	 lr:0.0001
epoch17: train: loss:0.5879377900667427 	 acc:0.6760330578512397 | test: loss:0.5832295023052898 	 acc:0.6728476821192053 	 lr:0.0001
epoch18: train: loss:0.5878958083381337 	 acc:0.6393388429752066 | test: loss:0.5913991440210911 	 acc:0.6198675496688741 	 lr:0.0001
epoch19: train: loss:0.5997529037727797 	 acc:0.5986776859504133 | test: loss:0.5954011115017316 	 acc:0.609271523178808 	 lr:0.0001
epoch20: train: loss:0.5854381294881017 	 acc:0.6274380165289256 | test: loss:0.5897692952724483 	 acc:0.6172185430463576 	 lr:0.0001
epoch21: train: loss:0.5936134291089271 	 acc:0.7206611570247934 | test: loss:0.5953610517331307 	 acc:0.7192052980132451 	 lr:0.0001
epoch22: train: loss:0.6065771610480695 	 acc:0.7074380165289256 | test: loss:0.6122246271727101 	 acc:0.7033112582781457 	 lr:0.0001
epoch23: train: loss:0.5842192088868007 	 acc:0.7183471074380166 | test: loss:0.5812750619768307 	 acc:0.7192052980132451 	 lr:0.0001
epoch24: train: loss:0.5695609924812948 	 acc:0.7411570247933884 | test: loss:0.5680976979779881 	 acc:0.7377483443708609 	 lr:0.0001
epoch25: train: loss:0.627550852909561 	 acc:0.6852892561983471 | test: loss:0.6282974627633758 	 acc:0.6754966887417219 	 lr:0.0001
epoch26: train: loss:0.6489080208195143 	 acc:0.6581818181818182 | test: loss:0.639128203502554 	 acc:0.6516556291390728 	 lr:0.0001
epoch27: train: loss:0.5779331668152297 	 acc:0.7213223140495868 | test: loss:0.567745427974802 	 acc:0.7403973509933774 	 lr:0.0001
epoch28: train: loss:0.5657479309838666 	 acc:0.7150413223140496 | test: loss:0.5606235091259938 	 acc:0.7271523178807947 	 lr:0.0001
epoch29: train: loss:0.567551007329925 	 acc:0.7130578512396695 | test: loss:0.5585076533405986 	 acc:0.7311258278145696 	 lr:0.0001
epoch30: train: loss:0.5935491506915447 	 acc:0.7160330578512397 | test: loss:0.5629404371147914 	 acc:0.7403973509933774 	 lr:0.0001
epoch31: train: loss:0.572027343009129 	 acc:0.6512396694214876 | test: loss:0.5716535985075085 	 acc:0.6556291390728477 	 lr:0.0001
epoch32: train: loss:0.5729311230754064 	 acc:0.7414876033057851 | test: loss:0.5633915095139813 	 acc:0.7562913907284768 	 lr:0.0001
epoch33: train: loss:0.5678608079390093 	 acc:0.7517355371900827 | test: loss:0.5621388575099162 	 acc:0.7536423841059603 	 lr:0.0001
epoch34: train: loss:0.5624110357032335 	 acc:0.7365289256198347 | test: loss:0.5523221010403917 	 acc:0.7350993377483444 	 lr:0.0001
epoch35: train: loss:0.5728375865211172 	 acc:0.6760330578512397 | test: loss:0.5824714299069335 	 acc:0.6211920529801325 	 lr:0.0001
epoch36: train: loss:0.6058404333335309 	 acc:0.5871074380165289 | test: loss:0.6231854937723931 	 acc:0.5655629139072847 	 lr:0.0001
epoch37: train: loss:0.563002140305259 	 acc:0.7153719008264463 | test: loss:0.5581538391429067 	 acc:0.7231788079470198 	 lr:0.0001
epoch38: train: loss:0.574285225809113 	 acc:0.6472727272727272 | test: loss:0.5723086186592152 	 acc:0.6410596026490066 	 lr:0.0001
epoch39: train: loss:0.5597633746438775 	 acc:0.691900826446281 | test: loss:0.5611213989605177 	 acc:0.6874172185430464 	 lr:0.0001
epoch40: train: loss:0.5624399528818682 	 acc:0.7355371900826446 | test: loss:0.5515491089284025 	 acc:0.7364238410596027 	 lr:0.0001
epoch41: train: loss:0.5736069711968919 	 acc:0.6522314049586777 | test: loss:0.5844223003513765 	 acc:0.6185430463576159 	 lr:0.0001
epoch42: train: loss:0.5753944532733318 	 acc:0.6436363636363637 | test: loss:0.5801437597401095 	 acc:0.6264900662251656 	 lr:0.0001
epoch43: train: loss:0.5559584198116271 	 acc:0.7226446280991735 | test: loss:0.5446708010521946 	 acc:0.7470198675496689 	 lr:0.0001
epoch44: train: loss:0.5577416576235747 	 acc:0.692892561983471 | test: loss:0.5681258121073641 	 acc:0.6569536423841059 	 lr:0.0001
epoch45: train: loss:0.5490544205066586 	 acc:0.7593388429752066 | test: loss:0.5477963899145063 	 acc:0.7589403973509934 	 lr:0.0001
epoch46: train: loss:0.5697371418811072 	 acc:0.6588429752066116 | test: loss:0.5603063161009985 	 acc:0.6874172185430464 	 lr:0.0001
epoch47: train: loss:0.5504177616844492 	 acc:0.739504132231405 | test: loss:0.5493601292174384 	 acc:0.7536423841059603 	 lr:0.0001
epoch48: train: loss:0.5530481394657418 	 acc:0.7368595041322314 | test: loss:0.5500860931068067 	 acc:0.7178807947019867 	 lr:0.0001
epoch49: train: loss:0.5582695250471761 	 acc:0.7028099173553719 | test: loss:0.5689456218125805 	 acc:0.6768211920529801 	 lr:0.0001
epoch50: train: loss:0.5382687546399014 	 acc:0.7748760330578512 | test: loss:0.5457491752327672 	 acc:0.7629139072847683 	 lr:5e-05
epoch51: train: loss:0.5377824814260499 	 acc:0.7828099173553719 | test: loss:0.5464092121219003 	 acc:0.7536423841059603 	 lr:5e-05
epoch52: train: loss:0.5333348634617388 	 acc:0.7510743801652893 | test: loss:0.533855283339292 	 acc:0.7483443708609272 	 lr:5e-05
epoch53: train: loss:0.5342789793999727 	 acc:0.7742148760330578 | test: loss:0.5411459888054045 	 acc:0.7642384105960265 	 lr:5e-05
epoch54: train: loss:0.539176100009729 	 acc:0.7722314049586777 | test: loss:0.5377174250337462 	 acc:0.7721854304635761 	 lr:5e-05
epoch55: train: loss:0.5420970301588705 	 acc:0.7715702479338843 | test: loss:0.5422320091961236 	 acc:0.7615894039735099 	 lr:5e-05
epoch56: train: loss:0.5428898019238937 	 acc:0.7107438016528925 | test: loss:0.546569807403135 	 acc:0.7059602649006622 	 lr:5e-05
epoch57: train: loss:0.5359695563237529 	 acc:0.7831404958677686 | test: loss:0.5380919356219815 	 acc:0.7695364238410596 	 lr:5e-05
epoch58: train: loss:0.5294554183502828 	 acc:0.7666115702479339 | test: loss:0.5347449828457359 	 acc:0.7642384105960265 	 lr:5e-05
epoch59: train: loss:0.5294195765109102 	 acc:0.7765289256198347 | test: loss:0.5401703764271263 	 acc:0.7615894039735099 	 lr:2.5e-05
epoch60: train: loss:0.5341374466241884 	 acc:0.730909090909091 | test: loss:0.5394978302993522 	 acc:0.7324503311258278 	 lr:2.5e-05
epoch61: train: loss:0.5265766881714182 	 acc:0.7814876033057852 | test: loss:0.533371700751071 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch62: train: loss:0.535882717420247 	 acc:0.7917355371900826 | test: loss:0.5370044505359322 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch63: train: loss:0.5371820303231232 	 acc:0.7854545454545454 | test: loss:0.5359031498037427 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch64: train: loss:0.5407862208106301 	 acc:0.7811570247933884 | test: loss:0.5398656774040879 	 acc:0.7708609271523179 	 lr:2.5e-05
epoch65: train: loss:0.5272236088287732 	 acc:0.7682644628099173 | test: loss:0.5309703061912233 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch66: train: loss:0.5306199411518318 	 acc:0.7897520661157025 | test: loss:0.5358049005072638 	 acc:0.766887417218543 	 lr:2.5e-05
epoch67: train: loss:0.5301013589792015 	 acc:0.7834710743801653 | test: loss:0.5322468482895403 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch68: train: loss:0.5444263694109011 	 acc:0.7801652892561983 | test: loss:0.5580467720694889 	 acc:0.7536423841059603 	 lr:2.5e-05
epoch69: train: loss:0.5335286136698132 	 acc:0.7709090909090909 | test: loss:0.5301518406299566 	 acc:0.766887417218543 	 lr:2.5e-05
epoch70: train: loss:0.5321028646949895 	 acc:0.7854545454545454 | test: loss:0.5366988638378927 	 acc:0.776158940397351 	 lr:2.5e-05
epoch71: train: loss:0.527010876166919 	 acc:0.7652892561983471 | test: loss:0.530022332289361 	 acc:0.7695364238410596 	 lr:2.5e-05
epoch72: train: loss:0.5264600693489895 	 acc:0.7795041322314049 | test: loss:0.5278892967085175 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch73: train: loss:0.528848102191263 	 acc:0.7871074380165289 | test: loss:0.5377201074006541 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch74: train: loss:0.5280928500033607 	 acc:0.7937190082644628 | test: loss:0.541205973262029 	 acc:0.7774834437086092 	 lr:2.5e-05
epoch75: train: loss:0.5226484194865897 	 acc:0.7847933884297521 | test: loss:0.5299252618227573 	 acc:0.766887417218543 	 lr:2.5e-05
epoch76: train: loss:0.5254021272974566 	 acc:0.7851239669421488 | test: loss:0.532572814170888 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch77: train: loss:0.5249252048602774 	 acc:0.7662809917355372 | test: loss:0.5294283996354665 	 acc:0.7576158940397351 	 lr:2.5e-05
epoch78: train: loss:0.5237166852990458 	 acc:0.7761983471074381 | test: loss:0.5289784661981444 	 acc:0.766887417218543 	 lr:2.5e-05
epoch79: train: loss:0.5231701330902163 	 acc:0.7788429752066116 | test: loss:0.5278272234840898 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch80: train: loss:0.5231121457312717 	 acc:0.7854545454545454 | test: loss:0.5270444653681572 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch81: train: loss:0.5224808567417555 	 acc:0.7976859504132231 | test: loss:0.5280129623728872 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch82: train: loss:0.5225627077118424 	 acc:0.7880991735537191 | test: loss:0.5274246643710611 	 acc:0.7682119205298014 	 lr:1.25e-05
epoch83: train: loss:0.5269904740585768 	 acc:0.7861157024793388 | test: loss:0.5306536826076886 	 acc:0.766887417218543 	 lr:1.25e-05
epoch84: train: loss:0.5206991453604265 	 acc:0.7894214876033058 | test: loss:0.526551198169885 	 acc:0.7682119205298014 	 lr:1.25e-05
epoch85: train: loss:0.5210055580020936 	 acc:0.7993388429752066 | test: loss:0.5313442716535354 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch86: train: loss:0.5155324640352864 	 acc:0.7970247933884298 | test: loss:0.5278278672932 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch87: train: loss:0.5213456227562644 	 acc:0.7586776859504132 | test: loss:0.5281580384993395 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch88: train: loss:0.5157649430755742 	 acc:0.7841322314049587 | test: loss:0.5271536124463113 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch89: train: loss:0.5205007273106536 	 acc:0.7907438016528926 | test: loss:0.526174446999632 	 acc:0.7801324503311259 	 lr:1.25e-05
epoch90: train: loss:0.5209370760484175 	 acc:0.792396694214876 | test: loss:0.5248021035004925 	 acc:0.7708609271523179 	 lr:1.25e-05
epoch91: train: loss:0.5196536702755069 	 acc:0.8016528925619835 | test: loss:0.5280950946523654 	 acc:0.7801324503311259 	 lr:1.25e-05
epoch92: train: loss:0.5322758595017362 	 acc:0.7963636363636364 | test: loss:0.5431390678645759 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch93: train: loss:0.5185621511837668 	 acc:0.7880991735537191 | test: loss:0.5285075266629655 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch94: train: loss:0.5211544862069374 	 acc:0.7920661157024793 | test: loss:0.5306730880642568 	 acc:0.7708609271523179 	 lr:1.25e-05
epoch95: train: loss:0.5229040108830476 	 acc:0.7775206611570248 | test: loss:0.5266696798880368 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch96: train: loss:0.5200187930193815 	 acc:0.7890909090909091 | test: loss:0.526702306760068 	 acc:0.776158940397351 	 lr:1.25e-05
epoch97: train: loss:0.518842065275208 	 acc:0.7884297520661157 | test: loss:0.5243220765859086 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch98: train: loss:0.5267488158832897 	 acc:0.7880991735537191 | test: loss:0.5316918412581185 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch99: train: loss:0.5147103235839813 	 acc:0.7943801652892561 | test: loss:0.5243981908488747 	 acc:0.776158940397351 	 lr:6.25e-06
epoch100: train: loss:0.5189633676828431 	 acc:0.7920661157024793 | test: loss:0.5278555942686978 	 acc:0.7748344370860927 	 lr:6.25e-06
epoch101: train: loss:0.5182033732311785 	 acc:0.7861157024793388 | test: loss:0.5257065550380985 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch102: train: loss:0.5176028422678798 	 acc:0.7947107438016529 | test: loss:0.5274101575478812 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch103: train: loss:0.5197332737662576 	 acc:0.795702479338843 | test: loss:0.5282797142369857 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch104: train: loss:0.5201620496600128 	 acc:0.7947107438016529 | test: loss:0.5261040664666535 	 acc:0.7708609271523179 	 lr:3.125e-06
epoch105: train: loss:0.516747398553801 	 acc:0.7937190082644628 | test: loss:0.5256465507658902 	 acc:0.776158940397351 	 lr:3.125e-06
epoch106: train: loss:0.516050042968151 	 acc:0.7990082644628099 | test: loss:0.5287753947523256 	 acc:0.776158940397351 	 lr:3.125e-06
epoch107: train: loss:0.5215790836870178 	 acc:0.7920661157024793 | test: loss:0.527216766606893 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch108: train: loss:0.5156810520798707 	 acc:0.7993388429752066 | test: loss:0.5267593436683251 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch109: train: loss:0.5134108409014615 	 acc:0.811900826446281 | test: loss:0.5276572581158568 	 acc:0.7748344370860927 	 lr:3.125e-06
epoch110: train: loss:0.5146807017602211 	 acc:0.7983471074380165 | test: loss:0.5251012343444572 	 acc:0.7748344370860927 	 lr:1.5625e-06
epoch111: train: loss:0.5192451419515058 	 acc:0.7884297520661157 | test: loss:0.5259193597250427 	 acc:0.776158940397351 	 lr:1.5625e-06
epoch112: train: loss:0.5144620891247899 	 acc:0.7983471074380165 | test: loss:0.5258394442646709 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch113: train: loss:0.5157768537190335 	 acc:0.795702479338843 | test: loss:0.5252259933395891 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch114: train: loss:0.5195603566130331 	 acc:0.7867768595041322 | test: loss:0.5255689439394616 	 acc:0.7721854304635761 	 lr:1.5625e-06
epoch115: train: loss:0.5182951794379999 	 acc:0.7976859504132231 | test: loss:0.5257350860842016 	 acc:0.776158940397351 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_8_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_8_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.658942459673921 	 acc:0.5272727272727272 | test: loss:0.6521610044485686 	 acc:0.543046357615894 	 lr:0.0001
epoch1: train: loss:0.6483430859274115 	 acc:0.5302479338842975 | test: loss:0.6430364702711042 	 acc:0.5298013245033113 	 lr:0.0001
epoch2: train: loss:0.6451240794717773 	 acc:0.5262809917355372 | test: loss:0.6419367629960673 	 acc:0.5231788079470199 	 lr:0.0001
epoch3: train: loss:0.659931289263008 	 acc:0.6188429752066116 | test: loss:0.6466140136813486 	 acc:0.6397350993377483 	 lr:0.0001
epoch4: train: loss:0.6436014382701275 	 acc:0.6347107438016529 | test: loss:0.631766250828244 	 acc:0.6754966887417219 	 lr:0.0001
epoch5: train: loss:0.6369734421446304 	 acc:0.6386776859504132 | test: loss:0.6213106870651245 	 acc:0.6847682119205298 	 lr:0.0001
epoch6: train: loss:0.6299085708295018 	 acc:0.5444628099173554 | test: loss:0.6202674898090742 	 acc:0.5496688741721855 	 lr:0.0001
epoch7: train: loss:0.6169645472400445 	 acc:0.631404958677686 | test: loss:0.6051154991648845 	 acc:0.6529801324503312 	 lr:0.0001
epoch8: train: loss:0.6342521716543466 	 acc:0.6657851239669421 | test: loss:0.6360226650901188 	 acc:0.6821192052980133 	 lr:0.0001
epoch9: train: loss:0.6416732410951094 	 acc:0.6614876033057852 | test: loss:0.6367752466770198 	 acc:0.6794701986754967 	 lr:0.0001
epoch10: train: loss:0.6121688972622895 	 acc:0.7054545454545454 | test: loss:0.6074119147875451 	 acc:0.7231788079470198 	 lr:0.0001
epoch11: train: loss:0.6092130433823452 	 acc:0.699504132231405 | test: loss:0.6000518021204614 	 acc:0.7205298013245033 	 lr:0.0001
epoch12: train: loss:0.6112477505896702 	 acc:0.6905785123966942 | test: loss:0.5994046686500903 	 acc:0.7178807947019867 	 lr:0.0001
epoch13: train: loss:0.5946574634362843 	 acc:0.6932231404958678 | test: loss:0.5870802909333185 	 acc:0.7072847682119205 	 lr:0.0001
epoch14: train: loss:0.5869077764266779 	 acc:0.6938842975206612 | test: loss:0.58022451740227 	 acc:0.7086092715231788 	 lr:0.0001
epoch15: train: loss:0.5904249228524767 	 acc:0.6684297520661157 | test: loss:0.5838376990217247 	 acc:0.6635761589403973 	 lr:0.0001
epoch16: train: loss:0.5996653710909127 	 acc:0.7021487603305785 | test: loss:0.5900369393904478 	 acc:0.7284768211920529 	 lr:0.0001
epoch17: train: loss:0.5939363540499664 	 acc:0.6333884297520661 | test: loss:0.595216895255032 	 acc:0.6132450331125828 	 lr:0.0001
epoch18: train: loss:0.5818994049789492 	 acc:0.6522314049586777 | test: loss:0.5848714942963708 	 acc:0.633112582781457 	 lr:0.0001
epoch19: train: loss:0.5932225385382156 	 acc:0.6188429752066116 | test: loss:0.5839398279095328 	 acc:0.633112582781457 	 lr:0.0001
epoch20: train: loss:0.5767758652198414 	 acc:0.6614876033057852 | test: loss:0.5870238488083643 	 acc:0.633112582781457 	 lr:0.0001
epoch21: train: loss:0.5762814961780202 	 acc:0.7414876033057851 | test: loss:0.5726974915194986 	 acc:0.7311258278145696 	 lr:5e-05
epoch22: train: loss:0.5777134294943376 	 acc:0.7322314049586777 | test: loss:0.5872640173956258 	 acc:0.7284768211920529 	 lr:5e-05
epoch23: train: loss:0.5704136728452257 	 acc:0.7358677685950413 | test: loss:0.5693892578415524 	 acc:0.7284768211920529 	 lr:5e-05
epoch24: train: loss:0.5649788018297558 	 acc:0.7133884297520661 | test: loss:0.5646310793642966 	 acc:0.7099337748344371 	 lr:5e-05
epoch25: train: loss:0.5943839697995462 	 acc:0.7229752066115702 | test: loss:0.6002974477824786 	 acc:0.7059602649006622 	 lr:5e-05
epoch26: train: loss:0.586001031063805 	 acc:0.6317355371900827 | test: loss:0.5903511185519743 	 acc:0.609271523178808 	 lr:5e-05
epoch27: train: loss:0.5796204446761076 	 acc:0.7226446280991735 | test: loss:0.582504260461062 	 acc:0.7271523178807947 	 lr:5e-05
epoch28: train: loss:0.5754118593665194 	 acc:0.6476033057851239 | test: loss:0.5670802113236181 	 acc:0.6781456953642384 	 lr:5e-05
epoch29: train: loss:0.5644812039304371 	 acc:0.7352066115702479 | test: loss:0.558311369166469 	 acc:0.7390728476821192 	 lr:5e-05
epoch30: train: loss:0.5567606710008354 	 acc:0.7418181818181818 | test: loss:0.5559235530183805 	 acc:0.7258278145695364 	 lr:5e-05
epoch31: train: loss:0.5620809508946316 	 acc:0.7566942148760331 | test: loss:0.5589871036295859 	 acc:0.7496688741721854 	 lr:5e-05
epoch32: train: loss:0.5642346902918225 	 acc:0.740495867768595 | test: loss:0.558857864970403 	 acc:0.7364238410596027 	 lr:5e-05
epoch33: train: loss:0.5582913078552435 	 acc:0.7378512396694215 | test: loss:0.5551517043682124 	 acc:0.7509933774834437 	 lr:5e-05
epoch34: train: loss:0.5571005557194229 	 acc:0.7520661157024794 | test: loss:0.5522517806646839 	 acc:0.743046357615894 	 lr:5e-05
epoch35: train: loss:0.5565305675159801 	 acc:0.7355371900826446 | test: loss:0.5530527866439314 	 acc:0.7271523178807947 	 lr:5e-05
epoch36: train: loss:0.5743284312752652 	 acc:0.7471074380165289 | test: loss:0.5619378294376348 	 acc:0.7483443708609272 	 lr:5e-05
epoch37: train: loss:0.5661697510057244 	 acc:0.751404958677686 | test: loss:0.5612278136196516 	 acc:0.7483443708609272 	 lr:5e-05
epoch38: train: loss:0.551312655161235 	 acc:0.7216528925619835 | test: loss:0.5519483683914538 	 acc:0.7112582781456953 	 lr:5e-05
epoch39: train: loss:0.548707593472536 	 acc:0.7295867768595041 | test: loss:0.5484951129022813 	 acc:0.7311258278145696 	 lr:5e-05
epoch40: train: loss:0.5562170726602728 	 acc:0.7613223140495867 | test: loss:0.5486671798276586 	 acc:0.7496688741721854 	 lr:5e-05
epoch41: train: loss:0.5508102390588807 	 acc:0.7424793388429752 | test: loss:0.5497824066522106 	 acc:0.7245033112582782 	 lr:5e-05
epoch42: train: loss:0.5460487185825001 	 acc:0.7411570247933884 | test: loss:0.5474802622731948 	 acc:0.7205298013245033 	 lr:5e-05
epoch43: train: loss:0.5509896196806726 	 acc:0.7570247933884298 | test: loss:0.5484871366166121 	 acc:0.7456953642384105 	 lr:5e-05
epoch44: train: loss:0.5540264582436932 	 acc:0.6866115702479338 | test: loss:0.5562716505385392 	 acc:0.6887417218543046 	 lr:5e-05
epoch45: train: loss:0.5998571693207607 	 acc:0.7163636363636363 | test: loss:0.5964322683037512 	 acc:0.7086092715231788 	 lr:5e-05
epoch46: train: loss:0.5562384203445813 	 acc:0.6829752066115703 | test: loss:0.5617782758561192 	 acc:0.6741721854304635 	 lr:5e-05
epoch47: train: loss:0.580562295519616 	 acc:0.7381818181818182 | test: loss:0.5858124229292206 	 acc:0.7192052980132451 	 lr:5e-05
epoch48: train: loss:0.5505740971407614 	 acc:0.7087603305785124 | test: loss:0.5592387847553025 	 acc:0.6781456953642384 	 lr:5e-05
epoch49: train: loss:0.5385757680766838 	 acc:0.7676033057851239 | test: loss:0.5394334946247126 	 acc:0.766887417218543 	 lr:2.5e-05
epoch50: train: loss:0.5353506352290635 	 acc:0.7699173553719009 | test: loss:0.5415771700688545 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch51: train: loss:0.5345261825017693 	 acc:0.7497520661157024 | test: loss:0.5413060856181264 	 acc:0.743046357615894 	 lr:2.5e-05
epoch52: train: loss:0.5360805948312618 	 acc:0.763305785123967 | test: loss:0.5389059091245891 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch53: train: loss:0.539034289348224 	 acc:0.7791735537190083 | test: loss:0.5419666602911538 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch54: train: loss:0.5364928468396841 	 acc:0.7699173553719009 | test: loss:0.5386770232623777 	 acc:0.7509933774834437 	 lr:2.5e-05
epoch55: train: loss:0.5333816147244667 	 acc:0.775206611570248 | test: loss:0.5372009002609758 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch56: train: loss:0.5376231225462984 	 acc:0.7325619834710744 | test: loss:0.5403581207951174 	 acc:0.7350993377483444 	 lr:2.5e-05
epoch57: train: loss:0.5343239379520258 	 acc:0.7847933884297521 | test: loss:0.5378453958113462 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch58: train: loss:0.5310929706471026 	 acc:0.7619834710743801 | test: loss:0.535549313501017 	 acc:0.7496688741721854 	 lr:2.5e-05
epoch59: train: loss:0.5311733979036 	 acc:0.7788429752066116 | test: loss:0.5401518618823676 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch60: train: loss:0.5380045957604715 	 acc:0.7292561983471074 | test: loss:0.5448949292795547 	 acc:0.7152317880794702 	 lr:2.5e-05
epoch61: train: loss:0.5343911386127315 	 acc:0.7857851239669421 | test: loss:0.5374777992829581 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch62: train: loss:0.5439812328007596 	 acc:0.7847933884297521 | test: loss:0.5392445478218281 	 acc:0.7788079470198676 	 lr:2.5e-05
epoch63: train: loss:0.5401661757989363 	 acc:0.7788429752066116 | test: loss:0.5376651792336773 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch64: train: loss:0.5516874867628428 	 acc:0.7699173553719009 | test: loss:0.548656214309844 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch65: train: loss:0.5311821183094309 	 acc:0.7570247933884298 | test: loss:0.5338580249950585 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch66: train: loss:0.5278496617128041 	 acc:0.7814876033057852 | test: loss:0.5322415107922838 	 acc:0.7615894039735099 	 lr:1.25e-05
epoch67: train: loss:0.5300182936605343 	 acc:0.7801652892561983 | test: loss:0.5330645408061956 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch68: train: loss:0.5376965955663319 	 acc:0.7795041322314049 | test: loss:0.5430408053840233 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch69: train: loss:0.5340366147569389 	 acc:0.7676033057851239 | test: loss:0.5315999635007997 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch70: train: loss:0.5309921133419699 	 acc:0.7841322314049587 | test: loss:0.5335093110602424 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch71: train: loss:0.5272158472794146 	 acc:0.7791735537190083 | test: loss:0.5314778713990521 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch72: train: loss:0.530467517257722 	 acc:0.7692561983471075 | test: loss:0.5302132012828297 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch73: train: loss:0.5334225757654049 	 acc:0.7831404958677686 | test: loss:0.5400798353927815 	 acc:0.7748344370860927 	 lr:1.25e-05
epoch74: train: loss:0.5285253494436091 	 acc:0.7937190082644628 | test: loss:0.5388810715927983 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch75: train: loss:0.5259261576006236 	 acc:0.7857851239669421 | test: loss:0.5322852981801065 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch76: train: loss:0.5349105548661602 	 acc:0.7851239669421488 | test: loss:0.5406095764494889 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch77: train: loss:0.5280373734876144 	 acc:0.7699173553719009 | test: loss:0.5308277401703083 	 acc:0.7562913907284768 	 lr:1.25e-05
epoch78: train: loss:0.5327066064077961 	 acc:0.7854545454545454 | test: loss:0.5353141982034342 	 acc:0.7748344370860927 	 lr:1.25e-05
epoch79: train: loss:0.5266239417288914 	 acc:0.7699173553719009 | test: loss:0.5306811684804247 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch80: train: loss:0.5294497054273432 	 acc:0.780495867768595 | test: loss:0.5307530216823351 	 acc:0.7642384105960265 	 lr:6.25e-06
epoch81: train: loss:0.5233515228515814 	 acc:0.792396694214876 | test: loss:0.5295337474109322 	 acc:0.7629139072847683 	 lr:6.25e-06
epoch82: train: loss:0.5272991048403023 	 acc:0.7880991735537191 | test: loss:0.53036233273563 	 acc:0.7695364238410596 	 lr:6.25e-06
epoch83: train: loss:0.5342259564281495 	 acc:0.7874380165289256 | test: loss:0.5346532048768555 	 acc:0.776158940397351 	 lr:6.25e-06
epoch84: train: loss:0.5248043273106094 	 acc:0.7864462809917355 | test: loss:0.5296968127718035 	 acc:0.7629139072847683 	 lr:6.25e-06
epoch85: train: loss:0.5249495138018584 	 acc:0.7884297520661157 | test: loss:0.5308681113830466 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch86: train: loss:0.5218320689713659 	 acc:0.7943801652892561 | test: loss:0.5307518245368604 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch87: train: loss:0.5238561943542859 	 acc:0.7705785123966942 | test: loss:0.5297964584748477 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch88: train: loss:0.5206362722333798 	 acc:0.7927272727272727 | test: loss:0.5288018949773927 	 acc:0.766887417218543 	 lr:3.125e-06
epoch89: train: loss:0.5272935896865593 	 acc:0.7880991735537191 | test: loss:0.5302977808263918 	 acc:0.7827814569536424 	 lr:3.125e-06
epoch90: train: loss:0.5252752732639471 	 acc:0.7963636363636364 | test: loss:0.5291136747953907 	 acc:0.7708609271523179 	 lr:3.125e-06
epoch91: train: loss:0.5235600060667873 	 acc:0.7966942148760331 | test: loss:0.5294178061927391 	 acc:0.7748344370860927 	 lr:3.125e-06
epoch92: train: loss:0.5256007794309253 	 acc:0.7937190082644628 | test: loss:0.531930007760888 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch93: train: loss:0.5268844726065959 	 acc:0.7890909090909091 | test: loss:0.5318709697154973 	 acc:0.7827814569536424 	 lr:3.125e-06
epoch94: train: loss:0.5253641838672732 	 acc:0.7828099173553719 | test: loss:0.5301591183965569 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch95: train: loss:0.5266236518828337 	 acc:0.7910743801652893 | test: loss:0.5303284492713726 	 acc:0.776158940397351 	 lr:1.5625e-06
epoch96: train: loss:0.5248102098654125 	 acc:0.7847933884297521 | test: loss:0.5300294164000757 	 acc:0.7748344370860927 	 lr:1.5625e-06
epoch97: train: loss:0.5272256837797559 	 acc:0.7880991735537191 | test: loss:0.5295245113751746 	 acc:0.7735099337748345 	 lr:1.5625e-06
epoch98: train: loss:0.5275923329739531 	 acc:0.7761983471074381 | test: loss:0.5295890854683933 	 acc:0.7721854304635761 	 lr:1.5625e-06
epoch99: train: loss:0.5225825889839614 	 acc:0.7933884297520661 | test: loss:0.5292665919720732 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch100: train: loss:0.5243761753444829 	 acc:0.7950413223140496 | test: loss:0.5295362776478395 	 acc:0.776158940397351 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_9_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_9_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6915674593231895 	 acc:0.5616528925619835 | test: loss:0.6935584608292737 	 acc:0.5562913907284768 	 lr:0.0001
epoch1: train: loss:0.6786478168314154 	 acc:0.5986776859504133 | test: loss:0.6731074855817075 	 acc:0.6198675496688741 	 lr:0.0001
epoch2: train: loss:0.6416622435948081 	 acc:0.5824793388429752 | test: loss:0.633622252862185 	 acc:0.6105960264900663 	 lr:0.0001
epoch3: train: loss:0.6288417713701232 	 acc:0.5497520661157025 | test: loss:0.6252249976657084 	 acc:0.5576158940397351 	 lr:0.0001
epoch4: train: loss:0.6211140244263262 	 acc:0.6836363636363636 | test: loss:0.6171243403921064 	 acc:0.6794701986754967 	 lr:0.0001
epoch5: train: loss:0.6146793986746102 	 acc:0.6578512396694215 | test: loss:0.6037925555216556 	 acc:0.6622516556291391 	 lr:0.0001
epoch6: train: loss:0.6744773904745244 	 acc:0.5983471074380166 | test: loss:0.677836929251816 	 acc:0.6172185430463576 	 lr:0.0001
epoch7: train: loss:0.5961561229406309 	 acc:0.6823140495867769 | test: loss:0.5865870658925038 	 acc:0.6781456953642384 	 lr:0.0001
epoch8: train: loss:0.6005271704728938 	 acc:0.595702479338843 | test: loss:0.6027978931831208 	 acc:0.590728476821192 	 lr:0.0001
epoch9: train: loss:0.592499117437473 	 acc:0.7233057851239669 | test: loss:0.5903554233494184 	 acc:0.7178807947019867 	 lr:0.0001
epoch10: train: loss:0.5924654242420985 	 acc:0.6112396694214876 | test: loss:0.5923447735262233 	 acc:0.6172185430463576 	 lr:0.0001
epoch11: train: loss:0.5758433964035727 	 acc:0.727603305785124 | test: loss:0.5719344323834047 	 acc:0.7192052980132451 	 lr:0.0001
epoch12: train: loss:0.571430682682794 	 acc:0.7209917355371901 | test: loss:0.5623441887217642 	 acc:0.704635761589404 	 lr:0.0001
epoch13: train: loss:0.5944748375435506 	 acc:0.7150413223140496 | test: loss:0.5980978484974792 	 acc:0.7072847682119205 	 lr:0.0001
epoch14: train: loss:0.5636677205267031 	 acc:0.7087603305785124 | test: loss:0.5608189691770945 	 acc:0.7112582781456953 	 lr:0.0001
epoch15: train: loss:0.5597457707617893 	 acc:0.7266115702479339 | test: loss:0.5569744931941001 	 acc:0.7152317880794702 	 lr:0.0001
epoch16: train: loss:0.5709359660818557 	 acc:0.743801652892562 | test: loss:0.572625104717861 	 acc:0.7417218543046358 	 lr:0.0001
epoch17: train: loss:0.5546597479985765 	 acc:0.7067768595041323 | test: loss:0.5531682674458485 	 acc:0.704635761589404 	 lr:0.0001
epoch18: train: loss:0.5555372907110482 	 acc:0.7064462809917356 | test: loss:0.5491546699542873 	 acc:0.7072847682119205 	 lr:0.0001
epoch19: train: loss:0.555941731831259 	 acc:0.7120661157024794 | test: loss:0.5453802486129155 	 acc:0.7271523178807947 	 lr:0.0001
epoch20: train: loss:0.5454128126467555 	 acc:0.7219834710743802 | test: loss:0.5489848669001598 	 acc:0.6966887417218544 	 lr:0.0001
epoch21: train: loss:0.5563141090416711 	 acc:0.7583471074380165 | test: loss:0.5421410150875319 	 acc:0.7549668874172185 	 lr:0.0001
epoch22: train: loss:0.5560744645181767 	 acc:0.7540495867768595 | test: loss:0.5640907028652974 	 acc:0.7496688741721854 	 lr:0.0001
epoch23: train: loss:0.5543135336607941 	 acc:0.7583471074380165 | test: loss:0.5792423578287592 	 acc:0.7390728476821192 	 lr:0.0001
epoch24: train: loss:0.5322714025718122 	 acc:0.7603305785123967 | test: loss:0.5385082600132519 	 acc:0.7337748344370861 	 lr:0.0001
epoch25: train: loss:0.5529914931817488 	 acc:0.7352066115702479 | test: loss:0.5475947313750816 	 acc:0.7059602649006622 	 lr:0.0001
epoch26: train: loss:0.5599846099624949 	 acc:0.7672727272727272 | test: loss:0.558043953046104 	 acc:0.7615894039735099 	 lr:0.0001
epoch27: train: loss:0.5334340541815955 	 acc:0.7738842975206611 | test: loss:0.5383457498834623 	 acc:0.7682119205298014 	 lr:0.0001
epoch28: train: loss:0.5639052552428128 	 acc:0.7616528925619834 | test: loss:0.5587821037564057 	 acc:0.7655629139072848 	 lr:0.0001
epoch29: train: loss:0.5277984177967734 	 acc:0.7907438016528926 | test: loss:0.5357742274044365 	 acc:0.7655629139072848 	 lr:0.0001
epoch30: train: loss:0.574737134866478 	 acc:0.7484297520661157 | test: loss:0.5830886178458763 	 acc:0.7350993377483444 	 lr:0.0001
epoch31: train: loss:0.5286092772168561 	 acc:0.775206611570248 | test: loss:0.5340866001236517 	 acc:0.7629139072847683 	 lr:0.0001
epoch32: train: loss:0.5531339418592531 	 acc:0.6707438016528926 | test: loss:0.5566601718498382 	 acc:0.6741721854304635 	 lr:0.0001
epoch33: train: loss:0.554104384568112 	 acc:0.7666115702479339 | test: loss:0.5669976057595765 	 acc:0.7576158940397351 	 lr:0.0001
epoch34: train: loss:0.581470440103988 	 acc:0.7338842975206612 | test: loss:0.605724514951769 	 acc:0.7099337748344371 	 lr:0.0001
epoch35: train: loss:0.5172674684189568 	 acc:0.80099173553719 | test: loss:0.5266850709915161 	 acc:0.7774834437086092 	 lr:0.0001
epoch36: train: loss:0.5176623448655625 	 acc:0.792396694214876 | test: loss:0.5267912065745979 	 acc:0.785430463576159 	 lr:0.0001
epoch37: train: loss:0.5693753006438579 	 acc:0.6333884297520661 | test: loss:0.5767576445017429 	 acc:0.6370860927152318 	 lr:0.0001
epoch38: train: loss:0.5827156168764288 	 acc:0.739504132231405 | test: loss:0.6073189241207199 	 acc:0.7112582781456953 	 lr:0.0001
epoch39: train: loss:0.527404437735061 	 acc:0.7229752066115702 | test: loss:0.5439896064088834 	 acc:0.6966887417218544 	 lr:0.0001
epoch40: train: loss:0.5176223721484507 	 acc:0.7444628099173554 | test: loss:0.5504236376048713 	 acc:0.7006622516556291 	 lr:0.0001
epoch41: train: loss:0.5201538574597067 	 acc:0.7335537190082645 | test: loss:0.5450142850939012 	 acc:0.6966887417218544 	 lr:0.0001
epoch42: train: loss:0.5278723633584897 	 acc:0.7104132231404958 | test: loss:0.5415194302994684 	 acc:0.7033112582781457 	 lr:5e-05
epoch43: train: loss:0.49535279788261605 	 acc:0.8337190082644628 | test: loss:0.5211123211494345 	 acc:0.8 	 lr:5e-05
epoch44: train: loss:0.489359608770402 	 acc:0.8039669421487603 | test: loss:0.5215765243334486 	 acc:0.7443708609271523 	 lr:5e-05
epoch45: train: loss:0.491766052581062 	 acc:0.8092561983471075 | test: loss:0.5141580234300221 	 acc:0.7695364238410596 	 lr:5e-05
epoch46: train: loss:0.4924771052943773 	 acc:0.8029752066115703 | test: loss:0.5168463770127454 	 acc:0.7708609271523179 	 lr:5e-05
epoch47: train: loss:0.5083426440648796 	 acc:0.8241322314049587 | test: loss:0.5516533648730904 	 acc:0.7695364238410596 	 lr:5e-05
epoch48: train: loss:0.4887590324090532 	 acc:0.812892561983471 | test: loss:0.5190918845056698 	 acc:0.7417218543046358 	 lr:5e-05
epoch49: train: loss:0.4974611528451778 	 acc:0.7947107438016529 | test: loss:0.530290283272598 	 acc:0.7271523178807947 	 lr:5e-05
epoch50: train: loss:0.4874634699683544 	 acc:0.8089256198347108 | test: loss:0.5188635138486395 	 acc:0.743046357615894 	 lr:5e-05
epoch51: train: loss:0.4846830101545192 	 acc:0.8145454545454546 | test: loss:0.5123102661789648 	 acc:0.7589403973509934 	 lr:5e-05
epoch52: train: loss:0.4983648734053304 	 acc:0.7801652892561983 | test: loss:0.5227254421505707 	 acc:0.7602649006622516 	 lr:5e-05
epoch53: train: loss:0.5035304002131312 	 acc:0.8257851239669421 | test: loss:0.5270454344370508 	 acc:0.7920529801324503 	 lr:5e-05
epoch54: train: loss:0.4888807352121211 	 acc:0.8333884297520661 | test: loss:0.5204639951124886 	 acc:0.7880794701986755 	 lr:5e-05
epoch55: train: loss:0.49152560711892185 	 acc:0.8419834710743802 | test: loss:0.530875115994586 	 acc:0.7960264900662252 	 lr:5e-05
epoch56: train: loss:0.5354759420639227 	 acc:0.7927272727272727 | test: loss:0.5907575756508783 	 acc:0.7192052980132451 	 lr:5e-05
epoch57: train: loss:0.5049928282509165 	 acc:0.8181818181818182 | test: loss:0.5573955227207664 	 acc:0.7549668874172185 	 lr:5e-05
epoch58: train: loss:0.47256325634057855 	 acc:0.8360330578512397 | test: loss:0.5135256143595209 	 acc:0.776158940397351 	 lr:2.5e-05
epoch59: train: loss:0.4731081939925832 	 acc:0.8267768595041323 | test: loss:0.514637918977548 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch60: train: loss:0.47102505205091366 	 acc:0.8502479338842975 | test: loss:0.5161390374038393 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch61: train: loss:0.4655745062847768 	 acc:0.8499173553719008 | test: loss:0.5143556538007117 	 acc:0.7788079470198676 	 lr:2.5e-05
epoch62: train: loss:0.4783711399815299 	 acc:0.8482644628099174 | test: loss:0.5254479495105364 	 acc:0.7920529801324503 	 lr:2.5e-05
epoch63: train: loss:0.4651161733540622 	 acc:0.84 | test: loss:0.5118991764175971 	 acc:0.7708609271523179 	 lr:2.5e-05
epoch64: train: loss:0.47239559722340796 	 acc:0.8565289256198347 | test: loss:0.5271651709316582 	 acc:0.7920529801324503 	 lr:2.5e-05
epoch65: train: loss:0.4948478377160947 	 acc:0.7606611570247934 | test: loss:0.546581773015837 	 acc:0.6993377483443709 	 lr:2.5e-05
epoch66: train: loss:0.47103090445857404 	 acc:0.8578512396694215 | test: loss:0.5257139725400912 	 acc:0.785430463576159 	 lr:2.5e-05
epoch67: train: loss:0.46549754139805627 	 acc:0.8310743801652892 | test: loss:0.5194979160037262 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch68: train: loss:0.4857782384383777 	 acc:0.8436363636363636 | test: loss:0.5463315765589278 	 acc:0.7721854304635761 	 lr:2.5e-05
epoch69: train: loss:0.4650444800696097 	 acc:0.8300826446280992 | test: loss:0.5164822810533031 	 acc:0.7602649006622516 	 lr:2.5e-05
epoch70: train: loss:0.4575186170920853 	 acc:0.8525619834710744 | test: loss:0.5146582858451945 	 acc:0.752317880794702 	 lr:1.25e-05
epoch71: train: loss:0.4542659891045783 	 acc:0.8588429752066116 | test: loss:0.5142594624039353 	 acc:0.7642384105960265 	 lr:1.25e-05
epoch72: train: loss:0.4585980485293491 	 acc:0.8462809917355372 | test: loss:0.5131512203753389 	 acc:0.7655629139072848 	 lr:1.25e-05
epoch73: train: loss:0.45727689171625563 	 acc:0.8492561983471074 | test: loss:0.5144018222164634 	 acc:0.7629139072847683 	 lr:1.25e-05
epoch74: train: loss:0.4547956356627882 	 acc:0.871404958677686 | test: loss:0.5152607317002403 	 acc:0.7774834437086092 	 lr:1.25e-05
epoch75: train: loss:0.4571111865181568 	 acc:0.8760330578512396 | test: loss:0.5197883482800414 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch76: train: loss:0.44984076808306794 	 acc:0.8700826446280991 | test: loss:0.5121794814305589 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch77: train: loss:0.45946315971287816 	 acc:0.8608264462809917 | test: loss:0.5139425418234819 	 acc:0.7735099337748345 	 lr:6.25e-06
epoch78: train: loss:0.45778380342751496 	 acc:0.8591735537190083 | test: loss:0.5116151375486361 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch79: train: loss:0.4538674215147318 	 acc:0.8720661157024794 | test: loss:0.5151078941016797 	 acc:0.7788079470198676 	 lr:6.25e-06
epoch80: train: loss:0.45589518854440736 	 acc:0.8585123966942149 | test: loss:0.5120355139504995 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch81: train: loss:0.4495392236039658 	 acc:0.8704132231404959 | test: loss:0.5117576360702515 	 acc:0.766887417218543 	 lr:6.25e-06
epoch82: train: loss:0.4503002494327293 	 acc:0.8740495867768595 | test: loss:0.5123788535200208 	 acc:0.7642384105960265 	 lr:6.25e-06
epoch83: train: loss:0.4521366643018959 	 acc:0.8737190082644628 | test: loss:0.5122414302352248 	 acc:0.7695364238410596 	 lr:6.25e-06
epoch84: train: loss:0.4520522240863359 	 acc:0.8684297520661157 | test: loss:0.5131725047597822 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch85: train: loss:0.4551113752589738 	 acc:0.8641322314049587 | test: loss:0.5125274840569654 	 acc:0.7774834437086092 	 lr:3.125e-06
epoch86: train: loss:0.4512649943316278 	 acc:0.8707438016528926 | test: loss:0.5157510147189462 	 acc:0.7827814569536424 	 lr:3.125e-06
epoch87: train: loss:0.4485413658914487 	 acc:0.8776859504132232 | test: loss:0.5130976048526384 	 acc:0.7748344370860927 	 lr:3.125e-06
epoch88: train: loss:0.4496332648864462 	 acc:0.8763636363636363 | test: loss:0.5126100659370423 	 acc:0.7748344370860927 	 lr:3.125e-06
epoch89: train: loss:0.4532985284013196 	 acc:0.8710743801652893 | test: loss:0.5133569871352998 	 acc:0.7801324503311259 	 lr:3.125e-06
epoch90: train: loss:0.4466780986372104 	 acc:0.8776859504132232 | test: loss:0.5126632523852468 	 acc:0.7695364238410596 	 lr:3.125e-06
epoch91: train: loss:0.4542933470651138 	 acc:0.859504132231405 | test: loss:0.5133038897388029 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch92: train: loss:0.4497841106466025 	 acc:0.8760330578512396 | test: loss:0.5137409154942494 	 acc:0.7801324503311259 	 lr:1.5625e-06
epoch93: train: loss:0.45281239705637466 	 acc:0.8694214876033057 | test: loss:0.5120494870160589 	 acc:0.7748344370860927 	 lr:1.5625e-06
epoch94: train: loss:0.45282920639376995 	 acc:0.8647933884297521 | test: loss:0.5114205002784729 	 acc:0.7708609271523179 	 lr:1.5625e-06
epoch95: train: loss:0.45182350311397523 	 acc:0.8614876033057851 | test: loss:0.5120094454051644 	 acc:0.7748344370860927 	 lr:1.5625e-06
epoch96: train: loss:0.4505187609964166 	 acc:0.8740495867768595 | test: loss:0.5139032026000371 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch97: train: loss:0.45315637603278985 	 acc:0.8727272727272727 | test: loss:0.5150480784327779 	 acc:0.7801324503311259 	 lr:1.5625e-06
epoch98: train: loss:0.45239293321105073 	 acc:0.8763636363636363 | test: loss:0.516372894135532 	 acc:0.7894039735099337 	 lr:1.5625e-06
epoch99: train: loss:0.4571019219761052 	 acc:0.8697520661157024 | test: loss:0.5131834725670467 	 acc:0.7788079470198676 	 lr:1.5625e-06
epoch100: train: loss:0.4548065866123546 	 acc:0.8585123966942149 | test: loss:0.5115574537523535 	 acc:0.7721854304635761 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_10_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_10_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6771086276661266 	 acc:0.5861157024793389 | test: loss:0.6781280103898206 	 acc:0.590728476821192 	 lr:0.0001
epoch1: train: loss:0.6693332557441775 	 acc:0.6224793388429752 | test: loss:0.6629052983214524 	 acc:0.6397350993377483 	 lr:0.0001
epoch2: train: loss:0.6376160751098444 	 acc:0.5963636363636363 | test: loss:0.6310625287081232 	 acc:0.6225165562913907 	 lr:0.0001
epoch3: train: loss:0.6236657923509267 	 acc:0.5725619834710743 | test: loss:0.6169306099020093 	 acc:0.5708609271523178 	 lr:0.0001
epoch4: train: loss:0.6065995947585618 	 acc:0.6743801652892562 | test: loss:0.6007016766939731 	 acc:0.6821192052980133 	 lr:0.0001
epoch5: train: loss:0.5967387672101171 	 acc:0.6479338842975206 | test: loss:0.588895180209583 	 acc:0.6304635761589404 	 lr:0.0001
epoch6: train: loss:0.6333579143807908 	 acc:0.6737190082644628 | test: loss:0.62817017116294 	 acc:0.6728476821192053 	 lr:0.0001
epoch7: train: loss:0.5856092143846937 	 acc:0.704793388429752 | test: loss:0.5715104701503223 	 acc:0.7152317880794702 	 lr:0.0001
epoch8: train: loss:0.593376476843495 	 acc:0.608595041322314 | test: loss:0.5922173522166069 	 acc:0.6026490066225165 	 lr:0.0001
epoch9: train: loss:0.5759602133104624 	 acc:0.7011570247933885 | test: loss:0.5746268638711891 	 acc:0.7112582781456953 	 lr:0.0001
epoch10: train: loss:0.5833560414747758 	 acc:0.6304132231404959 | test: loss:0.5735218009411894 	 acc:0.6490066225165563 	 lr:0.0001
epoch11: train: loss:0.5656022168191012 	 acc:0.7130578512396695 | test: loss:0.560930340416384 	 acc:0.7112582781456953 	 lr:0.0001
epoch12: train: loss:0.5671272522161814 	 acc:0.7494214876033057 | test: loss:0.553718792602716 	 acc:0.752317880794702 	 lr:0.0001
epoch13: train: loss:0.559267088243784 	 acc:0.7520661157024794 | test: loss:0.5552568271460122 	 acc:0.7509933774834437 	 lr:0.0001
epoch14: train: loss:0.5484073490347744 	 acc:0.727603305785124 | test: loss:0.5443444590694857 	 acc:0.7298013245033113 	 lr:0.0001
epoch15: train: loss:0.5546431546368875 	 acc:0.6922314049586776 | test: loss:0.5618591184647668 	 acc:0.6768211920529801 	 lr:0.0001
epoch16: train: loss:0.552443116775229 	 acc:0.7672727272727272 | test: loss:0.5500923014634492 	 acc:0.7509933774834437 	 lr:0.0001
epoch17: train: loss:0.5549461382479707 	 acc:0.6882644628099174 | test: loss:0.551372856020138 	 acc:0.7006622516556291 	 lr:0.0001
epoch18: train: loss:0.5392309337805126 	 acc:0.7553719008264462 | test: loss:0.5289196650713485 	 acc:0.7642384105960265 	 lr:0.0001
epoch19: train: loss:0.5303164428324739 	 acc:0.7682644628099173 | test: loss:0.5297580100053193 	 acc:0.7496688741721854 	 lr:0.0001
epoch20: train: loss:0.5317988356085849 	 acc:0.7418181818181818 | test: loss:0.5481591788348773 	 acc:0.6993377483443709 	 lr:0.0001
epoch21: train: loss:0.5336757884340838 	 acc:0.7861157024793388 | test: loss:0.5304980568538439 	 acc:0.7576158940397351 	 lr:0.0001
epoch22: train: loss:0.5531430325429302 	 acc:0.7606611570247934 | test: loss:0.5530130332668886 	 acc:0.7403973509933774 	 lr:0.0001
epoch23: train: loss:0.5285653227616933 	 acc:0.7861157024793388 | test: loss:0.5623927531652892 	 acc:0.743046357615894 	 lr:0.0001
epoch24: train: loss:0.5847578236485316 	 acc:0.727603305785124 | test: loss:0.6134500194069565 	 acc:0.7006622516556291 	 lr:0.0001
epoch25: train: loss:0.5158339357376098 	 acc:0.7990082644628099 | test: loss:0.5206308602497277 	 acc:0.7615894039735099 	 lr:5e-05
epoch26: train: loss:0.5099885979270147 	 acc:0.8033057851239669 | test: loss:0.5200390114689505 	 acc:0.7562913907284768 	 lr:5e-05
epoch27: train: loss:0.5465421702842082 	 acc:0.780495867768595 | test: loss:0.5734096913937702 	 acc:0.7390728476821192 	 lr:5e-05
epoch28: train: loss:0.5373927932337296 	 acc:0.7937190082644628 | test: loss:0.551845504195485 	 acc:0.7682119205298014 	 lr:5e-05
epoch29: train: loss:0.5010085119294726 	 acc:0.8181818181818182 | test: loss:0.5184477209255395 	 acc:0.7814569536423841 	 lr:5e-05
epoch30: train: loss:0.5516724533088936 	 acc:0.780495867768595 | test: loss:0.5701193138463607 	 acc:0.7403973509933774 	 lr:5e-05
epoch31: train: loss:0.5003899448666691 	 acc:0.8181818181818182 | test: loss:0.5172244065644725 	 acc:0.7788079470198676 	 lr:5e-05
epoch32: train: loss:0.5076843562303496 	 acc:0.8122314049586777 | test: loss:0.5266090153068896 	 acc:0.7841059602649006 	 lr:5e-05
epoch33: train: loss:0.49508362565158814 	 acc:0.8343801652892562 | test: loss:0.5175443336663657 	 acc:0.8 	 lr:5e-05
epoch34: train: loss:0.48823037248997647 	 acc:0.8310743801652892 | test: loss:0.5097557262079605 	 acc:0.7907284768211921 	 lr:5e-05
epoch35: train: loss:0.4994013223175175 	 acc:0.7871074380165289 | test: loss:0.5241576246867906 	 acc:0.7324503311258278 	 lr:5e-05
epoch36: train: loss:0.49776983534994207 	 acc:0.7940495867768596 | test: loss:0.523074675711575 	 acc:0.7403973509933774 	 lr:5e-05
epoch37: train: loss:0.49644521179278034 	 acc:0.8102479338842975 | test: loss:0.5252092024348429 	 acc:0.7801324503311259 	 lr:5e-05
epoch38: train: loss:0.5508821546144722 	 acc:0.775206611570248 | test: loss:0.5972555659464653 	 acc:0.7192052980132451 	 lr:5e-05
epoch39: train: loss:0.4787677386378454 	 acc:0.8218181818181818 | test: loss:0.5108883252996482 	 acc:0.7615894039735099 	 lr:5e-05
epoch40: train: loss:0.4815867951487707 	 acc:0.8115702479338843 | test: loss:0.510926790347952 	 acc:0.7576158940397351 	 lr:5e-05
epoch41: train: loss:0.48770836533593737 	 acc:0.8423140495867769 | test: loss:0.5288477517911141 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch42: train: loss:0.475297404271512 	 acc:0.827107438016529 | test: loss:0.5114148369688072 	 acc:0.7602649006622516 	 lr:2.5e-05
epoch43: train: loss:0.49483848025976135 	 acc:0.8416528925619835 | test: loss:0.5411017042122139 	 acc:0.7721854304635761 	 lr:2.5e-05
epoch44: train: loss:0.4650506344412969 	 acc:0.8353719008264463 | test: loss:0.509535391283351 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch45: train: loss:0.46905543595306143 	 acc:0.8419834710743802 | test: loss:0.5080403192943295 	 acc:0.7867549668874172 	 lr:2.5e-05
epoch46: train: loss:0.47235651825085156 	 acc:0.8489256198347107 | test: loss:0.517217616213868 	 acc:0.7947019867549668 	 lr:2.5e-05
epoch47: train: loss:0.47505750224610005 	 acc:0.8631404958677686 | test: loss:0.5271784513201935 	 acc:0.7841059602649006 	 lr:2.5e-05
epoch48: train: loss:0.48258912060871595 	 acc:0.7960330578512397 | test: loss:0.5250218200367808 	 acc:0.7284768211920529 	 lr:2.5e-05
epoch49: train: loss:0.46934416410351587 	 acc:0.8601652892561984 | test: loss:0.5097398153993468 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch50: train: loss:0.49437718424915283 	 acc:0.8347107438016529 | test: loss:0.5383358806963788 	 acc:0.7721854304635761 	 lr:2.5e-05
epoch51: train: loss:0.4640559211250179 	 acc:0.8558677685950413 | test: loss:0.5068016244875674 	 acc:0.7973509933774835 	 lr:2.5e-05
epoch52: train: loss:0.46009832777267645 	 acc:0.8565289256198347 | test: loss:0.5077312647112158 	 acc:0.7920529801324503 	 lr:2.5e-05
epoch53: train: loss:0.47856529154068184 	 acc:0.8512396694214877 | test: loss:0.5241972991962307 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch54: train: loss:0.46484713453891846 	 acc:0.8376859504132231 | test: loss:0.5099863794465728 	 acc:0.7615894039735099 	 lr:2.5e-05
epoch55: train: loss:0.45283581976063 	 acc:0.8618181818181818 | test: loss:0.5069814479114204 	 acc:0.7907284768211921 	 lr:2.5e-05
epoch56: train: loss:0.4841200124232237 	 acc:0.8479338842975207 | test: loss:0.5516901537282577 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch57: train: loss:0.4603048939842823 	 acc:0.8409917355371901 | test: loss:0.507453183227817 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch58: train: loss:0.47032487845617876 	 acc:0.8575206611570247 | test: loss:0.528207164884403 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch59: train: loss:0.4536436759341847 	 acc:0.8538842975206612 | test: loss:0.5043821784834198 	 acc:0.7748344370860927 	 lr:1.25e-05
epoch60: train: loss:0.46028970713457784 	 acc:0.8644628099173554 | test: loss:0.5154850374783901 	 acc:0.7841059602649006 	 lr:1.25e-05
epoch61: train: loss:0.44823859108381037 	 acc:0.8700826446280991 | test: loss:0.5068949000724894 	 acc:0.7894039735099337 	 lr:1.25e-05
epoch62: train: loss:0.45519209990816667 	 acc:0.8638016528925619 | test: loss:0.5063344546501211 	 acc:0.7973509933774835 	 lr:1.25e-05
epoch63: train: loss:0.4478273451131237 	 acc:0.8684297520661157 | test: loss:0.5036860300215664 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch64: train: loss:0.4478117587743712 	 acc:0.8730578512396694 | test: loss:0.5098509667724963 	 acc:0.7907284768211921 	 lr:1.25e-05
epoch65: train: loss:0.44577575868811486 	 acc:0.8545454545454545 | test: loss:0.5036890822530582 	 acc:0.785430463576159 	 lr:1.25e-05
epoch66: train: loss:0.4486323802431753 	 acc:0.8773553719008265 | test: loss:0.5072129737462429 	 acc:0.7920529801324503 	 lr:1.25e-05
epoch67: train: loss:0.4494433874729251 	 acc:0.868099173553719 | test: loss:0.5083542139324921 	 acc:0.7933774834437086 	 lr:1.25e-05
epoch68: train: loss:0.45858996379473976 	 acc:0.8753719008264463 | test: loss:0.5158411584942546 	 acc:0.7920529801324503 	 lr:1.25e-05
epoch69: train: loss:0.44938767862714024 	 acc:0.8694214876033057 | test: loss:0.5063189995209902 	 acc:0.785430463576159 	 lr:1.25e-05
epoch70: train: loss:0.446753407628083 	 acc:0.8664462809917355 | test: loss:0.5021541768351927 	 acc:0.7814569536423841 	 lr:6.25e-06
epoch71: train: loss:0.44535236972422637 	 acc:0.8661157024793389 | test: loss:0.5021447250385158 	 acc:0.7814569536423841 	 lr:6.25e-06
epoch72: train: loss:0.44149130134543113 	 acc:0.8776859504132232 | test: loss:0.5044751500451802 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch73: train: loss:0.4452045142453564 	 acc:0.871404958677686 | test: loss:0.5024480212603184 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch74: train: loss:0.4446589344789174 	 acc:0.8786776859504132 | test: loss:0.5105081813224893 	 acc:0.7920529801324503 	 lr:6.25e-06
epoch75: train: loss:0.43887885431612816 	 acc:0.8849586776859504 | test: loss:0.5040458881302385 	 acc:0.7880794701986755 	 lr:6.25e-06
epoch76: train: loss:0.44388441945895674 	 acc:0.871404958677686 | test: loss:0.5027075330153207 	 acc:0.7880794701986755 	 lr:6.25e-06
epoch77: train: loss:0.44806548325483464 	 acc:0.8710743801652893 | test: loss:0.5052039247474923 	 acc:0.7960264900662252 	 lr:3.125e-06
epoch78: train: loss:0.445867977112778 	 acc:0.8753719008264463 | test: loss:0.5021855819304257 	 acc:0.7827814569536424 	 lr:3.125e-06
epoch79: train: loss:0.445940403376729 	 acc:0.8760330578512396 | test: loss:0.5041316220302455 	 acc:0.7960264900662252 	 lr:3.125e-06
epoch80: train: loss:0.443846937408132 	 acc:0.8740495867768595 | test: loss:0.5033978511166098 	 acc:0.7907284768211921 	 lr:3.125e-06
epoch81: train: loss:0.440776292597952 	 acc:0.8803305785123967 | test: loss:0.5022520296621007 	 acc:0.7880794701986755 	 lr:3.125e-06
epoch82: train: loss:0.44030340380905086 	 acc:0.8849586776859504 | test: loss:0.5015578837584186 	 acc:0.7841059602649006 	 lr:3.125e-06
epoch83: train: loss:0.4421852776432825 	 acc:0.876694214876033 | test: loss:0.5020982916781445 	 acc:0.7894039735099337 	 lr:3.125e-06
epoch84: train: loss:0.4414597691386199 	 acc:0.8796694214876033 | test: loss:0.5054514457058433 	 acc:0.7960264900662252 	 lr:3.125e-06
epoch85: train: loss:0.4452483670475069 	 acc:0.8740495867768595 | test: loss:0.5027596976583367 	 acc:0.7894039735099337 	 lr:3.125e-06
epoch86: train: loss:0.44209086455589486 	 acc:0.8813223140495868 | test: loss:0.5083024508905727 	 acc:0.7933774834437086 	 lr:3.125e-06
epoch87: train: loss:0.43784451347737274 	 acc:0.8859504132231405 | test: loss:0.503258423457872 	 acc:0.7827814569536424 	 lr:3.125e-06
epoch88: train: loss:0.43854098802755687 	 acc:0.8856198347107438 | test: loss:0.5025435634006727 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch89: train: loss:0.44342035866965934 	 acc:0.8783471074380166 | test: loss:0.5048178615159546 	 acc:0.7920529801324503 	 lr:1.5625e-06
epoch90: train: loss:0.43837716445449953 	 acc:0.8809917355371901 | test: loss:0.5045451071878143 	 acc:0.7920529801324503 	 lr:1.5625e-06
epoch91: train: loss:0.4445714042896082 	 acc:0.8694214876033057 | test: loss:0.5057487509108537 	 acc:0.7894039735099337 	 lr:1.5625e-06
epoch92: train: loss:0.4411420272697102 	 acc:0.8819834710743801 | test: loss:0.5057842800159328 	 acc:0.7933774834437086 	 lr:1.5625e-06
epoch93: train: loss:0.43806047438589996 	 acc:0.8852892561983471 | test: loss:0.5034425268110061 	 acc:0.7894039735099337 	 lr:1.5625e-06
epoch94: train: loss:0.4436236110403518 	 acc:0.8757024793388429 | test: loss:0.5029127649913561 	 acc:0.7880794701986755 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_11_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_11_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6766608970815485 	 acc:0.5914049586776859 | test: loss:0.6762631443162628 	 acc:0.5960264900662252 	 lr:0.0001
epoch1: train: loss:0.6629019330158706 	 acc:0.6294214876033057 | test: loss:0.6560090020002908 	 acc:0.6410596026490066 	 lr:0.0001
epoch2: train: loss:0.6358714952547688 	 acc:0.5930578512396695 | test: loss:0.628834985028829 	 acc:0.6158940397350994 	 lr:0.0001
epoch3: train: loss:0.6197668824905206 	 acc:0.5834710743801653 | test: loss:0.6125550679813158 	 acc:0.5841059602649007 	 lr:0.0001
epoch4: train: loss:0.6004367941864266 	 acc:0.6753719008264463 | test: loss:0.5967166281693819 	 acc:0.6728476821192053 	 lr:0.0001
epoch5: train: loss:0.5954953274253971 	 acc:0.628099173553719 | test: loss:0.5922380435545713 	 acc:0.6158940397350994 	 lr:0.0001
epoch6: train: loss:0.6079434556212308 	 acc:0.7087603305785124 | test: loss:0.602889355839483 	 acc:0.7019867549668874 	 lr:0.0001
epoch7: train: loss:0.5770438156246154 	 acc:0.6955371900826446 | test: loss:0.5674914914251163 	 acc:0.6900662251655629 	 lr:0.0001
epoch8: train: loss:0.5866733365610611 	 acc:0.6284297520661157 | test: loss:0.5731737996568743 	 acc:0.6662251655629139 	 lr:0.0001
epoch9: train: loss:0.5612016468402768 	 acc:0.7335537190082645 | test: loss:0.5583906681332367 	 acc:0.7311258278145696 	 lr:0.0001
epoch10: train: loss:0.5751724740493396 	 acc:0.644297520661157 | test: loss:0.5822775847864466 	 acc:0.6384105960264901 	 lr:0.0001
epoch11: train: loss:0.5612278260869428 	 acc:0.7563636363636363 | test: loss:0.5566936443183595 	 acc:0.7403973509933774 	 lr:0.0001
epoch12: train: loss:0.596286633054087 	 acc:0.7193388429752066 | test: loss:0.6017298196325239 	 acc:0.7178807947019867 	 lr:0.0001
epoch13: train: loss:0.543020928773013 	 acc:0.739504132231405 | test: loss:0.5437005913810224 	 acc:0.7178807947019867 	 lr:0.0001
epoch14: train: loss:0.5474444216736092 	 acc:0.7249586776859505 | test: loss:0.5422211449667318 	 acc:0.7456953642384105 	 lr:0.0001
epoch15: train: loss:0.5400302208947741 	 acc:0.731900826446281 | test: loss:0.5371218504495179 	 acc:0.7364238410596027 	 lr:0.0001
epoch16: train: loss:0.5373174491007466 	 acc:0.7646280991735537 | test: loss:0.5379294841494782 	 acc:0.7509933774834437 	 lr:0.0001
epoch17: train: loss:0.5665414322506298 	 acc:0.6505785123966942 | test: loss:0.581626076177256 	 acc:0.6278145695364239 	 lr:0.0001
epoch18: train: loss:0.5342610900855261 	 acc:0.7854545454545454 | test: loss:0.5323816661013673 	 acc:0.7801324503311259 	 lr:0.0001
epoch19: train: loss:0.5256550718930142 	 acc:0.7656198347107438 | test: loss:0.5296679047559271 	 acc:0.7549668874172185 	 lr:0.0001
epoch20: train: loss:0.5394889878044443 	 acc:0.7087603305785124 | test: loss:0.5634083405235746 	 acc:0.6622516556291391 	 lr:0.0001
epoch21: train: loss:0.5210752565604596 	 acc:0.7887603305785124 | test: loss:0.5247371909634166 	 acc:0.7629139072847683 	 lr:0.0001
epoch22: train: loss:0.6105492624566575 	 acc:0.7024793388429752 | test: loss:0.6137240561428449 	 acc:0.7059602649006622 	 lr:0.0001
epoch23: train: loss:0.5153749693326714 	 acc:0.7847933884297521 | test: loss:0.5324351646252815 	 acc:0.7867549668874172 	 lr:0.0001
epoch24: train: loss:0.5274174537343427 	 acc:0.7871074380165289 | test: loss:0.5626826887888624 	 acc:0.752317880794702 	 lr:0.0001
epoch25: train: loss:0.524422230562888 	 acc:0.804297520661157 | test: loss:0.545240031646577 	 acc:0.7589403973509934 	 lr:0.0001
epoch26: train: loss:0.6033724844554239 	 acc:0.7110743801652892 | test: loss:0.632046352080162 	 acc:0.671523178807947 	 lr:0.0001
epoch27: train: loss:0.5005413823679459 	 acc:0.8191735537190082 | test: loss:0.5277521173685592 	 acc:0.7735099337748345 	 lr:0.0001
epoch28: train: loss:0.5643827312051757 	 acc:0.7557024793388429 | test: loss:0.6017817474358919 	 acc:0.7086092715231788 	 lr:5e-05
epoch29: train: loss:0.48465560379107137 	 acc:0.828099173553719 | test: loss:0.5199790667224404 	 acc:0.7801324503311259 	 lr:5e-05
epoch30: train: loss:0.5083314435738178 	 acc:0.8290909090909091 | test: loss:0.5549802746204351 	 acc:0.7642384105960265 	 lr:5e-05
epoch31: train: loss:0.5014053944221213 	 acc:0.8307438016528925 | test: loss:0.5391665213944896 	 acc:0.7682119205298014 	 lr:5e-05
epoch32: train: loss:0.48290719480553934 	 acc:0.8393388429752067 | test: loss:0.5199311827192243 	 acc:0.7880794701986755 	 lr:5e-05
epoch33: train: loss:0.4691402473134443 	 acc:0.8459504132231405 | test: loss:0.5074150914387987 	 acc:0.7801324503311259 	 lr:5e-05
epoch34: train: loss:0.47596071682685664 	 acc:0.8142148760330579 | test: loss:0.517466913074847 	 acc:0.7456953642384105 	 lr:5e-05
epoch35: train: loss:0.47133621338970405 	 acc:0.8439669421487603 | test: loss:0.5079511904558599 	 acc:0.7814569536423841 	 lr:5e-05
epoch36: train: loss:0.4737586429789047 	 acc:0.8241322314049587 | test: loss:0.5105541719506118 	 acc:0.7629139072847683 	 lr:5e-05
epoch37: train: loss:0.4704255666811604 	 acc:0.8383471074380165 | test: loss:0.5197625563634153 	 acc:0.7867549668874172 	 lr:5e-05
epoch38: train: loss:0.4978887883986323 	 acc:0.8244628099173553 | test: loss:0.5526797806190339 	 acc:0.7576158940397351 	 lr:5e-05
epoch39: train: loss:0.4639223535789931 	 acc:0.8314049586776859 | test: loss:0.5145066252607383 	 acc:0.7589403973509934 	 lr:5e-05
epoch40: train: loss:0.45754296092947655 	 acc:0.8674380165289256 | test: loss:0.5245007034169128 	 acc:0.7841059602649006 	 lr:2.5e-05
epoch41: train: loss:0.4584169502987349 	 acc:0.8684297520661157 | test: loss:0.5188452555643802 	 acc:0.776158940397351 	 lr:2.5e-05
epoch42: train: loss:0.45225867559101957 	 acc:0.8575206611570247 | test: loss:0.5081428962827518 	 acc:0.7708609271523179 	 lr:2.5e-05
epoch43: train: loss:0.474609546750045 	 acc:0.8542148760330579 | test: loss:0.5462212385720765 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch44: train: loss:0.4427780049103351 	 acc:0.8687603305785124 | test: loss:0.5100772750298709 	 acc:0.7841059602649006 	 lr:2.5e-05
epoch45: train: loss:0.44862996832398344 	 acc:0.8654545454545455 | test: loss:0.5039511201397473 	 acc:0.7920529801324503 	 lr:2.5e-05
epoch46: train: loss:0.4466396033271285 	 acc:0.8644628099173554 | test: loss:0.511644213483823 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch47: train: loss:0.45162527215382287 	 acc:0.876694214876033 | test: loss:0.5327975215501343 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch48: train: loss:0.4517215297635922 	 acc:0.8429752066115702 | test: loss:0.512540698525132 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch49: train: loss:0.4483334528710231 	 acc:0.8816528925619834 | test: loss:0.5106681354787965 	 acc:0.7788079470198676 	 lr:2.5e-05
epoch50: train: loss:0.46084396420431534 	 acc:0.8667768595041322 | test: loss:0.5438293277033117 	 acc:0.7708609271523179 	 lr:2.5e-05
epoch51: train: loss:0.4421993180641458 	 acc:0.8793388429752066 | test: loss:0.5064957906078819 	 acc:0.7960264900662252 	 lr:2.5e-05
epoch52: train: loss:0.4395869318118765 	 acc:0.88 | test: loss:0.508940853977835 	 acc:0.7880794701986755 	 lr:1.25e-05
epoch53: train: loss:0.43734643812021934 	 acc:0.8763636363636363 | test: loss:0.5056953617279103 	 acc:0.7774834437086092 	 lr:1.25e-05
epoch54: train: loss:0.44170931908709943 	 acc:0.8790082644628099 | test: loss:0.5078228547083621 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch55: train: loss:0.43265722567384896 	 acc:0.8819834710743801 | test: loss:0.5075706947718235 	 acc:0.7814569536423841 	 lr:1.25e-05
epoch56: train: loss:0.4325829805716995 	 acc:0.8869421487603306 | test: loss:0.5116742422249143 	 acc:0.7827814569536424 	 lr:1.25e-05
epoch57: train: loss:0.4338023734880873 	 acc:0.8816528925619834 | test: loss:0.5053392701591087 	 acc:0.776158940397351 	 lr:1.25e-05
epoch58: train: loss:0.4324195898859954 	 acc:0.8902479338842976 | test: loss:0.5061151250308713 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch59: train: loss:0.4306266059363184 	 acc:0.8928925619834711 | test: loss:0.5096426861175638 	 acc:0.7973509933774835 	 lr:6.25e-06
epoch60: train: loss:0.4365770669712508 	 acc:0.8879338842975206 | test: loss:0.5134126339527155 	 acc:0.7947019867549668 	 lr:6.25e-06
epoch61: train: loss:0.43103783239017834 	 acc:0.8902479338842976 | test: loss:0.5092485165753902 	 acc:0.8 	 lr:6.25e-06
epoch62: train: loss:0.43268430322654977 	 acc:0.8872727272727273 | test: loss:0.5058584386149779 	 acc:0.7960264900662252 	 lr:6.25e-06
epoch63: train: loss:0.4268106272299428 	 acc:0.8942148760330578 | test: loss:0.5087440804140457 	 acc:0.7960264900662252 	 lr:6.25e-06
epoch64: train: loss:0.42721883178742465 	 acc:0.8803305785123967 | test: loss:0.5038641707786661 	 acc:0.7880794701986755 	 lr:3.125e-06
epoch65: train: loss:0.4240804720220487 	 acc:0.8995041322314049 | test: loss:0.5079127839069493 	 acc:0.7920529801324503 	 lr:3.125e-06
epoch66: train: loss:0.42617494357518915 	 acc:0.8902479338842976 | test: loss:0.5051175119071607 	 acc:0.7894039735099337 	 lr:3.125e-06
epoch67: train: loss:0.4300639614980083 	 acc:0.8899173553719009 | test: loss:0.5066113310933903 	 acc:0.7973509933774835 	 lr:3.125e-06
epoch68: train: loss:0.4275043642422384 	 acc:0.8975206611570248 | test: loss:0.5054123031382529 	 acc:0.8 	 lr:3.125e-06
epoch69: train: loss:0.43328178377190896 	 acc:0.8829752066115703 | test: loss:0.511963072833636 	 acc:0.8013245033112583 	 lr:3.125e-06
epoch70: train: loss:0.43110436072034286 	 acc:0.8938842975206611 | test: loss:0.51463922783239 	 acc:0.7973509933774835 	 lr:3.125e-06
epoch71: train: loss:0.42711178675170774 	 acc:0.8879338842975206 | test: loss:0.5064665447007741 	 acc:0.7933774834437086 	 lr:1.5625e-06
epoch72: train: loss:0.4226196075175419 	 acc:0.8981818181818182 | test: loss:0.5054231932621128 	 acc:0.7894039735099337 	 lr:1.5625e-06
epoch73: train: loss:0.42802600206422414 	 acc:0.8899173553719009 | test: loss:0.5045982792677469 	 acc:0.7841059602649006 	 lr:1.5625e-06
epoch74: train: loss:0.42325176936535797 	 acc:0.8988429752066116 | test: loss:0.5068295376190287 	 acc:0.7933774834437086 	 lr:1.5625e-06
epoch75: train: loss:0.4232492533597079 	 acc:0.8988429752066116 | test: loss:0.5090744309867454 	 acc:0.7960264900662252 	 lr:1.5625e-06
epoch76: train: loss:0.4255557565354119 	 acc:0.8909090909090909 | test: loss:0.5062876170834169 	 acc:0.7907284768211921 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_12_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_12_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6768892760119163 	 acc:0.6039669421487603 | test: loss:0.6775556054336346 	 acc:0.5973509933774834 	 lr:0.0001
epoch1: train: loss:0.661284403229548 	 acc:0.6333884297520661 | test: loss:0.6560389292950661 	 acc:0.6543046357615894 	 lr:0.0001
epoch2: train: loss:0.6383449480750344 	 acc:0.6479338842975206 | test: loss:0.6308879446509658 	 acc:0.6635761589403973 	 lr:0.0001
epoch3: train: loss:0.6128295815680638 	 acc:0.6175206611570248 | test: loss:0.6029874510322975 	 acc:0.633112582781457 	 lr:0.0001
epoch4: train: loss:0.5948541626260301 	 acc:0.7077685950413223 | test: loss:0.5898662031091602 	 acc:0.7125827814569536 	 lr:0.0001
epoch5: train: loss:0.5984039671164899 	 acc:0.6142148760330578 | test: loss:0.6112696574223752 	 acc:0.5721854304635762 	 lr:0.0001
epoch6: train: loss:0.5861110868532795 	 acc:0.724297520661157 | test: loss:0.5768258357679608 	 acc:0.7178807947019867 	 lr:0.0001
epoch7: train: loss:0.5714267684605496 	 acc:0.728595041322314 | test: loss:0.560521260871003 	 acc:0.7218543046357616 	 lr:0.0001
epoch8: train: loss:0.5886234605213827 	 acc:0.6128925619834711 | test: loss:0.5818191543320157 	 acc:0.623841059602649 	 lr:0.0001
epoch9: train: loss:0.5607090115153099 	 acc:0.696198347107438 | test: loss:0.5556155823713896 	 acc:0.7099337748344371 	 lr:0.0001
epoch10: train: loss:0.5689996719951472 	 acc:0.6578512396694215 | test: loss:0.5650277930379703 	 acc:0.6728476821192053 	 lr:0.0001
epoch11: train: loss:0.5478232398308999 	 acc:0.7547107438016529 | test: loss:0.5485214768656043 	 acc:0.7231788079470198 	 lr:0.0001
epoch12: train: loss:0.5664147999661028 	 acc:0.7457851239669422 | test: loss:0.5730623021031057 	 acc:0.7364238410596027 	 lr:0.0001
epoch13: train: loss:0.5355010517372573 	 acc:0.7563636363636363 | test: loss:0.5435029157739601 	 acc:0.7364238410596027 	 lr:0.0001
epoch14: train: loss:0.5361051120048712 	 acc:0.7573553719008265 | test: loss:0.5403504629798282 	 acc:0.7549668874172185 	 lr:0.0001
epoch15: train: loss:0.5378188003587329 	 acc:0.7791735537190083 | test: loss:0.562038741601224 	 acc:0.7629139072847683 	 lr:0.0001
epoch16: train: loss:0.5296610682463843 	 acc:0.7917355371900826 | test: loss:0.5309275693451332 	 acc:0.7536423841059603 	 lr:0.0001
epoch17: train: loss:0.5538427286896824 	 acc:0.6763636363636364 | test: loss:0.5756352622777421 	 acc:0.6423841059602649 	 lr:0.0001
epoch18: train: loss:0.518634887963287 	 acc:0.8016528925619835 | test: loss:0.5248742690938988 	 acc:0.7841059602649006 	 lr:0.0001
epoch19: train: loss:0.5228818090296974 	 acc:0.7970247933884298 | test: loss:0.5423940806199383 	 acc:0.7602649006622516 	 lr:0.0001
epoch20: train: loss:0.551238249628997 	 acc:0.6783471074380165 | test: loss:0.5822150125408804 	 acc:0.6410596026490066 	 lr:0.0001
epoch21: train: loss:0.5227443508077259 	 acc:0.7282644628099173 | test: loss:0.5632194654041568 	 acc:0.6635761589403973 	 lr:0.0001
epoch22: train: loss:0.6226216705771517 	 acc:0.6971900826446281 | test: loss:0.6688215296000045 	 acc:0.6675496688741722 	 lr:0.0001
epoch23: train: loss:0.5010134447015021 	 acc:0.8155371900826446 | test: loss:0.5268996801597393 	 acc:0.7774834437086092 	 lr:0.0001
epoch24: train: loss:0.49323980499890224 	 acc:0.7973553719008264 | test: loss:0.5183247720958382 	 acc:0.7642384105960265 	 lr:0.0001
epoch25: train: loss:0.49255815941440173 	 acc:0.8211570247933885 | test: loss:0.5223245162048087 	 acc:0.7788079470198676 	 lr:0.0001
epoch26: train: loss:0.5231173784476666 	 acc:0.8082644628099174 | test: loss:0.5648186869179176 	 acc:0.7589403973509934 	 lr:0.0001
epoch27: train: loss:0.48099969028441375 	 acc:0.8314049586776859 | test: loss:0.5262941999151217 	 acc:0.752317880794702 	 lr:0.0001
epoch28: train: loss:0.5631237622331982 	 acc:0.7596694214876033 | test: loss:0.6142391763775554 	 acc:0.6887417218543046 	 lr:0.0001
epoch29: train: loss:0.477106976183978 	 acc:0.8095867768595041 | test: loss:0.5262311498060921 	 acc:0.7125827814569536 	 lr:0.0001
epoch30: train: loss:0.4702574242245067 	 acc:0.851900826446281 | test: loss:0.516982016737098 	 acc:0.7708609271523179 	 lr:0.0001
epoch31: train: loss:0.46677957302282663 	 acc:0.848595041322314 | test: loss:0.5182909671833973 	 acc:0.7549668874172185 	 lr:0.0001
epoch32: train: loss:0.47459560744033374 	 acc:0.8201652892561984 | test: loss:0.5241015042690251 	 acc:0.766887417218543 	 lr:0.0001
epoch33: train: loss:0.5171382017569108 	 acc:0.8105785123966942 | test: loss:0.6167850731224414 	 acc:0.7006622516556291 	 lr:0.0001
epoch34: train: loss:0.45520109939181114 	 acc:0.8591735537190083 | test: loss:0.5355728614409239 	 acc:0.7549668874172185 	 lr:0.0001
epoch35: train: loss:0.47360110381418025 	 acc:0.8492561983471074 | test: loss:0.5441109510447015 	 acc:0.7496688741721854 	 lr:0.0001
epoch36: train: loss:0.4868849728521237 	 acc:0.8373553719008264 | test: loss:0.5316606836603177 	 acc:0.7589403973509934 	 lr:0.0001
epoch37: train: loss:0.4449937490392322 	 acc:0.8631404958677686 | test: loss:0.512101304610044 	 acc:0.7735099337748345 	 lr:5e-05
epoch38: train: loss:0.4366517738763951 	 acc:0.8809917355371901 | test: loss:0.523377209941283 	 acc:0.7655629139072848 	 lr:5e-05
epoch39: train: loss:0.43757557261088664 	 acc:0.8634710743801652 | test: loss:0.5180292856614321 	 acc:0.7602649006622516 	 lr:5e-05
epoch40: train: loss:0.43194766219982433 	 acc:0.8796694214876033 | test: loss:0.5134402798501072 	 acc:0.752317880794702 	 lr:5e-05
epoch41: train: loss:0.4212975591864468 	 acc:0.8872727272727273 | test: loss:0.5095406885968139 	 acc:0.7708609271523179 	 lr:5e-05
epoch42: train: loss:0.42479757956236847 	 acc:0.8823140495867768 | test: loss:0.5170809436318101 	 acc:0.7443708609271523 	 lr:5e-05
epoch43: train: loss:0.4917946699237035 	 acc:0.836694214876033 | test: loss:0.6139873820424869 	 acc:0.7006622516556291 	 lr:5e-05
epoch44: train: loss:0.4264374029734903 	 acc:0.8995041322314049 | test: loss:0.5426570728125162 	 acc:0.7721854304635761 	 lr:5e-05
epoch45: train: loss:0.46286317282471773 	 acc:0.8671074380165289 | test: loss:0.5729797784066358 	 acc:0.743046357615894 	 lr:5e-05
epoch46: train: loss:0.4244731293729514 	 acc:0.8988429752066116 | test: loss:0.5201327100494839 	 acc:0.7735099337748345 	 lr:5e-05
epoch47: train: loss:0.41126246920301895 	 acc:0.9067768595041322 | test: loss:0.5122196878029022 	 acc:0.7615894039735099 	 lr:5e-05
epoch48: train: loss:0.40656700998298395 	 acc:0.9094214876033058 | test: loss:0.5189806419492557 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch49: train: loss:0.409577595488099 	 acc:0.9147107438016528 | test: loss:0.512220794159845 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch50: train: loss:0.4043830870990911 	 acc:0.9203305785123967 | test: loss:0.5324647826074764 	 acc:0.776158940397351 	 lr:2.5e-05
epoch51: train: loss:0.39718331787211836 	 acc:0.9213223140495868 | test: loss:0.507316805669014 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch52: train: loss:0.4036125721320633 	 acc:0.9147107438016528 | test: loss:0.5069753395800559 	 acc:0.7788079470198676 	 lr:2.5e-05
epoch53: train: loss:0.4070251088300027 	 acc:0.9160330578512397 | test: loss:0.5245468465697687 	 acc:0.7814569536423841 	 lr:2.5e-05
epoch54: train: loss:0.4023469670646447 	 acc:0.915702479338843 | test: loss:0.5058780811480339 	 acc:0.7867549668874172 	 lr:2.5e-05
epoch55: train: loss:0.3962293152178615 	 acc:0.911404958677686 | test: loss:0.5058715082951729 	 acc:0.7642384105960265 	 lr:2.5e-05
epoch56: train: loss:0.39618847661767126 	 acc:0.9206611570247933 | test: loss:0.5073119315090558 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch57: train: loss:0.4036282805962996 	 acc:0.9047933884297521 | test: loss:0.5040212849907527 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch58: train: loss:0.4134958413415704 	 acc:0.9127272727272727 | test: loss:0.5304620063068062 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch59: train: loss:0.39099513603636055 	 acc:0.9272727272727272 | test: loss:0.5053337154009484 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch60: train: loss:0.426507126427879 	 acc:0.9028099173553719 | test: loss:0.5906230773357366 	 acc:0.7218543046357616 	 lr:2.5e-05
epoch61: train: loss:0.39437585070113507 	 acc:0.9166942148760331 | test: loss:0.4994642667423021 	 acc:0.7814569536423841 	 lr:2.5e-05
epoch62: train: loss:0.3956870742770266 	 acc:0.9176859504132231 | test: loss:0.49601169694338415 	 acc:0.7880794701986755 	 lr:2.5e-05
epoch63: train: loss:0.387186847757702 	 acc:0.9309090909090909 | test: loss:0.5023549077526623 	 acc:0.7827814569536424 	 lr:2.5e-05
epoch64: train: loss:0.38831348144318445 	 acc:0.9302479338842975 | test: loss:0.5048645613209302 	 acc:0.7894039735099337 	 lr:2.5e-05
epoch65: train: loss:0.39073670183331516 	 acc:0.9216528925619835 | test: loss:0.5039810070928359 	 acc:0.7629139072847683 	 lr:2.5e-05
epoch66: train: loss:0.3886063397521815 	 acc:0.9292561983471075 | test: loss:0.521029404062309 	 acc:0.7735099337748345 	 lr:2.5e-05
epoch67: train: loss:0.38782436548185745 	 acc:0.9312396694214876 | test: loss:0.5112903461551035 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch68: train: loss:0.41283033155212717 	 acc:0.9133884297520661 | test: loss:0.5547497372753573 	 acc:0.7456953642384105 	 lr:2.5e-05
epoch69: train: loss:0.3869913860293459 	 acc:0.9289256198347108 | test: loss:0.5058715977416133 	 acc:0.785430463576159 	 lr:1.25e-05
epoch70: train: loss:0.384395727963487 	 acc:0.9292561983471075 | test: loss:0.5029546035046609 	 acc:0.785430463576159 	 lr:1.25e-05
epoch71: train: loss:0.38451649848094654 	 acc:0.9256198347107438 | test: loss:0.502063693866035 	 acc:0.7708609271523179 	 lr:1.25e-05
epoch72: train: loss:0.38323843652551826 	 acc:0.932892561983471 | test: loss:0.5051048152494114 	 acc:0.785430463576159 	 lr:1.25e-05
epoch73: train: loss:0.39383940261257583 	 acc:0.9176859504132231 | test: loss:0.5068408387386246 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch74: train: loss:0.3856346042589708 	 acc:0.9279338842975207 | test: loss:0.5187488287489935 	 acc:0.7748344370860927 	 lr:1.25e-05
epoch75: train: loss:0.37922229228926096 	 acc:0.9368595041322314 | test: loss:0.5020327626474645 	 acc:0.776158940397351 	 lr:6.25e-06
epoch76: train: loss:0.38168411911026506 	 acc:0.9345454545454546 | test: loss:0.5044257453735301 	 acc:0.7894039735099337 	 lr:6.25e-06
epoch77: train: loss:0.38371701256302765 	 acc:0.9358677685950413 | test: loss:0.5137492022766972 	 acc:0.7748344370860927 	 lr:6.25e-06
epoch78: train: loss:0.38128247301440593 	 acc:0.9418181818181818 | test: loss:0.504738731415856 	 acc:0.7827814569536424 	 lr:6.25e-06
epoch79: train: loss:0.3834882874823799 	 acc:0.9358677685950413 | test: loss:0.5084842637674698 	 acc:0.7788079470198676 	 lr:6.25e-06
epoch80: train: loss:0.37914928842182 	 acc:0.9368595041322314 | test: loss:0.5024268661113764 	 acc:0.785430463576159 	 lr:6.25e-06
epoch81: train: loss:0.3801275307777499 	 acc:0.9345454545454546 | test: loss:0.5013417645795456 	 acc:0.7894039735099337 	 lr:3.125e-06
epoch82: train: loss:0.3793286856639484 	 acc:0.9404958677685951 | test: loss:0.5036002022541122 	 acc:0.785430463576159 	 lr:3.125e-06
epoch83: train: loss:0.38360091293153686 	 acc:0.9348760330578513 | test: loss:0.5029643084829217 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch84: train: loss:0.380179166508115 	 acc:0.9371900826446281 | test: loss:0.503220208985916 	 acc:0.7841059602649006 	 lr:3.125e-06
epoch85: train: loss:0.3808418599830186 	 acc:0.9378512396694215 | test: loss:0.5029894115119581 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch86: train: loss:0.37690532991708803 	 acc:0.9444628099173553 | test: loss:0.5053284873236094 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch87: train: loss:0.3761206222171626 	 acc:0.9434710743801653 | test: loss:0.5031646460886823 	 acc:0.7880794701986755 	 lr:1.5625e-06
epoch88: train: loss:0.3749089317085329 	 acc:0.9421487603305785 | test: loss:0.5023023865080827 	 acc:0.7907284768211921 	 lr:1.5625e-06
epoch89: train: loss:0.38343628194706497 	 acc:0.9335537190082644 | test: loss:0.502774237323281 	 acc:0.7894039735099337 	 lr:1.5625e-06
epoch90: train: loss:0.37562724937092173 	 acc:0.9388429752066115 | test: loss:0.5032239811309915 	 acc:0.7894039735099337 	 lr:1.5625e-06
epoch91: train: loss:0.3810714303166413 	 acc:0.936198347107438 | test: loss:0.5043152339411098 	 acc:0.7920529801324503 	 lr:1.5625e-06
epoch92: train: loss:0.38029872626312505 	 acc:0.9348760330578513 | test: loss:0.5053663046154755 	 acc:0.7867549668874172 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_13_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_13_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.665129290592572 	 acc:0.5709090909090909 | test: loss:0.6638116894968298 	 acc:0.590728476821192 	 lr:0.0001
epoch1: train: loss:0.6524015069993074 	 acc:0.6082644628099173 | test: loss:0.6467311679132727 	 acc:0.6013245033112583 	 lr:0.0001
epoch2: train: loss:0.6326357466918378 	 acc:0.6492561983471075 | test: loss:0.6267173750511068 	 acc:0.6675496688741722 	 lr:0.0001
epoch3: train: loss:0.6107697036246623 	 acc:0.6664462809917355 | test: loss:0.6003792724072539 	 acc:0.6980132450331126 	 lr:0.0001
epoch4: train: loss:0.5881196232078489 	 acc:0.7170247933884297 | test: loss:0.5839746511535139 	 acc:0.7205298013245033 	 lr:0.0001
epoch5: train: loss:0.6277536429058421 	 acc:0.5616528925619835 | test: loss:0.6533748044872916 	 acc:0.5350993377483444 	 lr:0.0001
epoch6: train: loss:0.572508739380797 	 acc:0.7398347107438017 | test: loss:0.568209296030714 	 acc:0.7377483443708609 	 lr:0.0001
epoch7: train: loss:0.5637988286570084 	 acc:0.7345454545454545 | test: loss:0.554074069443128 	 acc:0.7284768211920529 	 lr:0.0001
epoch8: train: loss:0.5694654992986317 	 acc:0.6565289256198347 | test: loss:0.5610085627101115 	 acc:0.6900662251655629 	 lr:0.0001
epoch9: train: loss:0.5503898248593669 	 acc:0.7130578512396695 | test: loss:0.5481594002010017 	 acc:0.7086092715231788 	 lr:0.0001
epoch10: train: loss:0.5526894748506467 	 acc:0.6942148760330579 | test: loss:0.5640776083958859 	 acc:0.6781456953642384 	 lr:0.0001
epoch11: train: loss:0.5426092960223678 	 acc:0.7844628099173554 | test: loss:0.5479328404199209 	 acc:0.7549668874172185 	 lr:0.0001
epoch12: train: loss:0.5538238366379226 	 acc:0.7553719008264462 | test: loss:0.5685452627819895 	 acc:0.7390728476821192 	 lr:0.0001
epoch13: train: loss:0.5282816368292186 	 acc:0.775206611570248 | test: loss:0.5389200433200558 	 acc:0.7483443708609272 	 lr:0.0001
epoch14: train: loss:0.5169924910994601 	 acc:0.780495867768595 | test: loss:0.528747750433865 	 acc:0.7602649006622516 	 lr:0.0001
epoch15: train: loss:0.5232782132172388 	 acc:0.80099173553719 | test: loss:0.5504510626887643 	 acc:0.7721854304635761 	 lr:0.0001
epoch16: train: loss:0.5467863942374868 	 acc:0.771900826446281 | test: loss:0.5598220814932261 	 acc:0.752317880794702 	 lr:0.0001
epoch17: train: loss:0.5477490423730582 	 acc:0.6823140495867769 | test: loss:0.5565141276018509 	 acc:0.6675496688741722 	 lr:0.0001
epoch18: train: loss:0.50040718774165 	 acc:0.8105785123966942 | test: loss:0.5267819725914507 	 acc:0.7735099337748345 	 lr:0.0001
epoch19: train: loss:0.4960949801216441 	 acc:0.7947107438016529 | test: loss:0.5312931851835441 	 acc:0.7258278145695364 	 lr:0.0001
epoch20: train: loss:0.5129418294685931 	 acc:0.7550413223140496 | test: loss:0.5616772031941951 	 acc:0.6675496688741722 	 lr:0.0001
epoch21: train: loss:0.4990659177007754 	 acc:0.7603305785123967 | test: loss:0.5387149748423241 	 acc:0.7033112582781457 	 lr:0.0001
epoch22: train: loss:0.515206991030165 	 acc:0.7996694214876033 | test: loss:0.5556398610405574 	 acc:0.752317880794702 	 lr:0.0001
epoch23: train: loss:0.46625350586638964 	 acc:0.8499173553719008 | test: loss:0.5182177550745326 	 acc:0.7695364238410596 	 lr:0.0001
epoch24: train: loss:0.5152884712790655 	 acc:0.7342148760330579 | test: loss:0.5387503915275169 	 acc:0.7271523178807947 	 lr:0.0001
epoch25: train: loss:0.4750954542869379 	 acc:0.848595041322314 | test: loss:0.5456050071495259 	 acc:0.7576158940397351 	 lr:0.0001
epoch26: train: loss:0.4857666767530205 	 acc:0.8310743801652892 | test: loss:0.5193649080415436 	 acc:0.7801324503311259 	 lr:0.0001
epoch27: train: loss:0.4617932503184011 	 acc:0.8492561983471074 | test: loss:0.5217006540456355 	 acc:0.7682119205298014 	 lr:0.0001
epoch28: train: loss:0.46610156744964854 	 acc:0.8555371900826446 | test: loss:0.5405395447813123 	 acc:0.7615894039735099 	 lr:0.0001
epoch29: train: loss:0.45075468561866067 	 acc:0.8538842975206612 | test: loss:0.511287161767088 	 acc:0.7708609271523179 	 lr:0.0001
epoch30: train: loss:0.4848420044705887 	 acc:0.8406611570247934 | test: loss:0.5392302743646483 	 acc:0.7496688741721854 	 lr:0.0001
epoch31: train: loss:0.4593731951713562 	 acc:0.8585123966942149 | test: loss:0.5289943532438468 	 acc:0.7642384105960265 	 lr:0.0001
epoch32: train: loss:0.4809755915452626 	 acc:0.7887603305785124 | test: loss:0.5400417223671414 	 acc:0.7086092715231788 	 lr:0.0001
epoch33: train: loss:0.427961114290332 	 acc:0.8829752066115703 | test: loss:0.5098573728112985 	 acc:0.7801324503311259 	 lr:0.0001
epoch34: train: loss:0.45022359787925215 	 acc:0.8307438016528925 | test: loss:0.5207953514642273 	 acc:0.7443708609271523 	 lr:0.0001
epoch35: train: loss:0.4893798192079402 	 acc:0.7676033057851239 | test: loss:0.5565054299815602 	 acc:0.6728476821192053 	 lr:0.0001
epoch36: train: loss:0.4370286750202337 	 acc:0.8826446280991735 | test: loss:0.5134740105527915 	 acc:0.776158940397351 	 lr:0.0001
epoch37: train: loss:0.4601180326840109 	 acc:0.8257851239669421 | test: loss:0.5365413934189752 	 acc:0.7364238410596027 	 lr:0.0001
epoch38: train: loss:0.4230979106741503 	 acc:0.888595041322314 | test: loss:0.5216120250967164 	 acc:0.7642384105960265 	 lr:0.0001
epoch39: train: loss:0.4841647282316665 	 acc:0.7695867768595042 | test: loss:0.5800895772232915 	 acc:0.6490066225165563 	 lr:0.0001
epoch40: train: loss:0.41019889274904553 	 acc:0.8978512396694215 | test: loss:0.5141779677757364 	 acc:0.766887417218543 	 lr:5e-05
epoch41: train: loss:0.40640039120823884 	 acc:0.9074380165289256 | test: loss:0.5117944400042098 	 acc:0.7708609271523179 	 lr:5e-05
epoch42: train: loss:0.40167917388529817 	 acc:0.9133884297520661 | test: loss:0.5108843985772291 	 acc:0.7788079470198676 	 lr:5e-05
epoch43: train: loss:0.4067880852754451 	 acc:0.9147107438016528 | test: loss:0.5121238200080316 	 acc:0.7880794701986755 	 lr:5e-05
epoch44: train: loss:0.39305948023953713 	 acc:0.9229752066115703 | test: loss:0.5049675214369566 	 acc:0.7907284768211921 	 lr:5e-05
epoch45: train: loss:0.4084254862848392 	 acc:0.9170247933884298 | test: loss:0.5358626866182744 	 acc:0.766887417218543 	 lr:5e-05
epoch46: train: loss:0.39275657750358267 	 acc:0.927603305785124 | test: loss:0.5165310637840372 	 acc:0.7841059602649006 	 lr:5e-05
epoch47: train: loss:0.3909728626278806 	 acc:0.9292561983471075 | test: loss:0.5120389934407165 	 acc:0.7788079470198676 	 lr:5e-05
epoch48: train: loss:0.38838720709824365 	 acc:0.9262809917355372 | test: loss:0.5317560501445998 	 acc:0.7589403973509934 	 lr:5e-05
epoch49: train: loss:0.42093815563138853 	 acc:0.9021487603305786 | test: loss:0.5795072556331458 	 acc:0.7337748344370861 	 lr:5e-05
epoch50: train: loss:0.3878371885886862 	 acc:0.924297520661157 | test: loss:0.5119378207535143 	 acc:0.7801324503311259 	 lr:5e-05
epoch51: train: loss:0.38339344330070435 	 acc:0.9252892561983471 | test: loss:0.49889218830904425 	 acc:0.7867549668874172 	 lr:2.5e-05
epoch52: train: loss:0.38730841014010847 	 acc:0.9315702479338843 | test: loss:0.5075772897297184 	 acc:0.8013245033112583 	 lr:2.5e-05
epoch53: train: loss:0.38408224489078047 	 acc:0.9279338842975207 | test: loss:0.5040230368146833 	 acc:0.7867549668874172 	 lr:2.5e-05
epoch54: train: loss:0.3820701017163017 	 acc:0.931900826446281 | test: loss:0.5009414769166353 	 acc:0.7973509933774835 	 lr:2.5e-05
epoch55: train: loss:0.3798406464107766 	 acc:0.9272727272727272 | test: loss:0.49994852511298576 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch56: train: loss:0.3802782842541529 	 acc:0.9358677685950413 | test: loss:0.5032659199853606 	 acc:0.7788079470198676 	 lr:2.5e-05
epoch57: train: loss:0.3772506359293441 	 acc:0.9368595041322314 | test: loss:0.5012692700948147 	 acc:0.7947019867549668 	 lr:2.5e-05
epoch58: train: loss:0.3795584135016134 	 acc:0.9358677685950413 | test: loss:0.5040865349453806 	 acc:0.7801324503311259 	 lr:1.25e-05
epoch59: train: loss:0.38041477488092157 	 acc:0.9388429752066115 | test: loss:0.5080844987307164 	 acc:0.785430463576159 	 lr:1.25e-05
epoch60: train: loss:0.3821141743659973 	 acc:0.9418181818181818 | test: loss:0.5183665301626091 	 acc:0.7788079470198676 	 lr:1.25e-05
epoch61: train: loss:0.3776546773240586 	 acc:0.9395041322314049 | test: loss:0.5140845815866988 	 acc:0.776158940397351 	 lr:1.25e-05
epoch62: train: loss:0.377108415246995 	 acc:0.9385123966942148 | test: loss:0.5054705236131781 	 acc:0.7880794701986755 	 lr:1.25e-05
epoch63: train: loss:0.37121786268289425 	 acc:0.9431404958677686 | test: loss:0.502568087435716 	 acc:0.7867549668874172 	 lr:1.25e-05
epoch64: train: loss:0.37285759143592895 	 acc:0.943801652892562 | test: loss:0.5033014881689817 	 acc:0.7933774834437086 	 lr:6.25e-06
epoch65: train: loss:0.37130493702967304 	 acc:0.9457851239669421 | test: loss:0.5071105260722685 	 acc:0.7841059602649006 	 lr:6.25e-06
epoch66: train: loss:0.36888717660233994 	 acc:0.9484297520661157 | test: loss:0.5032478659358246 	 acc:0.785430463576159 	 lr:6.25e-06
epoch67: train: loss:0.37346092423131644 	 acc:0.9461157024793388 | test: loss:0.5082627299605615 	 acc:0.7880794701986755 	 lr:6.25e-06
epoch68: train: loss:0.37303247581828725 	 acc:0.9418181818181818 | test: loss:0.5129915602159816 	 acc:0.7801324503311259 	 lr:6.25e-06
epoch69: train: loss:0.3761435459862071 	 acc:0.9368595041322314 | test: loss:0.5112498224965784 	 acc:0.7907284768211921 	 lr:6.25e-06
epoch70: train: loss:0.3694203106726497 	 acc:0.9421487603305785 | test: loss:0.5090470717442747 	 acc:0.7894039735099337 	 lr:3.125e-06
epoch71: train: loss:0.37097849572000424 	 acc:0.9424793388429752 | test: loss:0.5038112264595285 	 acc:0.7894039735099337 	 lr:3.125e-06
epoch72: train: loss:0.37031905407747945 	 acc:0.9467768595041323 | test: loss:0.5048808469677603 	 acc:0.785430463576159 	 lr:3.125e-06
epoch73: train: loss:0.37052086600587386 	 acc:0.944793388429752 | test: loss:0.5041365737157152 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch74: train: loss:0.3709022725416609 	 acc:0.947107438016529 | test: loss:0.5088368407937864 	 acc:0.7880794701986755 	 lr:3.125e-06
epoch75: train: loss:0.367157388442804 	 acc:0.95900826446281 | test: loss:0.5090039715861643 	 acc:0.7867549668874172 	 lr:3.125e-06
epoch76: train: loss:0.3711893762931351 	 acc:0.9431404958677686 | test: loss:0.5066329287377415 	 acc:0.7827814569536424 	 lr:1.5625e-06
epoch77: train: loss:0.3733480868950363 	 acc:0.9391735537190082 | test: loss:0.5086324475458915 	 acc:0.7867549668874172 	 lr:1.5625e-06
epoch78: train: loss:0.3733483644654928 	 acc:0.943801652892562 | test: loss:0.5072433461416636 	 acc:0.7880794701986755 	 lr:1.5625e-06
epoch79: train: loss:0.37031385695638736 	 acc:0.944793388429752 | test: loss:0.5067257684587643 	 acc:0.785430463576159 	 lr:1.5625e-06
epoch80: train: loss:0.3701130106232383 	 acc:0.943801652892562 | test: loss:0.506766465089179 	 acc:0.7880794701986755 	 lr:1.5625e-06
epoch81: train: loss:0.37087523903728514 	 acc:0.9391735537190082 | test: loss:0.5051136910520642 	 acc:0.7880794701986755 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_14_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_14_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6541345727148135 	 acc:0.5269421487603306 | test: loss:0.6514562579969697 	 acc:0.5311258278145695 	 lr:0.0001
epoch1: train: loss:0.6427261883365222 	 acc:0.5328925619834711 | test: loss:0.6385869207761146 	 acc:0.5403973509933775 	 lr:0.0001
epoch2: train: loss:0.6281084524895534 	 acc:0.6115702479338843 | test: loss:0.6188448233320224 	 acc:0.6344370860927152 	 lr:0.0001
epoch3: train: loss:0.6055273240459852 	 acc:0.6872727272727273 | test: loss:0.5965203424163212 	 acc:0.6940397350993377 	 lr:0.0001
epoch4: train: loss:0.5888356190476536 	 acc:0.716694214876033 | test: loss:0.5833234913301784 	 acc:0.7178807947019867 	 lr:0.0001
epoch5: train: loss:0.6045723795299688 	 acc:0.6003305785123967 | test: loss:0.6261769796049358 	 acc:0.5509933774834437 	 lr:0.0001
epoch6: train: loss:0.5682463621502081 	 acc:0.7428099173553719 | test: loss:0.5581618800857999 	 acc:0.743046357615894 	 lr:0.0001
epoch7: train: loss:0.554406265641047 	 acc:0.7424793388429752 | test: loss:0.5455394524612174 	 acc:0.752317880794702 	 lr:0.0001
epoch8: train: loss:0.5566062041747668 	 acc:0.6773553719008264 | test: loss:0.5650394770483308 	 acc:0.6596026490066225 	 lr:0.0001
epoch9: train: loss:0.5358024026933781 	 acc:0.7477685950413223 | test: loss:0.5345451771818249 	 acc:0.7390728476821192 	 lr:0.0001
epoch10: train: loss:0.5351351633544795 	 acc:0.7299173553719008 | test: loss:0.5389361006534652 	 acc:0.7298013245033113 	 lr:0.0001
epoch11: train: loss:0.5266506348759675 	 acc:0.7854545454545454 | test: loss:0.5356730154018529 	 acc:0.7509933774834437 	 lr:0.0001
epoch12: train: loss:0.6059510739381648 	 acc:0.7100826446280992 | test: loss:0.64374390437903 	 acc:0.6609271523178808 	 lr:0.0001
epoch13: train: loss:0.5239310856298967 	 acc:0.7884297520661157 | test: loss:0.5430827057914228 	 acc:0.7589403973509934 	 lr:0.0001
epoch14: train: loss:0.5128467487894799 	 acc:0.7993388429752066 | test: loss:0.5338616861412857 	 acc:0.7615894039735099 	 lr:0.0001
epoch15: train: loss:0.5424033493049873 	 acc:0.7894214876033058 | test: loss:0.5833965369407704 	 acc:0.7298013245033113 	 lr:0.0001
epoch16: train: loss:0.5279308162838959 	 acc:0.7947107438016529 | test: loss:0.5558527834368068 	 acc:0.7576158940397351 	 lr:0.0001
epoch17: train: loss:0.5238770432511637 	 acc:0.7256198347107438 | test: loss:0.5517397300297061 	 acc:0.6913907284768211 	 lr:0.0001
epoch18: train: loss:0.48379206295841 	 acc:0.8112396694214876 | test: loss:0.5239957404452444 	 acc:0.7456953642384105 	 lr:0.0001
epoch19: train: loss:0.4836263596814526 	 acc:0.8274380165289256 | test: loss:0.5227430172313917 	 acc:0.7496688741721854 	 lr:0.0001
epoch20: train: loss:0.4884517100428747 	 acc:0.8343801652892562 | test: loss:0.5408650580620924 	 acc:0.7483443708609272 	 lr:0.0001
epoch21: train: loss:0.4757375205252781 	 acc:0.8168595041322314 | test: loss:0.5305337740885501 	 acc:0.7258278145695364 	 lr:0.0001
epoch22: train: loss:0.49631611861473274 	 acc:0.8082644628099174 | test: loss:0.5354768265951548 	 acc:0.7298013245033113 	 lr:0.0001
epoch23: train: loss:0.4549173193135537 	 acc:0.8469421487603306 | test: loss:0.5221420263612507 	 acc:0.7496688741721854 	 lr:0.0001
epoch24: train: loss:0.4647648790848157 	 acc:0.8254545454545454 | test: loss:0.5264610272369638 	 acc:0.7364238410596027 	 lr:0.0001
epoch25: train: loss:0.4711152387552025 	 acc:0.8456198347107438 | test: loss:0.5558424592807593 	 acc:0.7483443708609272 	 lr:0.0001
epoch26: train: loss:0.45173347171673106 	 acc:0.8489256198347107 | test: loss:0.5058228105898724 	 acc:0.7735099337748345 	 lr:0.0001
epoch27: train: loss:0.5045646982547666 	 acc:0.7537190082644628 | test: loss:0.5571941582572382 	 acc:0.6940397350993377 	 lr:0.0001
epoch28: train: loss:0.4384730580424474 	 acc:0.8747107438016529 | test: loss:0.5273985708786162 	 acc:0.7536423841059603 	 lr:0.0001
epoch29: train: loss:0.44755938483663826 	 acc:0.8472727272727273 | test: loss:0.5227776641877282 	 acc:0.7470198675496689 	 lr:0.0001
epoch30: train: loss:0.5249958695458972 	 acc:0.8026446280991736 | test: loss:0.6255286798571909 	 acc:0.6847682119205298 	 lr:0.0001
epoch31: train: loss:0.4404679780164041 	 acc:0.888595041322314 | test: loss:0.5330030466547075 	 acc:0.7695364238410596 	 lr:0.0001
epoch32: train: loss:0.4521475759518048 	 acc:0.8449586776859505 | test: loss:0.5331990861734807 	 acc:0.7390728476821192 	 lr:0.0001
epoch33: train: loss:0.4176940720041921 	 acc:0.8915702479338843 | test: loss:0.5148282759236974 	 acc:0.7509933774834437 	 lr:5e-05
epoch34: train: loss:0.4210814974327718 	 acc:0.8803305785123967 | test: loss:0.5172822667273465 	 acc:0.7655629139072848 	 lr:5e-05
epoch35: train: loss:0.4228403985401816 	 acc:0.9054545454545454 | test: loss:0.5419075662726598 	 acc:0.7708609271523179 	 lr:5e-05
epoch36: train: loss:0.412806066363311 	 acc:0.8849586776859504 | test: loss:0.5158930857449967 	 acc:0.752317880794702 	 lr:5e-05
epoch37: train: loss:0.41146348291192175 	 acc:0.8965289256198347 | test: loss:0.5197501463605868 	 acc:0.7655629139072848 	 lr:5e-05
epoch38: train: loss:0.41760638631079805 	 acc:0.9054545454545454 | test: loss:0.5373212854593795 	 acc:0.7496688741721854 	 lr:5e-05
epoch39: train: loss:0.39428798115943087 	 acc:0.9223140495867769 | test: loss:0.5191204058413474 	 acc:0.7655629139072848 	 lr:2.5e-05
epoch40: train: loss:0.39041639661985983 	 acc:0.9285950413223141 | test: loss:0.5179028244208026 	 acc:0.7748344370860927 	 lr:2.5e-05
epoch41: train: loss:0.3916552038133637 	 acc:0.9143801652892563 | test: loss:0.5113366799638761 	 acc:0.7682119205298014 	 lr:2.5e-05
epoch42: train: loss:0.3849665173617276 	 acc:0.9355371900826446 | test: loss:0.5117496643634821 	 acc:0.7814569536423841 	 lr:2.5e-05
epoch43: train: loss:0.3867212205780439 	 acc:0.932892561983471 | test: loss:0.5133775831058326 	 acc:0.776158940397351 	 lr:2.5e-05
epoch44: train: loss:0.3807099520569005 	 acc:0.9371900826446281 | test: loss:0.511318764149748 	 acc:0.7801324503311259 	 lr:2.5e-05
epoch45: train: loss:0.38339257088574497 	 acc:0.932892561983471 | test: loss:0.5076179405711344 	 acc:0.7801324503311259 	 lr:1.25e-05
epoch46: train: loss:0.3863606530575713 	 acc:0.9302479338842975 | test: loss:0.5155815500297294 	 acc:0.7814569536423841 	 lr:1.25e-05
epoch47: train: loss:0.38089514944179 	 acc:0.9378512396694215 | test: loss:0.5131402891203267 	 acc:0.776158940397351 	 lr:1.25e-05
epoch48: train: loss:0.3798225562434551 	 acc:0.9401652892561984 | test: loss:0.5160790061319112 	 acc:0.776158940397351 	 lr:1.25e-05
epoch49: train: loss:0.3812724137404733 	 acc:0.9378512396694215 | test: loss:0.5134835858218717 	 acc:0.7735099337748345 	 lr:1.25e-05
epoch50: train: loss:0.3774372317771281 	 acc:0.9424793388429752 | test: loss:0.513836681842804 	 acc:0.7721854304635761 	 lr:1.25e-05
epoch51: train: loss:0.3804132929813763 	 acc:0.9371900826446281 | test: loss:0.5196484662049653 	 acc:0.7682119205298014 	 lr:6.25e-06
epoch52: train: loss:0.38342590298534424 	 acc:0.9305785123966942 | test: loss:0.5154723158735313 	 acc:0.7695364238410596 	 lr:6.25e-06
epoch53: train: loss:0.3832386796927649 	 acc:0.931900826446281 | test: loss:0.513202161662626 	 acc:0.7642384105960265 	 lr:6.25e-06
epoch54: train: loss:0.37882144593010264 	 acc:0.9338842975206612 | test: loss:0.5149916745969002 	 acc:0.7682119205298014 	 lr:6.25e-06
epoch55: train: loss:0.3783197803044122 	 acc:0.9368595041322314 | test: loss:0.5150941065605114 	 acc:0.7708609271523179 	 lr:6.25e-06
epoch56: train: loss:0.3770878884220911 	 acc:0.9404958677685951 | test: loss:0.5142859040506628 	 acc:0.7721854304635761 	 lr:6.25e-06
epoch57: train: loss:0.3773190229786329 	 acc:0.9348760330578513 | test: loss:0.5117826536001748 	 acc:0.766887417218543 	 lr:3.125e-06
epoch58: train: loss:0.3828723706095672 	 acc:0.9335537190082644 | test: loss:0.5134980752768106 	 acc:0.7695364238410596 	 lr:3.125e-06
epoch59: train: loss:0.37820319272269887 	 acc:0.9414876033057851 | test: loss:0.5125984633995208 	 acc:0.766887417218543 	 lr:3.125e-06
epoch60: train: loss:0.37993280712238026 	 acc:0.9378512396694215 | test: loss:0.5151519236185693 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch61: train: loss:0.3786525731736963 	 acc:0.9365289256198347 | test: loss:0.5209008806588634 	 acc:0.7801324503311259 	 lr:3.125e-06
epoch62: train: loss:0.3806743635520462 	 acc:0.9375206611570248 | test: loss:0.5138749232355333 	 acc:0.7735099337748345 	 lr:3.125e-06
epoch63: train: loss:0.3730643146983848 	 acc:0.947107438016529 | test: loss:0.5127147594035066 	 acc:0.7774834437086092 	 lr:1.5625e-06
epoch64: train: loss:0.3742305850884146 	 acc:0.944793388429752 | test: loss:0.5126421682092528 	 acc:0.7721854304635761 	 lr:1.5625e-06
epoch65: train: loss:0.3729560635700699 	 acc:0.9444628099173553 | test: loss:0.5131418984457357 	 acc:0.7721854304635761 	 lr:1.5625e-06
epoch66: train: loss:0.37462959523043354 	 acc:0.9411570247933885 | test: loss:0.5128650981858867 	 acc:0.7721854304635761 	 lr:1.5625e-06
epoch67: train: loss:0.37601509883384077 	 acc:0.9411570247933885 | test: loss:0.5127743809428436 	 acc:0.7721854304635761 	 lr:1.5625e-06
epoch68: train: loss:0.37693558699828533 	 acc:0.9395041322314049 | test: loss:0.5117813516136827 	 acc:0.7774834437086092 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_15_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_15_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6530287901429105 	 acc:0.6 | test: loss:0.6551870867906028 	 acc:0.6039735099337749 	 lr:0.0001
epoch1: train: loss:0.617375267340132 	 acc:0.5904132231404958 | test: loss:0.6148940542675801 	 acc:0.5947019867549669 	 lr:0.0001
epoch2: train: loss:0.6000751612994296 	 acc:0.6380165289256199 | test: loss:0.5964903267803571 	 acc:0.6344370860927152 	 lr:0.0001
epoch3: train: loss:0.6170377317735971 	 acc:0.6955371900826446 | test: loss:0.6261136782879861 	 acc:0.6662251655629139 	 lr:0.0001
epoch4: train: loss:0.5768556130227964 	 acc:0.6978512396694215 | test: loss:0.5731894662048643 	 acc:0.7192052980132451 	 lr:0.0001
epoch5: train: loss:0.5632435135407882 	 acc:0.696198347107438 | test: loss:0.5583470067441069 	 acc:0.7006622516556291 	 lr:0.0001
epoch6: train: loss:0.5705027909515318 	 acc:0.7451239669421488 | test: loss:0.5727017864486239 	 acc:0.7337748344370861 	 lr:0.0001
epoch7: train: loss:0.5565744776568137 	 acc:0.7322314049586777 | test: loss:0.5547500253513159 	 acc:0.7271523178807947 	 lr:0.0001
epoch8: train: loss:0.545188311860581 	 acc:0.7471074380165289 | test: loss:0.5449348397602308 	 acc:0.7562913907284768 	 lr:0.0001
epoch9: train: loss:0.5442485026879744 	 acc:0.7715702479338843 | test: loss:0.5621919317750741 	 acc:0.7311258278145696 	 lr:0.0001
epoch10: train: loss:0.5741837619158847 	 acc:0.6390082644628099 | test: loss:0.5923492754532012 	 acc:0.6158940397350994 	 lr:0.0001
epoch11: train: loss:0.5695967451008883 	 acc:0.7517355371900827 | test: loss:0.5891712768188375 	 acc:0.7019867549668874 	 lr:0.0001
epoch12: train: loss:0.5220258323811302 	 acc:0.7583471074380165 | test: loss:0.5347512505701836 	 acc:0.7350993377483444 	 lr:0.0001
epoch13: train: loss:0.5703747756225018 	 acc:0.7520661157024794 | test: loss:0.6066280060256554 	 acc:0.7072847682119205 	 lr:0.0001
epoch14: train: loss:0.5435208560218496 	 acc:0.7732231404958678 | test: loss:0.5581151487811512 	 acc:0.7576158940397351 	 lr:0.0001
epoch15: train: loss:0.5134640549234122 	 acc:0.7732231404958678 | test: loss:0.5350212878738807 	 acc:0.7337748344370861 	 lr:0.0001
epoch16: train: loss:0.5075180235480474 	 acc:0.8069421487603305 | test: loss:0.5421828027592589 	 acc:0.7443708609271523 	 lr:0.0001
epoch17: train: loss:0.4898338227134106 	 acc:0.8138842975206612 | test: loss:0.5249634603790889 	 acc:0.7695364238410596 	 lr:0.0001
epoch18: train: loss:0.48447717307027705 	 acc:0.8231404958677686 | test: loss:0.533233230003458 	 acc:0.7390728476821192 	 lr:0.0001
epoch19: train: loss:0.4887376218886415 	 acc:0.8168595041322314 | test: loss:0.5507343618285577 	 acc:0.7377483443708609 	 lr:0.0001
epoch20: train: loss:0.5534544183991172 	 acc:0.7682644628099173 | test: loss:0.636098426856742 	 acc:0.6794701986754967 	 lr:0.0001
epoch21: train: loss:0.4735982890759618 	 acc:0.8370247933884297 | test: loss:0.5255682677622663 	 acc:0.7629139072847683 	 lr:0.0001
epoch22: train: loss:0.47915589304994943 	 acc:0.8019834710743802 | test: loss:0.5344590896802233 	 acc:0.7245033112582782 	 lr:0.0001
epoch23: train: loss:0.5019686217741532 	 acc:0.819504132231405 | test: loss:0.605194346004764 	 acc:0.7033112582781457 	 lr:0.0001
epoch24: train: loss:0.4901003667933882 	 acc:0.84 | test: loss:0.5699949217947903 	 acc:0.7350993377483444 	 lr:5e-05
epoch25: train: loss:0.46377078264212807 	 acc:0.8555371900826446 | test: loss:0.551831966914878 	 acc:0.7390728476821192 	 lr:5e-05
epoch26: train: loss:0.44736428595771477 	 acc:0.8624793388429752 | test: loss:0.5307431784686664 	 acc:0.7655629139072848 	 lr:5e-05
epoch27: train: loss:0.44668893135283605 	 acc:0.8568595041322314 | test: loss:0.5317959353623801 	 acc:0.7470198675496689 	 lr:5e-05
epoch28: train: loss:0.44951372946589446 	 acc:0.876694214876033 | test: loss:0.5497444770194048 	 acc:0.752317880794702 	 lr:5e-05
epoch29: train: loss:0.44341121690332397 	 acc:0.8641322314049587 | test: loss:0.5380973179608781 	 acc:0.7311258278145696 	 lr:5e-05
epoch30: train: loss:0.42805954816912817 	 acc:0.8932231404958678 | test: loss:0.5263817755591791 	 acc:0.7576158940397351 	 lr:2.5e-05
epoch31: train: loss:0.42747586329121234 	 acc:0.8895867768595042 | test: loss:0.5302845990420967 	 acc:0.7589403973509934 	 lr:2.5e-05
epoch32: train: loss:0.4344708423772134 	 acc:0.8568595041322314 | test: loss:0.53030527988017 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch33: train: loss:0.4218893442666235 	 acc:0.8925619834710744 | test: loss:0.5263260251638905 	 acc:0.7602649006622516 	 lr:2.5e-05
epoch34: train: loss:0.41968727694070046 	 acc:0.8995041322314049 | test: loss:0.5441683080812164 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch35: train: loss:0.42424746958677434 	 acc:0.9024793388429752 | test: loss:0.5401046508195384 	 acc:0.743046357615894 	 lr:2.5e-05
epoch36: train: loss:0.41756553500151833 	 acc:0.8971900826446281 | test: loss:0.5279396147917438 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch37: train: loss:0.41197012509196257 	 acc:0.908099173553719 | test: loss:0.534223982514135 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch38: train: loss:0.4130413098473194 	 acc:0.9057851239669421 | test: loss:0.524512647082474 	 acc:0.7496688741721854 	 lr:1.25e-05
epoch39: train: loss:0.4154331947850787 	 acc:0.9097520661157025 | test: loss:0.5430586158834546 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch40: train: loss:0.40857983363561395 	 acc:0.9107438016528926 | test: loss:0.5304707701632518 	 acc:0.7536423841059603 	 lr:1.25e-05
epoch41: train: loss:0.4115939310956592 	 acc:0.9024793388429752 | test: loss:0.5269093054019852 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch42: train: loss:0.4164175835128658 	 acc:0.9028099173553719 | test: loss:0.5349864223145491 	 acc:0.7589403973509934 	 lr:1.25e-05
epoch43: train: loss:0.40918787585802313 	 acc:0.9047933884297521 | test: loss:0.5293502956036701 	 acc:0.7576158940397351 	 lr:1.25e-05
epoch44: train: loss:0.4068693995475769 	 acc:0.9074380165289256 | test: loss:0.5315993836383945 	 acc:0.7483443708609272 	 lr:1.25e-05
epoch45: train: loss:0.4111210401688726 	 acc:0.9071074380165289 | test: loss:0.5350680458624631 	 acc:0.7562913907284768 	 lr:6.25e-06
epoch46: train: loss:0.41015067995087173 	 acc:0.9087603305785124 | test: loss:0.5299607508229893 	 acc:0.7602649006622516 	 lr:6.25e-06
epoch47: train: loss:0.4069917825135318 	 acc:0.9087603305785124 | test: loss:0.5290964496056765 	 acc:0.7655629139072848 	 lr:6.25e-06
epoch48: train: loss:0.397497063412154 	 acc:0.9213223140495868 | test: loss:0.5287046063025266 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch49: train: loss:0.4016519864433068 	 acc:0.9180165289256198 | test: loss:0.5288632408672611 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch50: train: loss:0.4011465132729081 	 acc:0.9186776859504132 | test: loss:0.5357433023831703 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch51: train: loss:0.3973955153926345 	 acc:0.9223140495867769 | test: loss:0.5360539484497727 	 acc:0.7615894039735099 	 lr:3.125e-06
epoch52: train: loss:0.40030982132785575 	 acc:0.9120661157024793 | test: loss:0.5307144667139116 	 acc:0.7589403973509934 	 lr:3.125e-06
epoch53: train: loss:0.3993140471178638 	 acc:0.92 | test: loss:0.529518386070302 	 acc:0.7549668874172185 	 lr:3.125e-06
epoch54: train: loss:0.39915849021643646 	 acc:0.9272727272727272 | test: loss:0.5310482918031958 	 acc:0.7562913907284768 	 lr:3.125e-06
epoch55: train: loss:0.3973411248045519 	 acc:0.9249586776859504 | test: loss:0.5309644693570421 	 acc:0.7562913907284768 	 lr:3.125e-06
epoch56: train: loss:0.3994295302599915 	 acc:0.9193388429752066 | test: loss:0.5294182463986984 	 acc:0.7549668874172185 	 lr:3.125e-06
epoch57: train: loss:0.4001907801726633 	 acc:0.9206611570247933 | test: loss:0.5329009631611653 	 acc:0.7576158940397351 	 lr:1.5625e-06
epoch58: train: loss:0.3997612948752632 	 acc:0.9166942148760331 | test: loss:0.5321532461816901 	 acc:0.7549668874172185 	 lr:1.5625e-06
epoch59: train: loss:0.40104908776677345 	 acc:0.9150413223140496 | test: loss:0.531011608815351 	 acc:0.7576158940397351 	 lr:1.5625e-06
epoch60: train: loss:0.39718152273784985 	 acc:0.9219834710743802 | test: loss:0.528925588746734 	 acc:0.7629139072847683 	 lr:1.5625e-06
epoch61: train: loss:0.40275767474135094 	 acc:0.9137190082644628 | test: loss:0.5293814931484248 	 acc:0.7562913907284768 	 lr:1.5625e-06
epoch62: train: loss:0.39550410678564024 	 acc:0.9256198347107438 | test: loss:0.5297592613081269 	 acc:0.7536423841059603 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_16_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_16_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.6449546052798752 	 acc:0.5325619834710744 | test: loss:0.6433819264765607 	 acc:0.5337748344370861 	 lr:0.0001
epoch1: train: loss:0.6208725623453945 	 acc:0.672396694214876 | test: loss:0.6292057976027987 	 acc:0.6582781456953642 	 lr:0.0001
epoch2: train: loss:0.5974894831003237 	 acc:0.6145454545454545 | test: loss:0.603481277014246 	 acc:0.6052980132450331 	 lr:0.0001
epoch3: train: loss:0.5757268909974532 	 acc:0.6985123966942148 | test: loss:0.573316080522853 	 acc:0.704635761589404 	 lr:0.0001
epoch4: train: loss:0.5625721520234731 	 acc:0.6938842975206612 | test: loss:0.5701130372798996 	 acc:0.6900662251655629 	 lr:0.0001
epoch5: train: loss:0.5812515265291387 	 acc:0.6300826446280992 | test: loss:0.5871084221940956 	 acc:0.6370860927152318 	 lr:0.0001
epoch6: train: loss:0.5687298764867231 	 acc:0.7431404958677686 | test: loss:0.5897479532570239 	 acc:0.7112582781456953 	 lr:0.0001
epoch7: train: loss:0.5739847550904456 	 acc:0.739504132231405 | test: loss:0.5827115804943818 	 acc:0.7072847682119205 	 lr:0.0001
epoch8: train: loss:0.5370461952390749 	 acc:0.7742148760330578 | test: loss:0.5602384977782799 	 acc:0.7350993377483444 	 lr:0.0001
epoch9: train: loss:0.5327534979434053 	 acc:0.7490909090909091 | test: loss:0.5552487145196523 	 acc:0.7311258278145696 	 lr:0.0001
epoch10: train: loss:0.5698685595811891 	 acc:0.7447933884297521 | test: loss:0.6193940118448624 	 acc:0.6768211920529801 	 lr:0.0001
epoch11: train: loss:0.5124098239260272 	 acc:0.7917355371900826 | test: loss:0.5472537572020727 	 acc:0.7377483443708609 	 lr:0.0001
epoch12: train: loss:0.5282651636423158 	 acc:0.7818181818181819 | test: loss:0.5654577630245133 	 acc:0.7231788079470198 	 lr:0.0001
epoch13: train: loss:0.5709921529864477 	 acc:0.7454545454545455 | test: loss:0.6428085616092808 	 acc:0.6649006622516557 	 lr:0.0001
epoch14: train: loss:0.5046942383789819 	 acc:0.8085950413223141 | test: loss:0.5540732544778988 	 acc:0.7350993377483444 	 lr:0.0001
epoch15: train: loss:0.49782198485264106 	 acc:0.8168595041322314 | test: loss:0.5571503275277598 	 acc:0.7403973509933774 	 lr:0.0001
epoch16: train: loss:0.4931524072895365 	 acc:0.8013223140495868 | test: loss:0.5523273354334547 	 acc:0.7192052980132451 	 lr:0.0001
epoch17: train: loss:0.488686504827058 	 acc:0.8085950413223141 | test: loss:0.5301783000396577 	 acc:0.7483443708609272 	 lr:0.0001
epoch18: train: loss:0.4904573433852393 	 acc:0.7791735537190083 | test: loss:0.5453454717895053 	 acc:0.7152317880794702 	 lr:0.0001
epoch19: train: loss:0.5069372369632248 	 acc:0.8089256198347108 | test: loss:0.5832912035335768 	 acc:0.7086092715231788 	 lr:0.0001
epoch20: train: loss:0.4776405222061252 	 acc:0.827107438016529 | test: loss:0.5657833825673488 	 acc:0.7205298013245033 	 lr:0.0001
epoch21: train: loss:0.4754933726984607 	 acc:0.8112396694214876 | test: loss:0.5476778633547145 	 acc:0.7033112582781457 	 lr:0.0001
epoch22: train: loss:0.5002516042496548 	 acc:0.768595041322314 | test: loss:0.5558155589545799 	 acc:0.6874172185430464 	 lr:0.0001
epoch23: train: loss:0.4606516647634427 	 acc:0.8423140495867769 | test: loss:0.5631913950901157 	 acc:0.7165562913907285 	 lr:0.0001
epoch24: train: loss:0.4523712966658852 	 acc:0.8700826446280991 | test: loss:0.5526721562770818 	 acc:0.7443708609271523 	 lr:5e-05
epoch25: train: loss:0.44845000585248646 	 acc:0.8608264462809917 | test: loss:0.545995985830067 	 acc:0.7470198675496689 	 lr:5e-05
epoch26: train: loss:0.44253729104995726 	 acc:0.8700826446280991 | test: loss:0.5423698527134018 	 acc:0.7417218543046358 	 lr:5e-05
epoch27: train: loss:0.4501258893939089 	 acc:0.8700826446280991 | test: loss:0.553411391870865 	 acc:0.7390728476821192 	 lr:5e-05
epoch28: train: loss:0.4376161880335532 	 acc:0.8836363636363637 | test: loss:0.5413696918266498 	 acc:0.7456953642384105 	 lr:5e-05
epoch29: train: loss:0.4434111413679832 	 acc:0.8522314049586777 | test: loss:0.5320634030348418 	 acc:0.7337748344370861 	 lr:5e-05
epoch30: train: loss:0.43689022002141337 	 acc:0.8836363636363637 | test: loss:0.5604598642185035 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch31: train: loss:0.41907042388088445 	 acc:0.8912396694214876 | test: loss:0.5194248988138919 	 acc:0.7549668874172185 	 lr:2.5e-05
epoch32: train: loss:0.4138978905126083 	 acc:0.8938842975206611 | test: loss:0.5229567703821801 	 acc:0.752317880794702 	 lr:2.5e-05
epoch33: train: loss:0.41545398884568335 	 acc:0.8991735537190083 | test: loss:0.5266138552040454 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch34: train: loss:0.41126040614340914 	 acc:0.9057851239669421 | test: loss:0.5404416621915552 	 acc:0.7377483443708609 	 lr:2.5e-05
epoch35: train: loss:0.42044870755889197 	 acc:0.9018181818181819 | test: loss:0.5463119441310301 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch36: train: loss:0.41508272754259345 	 acc:0.9008264462809917 | test: loss:0.5254292628623003 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch37: train: loss:0.4063854928745711 	 acc:0.8991735537190083 | test: loss:0.5327101141411738 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch38: train: loss:0.4045725816340486 	 acc:0.9216528925619835 | test: loss:0.5348301118572816 	 acc:0.7390728476821192 	 lr:1.25e-05
epoch39: train: loss:0.40171140280636874 	 acc:0.9166942148760331 | test: loss:0.5392075563108685 	 acc:0.7456953642384105 	 lr:1.25e-05
epoch40: train: loss:0.4033157720940172 	 acc:0.9100826446280992 | test: loss:0.5349594083842852 	 acc:0.7470198675496689 	 lr:1.25e-05
epoch41: train: loss:0.4052257118737402 	 acc:0.9104132231404959 | test: loss:0.5315569253946771 	 acc:0.7403973509933774 	 lr:1.25e-05
epoch42: train: loss:0.4048633461451728 	 acc:0.9094214876033058 | test: loss:0.5331466168757306 	 acc:0.7509933774834437 	 lr:1.25e-05
epoch43: train: loss:0.4018331061907051 	 acc:0.9110743801652893 | test: loss:0.5283963994474601 	 acc:0.7549668874172185 	 lr:1.25e-05
epoch44: train: loss:0.3948975269755056 	 acc:0.9266115702479338 | test: loss:0.5299030377375369 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch45: train: loss:0.40536562644745694 	 acc:0.9133884297520661 | test: loss:0.5357475682599655 	 acc:0.7576158940397351 	 lr:6.25e-06
epoch46: train: loss:0.40167796754639995 	 acc:0.9186776859504132 | test: loss:0.5365078956086115 	 acc:0.7509933774834437 	 lr:6.25e-06
epoch47: train: loss:0.3960753925871258 	 acc:0.9216528925619835 | test: loss:0.533769695727241 	 acc:0.7549668874172185 	 lr:6.25e-06
epoch48: train: loss:0.3938299330108422 	 acc:0.9203305785123967 | test: loss:0.5354757818165204 	 acc:0.7615894039735099 	 lr:6.25e-06
epoch49: train: loss:0.3953999031476738 	 acc:0.9226446280991736 | test: loss:0.5360656199076318 	 acc:0.7483443708609272 	 lr:6.25e-06
epoch50: train: loss:0.3944038222742475 	 acc:0.9190082644628099 | test: loss:0.539103891044263 	 acc:0.752317880794702 	 lr:3.125e-06
epoch51: train: loss:0.39345085549945674 	 acc:0.9262809917355372 | test: loss:0.5402826403150496 	 acc:0.7536423841059603 	 lr:3.125e-06
epoch52: train: loss:0.38891571852786483 	 acc:0.9302479338842975 | test: loss:0.5343500347326923 	 acc:0.7496688741721854 	 lr:3.125e-06
epoch53: train: loss:0.3937155427321915 	 acc:0.927603305785124 | test: loss:0.533788583689178 	 acc:0.7509933774834437 	 lr:3.125e-06
epoch54: train: loss:0.3944033888745899 	 acc:0.9262809917355372 | test: loss:0.5365659304801992 	 acc:0.752317880794702 	 lr:3.125e-06
epoch55: train: loss:0.3917132636535266 	 acc:0.9266115702479338 | test: loss:0.5352344887935563 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch56: train: loss:0.3911619836929416 	 acc:0.9256198347107438 | test: loss:0.5311285319707252 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch57: train: loss:0.3953542850529852 	 acc:0.9256198347107438 | test: loss:0.5362884902006743 	 acc:0.7549668874172185 	 lr:1.5625e-06
epoch58: train: loss:0.39354565823373716 	 acc:0.924297520661157 | test: loss:0.5346193129653173 	 acc:0.7509933774834437 	 lr:1.5625e-06
epoch59: train: loss:0.39507225622815534 	 acc:0.9229752066115703 | test: loss:0.5332238626795889 	 acc:0.7496688741721854 	 lr:1.5625e-06
epoch60: train: loss:0.38855774771083484 	 acc:0.9299173553719008 | test: loss:0.5323716176266702 	 acc:0.743046357615894 	 lr:1.5625e-06
epoch61: train: loss:0.39717525120609065 	 acc:0.9236363636363636 | test: loss:0.5335505675006387 	 acc:0.752317880794702 	 lr:1.5625e-06

Bad key text.latex.preview in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key mathtext.fallback_to_cm in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key savefig.jpeg_quality in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key keymap.all_axes in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_path in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution

Bad key animation.avconv_args in file /home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.5.2/matplotlibrc.template
or from the matplotlib source distribution
weights for positive classes: tensor([1.4829, 1.0000], device='cuda:0')
../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_17_3/
Training on BIMCV, create new exp container at ../checkpoints_noisy_nopooling/BIMCV/layerttl_resnet50_imagenet_17_3/
21
training with  layerttl_resnet50
/home/jusun/peng0347/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
epoch0: train: loss:0.7173698434750896 	 acc:0.5867768595041323 | test: loss:0.7619673977624501 	 acc:0.543046357615894 	 lr:0.0001
epoch1: train: loss:0.6401637254667676 	 acc:0.5553719008264463 | test: loss:0.639164879779942 	 acc:0.5642384105960265 	 lr:0.0001
epoch2: train: loss:0.6168563834891831 	 acc:0.5725619834710743 | test: loss:0.62220113032701 	 acc:0.5708609271523178 	 lr:0.0001
epoch3: train: loss:0.5997254968674715 	 acc:0.6733884297520661 | test: loss:0.607574528416261 	 acc:0.6768211920529801 	 lr:0.0001
epoch4: train: loss:0.5849019383990075 	 acc:0.6347107438016529 | test: loss:0.5862919047178812 	 acc:0.6357615894039735 	 lr:0.0001
epoch5: train: loss:0.56618645153755 	 acc:0.6945454545454546 | test: loss:0.5748021463684688 	 acc:0.6913907284768211 	 lr:0.0001
epoch6: train: loss:0.5697524211623451 	 acc:0.7193388429752066 | test: loss:0.5820836405880404 	 acc:0.7165562913907285 	 lr:0.0001
epoch7: train: loss:0.5730584912457742 	 acc:0.724297520661157 | test: loss:0.5685060637676164 	 acc:0.7178807947019867 	 lr:0.0001
epoch8: train: loss:0.5590686259190898 	 acc:0.7398347107438017 | test: loss:0.5726662153439805 	 acc:0.7390728476821192 	 lr:0.0001
epoch9: train: loss:0.5332882984216548 	 acc:0.7375206611570247 | test: loss:0.5489199701523939 	 acc:0.7284768211920529 	 lr:0.0001
epoch10: train: loss:0.5404998071134584 	 acc:0.7573553719008265 | test: loss:0.5506908874638033 	 acc:0.7245033112582782 	 lr:0.0001
epoch11: train: loss:0.5250559667319306 	 acc:0.751404958677686 | test: loss:0.5487062645274282 	 acc:0.7165562913907285 	 lr:0.0001
epoch12: train: loss:0.5337571170704424 	 acc:0.743801652892562 | test: loss:0.5572346414951299 	 acc:0.6966887417218544 	 lr:0.0001
epoch13: train: loss:0.5313490172457104 	 acc:0.7811570247933884 | test: loss:0.5596190550469404 	 acc:0.7298013245033113 	 lr:0.0001
epoch14: train: loss:0.5324193849248334 	 acc:0.7887603305785124 | test: loss:0.5619337095330094 	 acc:0.7350993377483444 	 lr:0.0001
epoch15: train: loss:0.5322383349394996 	 acc:0.7834710743801653 | test: loss:0.590806064305716 	 acc:0.7125827814569536 	 lr:0.0001
epoch16: train: loss:0.49859384231330933 	 acc:0.7940495867768596 | test: loss:0.5364622508453217 	 acc:0.7311258278145696 	 lr:0.0001
epoch17: train: loss:0.5086345234981253 	 acc:0.8006611570247933 | test: loss:0.5678099771209111 	 acc:0.7271523178807947 	 lr:0.0001
epoch18: train: loss:0.5104623183928245 	 acc:0.7950413223140496 | test: loss:0.5574631632558558 	 acc:0.7443708609271523 	 lr:0.0001
epoch19: train: loss:0.5054249909397 	 acc:0.7887603305785124 | test: loss:0.5452559539813869 	 acc:0.7298013245033113 	 lr:0.0001
epoch20: train: loss:0.49211990282555257 	 acc:0.8148760330578513 | test: loss:0.569605596254993 	 acc:0.713907284768212 	 lr:0.0001
epoch21: train: loss:0.498750071880246 	 acc:0.7801652892561983 | test: loss:0.5601536084484581 	 acc:0.680794701986755 	 lr:0.0001
epoch22: train: loss:0.4754387433075708 	 acc:0.8343801652892562 | test: loss:0.5422267958817892 	 acc:0.7390728476821192 	 lr:0.0001
epoch23: train: loss:0.46808481801639906 	 acc:0.8489256198347107 | test: loss:0.5738812973167723 	 acc:0.7086092715231788 	 lr:5e-05
epoch24: train: loss:0.46606692099374186 	 acc:0.8446280991735537 | test: loss:0.5647581870982189 	 acc:0.7298013245033113 	 lr:5e-05
epoch25: train: loss:0.5044190401085152 	 acc:0.8095867768595041 | test: loss:0.631521373000366 	 acc:0.6794701986754967 	 lr:5e-05
epoch26: train: loss:0.46095543469279265 	 acc:0.8261157024793389 | test: loss:0.5446653410298935 	 acc:0.7218543046357616 	 lr:5e-05
epoch27: train: loss:0.456403277422771 	 acc:0.8406611570247934 | test: loss:0.546486300190553 	 acc:0.7165562913907285 	 lr:5e-05
epoch28: train: loss:0.4574100644036758 	 acc:0.8661157024793389 | test: loss:0.5806480891657192 	 acc:0.7125827814569536 	 lr:5e-05
epoch29: train: loss:0.44318349525948203 	 acc:0.8671074380165289 | test: loss:0.5479813575744629 	 acc:0.7390728476821192 	 lr:2.5e-05
epoch30: train: loss:0.43799045290828736 	 acc:0.8694214876033057 | test: loss:0.5459106123210579 	 acc:0.7403973509933774 	 lr:2.5e-05
epoch31: train: loss:0.43067346439873877 	 acc:0.8849586776859504 | test: loss:0.5441131624954426 	 acc:0.7417218543046358 	 lr:2.5e-05
epoch32: train: loss:0.42878625586998365 	 acc:0.8806611570247934 | test: loss:0.5397370122915862 	 acc:0.7470198675496689 	 lr:2.5e-05
epoch33: train: loss:0.4290947193449194 	 acc:0.876694214876033 | test: loss:0.5342711366565023 	 acc:0.7483443708609272 	 lr:2.5e-05
epoch34: train: loss:0.42556028320769634 	 acc:0.8889256198347107 | test: loss:0.5515812454634155 	 acc:0.743046357615894 	 lr:2.5e-05
epoch35: train: loss:0.4306129684724098 	 acc:0.8905785123966942 | test: loss:0.546889973476233 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch36: train: loss:0.4274094891646677 	 acc:0.8905785123966942 | test: loss:0.5413479237366986 	 acc:0.743046357615894 	 lr:2.5e-05
epoch37: train: loss:0.4247075734453753 	 acc:0.8866115702479339 | test: loss:0.5526340243832165 	 acc:0.7324503311258278 	 lr:2.5e-05
epoch38: train: loss:0.4154443409817278 	 acc:0.8978512396694215 | test: loss:0.5447520109991364 	 acc:0.7443708609271523 	 lr:2.5e-05
epoch39: train: loss:0.4160366335979178 	 acc:0.8965289256198347 | test: loss:0.545801171403847 	 acc:0.7496688741721854 	 lr:2.5e-05
epoch40: train: loss:0.41938464789351154 	 acc:0.9034710743801653 | test: loss:0.5645659092246302 	 acc:0.7284768211920529 	 lr:1.25e-05
epoch41: train: loss:0.42025299284083784 	 acc:0.8866115702479339 | test: loss:0.5385904796076136 	 acc:0.7443708609271523 	 lr:1.25e-05
epoch42: train: loss:0.41319784489544953 	 acc:0.8998347107438016 | test: loss:0.5443955339343343 	 acc:0.7417218543046358 	 lr:1.25e-05
epoch43: train: loss:0.4101070110561434 	 acc:0.9018181818181819 | test: loss:0.5432094813182654 	 acc:0.743046357615894 	 lr:1.25e-05
epoch44: train: loss:0.40779207864083533 	 acc:0.911404958677686 | test: loss:0.5448987797396072 	 acc:0.7417218543046358 	 lr:1.25e-05
epoch45: train: loss:0.4126332055832729 	 acc:0.9084297520661156 | test: loss:0.5502090671204574 	 acc:0.7377483443708609 	 lr:1.25e-05
epoch46: train: loss:0.412751105403112 	 acc:0.9031404958677686 | test: loss:0.549253270878697 	 acc:0.7390728476821192 	 lr:6.25e-06
epoch47: train: loss:0.41344003589685296 	 acc:0.9018181818181819 | test: loss:0.5483420009644616 	 acc:0.7403973509933774 	 lr:6.25e-06
epoch48: train: loss:0.40279976901929243 	 acc:0.9176859504132231 | test: loss:0.5453155873627062 	 acc:0.7417218543046358 	 lr:6.25e-06
epoch49: train: loss:0.41020940393455757 	 acc:0.9014876033057851 | test: loss:0.5458785537062891 	 acc:0.7443708609271523 	 lr:6.25e-06
epoch50: train: loss:0.4029361934701273 	 acc:0.9067768595041322 | test: loss:0.5550119865809056 	 acc:0.7337748344370861 	 lr:6.25e-06
epoch51: train: loss:0.40899098070199824 	 acc:0.9034710743801653 | test: loss:0.5641494064141583 	 acc:0.7271523178807947 	 lr:6.25e-06
epoch52: train: loss:0.4031675344360761 	 acc:0.9140495867768595 | test: loss:0.5476464727856466 	 acc:0.743046357615894 	 lr:3.125e-06
epoch53: train: loss:0.4046886809798312 	 acc:0.9140495867768595 | test: loss:0.5427172172148496 	 acc:0.7390728476821192 	 lr:3.125e-06
epoch54: train: loss:0.40361502064161064 	 acc:0.9127272727272727 | test: loss:0.5456278825437786 	 acc:0.743046357615894 	 lr:3.125e-06
epoch55: train: loss:0.40311455688200704 	 acc:0.915702479338843 | test: loss:0.5479614212023501 	 acc:0.7377483443708609 	 lr:3.125e-06
epoch56: train: loss:0.4025067734028682 	 acc:0.9127272727272727 | test: loss:0.5448333384974903 	 acc:0.7483443708609272 	 lr:3.125e-06
epoch57: train: loss:0.40471188860491286 	 acc:0.9140495867768595 | test: loss:0.5585653093476959 	 acc:0.7311258278145696 	 lr:3.125e-06
epoch58: train: loss:0.40276984753687517 	 acc:0.9133884297520661 | test: loss:0.5562037812163498 	 acc:0.7350993377483444 	 lr:1.5625e-06
epoch59: train: loss:0.4043035125929462 	 acc:0.9071074380165289 | test: loss:0.5518884153555561 	 acc:0.7403973509933774 	 lr:1.5625e-06
epoch60: train: loss:0.397865392294797 	 acc:0.9163636363636364 | test: loss:0.5472631076313802 	 acc:0.7337748344370861 	 lr:1.5625e-06
epoch61: train: loss:0.40620375074630927 	 acc:0.9077685950413223 | test: loss:0.54878040512666 	 acc:0.7337748344370861 	 lr:1.5625e-06
epoch62: train: loss:0.39991906464592486 	 acc:0.923305785123967 | test: loss:0.5482752672094383 	 acc:0.7390728476821192 	 lr:1.5625e-06
epoch63: train: loss:0.4020869545207536 	 acc:0.912396694214876 | test: loss:0.5447763990882216 	 acc:0.7417218543046358 	 lr:1.5625e-06
